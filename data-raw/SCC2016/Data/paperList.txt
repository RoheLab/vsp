"DOI","year","title","citCounts","abstract"
"10.1214/12-AOS1008","2012","Rerandomization to improve covariate balance in experiments","0","Randomized experiments are the ""gold standard"" for estimating causal effects, yet often in practice, chance imbalances exist in covariate distributions between treatment groups. If covariate data are available before units are exposed to treatments, these chance imbalances can be mitigated by first checking covariate balance before the physical experiment takes place. Provided a precise definition of imbalance has been specified in advance, unbalanced randomizations can be discarded, followed by a rerandomization, and this process can continue until a randomization yielding balance according to the definition is achieved. By improving covariate balance, rerandomization provides more precise and trustworthy estimates of treatment effects."
"10.1214/12-AOS1006","2012","Realized {L}aplace transforms for pure-jump semimartingales","0","We consider specification and inference for the stochastic scale of discretely-observed pure-jump semimartingales with locally stable Levy densities in the setting where both the time span of the data set increases, and the mesh of the observation grid decreases. The estimation is based on constructing a nonparametric estimate for the empirical Laplace transform of the stochastic scale over a given interval of time by aggregating high-frequency increments of the observed process on that time interval into a statistic we call realized Laplace transform. The realized Laplace transform depends on the activity of the driving pure-jump martingale, and we consider both cases when the latter is known or has to be inferred from the data."
"10.1214/12-AOS1003","2012","Degrees of freedom in lasso problems","0","We derive the degrees of freedom of the lasso fit, placing no assumptions on the predictor matrix X. Like the well-known result of Zou, Hastie and Tibshirani [Ann. Statist. 35 (2007) 2173-2192], which gives the degrees of freedom of the lasso fit when X has full column rank, we express our result in terms of the active set of a lasso solution. We extend this result to cover the degrees of freedom of the generalized lasso fit for an arbitrary predictor matrix X (and an arbitrary penalty matrix D). Though our focus is degrees of freedom, we establish some intermediate results on the lasso and generalized lasso that may be interesting on their own."
"10.1214/12-AOS1000","2012","Noisy matrix decomposition via convex relaxation: optimal rates in high dimensions","0","We analyze a class of estimators based on convex relaxation for solving high-dimensional matrix decomposition problems. The observations are noisy realizations of a linear transformation (sic) of the sum of an (approximately) low rank matrix Theta(star) with a second matrix Gamma(star) endowed with a complementary form of low-dimensional structure; this set-up includes many statistical models of interest, including factor analysis, multi-task regression and robust covariance estimation. We derive a general theorem that bounds the Frobenius norm error for an estimate of the pair (Theta(star), Gamma(star)) obtained by solving a convex optimization problem that combines the nuclear norm with a general decomposable regularizer. Our results use a ""spikiness"" condition that is related to, but milder than, singular vector incoherence. We specialize our general result to two cases that have been studied in past work: low rank plus an entrywise sparse matrix, and low rank plus a columnwise sparse matrix. For both models, our theory yields nonasymptotic Frobenius error bounds for both deterministic and stochastic noise matrices, and applies to matrices Theta(star) that can be exactly or approximately low rank, and matrices Gamma(star) that can be exactly or approximately sparse. Moreover, for the case of stochastic noise matrices and the identity observation operator, we establish matching lower bounds on the minimax error. The sharpness of our nonasymptotic predictions is confirmed by numerical simulations."
"10.1214/12-AOS995","2012","Nonparametric regression with nonparametrically generated covariates","0","We analyze the statistical properties of nonparametric regression estimators using covariates which are not directly observable, but have be estimated from data in a preliminary step. These so-called generated covariates appear in numerous applications, including two-stage nonparametric regression, estimation of simultaneous equation models or censored regression models. Yet so far there seems to be no general theory for their impact on the final estimator's statistical properties. Our paper provides such results. We derive a stochastic expansion that characterizes the influence of the generation step on the final estimator, and use it to derive rates of consistency and asymptotic distributions accounting for the presence of generated covariates."
"10.1214/12-AOS1005","2012","Bayesian empirical likelihood for quantile regression","0","Bayesian inference provides a flexible way of combining data with prior information. However, quantile regression is not equipped with a parametric likelihood, and therefore, Bayesian inference for quantile regression demands careful investigation. This paper considers the Bayesian empirical likelihood approach to quantile regression. Taking the empirical likelihood into a Bayesian framework, we show that the resultant posterior from any fixed prior is asymptotically normal; its mean shrinks toward the true parameter values, and its variance approaches that of the maximum empirical likelihood estimator. A more interesting case can be made for the Bayesian empirical likelihood when informative priors are used to explore commonality across quantiles. Regression quantiles that are computed separately at each percentile level tend to be highly variable in the data sparse areas (e.g., high or low percentile levels). Through empirical likelihood, the proposed method enables us to explore various forms of commonality across quantiles for efficiency gains. By using an MCMC algorithm in the computation, we avoid the daunting task of directly maximizing empirical likelihood. The finite sample performance of the proposed method is investigated empirically, where substantial efficiency gains are demonstrated with informative priors on common features across several percentile levels. A theoretical framework of shrinking priors is used in the paper to better understand the power of the proposed method."
"10.1214/12-AOS997","2012","Characterizing {$L_2$}{B}oosting","0","We consider L(2)Boosting, a special case of Friedman's generic boosting algorithm applied to linear regression under L-2-loss. We study L(2)Boosting for an arbitrary regularization parameter and derive an exact closed form expression for the number of steps taken along a fixed coordinate direction. This relationship is used to describe L(2)Boosting's solution path, to describe new tools for studying its path, and to characterize some of the algorithm's unique properties, including active set cycling, a property where the algorithm spends lengthy periods of time cycling between the same coordinates when the regularization parameter is arbitrarily small. Our fixed descent analysis also reveals a repressible condition that limits the effectiveness of L(2)Boosting in correlated problems by preventing desirable variables from entering the solution path. As a simple remedy, a data augmentation method similar to that used for the elastic net is used to introduce L-2-penalization and is shown, in combination with decorrelation, to reverse the repressible condition and circumvents L(2)Boosting's deficiencies in correlated problems. In itself, this presents a new explanation for why the elastic net is successful in correlated problems and why methods like LAR and lasso can perform poorly in such settings."
"10.1214/12-AOS991","2012","Estimation of means in graphical {G}aussian models with symmetries","0","We study the problem of estimability of means in undirected graphical Gaussian models with symmetry restrictions represented by a colored graph. Following on from previous studies, we partition the variables into sets of vertices whose corresponding means are restricted to being identical. We find a necessary and sufficient condition on the partition to ensure equality between the maximum likelihood and least-squares estimators of the mean."
"10.1214/12-AOS989","2012","Nonlinear shrinkage estimation of large-dimensional covariance matrices","0","Many statistical applications require an estimate of a covariance matrix and/or its inverse. When the matrix dimension is large compared to the sample size, which happens frequently, the sample covariance matrix is known to perform poorly and may suffer from ill-conditioning. There already exists an extensive literature concerning improved estimators in such situations. In the absence of further knowledge about the structure of the true covariance matrix, the most successful approach so far, arguably, has been shrinkage estimation. Shrinking the sample covariance matrix to a multiple of the identity, by taking a weighted average of the two, turns out to be equivalent to linearly shrinking the sample eigenvalues to their grand mean, while retaining the sample eigenvectors. Our paper extends this approach by considering nonlinear transformations of the sample eigenvalues. We show how to construct an estimator that is asymptotically equivalent to an oracle estimator suggested in previous work. As demonstrated in extensive Monte Carlo simulations, the resulting bona fide estimator can result in sizeable improvements over the sample covariance matrix and also over linear shrinkage."
"10.1214/12-AOS986","2012","Maximum likelihood estimation in log-linear models","0","We study maximum likelihood estimation in log-linear models under conditional Poisson sampling schemes. We derive necessary and sufficient conditions for existence of the maximum likelihood estimator (MLE) of the model parameters and investigate estimability of the natural and mean-value parameters under a nonexistent MLE. Our conditions focus on the role of sampling zeros in the observed table. We situate our results within the framework of extended exponential families, and we exploit the geometric properties of log-linear models. We propose algorithms for extended maximum likelihood estimation that improve and correct the existing algorithms for log-linear model analysis."
"10.1214/11-AOS955","2012","Bayesian nonparametric estimation of the spectral density of a long or intermediate memory {G}aussian process","0","A stationary Gaussian process is said to be long-range dependent (resp., anti-persistent) if its spectral density f(lambda) can be written as f(lambda) = vertical bar lambda vertical bar(-2d) g(vertical bar lambda vertical bar), where 0 <d < 1/2 (resp., -1/2 <d < 0), and g is continuous and positive. We propose a novel Bayesian nonparametric approach for the estimation of the spectral density of such processes. We prove posterior consistency for both d and g, under appropriate conditions on the prior distribution. We establish the rate of convergence for a general class of priors and apply our results to the family of fractionally exponential priors. Our approach is based on the true likelihood and does not resort to Whittle's approximation."
"10.1214/12-AOS994","2012","Manifold estimation and singular deconvolution under {H}ausdorff loss","0","We find lower and upper bounds for the risk of estimating a manifold in Hausdorff distance under several models. We also show that there are close connections between manifold estimation and the problem of deconvolving a singular measure."
"10.1214/12-AOS993","2012","Two sample tests for high-dimensional covariance matrices","0","We propose two tests for the equality of covariance matrices between two high-dimensional populations. One test is on the whole variance covariance matrices, and the other is on off-diagonal sub-matrices, which define the covariance between two nonoverlapping segments of the high-dimensional random vectors. The tests are applicable (i) when the data dimension is much larger than the sample sizes, namely the ""large p, small n"" situations and (ii) without assuming parametric distributions for the two populations. These two aspects surpass the capability of the conventional likelihood ratio test. The proposed tests can be used to test on covariances associated with gene ontology terms."
"10.1214/12-AOS987","2012","Uniform fractional factorial designs","0","The minimum aberration criterion has been frequently used in the selection of fractional factorial designs with nominal factors. For designs with quantitative factors, however, level permutation of factors could alter their geometrical structures and statistical properties. In this paper uniformity is used to further distinguish fractional factorial designs, besides the minimum aberration criterion. We show that minimum aberration designs have low discrepancies on average. An efficient method for constructing uniform minimum aberration designs is proposed and optimal designs with 27 and 81 runs are obtained for practical use. These designs have good uniformity and are effective for studying quantitative factors."
"10.1214/12-AOS974","2012","Bayes factors and the geometry of discrete hierarchical loglinear models","0","A standard tool for model selection in a Bayesian framework is the Bayes factor which compares the marginal likelihood of the data under two given different models. In this paper, we consider the class of hierarchical loglinear models for discrete data given under the form of a contingency table with multinomial sampling. We assume that the prior distribution on the loglinear parameters is the Diaconis-Ylvisaker conjugate prior, and the uniform is the prior distribution on the space of models. Under these conditions, the Bayes factor between two models is a function of the normalizing constants of the prior and posterior distribution of the loglinear parameters. These constants are functions of the hyperparameters (m, alpha) which can be interpreted, respectively, as the marginal counts and total count of a fictive contingency table.We study the behavior of the Bayes factor when alpha tends to zero. In this study, the most important tool is the characteristic function J(C) of the interior C of the convex hull (C) over bar of the support of the multinomial distribution for a given hierarchical loglinear model. If h(C) is the support function of C, the function J(C) is the Laplace transform of exp(-h(C)). We show that, when alpha tends to 0, if the data lies on a face F-i of (C) over bar (i), i = 1, 2, of dimension k(i), the Bayes factor behaves like alpha(k1-k2). This implies in particular that when the data is in C-1 and in C-2, that is, when k(i) equals the dimension of model J(i), the sparser model is favored, thus confirming the idea of Bayesian regularization.In order to find the faces of (C) over bar, we need to know its facets. We show that since here C is a polytope, the denominator of the rational function J(C) is the product of the equations of the facets. We also identify a category of facets common to all hierarchical models for discrete variables, not necessarily binary. Finally, we show that these facets are the only facets of (C) over bar when the model is graphical with respect to a decomposable graph."
"10.1214/11-AOS965","2012","General nonexact oracle inequalities for classes with a subexponential envelope","0","We show that empirical risk minimization procedures and regularized empirical risk minimization procedures satisfy nonexact oracle inequalities in an unbounded framework, under the assumption that the class has a subexponential envelope function. The main novelty, in addition to the boundedness assumption free setup, is that those inequalities can yield fast rates even in situations in which exact oracle inequalities only hold with slower rates.We apply these results to show that procedures based on l(1) and nuclear norms regularization functions satisfy oracle inequalities with a residual term that decreases like 1/n forevery L-q-loss functions (q >= 2), while only assuming that the tail behavior of the input and output variables are well behaved. In particular, no RIP type of assumption or ""incoherence condition"" are needed to obtain fast residual terms in those setups. We also apply these results to the problems of convex aggregation and model selection."
"10.1214/12-AOS982","2012","Estimation in high-dimensional linear models with deterministic design matrices","0","Because of the advance in technologies, modem statistical studies often encounter linear models with the number of explanatory variables much larger than the sample size. Estimation and variable selection in these high-dimensional problems with deterministic design points is very different from those in the case of random covariates, due to the identifiability of the high-dimensional regression parameter vector. We show that a reasonable approach is to focus on the projection of the regression parameter vector onto the linear space generated by the design matrix. In this work, we consider the ridge regression estimator of the projection vector and propose to threshold the ridge regression estimator when the projection vector is sparse in the sense that many of its components are small. The proposed estimator has an explicit form and is easy to use in application. Asymptotic properties such as the consistency of variable selection and estimation and the convergence rate of the prediction mean squared error are established under some sparsity conditions on the projection vector. A simulation study is also conducted to examine the performance of the proposed estimator."
"10.1214/12-AOS978","2012","Perturbation and scaled {C}ook's distance","0","Cook's distance [Technometrics 19 (1977) 15-18] is one of the most important diagnostic tools for detecting influential individual or subsets of observations in linear regression for cross-sectional data. However, for many complex data structures (e.g., longitudinal data), no rigorous approach has been developed to address a fundamental issue: deleting subsets with different numbers of observations introduces different degrees of perturbation to the current model fitted to the data, and the magnitude of Cook's distance is associated with the degree of the perturbation. The aim of this paper is to address this issue in general parametric models with complex data structures. We propose a new quantity for measuring the degree of the perturbation introduced by deleting a subset. We use stochastic ordering to quantify the stochastic relationship between the degree of the perturbation and the magnitude of Cook's distance. We develop several scaled Cook's distances to resolve the comparison of Cook's distance for different subset deletions. Theoretical and numerical examples are examined to highlight the broad spectrum of applications of these scaled Cook's distances in a formal influence analysis."
"10.1214/12-AOS977","2012","Modeling high-frequency financial data by pure jump processes","0","It is generally accepted that the asset price processes contain jumps. In fact, pure jump models have been widely used to model asset prices and/or stochastic volatilities. The question is: is there any statistical evidence from the high-frequency financial data to support using pure jump models alone? The purpose of this paper is to develop such a statistical test against the necessity of a diffusion component. The test is very simple to use and yet effective. Asymptotic properties of the proposed test statistic will be studied. Simulation studies and some real-life examples are included to illustrate our results."
"10.1214/12-AOS975","2012","A specification test for nonlinear nonstationary models","0","We provide a limit theory for a general class of kernel smoothed U-statistics that may be used for specification testing in time series regression with nonstationary data. The test framework allows for linear and nonlinear models with endogenous regressors that have autoregressive unit roots or near unit roots. The limit theory for the specification test depends on the self-intersection local time of a Gaussian process. A new weak convergence result is developed for certain partial sums of functions involving nonstationary time series that converges to the intersection local time process. This result is of independent interest and is useful in other applications. Simulations examine the finite sample performance of the test."
"10.1214/12-AOS970","2012","Factor modeling for high-dimensional time series: inference for the number of factors","0","This paper deals with the factor modeling for high-dimensional time series based on a dimension-reduction viewpoint. Under stationary settings, the inference is simple in the sense that both the number of factors and the factor loadings are estimated in terms of an eigenanalysis for a nonnegative definite matrix, and is therefore applicable when the dimension of time series is on the order of a few thousands. Asymptotic properties of the proposed method are investigated under two settings: (i) the sample size goes to infinity while the dimension of time series is fixed; and (ii) both the sample size and the dimension of time series go to infinity together. In particular, our estimators for zero-eigenvalues enjoy faster convergence (or slower divergence) rates, hence making the estimation for the number of factors easier. In particular, when the sample size and the dimension of time series go to infinity together, the estimators for the eigenvalues are no longer consistent. However, our estimator for the number of the factors, which is based on the ratios of the estimated eigenvalues, still works fine. Furthermore, this estimation shows the so-called ""blessing of dimensionality"" property in the sense that the performance of the estimation may improve when the dimension of time series increases. A two-step procedure is investigated when the factors are of different degrees of strength. Numerical illustration with both simulated and real data is also reported."
"10.1214/12-AOS969","2012","A {R}obbins-{M}onro procedure for estimation in semiparametric regression models","0","This paper is devoted to the parametric estimation of a shift together with the nonparametric estimation of a regression function in a semiparametric regression model. We implement a very efficient and easy to handle Robbins-Monro procedure. On the one hand, we propose a stochastic algorithm similar to that of Robbins-Monro in order to estimate the shift parameter. A preliminary evaluation of the regression function is not necessary to estimate the shift parameter. On the other hand, we make use of a recursive Nadaraya-Watson estimator for the estimation of the regression function. This kernel estimator takes into account the previous estimation of the shift parameter. We establish the almost sure convergence for both Robbins-Monro and Nadaraya-Watson estimators. The asymptotic normality of our estimates is also provided. Finally, we illustrate our semiparametric estimation procedure on simulated and real data."
"10.1214/11-AOS961","2012","Kullback-{L}eibler aggregation and misspecified generalized linear models","0","In a regression setup with deterministic design, we study the pure aggregation problem and introduce a natural extension from the Gaussian distribution to distributions in the exponential family. While this extension bears strong connections with generalized linear models, it does not require identifiability of the parameter or even that the model on the systematic component is true. It is shown that this problem can be solved by constrained and/or penalized likelihood maximization and we derive sharp oracle inequalities that hold both in expectation and with high probability. Finally all the bounds are proved to be optimal in a minimax sense."
"10.1214/11-AOS918","2011","Robust linear least squares regression","0","We consider the problem of robustly predicting as well as the best linear combination of d given functions in least squares regression, and variants of this problem including constraints on the parameters of the linear combination. For the ridge estimator and the ordinary least squares estimator, and their variants, we provide new risk bounds of order d/n without logarithmic factor unlike some standard results, where n is the size of the training data. We also provide a new estimator with better deviations in the presence of heavy-tailed noise. It is based on truncating differences of losses in a min-max framework and satisfies a d/n risk bound both in expectation and in deviations. The key common surprising factor of these results is the absence of exponential moment condition on the output distribution while achieving exponential deviations. All risk bounds are obtained through a PAC-Bayesian analysis on truncated differences of losses. Experimental results strongly back up our truncated min-max estimator."
"10.1214/11-AOS917","2011","Fully {B}ayes factors with a generalized {$g$}-prior","0","For the normal linear model variable selection problem, we propose selection criteria based on a fully Bayes formulation with a generalization of Zellner's g-prior which allows for p > n. A special case of the prior formulation is seen to yield tractable closed forms for marginal densities and Bayes factors which reveal new model evaluation characteristics of potential interest."
"10.1214/11-AOS915","2011","Principal components analysis in the space of phylogenetic trees","0","Phylogenetic analysis of DNA or other data commonly gives rise to a collection or sample of inferred evolutionary trees. Principal Components Analysis (PCA) cannot be applied directly to collections of trees since the space of evolutionary trees on a fixed set of taxa is not a vector space. This paper describes a novel geometrical approach to PCA in tree-space that constructs the first principal path in an analogous way to standard linear Euclidean PCA. Given a data set of phylogenetic trees, a geodesic principal path is sought that maximizes the variance of the data under a form of projection onto the path. Due to the high dimensionality of tree-space and the nonlinear nature of this problem, the computational complexity is potentially very high, so approximate optimization algorithms are used to search for the optimal path. Principal paths identified in this way reveal and quantify the main sources of variation in the original collection of trees in terms of both topology and branch lengths. The approach is illustrated by application to simulated sets of trees and to a set of gene trees from metazoan (animal) species."
"10.1214/11-AOS914","2011","Robust recovery of multiple subspaces by geometric {$l_p$} minimization","0","We assume i.i.d. data sampled from a mixture distribution with K components along fixed d-dimensional linear subspaces and an additional outlier component. For p > 0, we study the simultaneous recovery of the K fixed subspaces by minimizing the l(p)-averaged distances of the sampled data points from any K subspaces. Under some conditions, we show that if 0 < p <= 1, then all underlying subspaces can be precisely recovered by l(p) minimization with overwhelming probability. On the other hand, if K > 1 and p > 1, then the underlying subspaces cannot be recovered or even nearly recovered by l(p) minimization. The results of this paper partially explain the successes and failures of the basic approach of l(p) energy minimization for modeling data by multiple subspaces."
"10.1214/11-AOS913","2011","Computational approaches for empirical {B}ayes methods and {B}ayesian sensitivity analysis","0","We consider situations in Bayesian analysis where we have a family of priors v(h) on the parameter theta, where h varies continuously over a space H, and we deal with two related problems. The first involves sensitivity analysis and is stated as follows. Suppose we fix a function f of theta. How do we efficiently estimate the posterior expectation of f(theta) simultaneously for all h in H? The second problem is how do we identify subsets of H which give rise to reasonable choices of v(h)? We assume that we are able to generate Markov chain samples from the posterior for a finite number of the priors, and we develop a methodology, based on a combination of importance sampling and the use of control variates, for dealing with these two problems. The methodology applies very generally, and we show how it applies in particular to a commonly used model for variable selection in Bayesian linear regression, and give an illustration on the US crime data of Vandaele."
"10.1214/11-AOS920","2011","Bayesian inverse problems with {G}aussian priors","0","The posterior distribution in a nonparametric inverse problem is shown to contract to the true parameter at a rate that depends on the smoothness of the parameter, and the smoothness and scale of the prior. Correct combinations of these characteristics lead to the minimax rate. The frequentist coverage of credible sets is shown to depend on the combination of prior and true parameter, with smoother priors leading to zero coverage and rougher priors to conservative coverage. In the latter case credible sets are of the correct order of magnitude. The results are numerically illustrated by the problem of recovering a function from observation of a noisy version of its primitive."
"10.1214/11-AOS919","2011","Penalized maximum likelihood estimation and variable selection in geostatistics","0","We consider the problem of selecting covariates ill spatial linear models with Gaussian process errors. Penalized maximum likelihood estimation (PMLE) that enables simultaneous variable selection and parameter estimation is developed and, for ease of computation, PMLE is approximated by one-step sparse estimation (OSE). To further improve computational efficiency, particularly with large sample sizes, we propose penalized maximum covariance-tapered likelihood estimation (PMLE(T)) and its one-step sparse estimation (OSE(T)). General forms of penalty functions with an emphasis on smoothly clipped absolute deviation are used for penalized maximum likelihood. Theoretical properties of PMLE and USE, as well as their approximations PMLE(T) and OSE(T) using covariance tapering, are derived, including consistency, sparsity, asymptotic normality and the oracle properties. For covariance tapering, a by-product of our theoretical results is consistency and asymptotic normality of maximum covariance-tapered likelihood estimates. Finite-sample properties of the proposed methods are demonstrated in a simulation study and, for illustration, the methods are applied to analyze two real data sets."
"10.1214/11-AOS916","2011","A spectral analytic comparison of trace-class data augmentation algorithms and their sandwich variants","0","The data augmentation (DA) algorithm is a widely used Markov chain Monte Carlo algorithm that is easy to implement but often suffers from slow convergence. The sandwich algorithm is an alternative that can converge much faster while requiring roughly the same computational effort per iteration. Theoretically, the sandwich algorithm always converges at least as fast as the corresponding DA algorithm in the sense that parallel to K*parallel to <= parallel to K parallel to, where K and K* are the Markov operators associated with the DA and sandwich algorithms, respectively, and parallel to . parallel to denotes operator norm. In this paper, a substantial refinement of this operator norm inequality is developed. In particular, under regularity conditions implying that K is a trace-class operator, it is shown that K* is also a positive, trace-class operator, and that the spectrum of K* dominates that of K in the sense that the ordered elements of the former are all less than or equal to the corresponding elements of the latter. Furthermore, if the sandwich algorithm is constructed using a group action, as described by Liu and Wu [J. Amer Statist. Assoc. 94 (1999) 1264-1274] and Hobert and Marchev [Ann. Statist. 36 (2008) 532-554], then there is strict inequality between at least one pair of eigenvalues. These results are applied to a new DA algorithm for Bayesian quantile regression introduced by Kozumi and Kobayashi [J. Stat. Comput. Simul. 81 (2011) 1565-1578]."
"10.1214/11-AOS912","2011","Bernstein-von {M}ises theorems for {G}aussian regression with increasing number of regressors","1","This paper brings a contribution to the Bayesian theory of nonparametric and semiparametric estimation. We are interested in the asymptotic normality of the posterior distribution in Gaussian linear regression models when the number of regressors increases with the sample size. Two kinds of Bernstein-von Mises theorems are obtained in this framework: nonparametric theorems for the parameter itself, and semiparametric theorems for functionals of the parameter. We apply them to the Gaussian sequence model and to the regression of functions in Sobolev and C(alpha) classes, in which we get the minimax convergence rates. Adaptivity is reached for the Bayesian estimators of functionals in our applications."
"10.1214/11-AOS910","2011","Global testing under sparse alternatives: {ANOVA}, multiple comparisons and the higher criticism","0","Testing for the significance of a subset of regression coefficients in a linear model, a staple of statistical analysis, goes back at least to the work of Fisher who introduced the analysis of variance (ANOVA). We study this problem under the assumption that the coefficient vector is sparse, a common situation in modern high-dimensional settings. Suppose we have p covariates and that under the alternative, the response only depends upon the order of p(1-alpha) of those, 0 <= alpha <= 1. Under moderate sparsity levels, that is, 0 <= alpha <= 1/2, we show that ANOVA is essentially optimal under some conditions on the design. This is no longer the case under strong sparsity constraints, that is, alpha > 1/2. In such settings, a multiple comparison procedure is often preferred and we establish its optimality when alpha >= 3/4. However, these two very popular methods are suboptimal, and sometimes powerless, under moderately strong sparsity where 1/2 < alpha < 3/4. We suggest a method based on the higher criticism that is powerful in the whole range alpha > 1/2. This optimality property is true for a variety of designs, including the classical (balanced) multi-way designs and more modern ""p > n"" designs arising in genetics and signal processing. In addition to the standard fixed effects model, we establish similar results for a random effects model where the nonzero coefficients of the regression vector are normally distributed."
"10.1214/11-AOS908","2011","Asymptotic normality and valid inference for {G}aussian variational approximation","0","We derive the precise asymptotic distributional behavior of Gaussian variational approximate estimators of the parameters in a single-predictor Poisson mixed model. These results are the deepest yet obtained concerning the statistical properties of a variational approximation method. Moreover, they give rise to asymptotically valid statistical inference. A simulation study demonstrates that Gaussian variational approximate confidence intervals possess good to excellent coverage properties, and have a similar precision to their exact likelihood counterparts."
"10.1214/11-AOS907","2011","On deconvolution of distribution functions","0","The subject of this paper is the problem of nonparametric estimation of a continuous distribution function from observations with measurement errors. We study minimax complexity of this problem when unknown distribution has a density belonging to the Sobolev class, and the error density is ordinary smooth. We develop rate optimal estimators based on direct inversion of empirical characteristic function. We also derive minimax affine estimators of the distribution function which are given by an explicit convex optimization problem. Adaptive versions of these estimators are proposed, and some numerical results demonstrating good practical behavior of the developed procedures are presented."
"10.1214/11-AOS906","2011","Semiparametrically efficient inference based on signed ranks in symmetric independent component models","0","We consider semiparametric location-scatter models for which the p-variate observation is obtained as X = Lambda Z + mu, where mu is a p-vector, Lambda is a full-rank p x p matrix and the (unobserved) random p-vector Z has marginals that are centered and mutually independent but are otherwise unspecified. As in blind source separation and independent component analysis (ICA), the parameter of interest throughout the paper is Lambda. On the basis of n i.i.d. copies of X, we develop, under a symmetry assumption on Z, signed-rank one-sample testing and estimation procedures for Lambda. We exploit the uniform local and asymptotic normality (ULAN) of the model to define signed-rank procedures that are semiparametrically efficient under correctly specified densities. Yet, as is usual in rank-based inference, the proposed procedures remain valid (correct asymptotic size under the null, for hypothesis testing, and root-n consistency, for point estimation) under a very broad range of densities. We derive the asymptotic properties of the proposed procedures and investigate their finite-sample behavior through simulations."
"10.1214/11-AOS905","2011","Factor models and variable selection in high-dimensional regression analysis","0","The paper considers linear regression problems where the number of predictor variables is possibly larger than the sample size. The basic motivation of the study is to combine the points of view of model selection and functional regression by using a factor approach: it is assumed that the predictor vector can be decomposed into a sum of two uncorrelated random components reflecting common factors and specific variabilities of the explanatory variables. It is shown that the traditional assumption of a sparse vector of parameters is restrictive in this context. Common factors may possess a significant influence on the response variable which cannot be captured by the specific effects of a small number of individual variables. We therefore propose to include principal components as additional explanatory variables in an augmented regression model. We give finite sample inequalities for estimates of these components. It is then shown that model selection procedures can be used to estimate the parameters of the augmented model, and we derive theoretical properties of the estimators. Finite sample performance is illustrated by a simulation study."
"10.1214/11-AOS903","2011","On adaptive inference and confidence bands","0","The problem of existence of adaptive confidence bands for an unknown density f that belongs to a nested scale of Holder classes over R or [0, 1] is considered. Whereas honest adaptive inference in this problem is impossible already for a pair of Holder balls Sigma (r), Sigma(s), r not equal s, of fixed radius, a non-parametric distinguishability condition is introduced under which adaptive confidence bands can be shown to exist. It is further shown that this condition is necessary and sufficient for the existence of honest asymptotic confidence bands, and that it is strictly weaker than similar analytic conditions recently employed in Gine and Nickl [Ann. Statist. 38 (2010) 1122-1170]. The exceptional sets for which honest inference is not possible have vanishingly small probability under natural priors on Holder balls Sigma (s). If no upper bound for the radius of the Holder balls is known, a price for adaptation has to be paid, and near-optimal adaptation is possible for standard procedures. The implications of these findings for a general theory of adaptive inference are discussed."
"10.1214/11-AOS902","2011","Evaluating probability forecasts","0","Probability forecasts of events are routinely used in climate predictions, in forecasting default probabilities on bank loans or in estimating the probability of a patient's positive response to treatment. Scoring rules have long been used to assess the efficacy of the forecast probabilities after observing the occurrence, or nonoccurrence, of the predicted events. We develop herein a statistical theory for scoring rules and propose an alternative approach to the evaluation of probability forecasts. This approach uses loss functions relating the predicted to the actual probabilities of the events and applies martingale theory to exploit the temporal structure between the forecast and the subsequent occurrence or nonoccurrence of the event."
"10.1214/11-AOS898","2011","Optimal estimation of the mean function based on discretely sampled functional data: phase transition","0","The problem of estimating the mean of random functions based on discretely sampled data arises naturally in functional data analysis. In this paper, we study optimal estimation of the mean function under both common and independent designs. Minimax rates of convergence are established and easily implementable rate-optimal estimators are introduced. The analysis reveals interesting and different phase transition phenomena in the two cases. Under the common design, the sampling frequency solely determines the optimal rate of convergence when it is relatively small and the sampling frequency has no effect on the optimal rate when it is large. On the other hand, under the independent design, the optimal rate of convergence is determined jointly by the sampling frequency and the number of curves when the sampling frequency is relatively small. When it is large, the sampling frequency has no effect on the optimal rate. Another interesting contrast between the two settings is that smoothing is necessary under the independent design, while, somewhat surprisingly, it is not essential under the common design."
"10.1214/11-AOS894","2011","Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion","0","This paper deals with the trace regression model where n entries or linear combinations of entries of an unknown m(1) x m(2) matrix A(0) corrupted by noise are observed. We propose a new nuclear-norm penalized estimator of A(0) and establish a general sharp oracle inequality for this estimator for arbitrary values of n, m(1), m(2) under the condition of isometry in expectation. Then this method is applied to the matrix completion problem. In this case, the estimator admits a simple explicit form, and we prove that it satisfies oracle inequalities with faster rates of convergence than in the previous works. They are valid, in particular, in the high-dimensional setting m(1)m(2) >> n. We show that the obtained rates are optimal up to logarithmic factors in a minimax sense and also derive, for any fixed matrix A(0), a nonminimax lower bound on the rate of convergence of our estimator, which coincides with the upper bound up to a constant factor. Finally, we show that our procedure provides an exact recovery of the rank of A(0) with probability close to 1. We also discuss the statistical learning setting where there is no underlying model determined by A(0), and the aim is to find the best trace regression model approximating the data. As a by-product, we show that, under the restricted eigenvalue condition, the usual vector Lasso estimator satisfies a sharp oracle inequality (i.e., an oracle inequality with leading constant 1)."
"10.1214/11-AOS904","2011","The method of moments and degree distributions for network models","0","Probability models on graphs are becoming increasingly important in many applications, but statistical tools for fitting such models are not yet well developed. Here we propose a general method of moments approach that can be used to fit a large class of probability models through empirical counts of certain patterns in a graph. We establish some general asymptotic properties of empirical graph moments and prove consistency of the estimates as the graph size grows for all ranges of the average degree including Omega (1). Additional results are obtained for the important special case of degree distributions."
"10.1214/11-AOS881","2011","Remembering {E}rich {L}ehmann","0",""
"10.1214/11-AOS927","2011","Erich {L}eo {L}ehmann---a glimpse into his life and work","0","Through the use of a system-building approach, an approach that includes finding common ground for the various philosophical paradigms within statistics, Erich L. Lehmann is responsible for much of the synthesis of classical statistical knowledge that developed from the Neyman-Pearson-Wald school. A biographical sketch and a brief summary of some of his many contributions are presented here. His complete bibliography is also included and the references present many other sources of information on his life and his work."
"10.1214/11-AOS928","2011","Introduction to the {L}ehmann special section","0",""
"10.1214/11-AOS901","2011","Statistical inference for time-changed {L}\'evy processes via composite characteristic function estimation","1","In this article, the problem of semi-parametric inference on the parameters of a multidimensional Levy process L(t) with independent components based on the low-frequency observations of the corresponding time-changed Levy process L(T)(,), where T is a nonnegative, nondecreasing real-valued process independent of L(t), is studied. We show that this problem is closely related to the problem of composite function estimation that has recently gotten much attention in statistical literature. Under suitable identifiability conditions, we propose a consistent estimate for the Levy density of L(t) and derive the uniform as well as the pointwise convergence rates of the estimate proposed. Moreover, we prove that the rates obtained are optimal in a minimax sense over suitable classes of time-changed Levy models. Finally, we present a simulation study showing the performance of our estimation algorithm in the case of time-changed Normal Inverse Gaussian (NIG) Levy processes."
"10.1214/11-AOS896","2011","Oracle inequalities and optimal inference under group sparsity","0","We consider the problem of estimating a sparse linear regression vector beta* under a Gaussian noise model, for the purpose of both prediction and model selection. We assume that prior knowledge is available on the sparsity pattern, namely the set of variables is partitioned into prescribed groups, only few of which are relevant in the estimation process. This group sparsity assumption suggests us to consider the Group Lasso method as a means to estimate beta*. We establish oracle inequalities for the prediction and l(2) estimation errors of this estimator. These bounds hold under a restricted eigenvalue condition on the design matrix. Under a stronger condition, we derive bounds for the estimation error for mixed (2, p)-norms with 1 <= p <= infinity. When p = infinity, this result implies that a thresholded version of the Group Lasso estimator selects the sparsity pattern of beta* with high probability. Next, we prove that the rate of convergence of our upper bounds is optimal in a minimax sense, up to a logarithmic factor, for all estimators over a class of group sparse vectors. Furthermore, we establish lower bounds for the prediction and l(2) estimation errors of the usual Lasso estimator. Using this result, we demonstrate that the Group Lasso can achieve an improvement in the prediction and estimation errors as compared to the Lasso.An important application of our results is provided by the problem of estimating multiple regression equations simultaneously or multi-task learning. In this case, we obtain refinements of the results in [In Proc. of the 22nd Annual Conference on Learning Theory (COLT) (2009)1, which allow us to establish a quantitative advantage of the Group Lasso over the usual Lasso in the multi-task setting. Finally, within the same setting, we show how our results can be extended to more general noise distributions, of which we only require the fourth moment to be finite. To obtain this extension, we establish a new maximal moment inequality, which may be of independent interest."
"10.1214/11-AOS895","2011","Global self-weighted and local quasi-maximum exponential likelihood estimators for {ARMA}-{GARCH}/{IGARCH} models","0","This paper investigates the asymptotic theory of the quasi-maximum exponential likelihood estimators (QMELE) for ARMA-GARCH models. Under only a fractional moment condition, the strong consistency and the asymptotic normality of the global self-weighted QMELE are obtained. Based on this self-weighted QMELE, the local QMELE is showed to be asymptotically normal for the ARMA model with GARCH (finite variance) and IGARCH errors. A formal comparison of two estimators is given for some cases. A simulation study is carried out to assess the performance of these estimators, and a real example on the world crude oil price is given."
"10.1214/11-AOS900","2011","On the range of validity of the autoregressive sieve bootstrap","0","We explore the limits of the autoregressive (AR) sieve bootstrap, and show that its applicability extends well beyond the realm of linear time series as has been previously thought. In particular, for appropriate statistics, the AR-sieve bootstrap is valid for stationary processes possessing a general Wold-type autoregressive representation with respect to a white noise; in essence, this includes all stationary, purely nondeterministic processes, whose spectral density is everywhere positive. Our main theorem provides a simple and effective tool in assessing whether the AR-sieve bootstrap is asymptotically valid in any given situation. In effect, the large-sample distribution of the statistic in question must only depend on the first and second order moments of the process; prominent examples include the sample mean and the spectral density. As a counterexample, we show how the AR-sieve bootstrap is not always valid for the sample autocovariance even when the underlying process is linear."
"10.1214/11-AOS899","2011","Parametric or nonparametric? {A} parametricness index for model selection","1","In model selection literature, two classes of criteria perform well asymptotically in different situations: Bayesian information criterion (BIC) (as a representative) is consistent in selection when the true model is finite dimensional (parametric scenario); Akaike's information criterion (AIC) performs well in an asymptotic efficiency when the true model is infinite dimensional (nonparametric scenario). But there is little work that addresses if it is possible and how to detect the situation that a specific model selection problem is in. In this work, we differentiate the two scenarios theoretically under some conditions. We develop a measure, parametricness index (PI), to assess whether a model selected by a potentially consistent procedure can be practically treated as the true model, which also hints on AIC or BIC is better suited for the data for the goal of estimating the regression function. A consequence is that by switching between AIC and BIC based on the PI, the resulting regression estimator is simultaneously asymptotically efficient for both parametric and nonparametric scenarios. In addition, we systematically investigate the behaviors of PI in simulation and real data and show its usefulness."
"10.1214/11-AOS893","2011","Parameter estimation for rough differential equations","0","We construct the ""expected signature matching"" estimator for differential equations driven by rough paths and we prove its consistency and asymptotic normality. We use it to estimate parameters of a diffusion and a fractional diffusions, that is, a differential equation driven by fractional Brownian motion."
"10.1214/11-AOS897","2011","The sparse {L}aplacian shrinkage estimator for high-dimensional regression","0","We propose a new penalized method for variable selection and estimation that explicitly incorporates the correlation patterns among predictors. This method is based on a combination of the minimax concave penalty and Laplacian quadratic associated with a graph as the penalty function. We call it the sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave penalty for encouraging sparsity and Laplacian quadratic penalty for promoting smoothness among coefficients associated with the correlated predictors. The SLS has a generalized grouping property with respect to the graph represented by the Laplacian quadratic. We show that the SLS possesses an oracle property in the sense that it is selection consistent and equal to the oracle Laplacian shrinkage estimator with high probability. This result holds in sparse, high-dimensional settings with p >> n under reasonable conditions. We derive a coordinate descent algorithm for computing the SLS estimates. Simulation studies are conducted to evaluate the performance of the SLS method and a real data example is used to illustrate its application."
"10.1214/11-AOS892","2011","On {B}ayes' theorem for improper mixtures","0","Although Bayes's theorem demands a prior that is a probability distribution on the parameter space, the calculus associated with Bayes's theorem sometimes generates sensible procedures from improper priors, Pitman's estimator being a good example. However, improper priors may also lead to Bayes procedures that are paradoxical or otherwise unsatisfactory, prompting some authors to insist that all priors be proper. This paper begins with the observation that an improper measure on 8 satisfying Kingman's countability condition is in fact a probability distribution on the power set. We show how to extend a model in such a way that the extended parameter space is the power set. Under an additional finiteness condition, which is needed for the existence of a sampling region, the conditions for Bayes's theorem are satisfied by the extension. Lack of interference ensures that the posterior distribution in the extended space is compatible with the original parameter space. Provided that the key finiteness condition is satisfied, this probabilistic analysis of the extended model may be interpreted as a vindication of improper Bayes procedures derived from the original model."
"10.1214/11-AOS890","2011","New estimators of the {P}ickands dependence function and a test for extreme-value dependence","0","We propose a new class of estimators for Pickands dependence function which is based on the concept of minimum distance estimation. An explicit integral representation of the function A* (t), which minimizes a weighted L(2)-distance between the logarithm of the copula C(y(1-t), y(t)) and functions of the form A (t) log(y) is derived. If the unknown copula is an extreme-value copula, the function A* (t) coincides with Pickands dependence function. Moreover, even if this is not the case, the function A* (t) always satisfies the boundary conditions of a Pickands dependence function. The estimators are obtained by replacing the unknown copula by its empirical counterpart and weak convergence of the corresponding process is shown. A comparison with the commonly used estimators is performed from a theoretical point of view and by means of a simulation study. Our asymptotic and numerical results indicate that some of the new estimators outperform the estimators, which were recently proposed by Genest and Segers [Ann. Statist. 37 (2009) 2990-3022]. As a by-product of our results, we obtain a simple test for the hypothesis of an extreme-value copula, which is consistent against all positive quadrant dependent alternatives satisfying weak differentiability assumptions of first order."
"10.1214/11-AOS889","2011","Stochastic expansions using continuous dictionaries: {L}\'evy adaptive regression kernels","0","This article describes a new class of prior distributions for nonparametric function estimation. The unknown function is modeled as a limit of weighted sums of kernels or generator functions indexed by continuous parameters that control local and global features such as their translation, dilation, modulation and shape. Levy random fields and their stochastic integrals are employed to induce prior distributions for the unknown functions or, equivalently, for the number of kernels and for the parameters governing their features. Scaling, shape, and other features of the generating functions are location-specific to allow quite different function properties in different parts of the space, as with wavelet bases and other methods employing overcomplete dictionaries. We provide conditions under which the stochastic expansions converge in specified Besov or Sobolev norms. Under a Gaussian error model, this may be viewed as a sparse regression problem, with regularization induced via the Levy random field prior distribution. Posterior inference for the unknown functions is based on a reversible jump Markov chain Monte Carlo algorithm. We compare the Levy Adaptive Regression Kernel (LARK) method to wavelet-based methods using some of the standard test functions, and illustrate its flexibility and adaptability in nonstationary applications."
"10.1214/11-AOS887","2011","Spectral clustering and the high-dimensional stochastic blockmodel","1","Networks or graphs can easily represent a diverse set of data sources that are characterized by interacting units or actors. Social networks, representing people who communicate with each other, are one example. Communities or clusters of highly connected actors form an essential feature in the structure of several empirical networks. Spectral clustering is a popular and computationally feasible method to discover these communities.The stochastic blockmodel [Social Networks 5 (1983) 109-137] is a social network model with well-defined communities; each node is a member of one community. For a network generated from the Stochastic Blockmodel, we bound the number of nodes ""misclustered"" by spectral clustering. The asymptotic results in this paper are the first clustering results that allow the number of clusters in the model to grow with the number of nodes, hence the name high-dimensional.In order to study spectral clustering under the stochastic blockmodel, we first show that under the more general latent space model, the eigenvectors of the normalized graph Laplacian asymptotically converge to the eigenvectors of a ""population"" normalized graph Laplacian. Aside from the implication for spectral clustering, this provides insight into a graph visualization technique. Our method of studying the eigenvectors of random matrices is original."
"10.1214/11-AOS888","2011","Optimal model selection for density estimation of stationary data under various mixing conditions","0","We propose a block-resampling penalization method for marginal density estimation with nonnecessary independent observations. When the data are beta or tau-mixing, the selected estimator satisfies oracle inequalities with leading constant asymptotically equal to 1.We also prove in this setting the slope heuristic, which is a data-driven method to optimize the leading constant in the penalty."
"10.1214/11-AOS885","2011","Estimation and variable selection for generalized additive partial linear models","0","We study generalized additive partial linear models, proposing the use of polynomial spline smoothing for estimation of nonparametric functions, and deriving quasi-likelihood based estimators for the linear parameters. We establish asymptotic normality for the estimators of the parametric components. The procedure avoids solving large systems of equations as in kernel-based procedures and thus results in gains in computational simplicity. We further develop a class of variable selection procedures for the linear parameters by employing a nonconcave penalized quasi-likelihood, which is shown to have an asymptotic oracle property. Monte Carlo simulations and an empirical example are presented for illustration."
"10.1214/11-AOS891","2011","Estimation of extreme risk regions under multivariate regular variation","0","When considering d possibly dependent random variables, one is often interested in extreme risk regions, with very small probability p. We consider risk regions of the form {z is an element of R(d) : f (z) <= beta}, where f is the joint density and beta a small number. Estimation of such an extreme risk region is difficult since it contains hardly any or no data. Using extreme value theory, we construct a natural estimator of an extreme risk region and prove a refined form of consistency, given a random sample of multivariate regularly varying random vectors. In a detailed simulation and comparison study, the good performance of the procedure is demonstrated. We also apply our estimator to financial data."
"10.1214/11-AOS886","2011","Iterated filtering","0","Inference for partially observed Markov process models has been a long-standing methodological challenge with many scientific and engineering applications. Iterated filtering algorithms maximize the likelihood function for partially observed Markov process models by solving a recursive sequence of filtering problems. We present new theoretical results pertaining to the convergence of iterated filtering algorithms implemented via sequential Monte Carlo filters. This theory complements the growing body of empirical evidence that iterated filtering algorithms provide an effective inference strategy for scientific models of nonlinear dynamic systems. The first step in our theory involves studying a new recursive approach for maximizing the likelihood function of a latent variable model, when this likelihood is evaluated via importance sampling. This leads to the consideration of an iterated importance sampling algorithm which serves as a simple special case of iterated filtering, and may have applicability in its own right."
"10.1214/11-AOS884","2011","A majorization-minimization approach to variable selection using spike and slab priors","0","We develop a method to carry out MAP estimation for a class of Bayesian regression models in which coefficients are assigned with Gaussian-based spike and slab priors. The objective function in the corresponding optimization problem has a Lagrangian form in that regression coefficients are regularized by a mixture of squared l(2) and l(0) norms. A tight approximation to the l(0) norm using majorization minimization techniques is derived, and a coordinate descent algorithm in conjunction with a soft-thresholding scheme is used in searching for the optimizer of the approximate objective. Simulation studies show that the proposed method can lead to more accurate variable selection than other benchmark methods. Theoretical results show that under regular conditions, sign consistency can be established, even when the Irrepresentable Condition is violated. Results on posterior model consistency and estimation consistency, and an extension to parameter estimation in the generalized linear models are provided."
"10.1214/11-AOS882","2011","Single and multiple index functional regression models with nonparametric link","0","Fully nonparametric methods for regression from functional data have poor accuracy from a statistical viewpoint, reflecting the fact that their convergence rates are slower than nonparametric rates for the estimation of high-dimensional functions. This difficulty has led to an emphasis on the so-called functional linear model, which is much more flexible than common linear models in finite dimension, but nevertheless imposes structural constraints on the relationship between predictors and responses. Recent advances have extended the linear approach by using it in conjunction with link functions, and by considering multiple indices, but the flexibility of this technique is still limited. For example, the link may be modeled parametrically or on a grid only, or may be constrained by an assumption such as monotonicity; multiple indices have been modeled by making finite-dimensional assumptions. In this paper we introduce a new technique for estimating the link function nonparametrically, and we suggest an approach to multi-index modeling using adaptively defined linear projections of functional data. We show that our methods enable prediction with polynomial convergence rates. The finite sample performance of our methods is studied in simulations, and is illustrated by an application to a functional regression problem."
"10.1214/11-AOS873","2011","Testing whether jumps have finite or infinite activity","0","We propose statistical tests to discriminate between the finite and infinite activity of jumps in a semimartingale discretely observed at high frequency. The two statistics allow for a symmetric treatment of the problem: we can either take the null hypothesis to be finite activity, or infinite activity. When implemented on high-frequency stock returns, both tests point toward the presence of infinite-activity jumps in the data."
"10.1214/10-AOS871","2011","The {EFM} approach for single-index models","0","Single-index models are natural extensions of linear models and circumvent the so-called curse of dimensionality. They are becoming increasingly popular in many scientific fields including biostatistics, medicine, economics and financial econometrics. Estimating and testing the model index coefficients beta is one of the most important objectives in the statistical analysis. However, the commonly used assumption on the index coefficients, parallel to beta parallel to = 1, represents a nonregular problem: the true index is on the boundary of the unit ball. In this paper we introduce the EFM approach, a method of estimating functions, to study the single-index model. The procedure is to first relax the equality constraint to one with (d - 1) components of beta lying in an open unit ball, and then to construct the associated (d - 1) estimating functions by projecting the score function to the linear space spanned by the residuals with the unknown link being estimated by kernel estimating functions. The root-n consistency and asymptotic normality for the estimator obtained from solving the resulting estimating equations are achieved, and a Wilks type theorem for testing the index is demonstrated. A noticeable result we obtain is that our estimator for beta has smaller or equal limiting variance than the estimator of Carroll et al. [J. Amer Statist. Assoc. 92 (1997) 447-4891. A fixed-point iterative scheme for computing this estimator is proposed. This algorithm only involves one-dimensional nonparametric smoothers, thereby avoiding the data sparsity problem caused by high model dimensionality. Numerical studies based on simulation and on applications suggest that this new estimating system is quite powerful and easy to implement."
"10.1214/10-AOS852","2011","Nonparametric least squares estimation of a multivariate convex regression function","0","This paper deals with the consistency of the nonparametric least squares estimator of a convex regression function when the predictor is multidimensional. We characterize and discuss the computation of such an estimator via the solution of certain quadratic and linear programs. Mild sufficient conditions for the consistency of this estimator and its subdifferentials in fixed and stochastic design regression settings are provided."
"10.1214/11-AOS883","2011","Bandwidth selection in kernel density estimation: oracle inequalities and adaptive minimax optimality","0","We address the problem of density estimation with L(s)(-)loss by selection of kernel estimators. We develop a selection procedure and derive corresponding L(s)-risk oracle inequalities. It is shown that the proposed selection rule leads to the estimator being minimax adaptive over a scale of the anisotropic Nikol'skii classes. The main technical tools used in our derivations are uniform bounds on the L(s)-norms of empirical processes developed recently by Goldenshluger and Lepski [Ann. Probab. (2011), to appear]."
"10.1214/11-AOS874","2011","Change-point in stochastic design regression and the bootstrap","0","In this paper we study the consistency of different bootstrap procedures for constructing confidence intervals (CIs) for the unique jump discontinuity (change-point) in an otherwise smooth regression function in a stochastic design setting. This problem exhibits nonstandard asymptotics, and we argue that the standard bootstrap procedures in regression fail to provide valid confidence intervals for the change-point. We propose a version of smoothed bootstrap, illustrate its remarkable finite sample performance in our simulation study and prove the consistency of the procedure. The m out of it bootstrap procedure is also considered and shown to be consistent. We also provide sufficient conditions for any bootstrap procedure to be consistent in this scenario."
"10.1214/10-AOS869","2011","Asymptotic {B}ayes-optimality under sparsity of some multiple testing procedures","0","Within a Bayesian decision theoretic framework we investigate some asymptotic optimality properties of a large class of multiple testing rules. A parametric setup is considered, in which observations come from a normal scale mixture model and the total loss is assumed to be the sum of losses for individual tests. Our model can be used for testing point null hypotheses, as well as to distinguish large signals from a multitude of very small effects. A rule is defined to be asymptotically Bayes optimal under sparsity (ABOS), if within our chosen asymptotic framework the ratio of its Bayes risk and that of the Bayes oracle (a rule which minimizes the Bayes risk) converges to one. Our main interest is in the asymptotic scheme where the proportion p of ""true"" alternatives converges to zero.We fully characterize the class of fixed threshold multiple testing rules which are ABOS, and hence derive conditions for the asymptotic optimality of rules controlling the Bayesian False Discovery Rate (BFDR). We finally provide conditions under which the popular Benjamini-Hochberg (BH) and Bonferroni procedures are ABOS and show that for a wide class of sparsity levels, the threshold of the former can be approximated by a nonrandom threshold.It turns out that while the choice of asymptotically optimal FDR levels for BH depends on the relative cost of a type I error, it is almost independent of the level of sparsity. Specifically, we show that when the number of tests in increases to infinity, then BH with FDR level chosen in accordance with the assumed loss function is ABOS in the entire range of sparsity parameters p proportional to m(-beta), with beta is an element of (0, 1]."
"10.1214/10-AOS861","2011","Uniform moment bounds of {F}isher's information with applications to time series","0","In this paper, a uniform (over some parameter space) moment bound for the inverse of Fisher's information matrix is established. This result is then applied to develop moment bounds for the normalized least squares estimate in (nonlinear) stochastic regression models. The usefulness of these results is illustrated using time series models. In particular, an asymptotic expression for the mean squared prediction error of the least squares predictor in autoregressive moving average models is obtained. This asymptotic expression provides a solid theoretical foundation for some model selection criteria."
"10.1214/11-AOS879","2011","Limiting laws of coherence of random matrices with applications to testing covariance structure and construction of compressed sensing matrices","2","Testing covariance structure is of significant interest in many areas of statistical analysis and construction of compressed sensing matrices is an important problem in signal processing. Motivated by these applications, we study in this paper the limiting laws of the coherence of an n x p random matrix in the high-dimensional setting where p can be much larger than n. Both the law of large numbers and the limiting distribution are derived. We then consider testing the bandedness of the covariance matrix of a high-dimensional Gaussian distribution which includes testing for independence as a special case. The limiting laws of the coherence of the data matrix play a critical role in the construction of the test. We also apply the asymptotic results to the construction of compressed sensing matrices."
"10.1214/10-AOS872","2011","Asymptotic equivalence of functional linear regression and a white noise inverse problem","0","We consider the statistical experiment of functional linear regression (FLR). Furthermore, we introduce a white noise model where one observes an Ito process, which contains the covariance operator of the corresponding FLR model in its construction. We prove asymptotic equivalence of FLR and this white noise model in LeCam's sense under known design distribution. Moreover, we show equivalence of FLR and an empirical version of the white noise model for finite sample sizes. As an application, we derive sharp minimax constants in the FLR model which are still valid in the case of unknown design distribution."
"10.1214/10-AOS868","2011","T{FT}-bootstrap: resampling time series in the frequency domain to obtain replicates in the time domain","1","A new time series bootstrap scheme, the time frequency toggle (TFT)-bootstrap, is proposed. Its basic idea is to bootstrap the Fourier coefficients of the observed time series, and then to back-transform them to obtain a bootstrap sample in the time domain. Related previous proposals, such as the ""surrogate data"" approach, resampled only the phase of the Fourier coefficients and thus had only limited validity. By contrast, we show that the appropriate resampling of phase and magnitude, in addition to some smoothing of Fourier coefficients, yields a bootstrap scheme that mimics the correct second-order moment structure for a large class of time series processes. As a main result we obtain a functional limit theorem for the TFT-bootstrap under a variety of popular ways of frequency domain bootstrapping. Possible applications of the TFT-bootstrap naturally arise in change-point analysis and unit-root testing where statistics are frequently based on functionals of partial sums. Finally, a small simulation study explores the potential of the TFT-bootstrap for small samples showing that for the discussed tests in change-point analysis as well as unit-root testing, it yields better results than the corresponding asymptotic tests if measured by size and power."
"10.1214/10-AOS867","2011","Asymptotic properties of {$U$}-processes under long-range dependence","0","Let (Xi)(i >= 1) be a stationary mean-zero Gaussian process with covariances rho(k) = E(X(1) X(k+1)) satisfying rho(0) = 1 and rho(k) = k(-D) L(k), where D is in (0, 1), and L is slowly varying at infinity. Consider the U-process {U(n)(r), r is an element of 1} defined asU(n)(r) = 1/n (n-1) Sigma(1 <= i not equal j <= n) 1{G(X(i), X(j))<= r}where I is an interval included in R, and G is a symmetric function. In this paper, we provide central and noncentral limit theorems for U(n). They are used to derive, in the long-range dependence setting, new properties of many well-known estimators such as the Hodges-Lehmann estimator, which is a well-known robust location estimator, the Wilcoxon-signed rank statistic, the sample correlation integral and an associated robust scale estimator. These robust estimators are shown to have the same asymptotic distribution as the classical location and scale estimators. The limiting distributions are expressed through multiple Wiener-Ito integrals."
"10.1214/11-AOS880","2011","Higher order scrambled digital nets achieve the optimal rate of the root mean square error for smooth integrands","0","We study a random sampling technique to approximate integrals f[0,1](s) f (x) dx by averaging the function at some sampling points. We focus on cases where the integrand is smooth, which is a problem which occurs in statistics.The convergence rate of the approximation error depends on the smoothness of the function f and the sampling technique. For instance, Monte Carlo (MC) sampling yields a convergence of the root mean square error (RMSE) of order N(-1/2) (where N is the number of samples) for functions f with finite variance. Randomized QMC (RQMC), a combination of MC and quasi-Monte Carlo (QMC), achieves a RMSE of order N(-3/2+epsilon) under the stronger assumption that the integrand has bounded variation. A combination of RQMC with local antithetic sampling achieves a convergence of the RMSE of order N(-3/2-1/s+epsilon) (where s >= 1 is the dimension) for functions with mixed partial derivatives up to order two.Additional smoothness of the integrand does not improve the rate of convergence of these algorithms in general. On the other hand, it is known that without additional smoothness of the integrand it is not possible to improve the convergence rate.This paper introduces a new RQMC algorithm, for which we prove that it achieves a convergence of the root mean square error (RMSE) of order N(-alpha-1/2+epsilon) provided the integrand satisfies the strong assumption that it has square integrable partial mixed derivatives up to order alpha > 1 in each variable. Known lower bounds on the RMSE show that this rate of convergence cannot be improved in general for integrands with this smoothness. We provide numerical examples for which the RMSE converges approximately with order N(-5/2) and N(-7/2), in accordance with the theoretical upper bound."
"10.1214/11-AOS878","2011","The solution path of the generalized lasso","2","We present a path algorithm for the generalized lasso problem. This problem penalizes the l(1) norm of a matrix D times the coefficient vector, and has a wide range of applications, dictated by the choice of D. Our algorithm is based on solving the dual of the generalized lasso, which greatly facilitates computation of the path. For D = I (the usual lasso), we draw a connection between our approach and the well-known LARS algorithm. For an arbitrary D, we derive an unbiased estimate of the degrees of freedom of the generalized lasso fit. This estimate turns out to be quite intuitive in many applications."
"10.1214/11-AOS877","2011","On construction of optimal mixed-level supersaturated designs","0","Supersaturated design (SSD) has received much recent interest because of its potential in factor screening experiments. In this paper, we provide equivalent conditions for two columns to be fully aliased and consequently propose methods for constructing E(f(NoD))- and chi(2)-optimal mixed-level SSDs without fully aliased columns, via equidistant designs and difference matrices. The methods can be easily performed and many new optimal mixed-level SSDs have been obtained. Furthermore, it is proved that the nonorthogonality between columns of the resulting design is well controlled by the source designs. A rather complete list of newly generated optimal mixed-level SSDs are tabulated for practical use."
"10.1214/11-AOS876","2011","Optimal selection of reduced rank estimators of high-dimensional matrices","2","We introduce a new criterion, the Rank Selection Criterion (RSC), for selecting the optimal reduced rank estimator of the coefficient matrix in multivariate response regression models. The corresponding RSC estimator minimizes the Frobenius norm of the fit plus a regularization term proportional to the number of parameters in the reduced rank model.The rank of the RSC estimator provides a consistent estimator of the rank of the coefficient matrix; in general, the rank of our estimator is a consistent estimate of the effective rank, which we define to be the number of singular values of the target matrix that are appropriately large. The consistency results are valid not only in the classic asymptotic regime, when n, the number of responses, and p, the number of predictors, stay bounded, and m, the number of observations, grows, but also when either, or both, n and p grow, possibly much faster than m.We establish minimax optimal bounds on the mean squared errors of our estimators. Our finite sample performance bounds for the RSC estimator show that it achieves the optimal balance between the approximation error and the penalty term.Furthermore, our procedure has very low computational complexity, linear in the number of candidate models, making it particularly appealing for large scale problems. We contrast our estimator with the nuclear norm penalized least squares (NNP) estimator, which has an inherently higher computational complexity than RSC, for multivariate regression models. We show that NNP has estimation properties similar to those of RSC, albeit under stronger conditions. However, it is not as parsimonious as RSC.We offer a simple correction of the NNP estimator which leads to consistent rank estimation. We verify and illustrate our theoretical findings via an extensive simulation study."
"10.1214/11-AOS875","2011","A note on the de la {G}arza phenomenon for locally optimal designs","0","The celebrated de la Garza phenomenon states that for a polynomial regression model of degree p - 1 any optimal design can be based on at most p design points. In a remarkable paper, Yang [Ann. Statist. 38 (2010) 2499 - 2524] showed that this phenomenon exists in many locally optimal design problems for nonlinear models. In the present note, we present a different view point on these findings using results about moment theory and Chebyshev systems. In particular, we show that this phenomenon occurs in an even larger class of models than considered so far."
"10.1214/10-AOS870","2011","Sparse linear discriminant analysis by thresholding for high dimensional data","3","In many social, economical, biological and medical studies, one objective is to classify a subject into one of several classes based on a set of variables observed from the subject. Because the probability distribution of the variables is usually unknown, the rule of classification is constructed using a training sample. The well-known linear discriminant analysis (LDA) works well for the situation where the number of variables used for classification is much smaller than the training sample size. Because of the advance in technologies, modern statistical studies often face classification problems with the number of variables much larger than the sample size, and the LDA may perform poorly. We explore when and why the LDA has poor performance and propose a sparse LDA that is asymptotically optimal under some sparsity conditions on the unknown parameters. For illustration of application, we discuss an example of classifying human cancer into two classes of leukemia based on a set of 7,129 genes and a training sample of size 72. A simulation is also conducted to check the performance of the proposed method."
"10.1214/10-AOS865","2011","Delta method in large deviations and moderate deviations for estimators","0","The delta method is a popular and elementary tool for deriving limiting distributions of transformed statistics, while applications of asymptotic distributions do not allow one to obtain desirable accuracy of approximation for tail probabilities. The large and moderate deviation theory can achieve this goal. Motivated by the delta method in weak convergence, a general delta method in large deviations is proposed. The new method can be widely applied to driving the moderate deviations of estimators and is illustrated by examples including the Wilcoxon statistic, the Kaplan-Meier estimator, the empirical quantile processes and the empirical copula function. We also improve the existing moderate deviations results for M-estimators and L-statistics by the new method. Some applications of moderate deviations to statistical hypothesis testing are provided."
"10.1214/10-AOS864","2011","Performance guarantees for individualized treatment rules","0","Because many illnesses show heterogeneous response to treatment, there is increasing interest in individualizing treatment to patients [Arch. Gen. Psychiatry 66 (2009) 128-133]. An individualized treatment rule is a decision rule that recommends treatment according to patient characteristics. We consider the use of clinical trial data in the construction of an individualized treatment rule leading to highest mean response. This is a difficult computational problem because the objective function is the expectation of a weighted indicator function that is nonconcave in the parameters. Furthermore, there are frequently many pretreatment variables that may or may not be useful in constructing an optimal individualized treatment rule, yet cost and interpretability considerations imply that only a few variables should be used by the individualized treatment rule. To address these challenges, we consider estimation based on l(1)-penalized least squares. This approach is justified via a finite sample upper bound on the difference between the mean response due to the estimated individualized treatment rule and the mean response due to the optimal individualized treatment rule."
"10.1214/10-AOS863","2011","On multivariate quantiles under partial orders","0","This paper focuses on generalizing quantiles from the ordering point of view. We propose the concept of partial quantiles, which are based on a given partial order. We establish that partial quantiles are equivariant under order-preserving transformations of the data, robust to outliers, characterize the probability distribution if the partial order is sufficiently rich, generalize the concept of efficient frontier, and can measure dispersion from the partial order perspective.We also study several statistical aspects of partial quantiles. We provide estimators, associated rates of convergence, and asymptotic distributions that hold uniformly over a continuum of quantile indices. Furthermore, we provide procedures that can restore monotonicity properties that might have been disturbed by estimation error, establish computational complexity bounds, and point out a concentration of measure phenomenon (the latter under independence and the componentwise natural order).Finally, we illustrate the concepts by discussing several theoretical examples and simulations. Empirical applications to compare intake nutrients within diets, to evaluate the performance of investment funds, and to study the impact of policies on tobacco awareness are also presented to illustrate the concepts and their use."
"10.1214/10-AOS862","2011","Intrinsic inference on the mean geodesic of planar shapes and tree discrimination by leaf growth","0","For planar landmark based shapes, taking into account the non-Euclidean geometry of the shape space, a statistical test for a common mean first geodesic principal component (GPC) is devised which rests on one of two asymptotic scenarios. For both scenarios, strong consistency and central limit theorems are established, along with an algorithm for the computation of a Ziezold mean geodesic. In application, this allows to verify the geodesic hypothesis for leaf growth of Canadian black poplars and to discriminate genetically different trees by observations of leaf shape growth over brief time intervals. With a test based on Procrustes tangent space coordinates, not involving the shape space's curvature, neither can be achieved."
"10.1214/10-AOS850","2011","Estimation of (near) low-rank matrices with noise and high-dimensional scaling","2","We study an instance of high-dimensional inference in which the goal is to estimate a matrix circle minus* is an element of R(m1xm2) on the basis of N noisy observations. The unknown matrix circle minus* is assumed to be either exactly low rank, or ""near"" low-rank, meaning that it can be well-approximated by a matrix with low rank. We consider a standard M-estimator based on regularization by the nuclear or trace norm over matrices, and analyze its performance under high-dimensional scaling. We define the notion of restricted strong convexity (RSC) for the loss function, and use it to derive nonasymptotic bounds on the Frobenius norm error that hold for a general class of noisy observation models, and apply to both exactly low-rank and approximately low rank matrices. We then illustrate consequences of this general theory for a number of specific matrix models, including low-rank multivariate or multi-task regression, system identification in vector autoregressive processes and recovery of low-rank matrices from random projections. These results involve nonasymptotic random matrix theory to establish that the RSC condition holds, and to determine an appropriate choice of regularization parameter. Simulation results show excellent agreement with the high-dimensional scaling of the error predicted by our theory."
"10.1214/10-AOS858","2011","Localized spherical deconvolution","0","We provide a new algorithm for the treatment of the deconvolution problem on the sphere which combines the traditional SVD inversion with an appropriate thresholding technique in a well chosen new basis. We establish upper bounds for the behavior of our procedure for any L-p loss. It is important to emphasize the adaptation properties of our procedures with respect to the regularity (sparsity) of the object to recover as well as to inhomogeneous smoothness. We also perform a numerical study which proves that the procedure shows very promising properties in practice as well."
"10.1214/10-AOS849","2011","Testing composite hypotheses, {H}ermite polynomials and optimal estimation of a nonsmooth functional","0","A general lower bound is developed for the minimax risk when estimating an arbitrary functional. The bound is based on testing two composite hypotheses and is shown to be effective in estimating the nonsmooth functional 1/n Sigma vertical bar theta(i)vertical bar from an observation Y similar to N (theta, I-n). This problem exhibits some features that are significantly different from those that occur in estimating conventional smooth functionals. This is a setting where standard techniques fail to yield sharp results.A sharp minimax lower bound is established by applying the general lower bound technique based on testing two composite hypotheses. A key step is the construction of two special priors and bounding the chi-square distance between two normal mixtures. An estimator is constructed using approximation theory and Hermite polynomials and is shown to be asymptotically sharp minimax when the means are bounded by a given value M. It is shown that the minimax risk equals beta(2)(*) M-2(log log n/log n)(2) asymptotically, where beta(*) is the log n Bernstein constant.The general techniques and results developed in the present paper can also be used to solve other related problems."
"10.1214/10-AOS828","2011","Kernel estimators of asymptotic variance for adaptive {M}arkov chain {M}onte {C}arlo","0","We study the asymptotic behavior of kernel estimators of asymptotic variances (or long-run variances) for a class of adaptive Markov chains. The convergence is studied both in L(P) and almost surely. The results also apply to Markov chains and improve on the existing literature by imposing weaker conditions. We illustrate the results with applications to the GARCH(1, 1) Markov model and to an adaptive MCMC algorithm for Bayesian logistic regression."
"10.1214/10-AOS820","2011","A two-stage hybrid procedure for estimating an inverse regression function","0","We consider a two-stage procedure (TSP) for estimating an inverse regression function at a given point, where isotonic regression is used at stage one to obtain an initial estimate and a local linear approximation in the vicinity of this estimate is used at stage two. We establish that the convergence rate of the second-stage estimate can attain the parametric n(1/2) rate. Furthermore, a bootstrapped variant of TSP (BTSP) is introduced and its consistency properties studied. This variant manages to overcome the slow speed of the convergence in distribution and the estimation of the derivative of the regression function at the unknown target quantity. Finally, the finite sample performance of BTSP is studied through simulations and the method is illustrated on a data set."
"10.1214/10-AOS815","2011","A trigonometric approach to quaternary code designs with application to one-eighth and one-sixteenth fractions","0","The study of good nonregular fractional factorial designs has received significant attention over the last two decades. Recent research indicates that designs constructed from quaternary codes (QC) are very promising in this regard. The present paper shows how a trigonometric approach can facilitate a systematic understanding of such QC designs and lead to new theoretical results covering hitherto unexplored situations. We focus attention on one-eighth and one-sixteenth fractions of two-level factorials and show that optimal QC designs often have larger generalized resolution and projectivity than comparable regular designs. Moreover, some of these designs are found to have maximum projectivity among all designs."
"10.1214/10-AOS860","2011","Estimation of high-dimensional low-rank matrices","4","Suppose that we observe entries or, more generally, linear combinations of entries of an unknown m x T-matrix A corrupted by noise. We are particularly interested in the high-dimensional setting where the number mT of unknown entries can be much larger than the sample size N. Motivated by several applications, we consider estimation of matrix A under the assumption that it has small rank. This can be viewed as dimension reduction or sparsity assumption. In order to shrink toward a low-rank representation, we investigate penalized least squares estimators with a Schatten-p quasi-norm penalty term, p <= 1. We study these estimators under two possible assumptions-a modified version of the restricted isometry condition and a uniform bound on the ratio ""empirical norm induced by the sampling operator/Frobenius norm."" The main results are stated as nonasymptotic upper bounds on the prediction risk and on the Schatten-q risk of the estimators, where q is an element of [p, 2]. The rates that we obtain for the prediction risk are of the form rmIN (for m = T), up to logarithmic factors, where r is the rank of A. The particular examples of multi-task learning and matrix completion are worked out in detail. The proofs are based on tools from the theory of empirical processes. As a by-product, we derive bounds for the kth entropy numbers of the quasi-convex Schatten class embeddings, S(p)(M) -> S(2)(M) p < 1, which are of independent interest."
"10.1214/10-AOS859","2011","Global identifiability of linear structural equation models","0","Structural equation models are multivariate statistical models that are defined by specifying noisy functional relationships among random variables. We consider the classical case of linear relationships and additive Gaussian noise terms. We give a necessary and sufficient condition for global identifiability of the model in terms of a mixed graph encoding the linear structural equations and the correlation structure of the error terms. Global identifiability is understood to mean injectivity of the parametrization of the model and is fundamental in particular for applicability of standard statistical methodology."
"10.1214/10-AOS857","2011","Bayesian analysis of variable-order, reversible {M}arkov chains","0","We define a conjugate prior for the reversible Markov chain of order r. The prior arises from a partially exchangeable reinforced random walk, in the same way that the Beta distribution arises from the exchangeable Polya urn. An extension to variable-order Markov chains is also derived. We show the utility of this prior in testing the order and estimating the parameters of a reversible Markov model."
"10.1214/10-AOS856","2011","Estimation for {L}\'evy processes from high frequency data within a long time interval","0","In this paper, we study nonparametric estimation of the Levy density for Levy processes, with and without Brownian component. For this, we consider n discrete time observations with step Delta. The asymptotic framework is: n tends to infinity, Delta = Delta(n), tends to zero while n Delta(n) tends to infinity. We use a Fourier approach to construct an adaptive nonparametric estimator of the Levy density and to provide a bound for the global L(2)-risk. Estimators of the drift and of the variance of the Gaussian component are also studied. We discuss rates of convergence and give examples and simulation results for processes fitting in our framework."
"10.1214/10-AOS855","2011","Asymptotic equivalence for inference on the volatility from noisy observations","0","We consider discrete-time observations of a continuous martingale under measurement error. This serves as a fundamental model for high-frequency data in finance, where an efficient price process is observed under microstructure noise. It is shown that this nonparametric model is in Le Cam's sense asymptotically equivalent to a Gaussian shift experiment in terms of the square root of the volatility function a and a nonstandard noise level. As an application, new rate-optimal estimators of the volatility function and simple efficient estimators of the integrated volatility are constructed."
"10.1214/10-AOS854","2011","Exponential screening and optimal rates of sparse estimation","1","In high-dimensional linear regression, the goal pursued here is to estimate an unknown regression function using linear combinations of a suitable set of covariates. One of the key assumptions for the success of any statistical procedure in this setup is to assume that the linear combination is sparse in some sense, for example, that it involves only few covariates. We consider a general, nonnecessarily linear, regression with Gaussian noise and study a related question, that is, to find a linear combination of approximating functions, which is at the same time sparse and has small mean squared error (MSE). We introduce a new estimation procedure, called Exponential Screening, that shows remarkable adaptation properties. It adapts to the linear combination that optimally balances MSE and sparsity, whether the latter is measured in terms of the number of nonzero entries in the combination (l(0) norm) or in terms of the global weight of the combination (l(1) norm). The power of this adaptation result is illustrated by showing that Exponential Screening solves optimally and simultaneously all the problems of aggregation in Gaussian regression that have been discussed in the literature. Moreover, we show that the performance of the Exponential Screening estimator cannot be improved in a minimax sense, even if the optimal sparsity is known in advance. The theoretical and numerical superiority of Exponential Screening compared to state-of-the-art sparse procedures is also discussed."
"10.1214/10-AOS853","2011","Approximation by log-concave distributions, with applications to regression","0","We study the approximation of arbitrary distributions P on d-dimensional space by distributions with log-concave density. Approximation means minimizing a Kullback Leibler-type functional. We show that such an approximation exists if and only if P has finite first moments and is not supported by some hyperplane. Furthermore we show that this approximation depends continuously on P with respect to Mallows distance D(1)(.,.). This result implies consistency of the maximum likelihood estimator of a log-concave density under fairly general conditions. It also allows us to prove existence and consistency of estimators in regression models with a response Y = mu(X) + epsilon, where X and epsilon are independent, mu(.) belongs to a certain class of regression functions while E is a random error with log-concave density and mean zero."
"10.1214/10-AOS831","2011","Consistency of {M}arkov chain quasi-{M}onte {C}arlo on continuous state spaces","0","The random numbers driving Markov chain Monte Carlo (MCMC) simulation are usually modeled as independent U(0, 1) random variables. Tribble [Markov chain Monte Carlo algorithms using completely uniformly distributed driving sequences (2007) Stanford Univ.] reports substantial improvements when those random numbers are replaced by carefully balanced inputs from completely uniformly distributed sequences. The previous theoretical justification for using anything other than lid. U(0, 1) points shows consistency for estimated means, but only applies for discrete stationary distributions. We extend those results to some MCMC algorithms for continuous stationary distributions. The main motivation is the search for quasi-Monte Carlo versions of MCMC. As a side benefit, the results also establish consistency for the usual method of using pseudo-random numbers in place of random ones."
"10.1214/10-AOS851","2011","Immigrated urn models---theoretical properties and applications","0","Urn models have been widely studied and applied in both scientific and social science disciplines. In clinical studies, the adoption of urn models in treatment allocation schemes has proved to be beneficial to researchers, by providing more efficient clinical trials, and to patients, by increasing the likelihood of receiving the better treatment. In this paper, we propose a new and general class of immigrated urn (IMU) models that incorporates the immigration mechanism into the urn process. Theoretical properties are developed and the advantages of the IMU models are discussed. In general, the IMU models have smaller variabilities than the classical urn models, yielding more powerful statistical inferences in applications. Illustrative examples are presented to demonstrate the wide applicability of the IMU models. The proposed IMU framework, including many popular classical urn models not only offers a unify perspective for us to comprehend the urn process, but also enables us to generate several novel urn models with desirable properties."
"10.1214/10-AOS848","2011","Multiple testing via {${\rm FDR}_L$} for large-scale imaging data","0","The multiple testing procedure plays an important role in detecting the presence of spatial signals for large-scale imaging data. Typically, the spatial signals are sparse but clustered. This paper provides empirical evidence that for a range of commonly used control levels, the conventional FDR procedure can lack the ability to detect statistical significance, even if the p-values under the true null hypotheses are independent and uniformly distributed; more generally, ignoring the neighboring information of spatially structured data will tend to diminish the detection effectiveness of the FDR procedure. This paper first introduces a scalar quantity to characterize the extent to which the ""lack of identification phenomenon"" (LIP) of the FDR procedure occurs. Second, we propose a new multiple comparison procedure, called FDRL, to accommodate the spatial information of neighboring p-values, via a local aggregation of p-values. Theoretical properties of the FDRL procedure are investigated under weak dependence of p-values. It is shown that the FDRL procedure alleviates the LIP of the FDR procedure, thus substantially facilitating the selection of more stringent control levels. Simulation evaluations indicate that the FDRL procedure improves the detection sensitivity of the FDR procedure with little loss in detection specificity. The computational simplicity and detection effectiveness of the FDRL procedure are illustrated through a real brain fMRI dataset."
"10.1214/10-AOS847","2011","Exact calculations for false discovery proportion with application to least favorable configurations","0","In a context of multiple hypothesis testing, we provide several new exact calculations related to the false discovery proportion (FDP) of step-up and step-down procedures. For step-up procedures, we show that the number of erroneous rejections conditionally on the rejection number is simply a binomial variable, which leads to explicit computations of the c.d.f., the sth moment and the mean of the FDP, the latter corresponding to the false discovery rate (FDR). For step-down procedures, we derive what is to our knowledge the first explicit formula for the FDR valid for any alternative c.d.f. of the p-values. We also derive explicit computations of the power for both step-up and step-down procedures. These formulas are ""explicit"" in the sense that they only involve the parameters of the model and the c.d.f. of the order statistics of i.i.d. uniform variables. The p-values are assumed either independent or coming from an equicorrelated multivariate normal model and an additional mixture model for the true/false hypotheses is used. Our approach is then used to investigate new results which are of interest in their own right, related to least/most favorable configurations for the FDR and the variance of the FDP."
"10.1214/10-AOS844","2011","Power-enhanced multiple decision functions controlling family-wise error and false discovery rates","1","Improved procedures, in terms of smaller missed discovery rates (MDR), for performing multiple hypotheses testing with weak and strong control of the family-wise error rate (FWER) or the false discovery rate (FDR) are developed and studied. The improvement over existing procedures such as the Sidak procedure for FWER control and the Benjamini-Hochberg (BH) procedure for FDR control is achieved by exploiting possible differences in the powers of the individual tests. Results signal the need to take into account the powers of the individual tests and to have multiple hypotheses decision functions which are not limited to simply using the individual p-values, as is the case, for example, with the Sidak, Bonferroni, or BH procedures. They also enhance understanding of the role of the powers of individual tests, or more precisely the receiver operating characteristic (ROC) functions of decision processes, in the search for better multiple hypotheses testing procedures. A decision-theoretic framework is utilized, and through auxiliary randomizers the procedures could be used with discrete or mixed-type data or with rank-based nonparametric tests. This is in contrast to existing p-value based procedures whose theoretical validity is contingent on each of these p-value statistics being stochastically equal to or greater than a standard uniform variable under the null hypothesis. Proposed procedures are relevant in the analysis of high-dimensional ""large M, small n"" data sets arising in the natural, physical, medical, economic and social sciences, whose generation and creation is accelerated by advances in high-throughput technology, notably, but not limited to, microarray technology."
"10.1214/10-AOS841","2011","Wishart distributions for decomposable covariance graph models","3","Gaussian covariance graph models encode marginal independence among the components of a multivariate random vector by means of a graph G. These models are distinctly different from the traditional concentration graph models (often also referred to as Gaussian graphical models or covariance selection models) since the zeros in the parameter are now reflected in the covariance matrix E, as compared to the concentration matrix Omega = Sigma(-1) The parameter space of interest for covariance graph models is the cone PG of positive definite matrices with fixed zeros corresponding to the missing edges of G. As in Letac and Massam [Ann. Statist. 35 (2007) 1278-1323], we consider the case where G is decomposable. In this paper, we construct on the cone PG a family of Wishart distributions which serve a similar purpose in the covariance graph setting as those constructed by Letac and Massam [Ann. Statist. 35 (2007) 1278-1323] and Dawid and Lauritzen [Ann. Statist. 21 (1993) 1272-1317] do in the concentration graph setting. We proceed to undertake a rigorous study of these ""covariance"" Wishart distributions and derive several deep and useful properties of this class. First, they form a rich conjugate family of priors with multiple shape parameters for covariance graph models. Second, we show how to sample from these distributions by using a block Gibbs sampling algorithm and prove convergence of this block Gibbs sampler. Development of this class of distributions enables Bayesian inference, which, in turn, allows for the estimation of Sigma, even in the case when the sample size is less than the dimension of the data (i.e., when ""n < p""), otherwise not generally possible in the maximum likelihood framework. Third, we prove that when G is a homogeneous graph, our covariance priors correspond to standard conjugate priors for appropriate directed acyclic graph (DAG) models. This correspondence enables closed form expressions for normalizing constants and expected values, and also establishes hyper-Markov properties for our class of priors. We also note that when G is homogeneous, the family IW(QG) of Letac and Massam [Ann. Statist. 35 (2007) 1278-1323] is a special case of our covariance Wishart distributions. Fourth, and finally, we illustrate the use of our family of conjugate priors on real and simulated data."
"10.1214/10-AOS834","2011","Consistency of the maximum likelihood estimator for general hidden {M}arkov models","0","Consider a parametrized family of general hidden Markov models, where both the observed and unobserved components take values in a complete separable metric space. We prove that the maximum likelihood estimator (MLE) of the parameter is strongly consistent under a rather minimal set of assumptions. As special cases of our main result, we obtain consistency in a large class of nonlinear state space models, as well as general results on linear Gaussian state space models and finite state models.A novel aspect of our approach is an information-theoretic technique for proving identifiability, which does not require an explicit representation for the relative entropy rate. Our method of proof could therefore form a foundation for the investigation of MLE consistency in more general dependent and non-Markovian time series. Also of independent interest is a general concentration inequality for V-uniformly ergodic Markov chains."
"10.1214/10-AOS822","2011","Effects of statistical dependence on multiple testing under a hidden {M}arkov model","0","The performance of multiple hypothesis testing is known to be affected by the statistical dependence among random variables involved. The mechanisms responsible for this, however, are not well understood. We study the effects of the dependence structure of a finite state hidden Markov model (HMM) on the likelihood ratios critical for optimal multiple testing on the hidden states. Various convergence results are obtained for the likelihood ratios as the observations of the HMM form an increasing long chain. Analytic expansions of the first and second order derivatives are obtained for the case of binary states, explicitly showing the effects of the parameters of the HMM on the likelihood ratios."
"10.1214/10-AOS804","2011","Monotone spectral density estimation","0","We propose two estimators of a monotone spectral density, that are based on the periodogram. These are the isotonic regression of the periodogram and the isotonic regression of the log-periodogram. We derive pointwise limit distribution results for the proposed estimators for short memory linear processes and long memory Gaussian processes and also that the estimators are rate optimal."
"10.1214/10-AOS846","2011","G{EE} analysis of clustered binary data with diverging number of covariates","0","Clustered binary data with a large number of covariates have become increasingly common in many scientific disciplines. This paper develops an asymptotic theory for generalized estimating equations (GEE) analysis of clustered binary data when the number of covariates grows to infinity with the number of clusters. In this ""large n, diverging p"" framework, we provide appropriate regularity conditions and establish the existence, consistency and asymptotic normality of the GEE estimator. Furthermore, we prove that the sandwich variance formula remains valid. Even when the working correlation matrix is misspecified, the use of the sandwich variance formula leads to an asymptotically valid confidence interval and Wald test for an estimable linear combination of the unknown parameters. The accuracy of the asymptotic approximation is examined via numerical simulations. We also discuss the ""diverging p"" asymptotic theory for general GEE. The results in this paper extend the recent elegant work of Xie and Yang [Ann. Statist. 31 (2003) 310347] and Balan and Schiopu-Kratina [Ann. Statist. 32 (2005) 522-541] in the ""fixed p"" setting."
"10.1214/10-AOS845","2011","Functional single index models for longitudinal data","0","A new single-index model that reflects the time-dynamic effects of the single index is proposed for longitudinal and functional response data, possibly measured with errors, for both longitudinal and time-invariant covariates. With appropriate initial estimates of the parametric index, the proposed estimator is shown to be root n-consistent and asymptotically normally distributed. We also address the nonparametric estimation of regression functions and provide estimates with optimal convergence rates. One advantage of the new approach is that the same bandwidth is used to estimate both the nonparametric mean function and the parameter in the index. The finite-sample performance for the proposed procedure is studied numerically."
"10.1214/10-AOS843","2011","Rates of convergence in active learning","0","We study the rates of convergence in generalization error achievable by active learning under various types of label noise. Additionally, we study the general problem of model selection for active learning with a nested hierarchy of hypothesis classes and propose an algorithm whose error rate provably converges to the best achievable error among classifiers in the hierarchy at a rate adaptive to both the complexity of the optimal classifier and the noise conditions. In particular, we state sufficient conditions for these rates to be dramatically faster than those achievable by passive learning."
"10.1214/10-AOS842","2011","New efficient estimation and variable selection methods for semiparametric varying-coefficient partially linear models","2","The complexity of semiparametric models poses new challenges to statistical inference and model selection that frequently arise from real applications. In this work, we propose new estimation and variable selection procedures for the semiparametric varying-coefficient partially linear model. We first study quantile regression estimates for the nonparametric varying-coefficient functions and the parametric regression coefficients. To achieve nice efficiency properties, we further develop a semiparametric composite quantile regression procedure. We establish the asymptotic normality of proposed estimators for both the parametric and nonparametric parts and show that the estimators achieve the best convergence rate. Moreover, we show that the proposed method is much more efficient than the least-squares-based method for many non-normal errors and that it only loses a small amount of efficiency for normal errors. In addition, it is shown that the loss in efficiency is at most 11.1% for estimating varying coefficient functions and is no greater than 13.6% for estimating parametric components. To achieve sparsity with high-dimensional covariates, we propose adaptive penalization methods for variable selection in the semiparametric varying-coefficient partially linear model and prove that the methods possess the oracle property. Extensive Monte Carlo simulation studies are conducted to examine the finite-sample performance of the proposed procedures. Finally, we apply the new methods to analyze the plasma beta-carotene level data."
"10.1214/10-AOS839","2011","Detection of an anomalous cluster in a network","0","We consider the problem of detecting whether or not, in a given sensor network, there is a cluster of sensors which exhibit an ""unusual behavior."" Formally, suppose we are given a set of nodes and attach a random variable to each node. We observe a realization of this process and want to decide between the following two hypotheses: under the null, the variables are i.i.d. standard normal; under the alternative, there is a cluster of variables that are i.i.d. normal with positive mean and unit variance, while the rest are i.i.d. standard normal. We also address surveillance settings where each sensor in the network collects information over time. The resulting model is similar, now with a time series attached to each node. We again observe the process over time and want to decide between the null, where all the variables are i.i.d. standard normal, and the alternative, where there is an emerging cluster of i.i.d. normal variables with positive mean and unit variance. The growth models used to represent the emerging cluster are quite general and, in particular, include cellular automata used in modeling epidemics. In both settings, we consider classes of clusters that are quite general, for which we obtain a lower bound on their respective minimax detection rate and show that some form of scan statistic, by far the most popular method in practice, achieves that same rate to within a logarithmic factor. Our results are not limited to the normal location model, but generalize to any one-parameter exponential family when the anomalous clusters are large enough."
"10.1214/10-AOS838","2011","A vanilla {R}ao-{B}lackwellization of {M}etropolis-{H}astings algorithms","0","Casella and Robert [Biometrika 83 (1996) 81-94] presented a general Rao-Blackwellization principle for accept-reject and Metropolis-Hastings schemes that leads to significant decreases in the variance of the resulting estimators, but at a high cost in computation and storage. Adopting a completely different perspective, we introduce instead a universal scheme that guarantees variance reductions in all Metropolis-Hastings-based estimators while keeping the computation cost under control. We establish a central limit theorem for the improved estimators and illustrate their performances on toy examples and on a probit model estimation."
"10.1214/10-AOS837","2011","Nonparametric estimation of surface integrals","0","The estimation of surface integrals on the boundary of an unknown body is a challenge for nonparametric methods in statistics, with powerful applications to physics and image analysis, among other fields. Provided that one can determine whether random shots hit the body, Cuevas et al. [Ann. Statist. 35 (2007) 1031-1051] estimate the boundary measure (the boundary length for planar sets and the surface area for 3-dimensional objects) via the consideration of shots at a box containing the body. The statistics considered by these authors, as well as those in subsequent papers, are based on the estimation of Minkowski content and depend on a smoothing parameter which must be carefully chosen. For the same sampling scheme, we introduce a new approach which bypasses this issue, providing strongly consistent estimators of both the boundary measure and the surface integrals of scalar functions, provided one can collect the function values at the sample points. Examples arise in experiments in which the density of the body can be measured by physical properties of the impacts, or in situations where such quantities as temperature and humidity are observed by randomly distributed sensors. Our method is based on random Delaunay triangulations and involves a simple procedure for surface reconstruction from a dense cloud of points inside and outside the body. We obtain basic asymptotics of the estimator, perform simulations and discuss, via Google Earth's data, an application to the image analysis of the Aral Sea coast and its cliffs."
"10.1214/10-AOS836","2011","Global uniform risk bounds for wavelet deconvolution estimators","1","We consider the statistical deconvolution problem where one observes n replications from the model Y = X + epsilon, where X is the unobserved random signal of interest and epsilon is an independent random error with distribution phi. Under weak assumptions on the decay of the Fourier transform of phi, we derive upper bounds for the finite-sample sup-norm risk of wavelet deconvolution density estimators f(n) for the density f of X, where f : R -> R is assumed to be bounded. We then derive lower bounds for the minimax sup-norm risk over Besov balls in this estimation problem and show that wavelet deconvolution density estimators attain these bounds. We further show that linear estimators adapt to the unknown smoothness of f if the Fourier transform of phi decays exponentially and that a corresponding result holds true for the hard thresholding wavelet estimator if phi decays polynomially. We also analyze the case where f is a ""supersmooth""/analytic density. We finally show how our results and recent techniques from Rademacher processes can be applied to construct global confidence bands for the density f."
"10.1214/10-AOS832","2011","Focused information criterion and model averaging for generalized additive partial linear models","0","We study model selection and model averaging in generalized additive partial linear models (GAPLMs). Polynomial spline is used to approximate nonparametric functions. The corresponding estimators of the linear parameters are shown to be asymptotically normal. We then develop a focused information criterion (FIC) and a frequentist model average (FMA) estimator on the basis of the quasi-likelihood principle and examine theoretical properties of the FIC and FMA. The major advantages of the proposed procedures over the existing ones are their computational expediency and theoretical reliability. Simulation experiments have provided evidence of the superiority of the proposed procedures. The approach is further applied to a real-world data example."
"10.1214/10-AOS830","2011","Causal inference for continuous-time processes when covariates are observed only at discrete times","0","Most of the work on the structural nested model and g-estimation for causal inference in longitudinal data assumes a discrete-time underlying data generating process. However, in some observational studies, it is more reasonable to assume that the data are generated from a continuous-time process and are only observable at discrete time points. When these circumstances arise, the sequential randomization assumption in the observed discrete-time data, which is essential in justifying discrete-time g-estimation, may not be reasonable. Under a deterministic model, we discuss other useful assumptions that guarantee the consistency of discrete-time g-estimation. In more general cases, when those assumptions are violated, we propose a controlling-the-future method that performs at least as well as g-estimation in most scenarios and which provides consistent estimation in some cases where g-estimation is severely inconsistent. We apply the methods discussed in this paper to simulated data, as well as to a data set collected following a massive flood in Bangladesh, estimating the effect of diarrhea on children's height. Results from different methods are compared in both simulation and the real application."
"10.1214/10-AOS827","2011","{$\ell_1$}-penalized quantile regression in high-dimensional sparse models","2","We consider median regression and, more generally, a possibly infinite collection of quantile regressions in high-dimensional sparse models. In these models, the number of regressors p is very large, possibly larger than the sample size n, but only at most s regressors have a nonzero impact on each conditional quantile of the response variable, where s grows more slowly than n. Since ordinary quantile regression is not consistent in this case, we consider l(1)-penalized quantile regression (l(1)-QR), which penalizes the l(1)-norm of regression coefficients, as well as the post-penalized QR estimator (post-l(1)-QR), which applies ordinary QR to the model selected by l(1)-QR. First, we show that under general conditions l(1)-QR is consistent at the near-oracle rate. root s/n root log(p boolean OR n), uniformly in the compact set u subset of (0, 1) of quantile indices. In deriving this result, we propose a partly pivotal, data-driven choice of the penalty level and show that it satisfies the requirements for achieving this rate. Second, we show that under similar conditions post-l(1)-QR is consistent at the near-oracle rate root s/n root log(p boolean OR n), uniformly over u, even if the l(1)-QR-selected models miss some components of the true models, and the rate could be even closer to the oracle rate otherwise. Third, we characterize conditions under which l(1)-QR contains the true model as a submodel, and derive bounds on the dimension of the selected model, uniformly over u; we also provide conditions under which hard-thresholding selects the minimal true model, uniformly over u."
"10.1214/10-AOS823","2011","Regression on manifolds: estimation of the exterior derivative","0","Collinearity and near-collinearity of predictors cause difficulties when doing regression. In these cases, variable selection becomes untenable because of mathematical issues concerning the existence and numerical stability of the regression coefficients, and interpretation of the coefficients is ambiguous because gradients are not defined. Using a differential geometric interpretation, in which the regression coefficients are interpreted as estimates of the exterior derivative of a function, we develop a new method to do regression in the presence of collinearities. Our regularization scheme can improve estimation error, and it can be easily modified to include lasso-type regularization. These estimators also have simple extensions to the ""large p, small n"" context."
"10.1214/09-AOS776","2011","Support union recovery in high-dimensional multivariate regression","2","In multivariate regression, a K-dimensional response vector is regressed upon a common set of p covariates, with a matrix B* is an element of R-pxK of regression coefficients. We study the behavior of the multivariate group Lasso, in which block regularization based on the l(1)/l(2) norm is used for support union recovery, or recovery of the set of s rows for which B* is nonzero. Under high-dimensional scaling, we show that the multivariate group Lasso exhibits a threshold for the recovery of the exact row pattern with high probability over the random design and noise that is specified by the sample complexity parameter theta(n, p, s): = n/[2 psi(B*) log(p - s)]. Here n is the sample size, and psi(B*) is a sparsity-overlap function measuring a combination of the sparsities and overlaps of the K-regression coefficient vectors that constitute the model. We prove that the multivariate group Lasso succeeds for problem sequences (n, p, s) such that theta(n, p, s) exceeds a critical level theta(u), and fails for sequences such that theta(n, p, s) lies below a critical level theta(l). For the special case of the standard Gaussian ensemble, we show that theta(l) = theta(u) so that the characterization is sharp. The sparsity-overlap function psi(B*) reveals that, if the design is uncorrelated on the active rows, l(1)/l(2) regularization for multivariate regression never harms performance relative to an ordinary Lasso approach and can yield substantial improvements in sample complexity (up to a factor of K) when the coefficient vectors are suitably orthogonal. For more general designs, it is possible for the ordinary Lasso to outperform the multivariate group Lasso. We complement our analysis with simulations that demonstrate the sharpness of our theoretical results, even for relatively small problems."
"10.1214/10-AOS835","2010","Estimation and testing for partially linear single-index models","0","In partially linear single-index models, we obtain the semiparametrically efficient profile least-squares estimators of regression coefficients. We also employ the smoothly clipped absolute deviation penalty (SCAD) approach to simultaneously select variables and estimate regression coefficients. We show that the resulting SCAD estimators are consistent and possess the oracle property. Subsequently, we demonstrate that a proposed tuning parameter selector, BIC, identifies the true model consistently. Finally, we develop a linear hypothesis test for the parametric coefficients and a goodness-of-fit test for the nonparametric component, respectively. Monte Carlo studies are also presented."
"10.1214/10-AOS829","2010","The sequential rejection principle of familywise error control","0","Closed testing and partitioning are recognized as fundamental principles of familywise error control. In this paper, we argue that sequential rejection can be considered equally fundamental as a general principle of multiple testing. We present a general sequentially rejective multiple testing procedure and show that many well-known familywise error controlling methods can be constructed as special cases of this procedure, among which are the procedures of Holm, Shaffer and Hochberg, parallel and serial gatekeeping procedures, modern procedures for multiple testing in graphs, resampling-based multiple testing procedures and even the closed testing and partitioning procedures themselves. We also give a general proof that sequentially rejective multiple testing procedures strongly control the familywise error if they fulfill simple criteria of monotonicity of the critical values and a limited form of weak familywise error control in each single step. The sequential rejection principle gives a novel theoretical perspective on many well-known multiple testing procedures, emphasizing the sequential aspect. Its main practical usefulness is for the development of multiple testing procedures for null hypotheses, possibly logically related, that are structured in a graph. We illustrate this by presenting a uniform improvement of a recently published procedure."
"10.1214/10-AOS840","2010","Nonparametric estimation of multivariate convex-transformed densities","2","We study estimation of multivariate densities p of the form p(x) = h(g(x)) for x is an element of R-d and for a fixed monotone function h and an unknown convex function g. The canonical example is h(y) = e(-y) for y is an element of R; in this case, the resulting class of densitiesP(e(-y)) = {p = exp(-g) : g is convex}is well known as the class of log-concave densities. Other functions h allow for classes of densities with heavier tails than the log-concave class.We first investigate when the maximum likelihood estimator (p) over cap exists for the class P(h) for various choices of monotone transformations h, including decreasing and increasing functions h. The resulting models for increasing transformations h extend the classes of log-convex densities studied previously in the econometrics literature, corresponding to h(y) = exp(y).We then establish consistency of the maximum likelihood estimator for fairly general functions h, including the log-concave class P(e(-y)) and many others. In a final section, we provide asymptotic minimax lower bounds for the estimation of p and its vector of derivatives at a fixed point x(0) under natural smoothness hypotheses on h and g. The proofs rely heavily on results from convex analysis."
"10.1214/10-AOS833","2010","Nonparametric estimate of spectral density functions of sample covariance matrices: a first step","0","The density function of the limiting spectral distribution of general sample covariance matrices is usually unknown. We propose to use kernel estimators which are proved to be consistent. A simulation study is also conducted to show the performance of the estimators."
"10.1214/10-AOS826","2010","Coordinate-independent sparse sufficient dimension reduction and variable selection","0","Sufficient dimension reduction (SDR) in regression, which reduces the dimension by replacing original predictors with a minimal set of their linear combinations without loss of information, is very helpful when the number of predictors is large. The standard SDR methods suffer because the estimated linear combinations usually consist of all original predictors, making it difficult to interpret. In this paper, we propose a unified method-coordinate-independent sparse estimation (CISE)-that can simultaneously achieve sparse sufficient dimension reduction and screen out irrelevant and redundant variables efficiently. CISE is subspace oriented in the sense that it incorporates a coordinate-independent penalty term with a broad series of model-based and model-free SDR approaches. This results in a Grassmann manifold optimization problem and a fast algorithm is suggested. Under mild conditions, based on manifold theories and techniques, it can be shown that CISE would perform asymptotically as well as if the true irrelevant predictors were known, which is referred to as the oracle property. Simulation studies and a real-data example demonstrate the effectiveness and efficiency of the proposed approach."
"10.1214/10-AOS825","2010","Sparsity in multiple kernel learning","0","The problem of multiple kernel learning based on penalized empirical risk minimization is discussed. The complexity penalty is determined jointly by the empirical L-2 norms and the reproducing kernel Hilbert space (RKHS) norms induced by the kernels with a data-driven choice of regularization parameters. The main focus is on the case when the total number of kernels is large, but only a relatively small number of them is needed to represent the target function, so that the problem is sparse. The goal is to establish oracle inequalities for the excess risk of the resulting prediction rule showing that the method is adaptive both to the unknown design distribution and to the sparsity of the problem."
"10.1214/10-AOS824","2010","A{NOVA} for longitudinal data with missing values","0","We carry out ANOVA comparisons of multiple treatments for longitudinal studies with missing values. The treatment effects are modeled semiparametrically via a partially linear regression which is flexible in quantifying the time effects of treatments. The empirical likelihood is employed to formulate model-robust nonparametric ANOVA tests for treatment effects with respect to covariates, the nonparametric time-effect functions and interactions between covariates and time. The proposed tests can be readily modified for a variety of data and model combinations, that encompasses parametric, semiparametric and nonparametric regression models; cross-sectional and longitudinal data, and with or without missing values."
"10.1214/10-AOS821","2010","Convergence and prediction of principal component scores in high-dimensional settings","0","A number of settings arise in which it is of interest to predict Principal Component (PC) scores for new observations using data from an initial sample. In this paper, we demonstrate that naive approaches to PC score prediction can be substantially biased toward 0 in the analysis of large matrices. This phenomenon is largely related to known inconsistency results for sample eigenvalues and eigenvectors as both dimensions of the matrix increase. For the spiked eigenvalue model for random matrices, we expand the generality of these results, and propose bias-adjusted PC score prediction. In addition, we compute the asymptotic correlation coefficient between PC scores from sample and population eigenvectors. Simulation and real data examples from the genetics literature show the improved bias and numerical properties of our estimators."
"10.1214/10-AOS798","2010","Sure independence screening in generalized linear models with {NP}-dimensionality","4","Ultrahigh-dimensional variable selection plays an increasingly important role in contemporary scientific discoveries and statistical research. Among others, Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] propose an independent screening framework by ranking the marginal correlations. They showed that the correlation ranking procedure possesses a sure independence screening property within the context of the linear model with Gaussian covariates and responses. In this paper, we propose a more general version of the independent learning with ranking the maximum marginal likelihood estimates or the maximum marginal likelihood itself in generalized linear models. We show that the proposed methods, with Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] as a very special case, also possess the sure screening property with vanishing false selection rate. The conditions under which the independence learning possesses a sure screening is surprisingly simple. This justifies the applicability of such a simple method in a wide spectrum. We quantify explicitly the extent to which the dimensionality can be reduced by independence screening, which depends on the interactions of the covariance matrix of covariates and true parameters. Simulation studies are used to illustrate the utility of the proposed approaches. In addition, we establish an exponential inequality for the quasi-maximum likelihood estimator which is useful for high-dimensional statistical learning."
"10.1214/10-AOS795","2010","High-dimensionality effects in the {M}arkowitz problem and other quadratic programs with linear constraints: risk underestimation","0","We first study the properties of solutions of quadratic programs with linear equality constraints whose parameters are estimated from data in the high-dimensional setting where p, the number of variables in the problem, is of the same order of magnitude as n, the number of observations used to estimate the parameters. The Markowitz problem in Finance is a subcase of our study. Assuming normality and independence of the observations we relate the efficient frontier computed empirically to the ""true"" efficient frontier. Our computations show that there is a separation of the errors induced by estimating the mean of the observations and estimating the covariance matrix. In particular, the price paid for estimating the covariance matrix is an underestimation of the variance by a factor roughly equal to 1 - p/n. Therefore the risk of the optimal population solution is underestimated when we estimate it by solving a similar quadratic program with estimated parameters.We also characterize the statistical behavior of linear functionals of the empirical optimal vector and show that they are biased estimators of the corresponding population quantities.We investigate the robustness of our Gaussian results by extending the study to certain elliptical models and models where our n observations are correlated (in ""time""). We show a lack of robustness of the Gaussian results, but are still able to get results concerning first order properties of the quantities of interest, even in the case of relatively heavy-tailed data (we require two moments). Risk underestimation is still present in the elliptical case and more pronounced than in the Gaussian case.We discuss properties of the nonparametric and parametric bootstrap in this context. We show several results, including the interesting fact that standard applications of the bootstrap generally yield inconsistent estimates of bias.We propose some strategies to correct these problems and practically validate them in some simulations. Throughout this paper, we will assume that p, n and n - p tend to infinity, and p < n.Finally, we extend our study to the case of problems with more general linear constraints, including, in particular, inequality constraints."
"10.1214/09-AOS786","2010","Empirical dynamics for longitudinal data","2","We demonstrate that the processes underlying on-line auction price bids and many other longitudinal data can be represented by an empirical first order stochastic ordinary differential equation with time-varying coefficients and a smooth drift process. This equation may be empirically obtained from longitudinal observations for a sample of subjects and does not presuppose specific knowledge of the underlying processes. For the nonparametric estimation of the components of the differential equation, it suffices to have available sparsely observed longitudinal measurements which may be noisy and are generated by underlying smooth random trajectories for each subject or experimental unit in the sample. The drift process that drives the equation determines how closely individual process trajectories follow a deterministic approximation of the differential equation. We provide estimates for trajectories and especially the variance function of the drift process. At each fixed time point, the proposed empirical dynamic model implies a decomposition of the derivative of the process underlying the longitudinal data into a component explained by a linear component determined by a varying coefficient function dynamic equation and an orthogonal complement that corresponds to the drift process. An enhanced perturbation result enables us to obtain improved asymptotic convergence rates for eigenfunction derivative estimation and consistency for the varying coefficient function and the components of the drift process. We illustrate the differential equation with an application to the dynamics of on-line auction data."
"10.1214/09-AOS775","2010","On optimality of the {S}hiryaev-{R}oberts procedure for detecting a change in distribution","0","In 1985, for detecting a change in distribution, Pollak introduced a specific minimax performance metric and a randomized version of the Shiryaev-Roberts procedure where the zero initial condition is replaced by a random variable sampled from the quasi-stationary distribution of the Shiryaev-Roberts statistic. Pollak proved that this procedure is third-order asymptotically optimal as the mean time to false alarm becomes large. The question of whether Pollak's procedure is strictly minimax for any false alarm rate has been open for more than two decades, and there were several attempts to prove this strict optimality. In this paper, we provide a counterexample which shows that Pollak's procedure is not optimal and that there is a strictly optimal procedure which is nothing but the Shiryaev-Roberts procedure that starts with a specially designed deterministic point."
"10.1214/09-AOS772","2010","A reproducing kernel {H}ilbert space approach to functional linear regression","0","We study in this paper a smoothness regularization method for functional linear regression and provide a unified treatment for both the prediction and estimation problems. By developing a tool on simultaneous diagonalization of two positive definite kernels, we obtain shaper results on the minimax rates of convergence and show that smoothness regularized estimators achieve the optimal rates of convergence for both prediction and estimation under conditions weaker than those for the functional principal components based methods developed in the literature. Despite the generality of the method of regularization, we show that the procedure is easily implementable. Numerical results are obtained to illustrate the merits of the method and to demonstrate the theoretical developments."
"10.1214/09-AOS747","2010","Sequentially interacting {M}arkov chain {M}onte {C}arlo methods","0","Sequential Monte Carlo (SMC) is a methodology for sampling approximately from a sequence of probability distributions of increasing dimension and estimating their normalizing constants. We propose here an alternative methodology named Sequentially Interacting Markov Chain Monte Carlo (SIMCMC). SIMCMC methods work by generating interacting non-Markovian sequences which behave asymptotically like independent Metropolis-Hastings (MH) Markov chains with the desired limiting distributions. Contrary to SMC, SIMCMC allows us to iteratively improve our estimates in an MCMC-like fashion. We establish convergence results under realistic verifiable assumptions and demonstrate its performance on several examples arising in Bayesian time series analysis."
"10.1214/10-AOS819","2010","Identifying the finite dimensionality of curve time series","2","The curve time series framework provides a convenient vehicle to accommodate some nonstationary features into a stationary setup. We propose a new method to identify the dimensionality of curve time series based on the dynamical dependence across different curves. The practical implementation of our method boils down to an eigenanalysis of a finite-dimensional matrix. Furthermore, the determination of the dimensionality is equivalent to the identification of the nonzero eigenvalues of the matrix, which we carry out in terms of some bootstrap tests. Asymptotic properties of the proposed method are investigated. In particular, our estimators for zero-eigenvalues enjoy the fast convergence rate n while the estimators for nonzero eigenvalues converge at the standard root n-rate. The proposed methodology is illustrated with both simulated and real data sets."
"10.1214/10-AOS813","2010","Uniform convergence rates for nonparametric regression and principal component analysis in functional/longitudinal data","0","We consider nonparametric estimation of the mean and covariance functions for functional/longitudinal data. Strong uniform convergence rates are developed for estimators that are local-linear smoothers. Our results are obtained in a unified framework in which the number of observations within each curve/cluster can be of any rate relative to the sample size. We show that the convergence rates for the procedures depend on both the number of sample curves and the number of observations on each curve. For sparse functional data, these rates are equivalent to the optimal rates in nonparametric regression. For dense functional data, root-n rates of convergence can be achieved with proper choices of bandwidths. We further derive almost sure rates of convergence for principal component analysis using the estimated covariance function. The results are illustrated with simulation studies."
"10.1214/10-AOS811","2010","Adaptive nonparametric {B}ayesian inference using location-scale mixture priors","0","We study location-scale mixture priors for nonparametric statistical problems, including multivariate regression, density estimation and classification. We show that a rate-adaptive procedure can be obtained if the prior is properly constructed. In particular, we show that adaptation is achieved if a kernel mixture prior on a regression function is constructed using a Gaussian kernel, an inverse gamma bandwidth, and Gaussian mixing weights."
"10.1214/10-AOS810","2010","Optimal rank-based testing for principal components","0","This paper provides parametric and rank-based optimal tests for eigenvectors and eigenvalues of covariance or scatter matrices in elliptical families. The parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963) and their pseudo-Gaussian robustifications by Davis (1977) and Tyler (1981, 1983). The rank-based tests address a much broader class of problems, where covariance matrices need not exist and principal components are associated with more general scatter matrices. The proposed tests are shown to outperform daily practice both from the point of view of validity as from the point of view of efficiency. This is achieved by utilizing the Le Cam theory of locally asymptotically normal experiments, in the nonstandard context, however, of a curved parametrization. The results we derive for curved experiments are of independent interest, and likely to apply in other contexts."
"10.1214/10-AOS805","2010","Gamma-based clustering via ordered means with application to gene-expression analysis","0","Discrete mixture models provide a well-known basis for effective clustering algorithms, although technical challenges have limited their scope. In the context of gene-expression data analysis, a model is presented that mixes over a finite catalog of structures, each one representing equality and inequality constraints among latent expected values. Computations depend on the probability that independent gamma-distributed variables attain each of their possible orderings. Each ordering event is equivalent to an event in independent negative-binomial random variables, and this finding guides a dynamic-programming calculation. The structuring of mixture-model components according to constraints among latent means leads to strict concavity of the mixture log likelihood. In addition to its beneficial numerical properties, the clustering method shows promising results in an empirical study."
"10.1214/10-AOS801","2010","On information plus noise kernel random matrices","0","Kernel random matrices have attracted a lot of interest in recent years, from both practical and theoretical standpoints. Most of the theoretical work so far has focused on the case were the data is sampled from a low-dimensional structure. Very recently, the first results concerning kernel random matrices with high-dimensional input data were obtained, in a setting where the data was sampled from a genuinely high-dimensional structure-similar to standard assumptions in random matrix theory.In this paper, we consider the case where the data is of the type ""information + noise."" In other words, each observation is the sum of two independent elements: one sampled from a ""low-dimensional"" structure, the signal part of the data, the other being high-dimensional noise, normalized to not overwhelm but still affect the signal. We consider two types of noise, spherical and elliptical.In the spherical setting, we show that the spectral properties of kernel random matrices can be understood from a new kernel matrix, computed only from the signal part of the data, but using (in general) a slightly different kernel. The Gaussian kernel has some special properties in this setting.The elliptical setting, which is important from a robustness standpoint, is less prone to easy interpretation."
"10.1214/09-AOS785","2010","Decomposition tables for experiments. {II}. {T}wo--one randomizations","0","We investigate structure for pairs of randomizations that do not follow each other in a chain. These are unrandomized-inclusive, independent, coincident or double randomizations. This involves taking several structures that satisfy particular relations and combining them to form the appropriate orthogonal decomposition of the data space for the experiment. We show how to establish the decomposition table giving the sources of variation, their relationships and their degrees of freedom, so that competing designs can be evaluated. This leads to recommendations for when the different types of multiple randomization should be used."
"10.1214/09-AOS763","2010","Nonparametric tests of the {M}arkov hypothesis in continuous-time models","0","We propose several statistics to test the Markov hypothesis for beta-mixing stationary processes sampled at discrete time intervals. Our tests are based on the Chapman Kolmogorov equation. We establish the asymptotic null distributions of the proposed test statistics, showing that Wilks's phenomenon holds. We compute the power of the test and provide simulations to investigate the finite sample performance of the test statistics when the null model is a diffusion process, with alternatives consisting of models with a stochastic mean reversion level, stochastic volatility and jumps."
"10.1214/09-AOS749","2010","Is {B}rownian motion necessary to model high-frequency data?","1","This paper considers the problem of testing for the presence of a continuous part in a semimartingale sampled at high frequency. We provide two tests, one where the null hypothesis is that a continuous component is present, the other where the continuous component is absent, and the model is then driven by a pure jump process. When applied to high-frequency individual stock data, both tests point toward the need to include a continuous component in the model."
"10.1214/10-AOS817","2010","On combinatorial testing problems","1","We study a class of hypothesis testing problems in which, upon observing the realization of an n-dimensional Gaussian vector, one has to decide whether the vector was drawn from a standard normal distribution or, alternatively, whether there is a subset of the components belonging to a certain given class of sets whose elements have been ""contaminated,"" that is, have a mean different from zero. We establish some general conditions under which testing is possible and others under which testing is hopeless with a small risk. The combinatorial and geometric structure of the class of sets is shown to play a crucial role. The bounds are illustrated on various examples."
"10.1214/10-AOS816","2010","Deciding the dimension of effective dimension reduction space for functional and high-dimensional data","0","In this paper, we consider regression models with a Hilbert-space-valued predictor and a scalar response, where the response depends on the predictor only through a finite number of projections. The linear subspace spanned by these projections is called the effective dimension reduction (EDR) space. To determine the dimensionality of the EDR space, we focus on the leading principal component scores of the predictor, and propose two sequential chi(2) testing procedures under the assumption that the predictor has an elliptically contoured distribution. We further extend these procedures and introduce a test that simultaneously takes into account a large number of principal component scores. The proposed procedures are supported by theory, validated by simulation studies, and illustrated by a real-data example. Our methods and theory are applicable to functional data and high-dimensional multivariate data."
"10.1214/10-AOS814","2010","Quasi-concave density estimation","3","Maximum likelihood estimation of a log-concave probability density is formulated as a convex optimization problem and shown to have an equivalent dual formulation as a constrained maximum Shannon entropy problem. Closely related maximum Renyi entropy estimators that impose weaker concavity restrictions on the fitted density are also considered, notably a minimum Hellinger discrepancy estimator that constrains the reciprocal of the square-root of the density to be concave. A limiting form of these estimators constrains solutions to the class of quasi-concave densities."
"10.1214/10-AOS812","2010","An efficient estimator for locally stationary {G}aussian long-memory processes","0","This paper addresses the estimation of locally stationary long-range dependent processes, a methodology that allows the statistical analysis of time series data exhibiting both nonstationarity and strong dependency. A time-varying parametric formulation of these models is introduced and a Whittle likelihood technique is proposed for estimating the parameters involved. Large sample properties of these Whittle estimates such as consistency, normality and efficiency are established in this work. Furthermore, the finite sample behavior of the estimators is investigated through Monte Carlo experiments. As a result from these simulations, we show that the estimates behave well even for relatively small sample sizes."
"10.1214/10-AOS799","2010","Kernel density estimation via diffusion","0","We present a new adaptive kernel density estimator based on linear diffusion processes. The proposed estimator builds on existing ideas for adaptive smoothing by incorporating information from a pilot density estimate. In addition, we propose a new plug-in bandwidth selection method that is free from the arbitrary normal reference rules used by existing methods. We present simulation examples in which the proposed approach outperforms existing methods in terms of accuracy and reliability."
"10.1214/10-AOS809","2010","Bootstrap consistency for general semiparametric {$M$}-estimation","1","Consider M-estimation in a semiparametric model that is characterized by a Euclidean parameter of interest and an infinite-dimensional nuisance parameter. As a general purpose approach to statistical inferences, the bootstrap has found wide applications in semiparametric M-estimation and, because of its simplicity, provides an attractive alternative to the inference approach based on the asymptotic distribution theory. The purpose of this paper is to provide theoretical justifications for the use of bootstrap as a semiparametric inferential tool. We show that, under general conditions, the bootstrap is asymptotically consistent in estimating the distribution of the M-estimate of Euclidean parameter; that is, the bootstrap distribution asymptotically imitates the distribution of the M-estimate. We also show that the bootstrap confidence set has the asymptotically correct coverage probability. These general conclusions hold, in particular, when the nuisance parameter is not estimable at root-n rate, and apply to a broad class of bootstrap methods with exchangeable bootstrap weights. This paper provides a first general theoretical study of the bootstrap in semiparametric models."
"10.1214/10-AOS808","2010","Backfitting and smooth backfitting for additive quantile models","0","In this paper, we study the ordinary backfitting and smooth backfitting as methods of fitting additive quantile models. We show that these backfitting quantile estimators are asymptotically equivalent to the corresponding backfitting estimators of the additive components in a specially-designed additive mean regression model. This implies that the theoretical properties of the backfitting quantile estimators are not unlike those of backfitting mean regression estimators. We also assess the finite sample properties of the two backfitting quantile estimators."
"10.1214/10-AOS807","2010","Trajectory averaging for stochastic approximation {MCMC} algorithms","0","The subject of stochastic approximation was founded by Robbins and Monro [Ann. Math. Statist. 22 (1951) 400-407]. After five decades of continual development, it has developed into an important area in systems control and optimization, and it has also served as a prototype for the development of adaptive algorithms for on-line estimation and control of stochastic systems. Recently, it has been used in statistics with Markov chain Monte Carlo for solving maximum likelihood estimation problems and for general simulation and optimizations. In this paper, we first show that the trajectory averaging estimator is asymptotically efficient for the stochastic approximation MCMC (SAMCMC) algorithm under mild conditions, and then apply this result to the stochastic approximation Monte Carlo algorithm [Liang, Liu and Carroll J. Amer Statist. Assoc. 102 (2007) 305-320]. The application of the trajectory averaging estimator to other stochastic approximation MCMC algorithms, for example, a stochastic approximation MLE algorithm for missing data problems, is also considered in the paper."
"10.1214/10-AOS806","2010","Adaptive estimation for {H}awkes processes; application to genome analysis","0","The aim of this paper is to provide a new method for the detection of either favored or avoided distances between genomic events along DNA sequences. These events are modeled by a Hawkes process. The biological problem is actually complex enough to need a nonasymptotic penalized model selection approach. We provide a theoretical penalty that satisfies an oracle inequality even for quite complex families of models. The consecutive theoretical estimator is shown to be adaptive minimax for Holderian functions with regularity in (1/2, 1]: those aspects have not yet been studied for the Hawkes' process. Moreover, we introduce an efficient strategy, named Islands, which is not classically used in model selection, but that happens to be particularly relevant to the biological question we want to answer. Since a multiplicative constant in the theoretical penalty is not computable in practice, we provide extensive simulations to find a data-driven calibration of this constant. The results obtained on real genomic data are coherent with biological knowledge and eventually refine them."
"10.1214/10-AOS803","2010","On universal oracle inequalities related to high-dimensional linear models","0","This paper deals with recovering an unknown vector theta from the noisy data Y = A theta + sigma xi, where A is a known (m x n)-matrix and xi is a white Gaussian noise. It is assumed that n is large and A may be severely ill-posed. Therefore, in order to estimate theta, a spectral regularization method is used, and our goal is to choose its regularization parameter with the help of the data Y. For spectral regularization methods related to the so-called ordered smoothers [see Kneip Ann. Statist. 22 (1994) 835-866], we propose new penalties in the principle of empirical risk minimization. The heuristical idea behind these penalties is related to balancing excess risks. Based on this approach, we derive a sharp oracle inequality controlling the mean square risks of data-driven spectral regularization methods."
"10.1214/10-AOS802","2010","Nonparametric estimation of genewise variance for microarray data","0","Estimation of genewise variance arises from two important applications in microarray data analysis: selecting significantly differentially expressed genes and validation tests for normalization of microarray data. We approach the problem by introducing a two-way nonparametric model, which is an extension of the famous Neyman-Scott model and is applicable beyond microarray data. The problem itself poses interesting challenges because the number of nuisance parameters is proportional to the sample size and it is not obvious how the variance function can be estimated when measurements are correlated. In such a high-dimensional nonparametric problem, we proposed two novel nonparametric estimators for genewise variance function and semiparametric estimators for measurement correlation, via solving a system of nonlinear equations. Their asymptotic normality is established. The finite sample property is demonstrated by simulation studies. The estimators also improve the power of the tests for detecting statistically differentially expressed genes. The methodology is illustrated by the data from microarray quality control (MAQC) project."
"10.1214/10-AOS797","2010","Generalized density clustering","0","We study generalized density-based clustering in which sharply defined clusters such as clusters on lower-dimensional manifolds are allowed. We show that accurate clustering is possible even in high dimensions. We propose two data-based methods for choosing the bandwidth and we study the stability properties of density clusters. We show that a simple graph-based algorithm successfully approximates the high density clusters."
"10.1214/10-AOS794","2010","Modeling the variability of rankings","0","For better or for worse, rankings of institutions, such as universities, schools and hospitals, play an important role today in conveying information about relative performance. They inform policy decisions and budgets, and are often reported in the media. While overall rankings can vary markedly over relatively short time periods, it is not unusual to find that the ranks of a small number of ""highly performing"" institutions remain fixed, even when the data on which the rankings are based are extensively revised, and even when a large number of new institutions are added to the competition. In the present paper, we endeavor to model this phenomenon. In particular, we interpret as a random variable the value of the attribute on which the ranking should ideally be based. More precisely, if p items are to be ranked then the true, but unobserved, attributes are taken to be values of p independent and identically distributed variates. However, each attribute value is observed only with noise, and via a sample of size roughly equal to n, say. These noisy approximations to the true attributes are the quantities that are actually ranked. We show that, if the distribution of the true attributes is light-tailed (e.g., normal or exponential) then the number of institutions whose ranking is correct, even after recalculation using new data and even after many new institutions are added, is essentially fixed. Formally, p is taken to be of order n(C) for any fixed C > 0, and the number of institutions whose ranking is reliable depends very little on p. On the other hand, cases where the number of reliable rankings increases significantly when new institutions are added are those for which the distribution of the true attributes is relatively heavy-tailed, for example, with tails that decay like x(-alpha) for some alpha > 0. These properties and others are explored analytically, under general conditions. A numerical study links the results to outcomes for real-data problems."
"10.1214/10-AOS793","2010","Sparse recovery under matrix uncertainty","1","We consider the modely = X theta* + xi,Z = X + Xi,where the random vector y is an element of R(n) and the random n x p matrix Z are observed, the n x p matrix X is unknown, Xi is an n x p random noise matrix, xi is an element of R(n) is a noise independent of Xi, and theta* is a vector of unknown parameters to be estimated. The matrix uncertainty is in the fact that X is observed with additive error. For dimensions p that can be much larger than the sample size n, we consider the estimation of sparse vectors theta*. Under matrix uncertainty, the Lasso and Dantzig selector turn out to be extremely unstable in recovering the sparsity pattern (i.e., of the set of nonzero components of theta*), even if the noise level is very small. We suggest new estimators called matrix uncertainty selectors (or, shortly, the MU-selectors) which are close to theta* in different norms and in the prediction risk if the restricted eigenvalue assumption on X is satisfied. We also show that under somewhat stronger assumptions, these estimators recover correctly the sparsity pattern."
"10.1214/10-AOS792","2010","Bayes and empirical-{B}ayes multiplicity adjustment in the variable-selection problem","2","This paper studies the multiplicity-correction effect of standard Bayesian variable-selection priors in linear regression. Our first goal is to clarify when, and how, multiplicity correction happens automatically in Bayesian analysis, and to distinguish this correction from the Bayesian Ockham's-razor effect. Our second goal is to contrast empirical-Bayes and fully Bayesian approaches to variable selection through examples, theoretical results and simulations. Considerable differences between the two approaches are found. In particular, we prove a theorem that characterizes a surprising aymptotic discrepancy between fully Bayes and empirical Bayes. This discrepancy arises from a different source than the failure to account for hyperparameter uncertainty in the empirical-Bayes estimate. Indeed, even at the extreme, when the empirical-Bayes estimate converges asymptotically to the true variable-inclusion probability, the potential for a serious difference remains."
"10.1214/10-AOS791","2010","Fractals with point impact in functional linear regression","0","This paper develops a point impact linear regression model in which the trajectory of a continuous stochastic process, when evaluated at a sensitive time point, is associated with a scalar response. The proposed model complements and is more interpretable than the functional linear regression approach that has become popular in recent years. The trajectories are assumed to have fractal (self-similar) properties in common with a fractional Brownian motion with an unknown Hurst exponent. Bootstrap confidence intervals based on the least-squares estimator of the sensitive time point are developed. Misspecification of the point impact model by a functional linear model is also investigated. Non-Gaussian limit distributions and rates of convergence determined by the Hurst exponent play an important role."
"10.1214/09-AOS790","2010","Spades and mixture models","0","This paper studies sparse density estimation via l(1) penalization (SPADES). We focus on estimation in high-dimensional mixture models and nonparametric adaptive density estimation. We show, respectively, that SPADES can recover, with high probability, the unknown components of a mixture of probability densities and that it yields minimax adaptive density estimates. These results are based on a general sparsity oracle inequality that the SPADES estimates satisfy. We offer a data driven method for the choice of the tuning parameter used in the construction of SPADES. The method uses the generalized bisection method first introduced in [10]. The suggested procedure bypasses the need for a grid search and offers substantial computational savings. We complement our theoretical results with a simulation study that employs this method for approximations of one and two-dimensional densities with mixtures. The numerical results strongly support our theoretical findings."
"10.1214/09-AOS787","2010","On the de la {G}arza phenomenon","1","Deriving optimal designs for nonlinear models is, in general, challenging. One crucial step is to determine the number of support points needed. Current tools handle this on a case-by-case basis. Each combination of model, optimality criterion and objective requires its own proof. The celebrated de la Garza Phenomenon states that under a (p - 1)th-degree polynomial regression model, any optimal design can be based on at most p design points, the minimum number of support points such that all parameters are estimable. Does this conclusion also hold for nonlinear models? If the answer is yes, it would be relatively easy to derive any optimal design, analytically or numerically. In this paper, a novel approach is developed to address this question. Using this new approach, it can be easily shown that the de la Garza phenomenon exists for many commonly studied nonlinear models, such as the Emax model, exponential model, three- and four-parameter log-linear models, Emax-PK1 model, as well as many classical polynomial regression models. The proposed approach unities and extends many well-known results in the optimal design literature. It has four advantages over current tools: (i) it can be applied to many forms of nonlinear models; to continuous or discrete data; to data with homogeneous or nonhomogeneous errors; (ii) it can be applied to any design region; (iii) it can be applied to multiple-stage optimal design and (iv) it can be easily implemented."
"10.1214/09-AOS783","2010","M\""obius deconvolution on the hyperbolic plane with application to impedance density estimation","0","In this paper we consider a novel statistical inverse problem on the Poincare, or Lobachevsky, upper (complex) half plane. Here the Riemannian structure is hyperbolic and a transitive group action comes from the space of 2 x 2 real matrices of determinant one via Mobius transformations. Our approach is based on a deconvolution technique which relies on the Helgason-Fourier calculus adapted to this hyperbolic space. This gives a minimax nonparametric density estimator of a hyperbolic density that is corrupted by a random Mains transform. A motivation for this work comes from the reconstruction of impedances of capacitors where the above scenario on the Poincare plane exactly describes the physical system that is of statistical interest."
"10.1214/10-AOS800","2010","A deconvolution approach to estimation of a common shape in a shifted curves model","0","This paper considers the problem of adaptive estimation of a mean pattern in a randomly shifted curve model. We show that this problem can be transformed into a linear inverse problem, where the density of the random shifts plays the role of a convolution operator. An adaptive estimator of the mean pattern, based on wavelet thresholding is proposed. We study its consistency for the quadratic risk as the number of observed curves tends to infinity, and this estimator is shown to achieve a near-minimax rate of convergence over a large class of Besov balls. This rate depends both on the smoothness of the common shape of the curves and on the decay of the Fourier coefficients of the density of the random shifts. Hence, this paper makes a connection between mean pattern estimation and the statistical analysis of linear inverse problems, which is a new point of view on curve registration and image warping problems. We also provide a new method to estimate the unknown random shifts between curves. Some numerical experiments are given to illustrate the performances of our approach and to compare them with another algorithm existing in the literature."
"10.1214/09-AOS789","2010","Simultaneous nonparametric inference of time series","0","We consider kernel estimation of marginal densities and regression functions of stationary processes. It is shown that for a wide class of time series, with proper centering and scaling, the maximum deviations of kernel density and regression estimates are asymptotically Gumbel. Our results substantially generalize earlier ones which were obtained under independence or beta mixing assumptions. The asymptotic results can be applied to assess patterns of marginal densities or regression functions via the construction of simultaneous confidence bands for which one can perform goodness-of-fit tests. As an application, we construct simultaneous confidence bands for drift and volatility functions in a dynamic short-term rate model for the U.S. Treasury yield curve rates data."
"10.1214/09-AOS784","2010","Sieve estimation of constant and time-varying coefficients in nonlinear ordinary differential equation models by considering both numerical error and measurement error","1","This article considers estimation of constant and time-varying coefficients in nonlinear ordinary differential equation (ODE) models where analytic closed-form solutions are not available. The numerical solution-based nonlinear least squares (NLS) estimator is investigated in this study. A numerical algorithm such as the Runge-Kutta method is used to approximate the ODE solution. The asymptotic properties are established for the proposed estimators considering both numerical error and measurement error. The B-spline is used to approximate the time-varying coefficients, and the corresponding asymptotic theories in this case are investigated under the framework of the sieve approach. Our results show that if the maximum step size of the p-order numerical algorithm goes to zero at a rate faster than n(-1)/(p boolean AND 4), the numerical error is negligible compared to the measurement error. This result provides a theoretical guidance in selection of the step size for numerical evaluations of ODEs. Moreover, we have shown that the numerical solution-based NLS estimator and the sieve NLS estimator are strongly consistent. The sieve estimator of constant parameters is asymptotically normal with the same asymptotic co-variance as that of the case where the true ODE solution is exactly known, while the estimator of the time-varying parameter has the optimal convergence rate under some regularity conditions. The theoretical results are also developed for the case when the step size of the ODE numerical solver does not go to zero fast enough or the numerical error is comparable to the measurement error. We illustrate our approach with both simulation studies and clinical data on HIV viral dynamics."
"10.1214/09-AOS782","2010","Order thresholding","0","A new thresholding method, based on L-statistics and called order thresholding, is proposed as a technique for improving the power when testing against high-dimensional alternatives. The new method allows great flexibility in the choice of the threshold parameter. This results in improved power over the soft and hard thresholding methods. Moreover, order thresholding is not restricted to the normal distribution. An extension of the basic order threshold statistic to high-dimensional ANOVA is presented. The performance of the basic order threshold statistic and its extension is evaluated with extensive simulations."
"10.1214/09-AOS781","2010","Variable selection in nonparametric additive models","4","We consider a nonparametric additive model of a conditional mean function in which the number of variables and additive components may be larger than the sample size but the number of nonzero additive components is ""small"" relative to the sample size. The statistical problem is to determine which additive components are nonzero. The additive components are approximated by truncated series expansions with B-spline bases. With this approximation, the problem of component selection becomes that of selecting the groups of coefficients in the expansion. We apply the adaptive group Lasso to select nonzero components, using the group Lasso to obtain an initial estimator and reduce the dimension of the problem. We give conditions under which the group Lasso selects a model whose number of components is comparable with the underlying model, and the adaptive group Lasso selects the nonzero components correctly with probability approaching one as the sample size increases and achieves the optimal rate of convergence. The results of Monte Carlo experiments show that the adaptive group Lasso procedure works well with samples of moderate size. A data example is used to illustrate the application of the proposed method."
"10.1214/09-AOS779","2010","Stochastic kinetic models: dynamic independence, modularity and graphs","0","The dynamic properties and independence structure of stochastic kinetic models (SKMs) are analyzed. An SKM is a highly multivariate jump process used to model chemical reaction networks, particularly those in biochemical and cellular systems. We identify SKM subprocesses with the corresponding counting processes and propose a directed, cyclic graph (the kinetic independence graph or KIG) that encodes the local independence structure of their conditional intensities. Given a partition [A, D, B] of the vertices, the graphical separation A perpendicular to B vertical bar D in the undirected KIG has an intuitive chemical interpretation and implies that A is locally independent of B given A boolean OR D. It is proved that this separation also results in global independence of the internal histories of A and B conditional on a history of the jumps in D which, under conditions we derive, corresponds to the internal history of D. The results enable mathematical definition of a modularization of an SKM using its implied dynamics. Graphical decomposition methods are developed for the identification and efficient computation of nested modularizations. Application to an SKM of the red blood cell advances understanding of this biochemical system."
"10.1214/10-AOS796","2010","Sequential monitoring of response-adaptive randomized clinical trials","1","Clinical trials are complex and usually involve multiple objectives such as controlling type I error rate, increasing power to detect treatment difference, assigning more patients to better treatment, and more. In literature, both response-adaptive randomization (RAR) procedures (by changing randomization procedure sequentially) and sequential monitoring (by changing analysis procedure sequentially) have been proposed to achieve these objectives to some degree. In this paper, we propose to sequentially monitor response-adaptive randomized clinical trial and study it's properties. We prove that the sequential test statistics of the new procedure converge to a Brownian motion in distribution. Further, we show that the sequential test statistics asymptotically satisfy the canonical joint distribution defined in Jennison and Turnbull (2000). Therefore, type I error and other objectives can be achieved theoretically by selecting appropriate boundaries. These results open a door to sequentially monitor response-adaptive randomized clinical trials in practice. We can also observe from the simulation studies that, the proposed procedure brings together the advantages of both techniques, in dealing with power, total sample size and total failure numbers, while keeps the type I error. In addition, we illustrate the characteristics of the proposed procedure by redesigning a well-known clinical trial of maternal-infant HIV transmission."
"10.1214/09-AOS769","2010","Nonparametric inference of quantile curves for nonstationary time series","1","The paper considers nonparametric specification tests of quantile curves for a general class of nonstationary processes. Using Bahadur representation and Gaussian approximation results for nonstationary time series, simultaneous confidence bands and integrated squared difference tests are proposed to test various parametric forms of the quantile curves with asymptotically correct type I error rates. A wild bootstrap procedure is implemented to alleviate the problem of slow convergence of the asymptotic results. In particular, our results can be used to test the trends of extremes of climate variables, an important problem in understanding climate change. Our methodology is applied to the analysis of the maximum speed of tropical cyclone winds. It was found that an inhomogeneous upward trend for cyclone wind speeds is pronounced at high quantile values. However, there is no trend in the mean lifetime-maximum wind speed. This example shows the effectiveness of the quantile regression technique."
"10.1214/09-AOS788","2010","Limit theorems for empirical processes of cluster functionals","0","Let (X(n), i) 1 <= i <= n,m is an element of N be a triangular array of row-wise stationary R(d)-valued random variables. We use a ""blocks method"" to define clusters of extreme values: the rows of (X(n), i) are divided into m(n) blocks (Y(n), j), and if a block contains at least one extreme value, the block is considered to contain a cluster. The cluster starts at the first extreme value in the block and ends at the last one. The main results are uniform central limit theorems for empirical processes Z(n)(f) := 1/root nv(n) Sigma(mn)(j=1) (f(Y(n, j)) - Ef(Y(n, j))), for v(n) = P{X(n, i) not equal 0} and f belonging to classes of cluster functionals, that is, functions of the blocks Y(n, j) which only depend on the cluster values and which are equal to 0 if Y(n, j) does not contain a cluster. Conditions for finite-dimensional convergence include beta-mixing, suitable Lindeberg conditions and convergence of covariances. To obtain full uniform convergence, we use either ""bracketing entropy"" or bounds on covering numbers with respect to a random semi-metric. The latter makes it possible to bring the powerful Vapnik-Cervonenkis theory to bear. Applications include multivariate tail empirical processes and empirical processes of cluster values and of order statistics in clusters. Although our main field of applications is the analysis of extreme values, the theory can be applied more generally to rare events occurring, for example, in nonparametric curve estimation."
"10.1214/09-AOS752","2010","Optimal rates of convergence for covariance matrix estimation","5","Covariance matrix plays a central role in multivariate statistical analysis. Significant advances have been made recently on developing both theory and methodology for estimating large covariance matrices. However, a minimax theory has yet been developed. In this paper we establish the optimal rates of convergence for estimating the covariance matrix under both the operator norm and Frobenius norm. It is shown that optimal procedures under the two norms are different and consequently matrix estimation under the operator norm is fundamentally different from vector estimation. The minimax upper bound is obtained by constructing a special class of tapering estimators and by studying their risk properties. A key step in obtaining the optimal rate of convergence is the derivation of the minimax lower bound. The technical analysis requires new ideas that are quite different from those used in the more conventional function/sequence estimation problems."
"10.1214/09-AOS780","2010","Penalized variable selection procedure for {C}ox models with semiparametric relative risk","0","We study the Cox models with semiparametric relative risk, which can be partially linear with one nonparametric component, or multiple additive or nonadditive nonparametric components. A penalized partial likelihood procedure is proposed to simultaneously estimate the parameters and select variables for both the parametric and the nonparametric parts. Two penalties are applied sequentially. The first penalty, governing the smoothness of the multivariate nonlinear covariate effect function, provides a smoothing spline ANOVA framework that is exploited to derive an empirical model selection tool for the nonparametric part. The second penalty, either the smoothly-clipped-absolute-deviation (SCAD) penalty or the adaptive LASSO penalty, achieves variable selection in the parametric part. We show that the resulting estimator of the parametric part possesses the oracle property, and that the estimator of the nonparametric part achieves the optimal rate of convergence. The proposed procedures are shown to work well in simulation experiments, and then applied to a real data example on sexually transmitted diseases."
"10.1214/09-AOS770","2010","Testing conditional independence using maximal nonlinear conditional correlation","0","In this paper, the maximal nonlinear conditional correlation of two random vectors X and Y given another random vector Z. denoted by rho(1) (X. Y vertical bar Z), is defined as a measure of conditional association, which satisfies certain desirable properties. When Z is continuous, a test for testing the conditional independence of X and Y given Z is constructed based on the estimator of a weighted average of the form Sigma(nZ)(k=1) f(Z)(z(k))rho(2)(1) (X, Y vertical bar Z = z(k)). where f(Z) is the probability density function of Z and the z(k)'s are some points in the range of Z. Under some conditions, it is shown that the test statistic is asymptotically normal under conditional independence, and the test is consistent."
"10.1214/09-AOS762","2010","Nonparametric regression in exponential families","0","Most results in nonparametric regression theory are developed only for the case of additive noise. In such a setting many smoothing techniques including wavelet thresholding methods have been developed and shown to be highly adaptive. In this paper we consider nonparametric regression in exponential families with the main focus on the natural exponential families with a quadratic variance function, which include, for example, Poisson regression, binomial regression and gamma regression. We propose a unified approach of using a mean-matching variance stabilizing transformation to turn the relatively complicated problem of nonparametric regression in exponential families into a standard homoscedastic Gaussian regression problem. Then in principle any good nonparametric Gaussian regression procedure can be applied to the transformed data. To illustrate our general methodology, in this paper we use wavelet block thresholding to construct the final estimators of the regression function. The procedures are easily implementable. Both theoretical and numerical properties of the estimators are investigated. The estimators are shown to enjoy a high degree of adaptivity and spatial adaptivity with near-optimal asymptotic performance over a wide range of Besov spaces. The estimators also perform well numerically."
"10.1214/09-AOS778","2010","The benefit of group sparsity","1","This paper develops a theory for group Lasso using a concept called strong group sparsity. Our result shows that group Lasso is superior to standard Lasso for strongly group-sparse signals. This provides a convincing theoretical justification for using group sparse regularization when the underlying group structure is consistent with the data. Moreover, the theory predicts some limitations of the group Lasso formulation that are confirmed by simulation studies."
"10.1214/09-AOS777","2010","Inconsistency of bootstrap: the {G}renander estimator","3","In this paper, we investigate the (in)-consistency of different bootstrap methods for constructing confidence intervals in the class of estimators that converge at rate n(1/3). The Grenander estimator, the nonparametric maximum likelihood estimator of an unknown nonincreasing density function f on [0, infinity), is a prototypical example. We focus on this example and explore different approaches to constructing bootstrap confidence intervals for f(t(0)), where t(o) is an element of (0, infinity) is an interior point. We find that the bootstrap estimate, when generating bootstrap samples from the empirical distribution function F(n) or its least concave majorant (F) over tilde (n), does not have any weak limit in probability. We provide a set of sufficient conditions for the consistency of any bootstrap method in this example and show that bootstrapping from a smoothed version of (F) over tilde (n) leads to strongly consistent estimators. The m out of n bootstrap method is also shown to be consistent while generating samples from F(n) and (F) over tilde (n)."
"10.1214/09-AOS754","2010","Consistency of objective {B}ayes factors as the model dimension grows","0","In the class of normal regression models with a finite number of regressors, and for a wide class of prior distributions, a Bayesian model selection procedure based on the Bayes factor is consistent [Casella and Moreno J. Amer Statist. Assoc. 104 (2009) 1261-1271]. However, in models where the number of parameters increases as the sample size increases, properties of the Bayes factor are not totally understood. Here we study consistency of the Bayes factors for nested normal linear models when the number of regressors increases with the sample size. We pay attention to two successful tools for model selection [Schwarz Ann. Statist. 6 (1978) 461-464] approximation to the Bayes factor, and the Bayes factor for intrinsic priors [Berger and Pericchi I. Amer Statist. Assoc. 91 (1996) 109-122, Moreno, Bertolino and Racugno J. Amer Statist. Assoc. 93 (1998) 1451-1460].We find that the the Schwarz approximation and the Bayes factor for intrinsic priors are consistent when the rate of growth of the dimension of the bigger model is O(n(b)) for b < 1. When b = 1 the Schwarz approximation is always inconsistent under the alternative while the Bayes factor for intrinsic priors is consistent except for a small set of alternative models which is characterized."
"10.1214/09-AOS774","2010","Cram\'er-type moderate deviation for the maximum of the periodogram with application to simultaneous tests in gene expression time series","0","In this paper, Cramer-type moderate deviations for the maximum of the periodogram and its studentized version are derived. The results are then applied to a simultaneous testing problem in gene expression time series. It is shown that the level of the simultaneous tests is accurate provided that the number of genes C and the sample size n satisfy G = exp(o(n(1/3)))."
"10.1214/07-AOS566","2010","Efficient estimation for a subclass of shape invariant models","2","In this paper, we observe a fixed number of unknown 2 pi-periodic functions differing from each other by both phases and amplitude. This semiparametric model appears in literature under the name ""shape invariant model."" While the common shape is unknown, we introduce an asymptotically efficient estimator of the unite-dimensional parameter (phases and amplitude) using the profile likelihood and the Fourier basis. Moreover, this estimation method leads to a consistent and asymptotically linear estimator or the common shape."
"10.1214/09-AOS768","2010","Weakly dependent functional data","1","Functional data often arise from measurements on tine time grids and are obtained by separating an almost continuous time record into natural consecutive intervals, or example, days. The functions thus obtained form a functional time series, and the central issue in the analysis of such data consists in taking into account the temporal dependence of these functional observations. Examples include daily curves of financial transaction data and daily patterns of geophysical and environmental data. For scalar and vector valued stochastic processes, a large number of dependence notions have been proposed, mostly involving mixing type distances between sigma-algebras. In time series analysis, measures of dependence based on moments have proven most useful (autocovariances and cumulants). We introduce a moment-based notion of dependence for functional time series which involves in-dependence. We show that it is applicable to linear as well as nonlinear functional time series. Then we investigate the impact of dependence thus quantified on several important statistical procedures for functional data. We study the estimation of the functional principal components, the long-run covariance matrix, change point detection and the functional linear model. We explain when temporal dependence affects the results obtained for i.i.d. functional observations and when these results are robust to weak dependence."
"10.1214/09-AOS767","2010","On convergence rates equivalency and sampling strategies in functional deconvolution models","0","Using the asymptotical minimax framework, we examine convergence rates equivalency between a continuous functional deconvolution model and its real-life discrete counterpart over a wide range of Besov balls and for the L-2-risk. For this purpose, all possible models are divided into three groups. For the models in the first group, which we call uniform, the convergence rates in the discrete and the continuous models coincide no matter what the sampling scheme is chosen, and hence the replacement of the discrete model by its continuous counterpart is legitimate. For the models in the second group, to which we refer as regular, one can point out the best sampling strategy in the discrete model, but not every sampling scheme leads to the same convergence rates; there are at least two sampling schemes which deliver different convergence rates in the discrete model (i.e., at least one of the discrete models leads to convergence rates that are different from the convergence rates in the continuous model). The third group consists of models for which, in general, it is impossible to devise the best sampling strategy; we call these models irregular.We formulate the conditions when each of these situations takes place. In the regular case, we not only point out the number and the selection of sampling points which deliver the fastest convergence rates in the discrete model but also investigate when, in the case of an arbitrary sampling scheme, the convergence rates in the continuous model coincide or do not coincide with the convergence rates in the discrete model. We also study what happens if one chooses a uniform, or a more general pseudo-uniform, sampling scheme which can be viewed as an intuitive replacement of the continuous model. Finally, as a representative of the irregular case, we study functional deconvolution with a boxcar-like blurring function since this model has a number of important applications. All theoretical results presented in the paper are illustrated by numerous examples; many of which are motivated directly by a multitude of inverse problems in mathematical physics where one needs to recover initial or boundary conditions on the basis of observations from a noisy solution of a partial differential equation. The theoretical performance of the suggested estimator in the multichannel deconvolution model with a boxcar-like blurring function is also supplemented by a limited simulation study and compared to an estimator available in the current literature. The paper concludes that in both regular and irregular cases one should be extremely careful when replacing a discrete functional deconvolution model by its continuous counterpart."
"10.1214/09-AOS766","2010","Asymptotics and optimal bandwidth selection for highest density region estimation","0","We study kernel estimation of highest-density regions (HDR). Our main contributions are two-fold. First, we derive a uniform-in-bandwidth asymptotic approximation to a risk that is appropriate for HDR estimation. This approximation is then used to derive a bandwidth selection rule for HDR estimation possessing attractive asymptotic properties. We also present the results of numerical studies that illustrate the benefits of our theory and methodology."
"10.1214/09-AOS765","2010","Approximation of conditional densities by smooth mixtures of regressions","1","This paper shows that large nonparametric classes of conditional multivariate densities can be approximated in the Kullback-Leibler distance by different specifications of finite mixtures of normal regressions in which normal means and variances and mixing probabilities can depend on variables in the conditioning set (covariates). These models are a special case of models known as ""mixtures of experts"" in statistics and computer science literature. Flexible specifications include models in which only mixing probabilities, modeled by multinomial logit, depend on the covariates and, in the univariate case, models in which only means of the mixed normals depend flexibly on the covariates. Modeling the variance of the mixed normals by flexible functions of the covariates can weaken restrictions on the class of the approximable densities. Obtained results can be generalized to mixtures of general location scale densities. Rates of convergence and easy to interpret bounds are also obtained for different model specifications. These approximation results can be useful for proving consistency of Bayesian and maximum likelihood density estimators based on these models. The results also have interesting implications for applied researchers."
"10.1214/09-AOS764","2010","Innovated higher criticism for detecting sparse signals in correlated noise","4","Higher criticism is a method for detecting signals that are both sparse and weak. Although first proposed in cases where the noise variables are independent, higher criticism also has reasonable performance in settings where those variables are correlated. In this paper we show that, by exploiting the nature of the correlation, performance can be improved by using a modified approach which exploits the potential advantages that correlation has to offer. Indeed, it turns out that the case of independent noise is the most difficult of all, from a statistical viewpoint, and that more accurate signal detection (for a given level of signal sparsity and strength) can be obtained when correlation is present. We characterize the advantages of correlation by showing how to incorporate them into the definition of an optimal detection boundary. The boundary has particularly attractive properties when correlation decays at a polynomial rate or the correlation matrix is Toeplitz."
"10.1214/09-AOS760","2010","Trek separation for {G}aussian graphical models","0","Gaussian graphical models are semi-algebraic subsets of the cone of positive definite covariance matrices. Submatrices with low rank correspond to generalizations of conditional independence constraints on collections of random variables. We give a precise graph-theoretic characterization of when submatrices of the covariance matrix have small rank for a general class of mixed graphs that includes directed acyclic and undirected graphs as special cases. Our new trek separation criterion generalizes the familiar d-separation criterion. Proofs are based on the trek rule, the resulting matrix factorizations and classical theorems of algebraic combinatories on the expansions of determinants of path polynomials."
"10.1214/09-AOS743","2010","Successive normalization of rectangular arrays","1","Standard statistical techniques often require transforming data to have mean 0 and standard deviation I. Typically, this process of ""standardization"" or ""normalization"" is applied across subjects when each subject produces a single number. High throughput genomic and financial data often come as rectangular arrays where each coordinate in one direction concerns subjects who might have different status (case or control, say), and each coordinate in the other designates ""outcome"" for a specific feature, for example, ""gene,"" ""polymorphic site"" or some aspect of financial profile. It may happen, when analyzing data that arrive as a rectangular array, that one requires BOTH the subjects and the features to be ""on the same fooling."" Thus there may be a need to standardize across rows and columns of the rectangular matrix. There arises the question as to how to achieve this double normalization. We propose and investigate the convergence of what seems to us a natural approach to successive normalization which we learned from our colleague Bradley Efron. We also study the implementation of the method on simulated data and also on data that arose from scientific experimentation."
"10.1214/09-AOS771","2010","Quantile calculus and censored regression","0","Quantile regression has been advocated in survival analysis to assess evolving covariate effects. However, challenges arise when the censoring time is not always observed and may he covariate-dependent, particularly in the presence of continuously-distributed covariates. In spite of several recent advances, existing methods either involve algorithmic complications or impose a probability grid. The former leads to difficulties in the implementation and asymptotics, whereas the latter introduces undesirable grid dependence. To resolve these issues, we develop fundamental and general pantile calculus on cumulative probability scale in this article, upon recognizing that probability and time scales do not always have a one-to-one mapping given a survival distribution. These results give rise to a novel estimation procedure for censored pantile regression, based on estimating integral equations. A numerically reliable and efficient Progressive Localized Minimization (PLMIN) algorithm is proposed for the computation. This procedure reduces exactly to the Kaplan-Meier method in the k-sample problem, and to standard uncensored guanine regression in the absence of censoring. Under regularity conditions, the proposed pantile coefficient estimator is uniformly consistent and converges weakly to a Gaussian process. Simulations show good statistical and algorithmic performance. The proposal is illustrated in the application to a clinical study."
"10.1214/09-AOS761","2010","Monotonic convergence of a general algorithm for computing optimal designs","0","Monotonic convergence is established for a general class of multiplicative algorithms introduced by Silvey, Titterington and Torsney [Comm. Statist. Theory Methods 14 (1978) 1379-1389] or computing optimal designs. A conjecture of Titterington [Appl. Stat. 27(1978) 227-234] is confirmed as a consequence. Optimal designs for logistic regression are used as an illustration."
"10.1214/09-AOS759","2010","Statistical analysis of {$k$}-nearest neighbor collaborative recommendation","0","Collaborative recommendation is an information-filtering technique that attempts to present information items that are likely of interest to an Internet user. Traditionally, collaborative systems deal with situations with two types of variables, users and items. In its most common form, the problem is framed as trying to estimate ratings for items that have not yet been consumed by a user. Despite wide-ranging literature, little is known about the statistical properties of recommendation systems. In fact, no clear probabilistic model even exists which would allow us to precisely describe the mathematical forces driving collaborative filtering. To provide an initial contribution to this, we propose to set out a general sequential stochastic model for collaborative recommendation. We offer an in-depth analysis of the so-called cosine-type nearest neighbor collaborative method, which is one of the most widely used algorithms in collaborative filtering, and analyze its asymptotic performance as the number of users grows. We establish consistency of the procedure under mild assumptions on the model. Rates of convergence and examples are also provided."
"10.1214/09-AOS758","2010","Exact properties of {E}fron's biased coin randomization procedure","1","Efron [Biometrika 58 (1971) 403-417] developed a restricted randomization procedure to promote balance between two treatment groups in a sequential clinical trial. He called this the biased coin design. He also introduced the concept of accidental bias, and investigated properties of the procedure with respect to both accidental and selection bias, balance, and randomization-based inference using the steady-state properties of the induced Markov chain. In this paper we revisit this procedure, and derive closed-form expressions for the exact properties of the measures derived asymptotically in Efron's paper. In particular, we derive the exact distribution of the treatment imbalance and the variance-covariance matrix of the treatment assignments. These results have application in the design and analysis of clinical trials, by providing exact formulas to determine the role of the coin's bias probability in the context of selection and accidental bias, balancing properties and randomization-based inference."
"10.1214/09-AOS756","2010","Limit theorems for moving averages of discretized processes plus noise","0","This paper presents some limit theorems for certain functionals of moving averages of semimartingales plus noise which are observed at high frequency. Our method generalizes the pre-averaging approach (see [Bernoulli 15 (2009) 634-658, Stochastic Process. Appl. 119 (2009) 2249-2276]) and provides consistent estimates for various characteristics of general semimartingales. Furthermore, we prove the associated multidimensional (stable) central limit theorems. As expected, we find central limit theorems with a convergence rate n(-1/4), if it is the number of observations."
"10.1214/09-AOS757","2010","A new and flexible method for constructing designs for computer experiments","1","We develop a new method for constructing ""good"" designs for computer experiments. The method derives its power from its basic structure that builds large designs using small designs. We specialize the method for the construction of orthogonal Latin hypercubes and obtain many results along the way. In terms of run sizes, the existence problem of orthogonal Latin hypercubes is completely solved. We also present an explicit result showing how large orthogonal Latin hypercubes can be constructed using small orthogonal Latin hypercubes. Another appealing feature of our method is that it can easily be adapted to construct other designs; we examine how to make use of the method to construct nearly orthogonal and cascading Latin hypercubes."
"10.1214/09-AOS755","2010","Optional {P}\'olya tree and {B}ayesian inference","1","We introduce an extension of the Polya tree approach for constructing distributions on the space of probability measures. By using optional stopping and optional choice of splitting variables, the construction gives rise to random measures that are absolutely continuous with piecewise smooth densities on partitions that can adapt to fit the data. The resulting ""optional Polya tree"" distribution has large support in total variation topology and yields posterior distributions that are also optional Polya trees with computable parameter values."
"10.1214/09-AOS753","2010","Estimation in additive models with highly or nonhighly correlated covariates","0","Motivated by normalizing DNA microarray data and by predicting the interest rates, we explore nonparametric estimation of additive models with highly correlated covariates. We introduce two novel approaches for estimating the additive components, integration estimation and pooled backfitting estimation. The former is designed for highly correlated covariates, and the latter is useful for nonhighly correlated covariates. Asymptotic normalities of the proposed estimators are established. Simulations are conducted to demonstrate finite sample behaviors of the proposed estimators, and real data examples are given to illustrate the value of the methodology."
"10.1214/09-AOS751","2010","Adaptive estimation of stationary {G}aussian fields","0","We study the nonparametric covariance estimation of a stationary Gaussian field X observed on a regular lattice. In the time series setting, some procedures like AIC are proved to achieve optimal model selection among autoregressive models. However, there exists no such equivalent results of adaptivity in a spatial setting. By considering collections of Gaussian Markov random fields (GMRF) as approximation sets for the distribution of X. we introduce a novel model selection procedure for spatial fields. For all neighborhoods m in a given collection M, this procedure first amounts to computing a covariance estimator of X within the GMRFs of neighborhood m. Then it selects a neighborhood (m) over cap by applying a penalization strategy. The so-defined method satisfies a nonasymptotic oracle-type inequality. If X is a GMRF, the procedure is also minimax adaptive to the sparsity of its neighborhood. More generally, the procedure is adaptive to the rate of approximation of the true distribution by GMRFs with growing neighborhoods."
"10.1214/09-AOS750","2010","Adjusted empirical likelihood with high-order precision","0","Empirical likelihood is a popular nonparametric or semi-parametric statistical method with many nice statistical properties. Yet when the sample size is small, or the dimension of the accompanying estimating function is high, the application of the empirical likelihood method can be hindered by low precision of the chi-square approximation and by nonexistence of solutions to the estimating equations. In this paper, we show that the adjusted empirical likelihood is effective at addressing both problems. With a specific level of adjustment, the adjusted empirical likelihood achieves the high-order precision of the Bartlett correction, in addition to the advantage of a guaranteed solution to the estimating equations. Simulation results indicate that the confidence regions constructed by the adjusted empirical likelihood have coverage probabilities comparable to or substantially more accurate than the original empirical likelihood enhanced by the Bartlett correction."
"10.1214/09-AOS746","2010","Asymptotic distribution of conical-hull estimators of directional edges","0","Nonparametric data envelopment analysis (DEA) estimators have been widely applied in analysis of productive efficiency. Typically they are defined in terms of convex-hulls of the observed combinations of inputs x outputs in a sample of enterprises. The shape of the convex-hull relies on a hypothesis on the shape of the technology, defined as the boundary of the set of technically attainable points in the inputs x outputs space. So far, only the statistical properties of the smallest convex polyhedron enveloping the data points has been considered which corresponds to a situation where the technology presents variable returns-to-scale (VRS). This paper analyzes the case where the most common constant returns-to-scale (CRS) hypothesis is assumed. Here the DEA is defined as the smallest conical-hull with vertex at the origin enveloping the cloud of observed points. In this paper we determine the asymptotic properties of this estimator, showing that the rate of convergence is better than for the VRS estimator. We derive also its asymptotic sampling distribution with a practical way to simulate it. This allows to define a bias-corrected estimator and to build confidence intervals for the frontier. We compare in a simulated example the bias-corrected estimator with the original conical-hull estimator and show its superiority in terms of median squared error."
"10.1214/09-AOS691","2010","High-dimensional {I}sing model selection using {$\ell_1$}-regularized logistic regression","1","We consider the problem of estimating the graph associated with a binary Ising Markov random field. We describe a method based on l(1)-regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an l(1)-constraint. The method is analyzed under high-dimensional scaling in which both the number of nodes p and maximum neighborhood size d are allowed to grow as a function of the number of observations n. Our main results provide sufficient conditions on the triple (n, p, d) and the model parameters for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. With coherence conditions imposed on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes n = Omega (d(3) log p) with exponentially decaying error. When these same conditions are imposed directly on the sample matrices, we show that a reduced sample size of n = Omega (d(2) log p) suffices for the method to estimate neighborhoods consistently. Although this paper focuses on the binary graphical models, we indicate how a generalization of the method of the paper would apply to general discrete Markov random fields."
"10.1214/09-AOS745","2010","Quantile estimation with adaptive importance sampling","0","We introduce new quantile estimators with adaptive importance sampling. The adaptive estimators are based on weighted samples that are neither independent nor identically distributed. Using a new law of iterated logarithm for martingales, we prove the convergence of the adaptive quantile estimators for general distributions with nonunique quantiles thereby extending the work of Feldman and Tucker [Ann. Math. Statist. 37 (1996) 451-457]. We illustrate the algorithm with an example from credit portfolio risk analysis."
"10.1214/09-AOS744","2010","On construction of the smallest one-sided confidence interval for the difference of two proportions","0","For my class of one-sided 1 - alpha confidence intervals with a certain monotonicity ordering on the random confidence limit, the smallest interval, in the sense of the set inclusion for the difference of two proportions of two independent binomial random variables, is constructed based on a direct analysis of coverage probability function. A special ordering on the confidence limit is developed and the corresponding smallest confidence interval is derived. This interval is then applied to identify the minimum effective dose (MED) for binary data in dose-response studies, and a multiple test procedure that controls the familywise error rate at level alpha is obtained. A generalization of constructing the smallest one-sided confidence interval to other discrete sample spaces is discussed in the presence of nuisance parameters."
"10.1214/09-AOS742","2010","Covariate adjusted functional principal components analysis for longitudinal data","1","Classical multivariate principal component analysis has been extended to functional data and termed functional principal component analysis (FPCA). Most existing FPCA approaches do not accommodate covariate information, and it is the goal of this paper to develop two methods that do. In the first approach, both the mean and covariance functions depend on the covariate Z and time scale t while in the second approach only the mean function depends on the covariate Z. Both new approaches accommodate additional measurement errors and functional data sampled at regular time grids as well as sparse longitudinal data sampled at irregular time grids. The first approach to fully adjust both the mean and covariance functions adapts more to the data but is computationally more intensive than the approach to adjust the covariate effects on the mean function only. We develop general asymptotic theory for both approaches and compare their performance numerically through simulation Studies and a data set."
"10.1214/09-AOS741","2010","Defining probability density for a distribution of random functions","2","The notion of probability density for a random function is not as straightforward as in finite-dimensional cases. While a probability density function generally does not exist for functional data, we show that it is possible to develop the notion of density when functional data are considered in the space determined by the eigenfunctions of principal component analysis. This leads to a transparent and meaningful surrogate for density defined in terms of the average value of the logarithms of the densities of the distributions of principal components for a given dimension. This density approximation is estimable readily from data. It accurately represents, in a monotone way, key features of small-ball approximations to density. Our results on estimators of the densities of principal component scores are also of independent interest; they reveal interesting shape differences that have not previously been considered. The statistical implications of these results and properties are identified and discussed, and practical ramifications are illustrated in numerical work."
"10.1214/09-AOS730","2010","Vast volatility matrix estimation for high-frequency financial data","3","High-frequency data observed on the prices of financial assets are commonly modeled by diffusion processes with micro-structure noise, and realized volatility-based methods are often used to estimate integrated volatility. For problems involving a large number of assets, the estimation objects we face are volatility matrices of large size. The existing volatility estimators work well for a small number of assets but perform poorly when the number of assets is very large. In fact, they are inconsistent when both the number, p, of the assets and the average sample size, n, of the price data on the p assets go to infinity. This paper proposes a new type of estimators for the integrated volatility matrix and establishes asymptotic theory for the proposed estimators in the framework that allows both n and p to approach to infinity. The theory shows that the proposed estimators achieve high convergence rates under a sparsity assumption on the integrated volatility matrix. The numerical studies demonstrate that the proposed estimators perform well for large p and complex price and volatility models. The proposed method is applied to real high-frequency financial data."
"10.1214/09-AOS738","2010","Confidence bands in density estimation","3","Given a sample from some unknown continuous density f : R -> R, we construct adaptive confidence bands that are honest for all densities in a ""generic"" subset of the union of t-Holder balls, 0 < t <= r, where r is a fixed but arbitrary integer. The exceptional (""nongeneric"") set of densities for which our results do not hold is shown to be nowhere dense in the relevant Holder-norm topologies. In the course of the proofs we also obtain limit theorems for maxima of linear wavelet and kernel density estimators, which are of independent interest."
"10.1214/09-AOS737","2010","On dimension folding of matrix- or array-valued statistical objects","1","We consider dimension reduction for regression or classification in which the predictors are matrix- or array-valued. This type of predictor arises when measurements are obtained for each combination of two or more underlying variables-for example, the voltage measured at different channels and times in electroencephalography data. For these applications, it is desirable to preserve the array structure of the reduced predictor (e.g., time versus channel), but this cannot be achieved within the conventional dimension reduction formulation. In this paper, we introduce a dimension reduction method, to be called dimension folding, for matrix- and array-valued predictors that preserves the array structure. In an application of dimension folding to an electroencephalography data set, we correctly classify 97 out of 122 subjects as alcoholic or nonalcoholic based on their electroencephalography in a cross-validation sample."
"10.1214/09-AOS736","2010","Optimal properties of centroid-based classifiers for very high-dimensional data","0","We show that scale-adjusted versions of the centroid-based classifier enjoys optimal properties when used to discriminate between two very high-dimensional populations where the principal differences are in location. The scale adjustment removes the tendency of scale differences to confound differences in means. Certain other distance-based methods, for example, those founded on nearest-neighbor distance, do not have optimal performance in the sense that we propose. Our results permit varying degrees of sparsity and signal strength to be treated, and require only mild conditions on dependence of vector components. Additionally, we permit the marginal distributions of vector components to vary extensively. In addition to providing theory we explore numerical properties of a centroid-based classifier, and show that these features reflect theoretical accounts of performance."
"10.1214/09-AOS735","2010","Batch means and spectral variance estimators in {M}arkov chain {M}onte {C}arlo","1","Calculating a Monte Carlo standard error (MCSE) is an important step in the statistical analysis of the simulation output obtained from a Markov chain Monte Carlo experiment. An MCSE is usually based on an estimate of the variance of the asymptotic normal distribution. We consider spectral and batch means methods for estimating this variance. In particular, we establish conditions which guarantee that these estimators are strongly consistent as the simulation effort increases. In addition, for the batch means and overlapping batch means methods we establish conditions ensuring consistency in the mean-square sense which in turn allows us to calculate the optimal batch size up to a constant of proportionality. Finally, we examine the empirical finite-sample properties of spectral variance and batch means estimators and provide recommendations for practitioners."
"10.1214/09-AOS732","2010","Optimal and fast detection of spatial clusters with scan statistics","1","We consider the detection of multivariate spatial clusters in the Bernoulli model with N locations, where the design distribution has weakly dependent marginals. The locations are scanned with a rectangular window with sides parallel to the axes and with varying sizes and aspect ratios. Multivariate scan statistics pose a statistical. problem due to the multiple testing over many scan windows, as well as a computational problem because statistics have to be evaluated on many windows. This paper introduces methodology that leads to both statistically optimal inference and computationally efficient algorithms. The main difference to the traditional calibration of scan statistics is the concept of grouping scan windows according to their sizes, and then applying different critical values to different groups. It is shown that this calibration of the scan statistic results in optimal inference for spatial clusters on both small scales and on large scales, as well as in the case where the cluster lives on one of the marginals. Methodology is introduced that allows for an efficient approximation of the set of all rectangles while still guaranteeing the statistical optimality results described above. It is shown that the resulting scan statistic has a computational complexity that is almost linear in N."
"10.1214/09-AOS731","2010","Estimation in {D}irichlet random effects models","0","We develop a new Gibbs sampler for a linear mixed model with a Dirichlet process random effect term, which is easily extended to a generalized linear mixed model with a probit link function. Our Gibbs sampler exploits the properties of the multinomial and Dirichlet distributions, and is shown to be an improvement, in terms of operator norm and efficiency, over other commonly used MCMC algorithms. We also investigate methods for the estimation of the precision parameter of the Dirichlet process, finding that maximum likelihood may not be desirable, but a posterior mode is a reasonable approach. Examples are given to show how these models perform on real data. Our results complement both the theoretical basis of the Dirichlet process nonparametric prior and the Computational work that has been done to date."
"10.1214/09-AOS729","2010","Nearly unbiased variable selection under minimax concave penalty","13","We propose MC+, a fast, continuous, nearly unbiased and accurate method of penalized variable selection in high-dimensional linear regression. The LASSO is fast and continuous, but biased. The bias of the LASSO may prevent consistent variable selection. Subset selection is unbiased but computationally costly. The MC+ has two elements: a minimax concave penalty (MCP) and a penalized linear unbiased selection (PLUS) algorithm. The MCP provides the convexity of the penalized loss in sparse regions to the greatest extent given certain thresholds for variable selection and unbiasedness. The PLUS computes multiple exact local minimizers of a possibly nonconvex penalized loss function in a certain main branch of the graph of critical points of the penalized loss. Its output is a continuous piecewise linear path encompassing from the origin for infinite penalty to a least squares solution for zero penalty. We prove that at a universal penalty level, the MC+ has high probability of matching the signs of the unknowns, and thus correct selection, without assuming the strong irrepresentable condition required by the LASSO. This selection consistency applies to the case of p >> n, and is proved to hold for exactly the MC+ solution among possibly many local minimizers. We prove that the MC+ attains certain minimax convergence rates in probability for the estimation of regression coefficients in e, balls. We use the SURE method to derive degrees of freedom and C-p-type risk estimates for general penalized LSE, including the LASSO and MC+ estimators, and prove their unbiasedness. Based on the estimated degrees of freedom, we propose an estimator of the noise level for proper choice of the penalty level. For full rank designs and general sub-quadratic penalties, we provide necessary and sufficient conditions for the continuity of the penalized LSE. Simulation results overwhelmingly support our claim Of Superior variable selection properties and demonstrate the computational efficiency of the proposed method."
"10.1214/09-AOS725","2010","On the consistent separation of scale and variance for {G}aussian random fields","0","We present fixed domain asymptotic results that establish consistent estimates of the variance and scale parameters for a Gaussian random field with a geometric anisotropic Matern autocovariance in dimension d > 4. When d < 4 this is impossible due to the mutual absolute continuity of Matern Gaussian random fields with different scale and variance (see Zhang [J. Amer. Statist. Assoc. 99 (2004) 250-261]). Informally, when d > 4, we show that one can estimate the coefficient on the principle irregular term accurately enough to get a consistent estimate of the coefficient on the second irregular term. These two coefficients can then be used to separate the scale and variance. We extend our results to the general problem of estimating a variance and geometric anisotropy for more general autocovariance functions. Our results illustrate the interaction between the accuracy of estimation, the smoothness of the random field, the dimension of the observation space and the number of increments used for estimation. As a corollary, our results establish the orthogonality of Matern Gaussian random fields with different parameters when d > 4. The case d = 4 is still open."
"10.1214/08-AOS629","2010","Goodness-of-fit tests for high-dimensional {G}aussian linear models","0","Let (Y, (X(i))(1 <= i <= p)) be a real zero mean Gaussian vector and V be a subset of {1,..., p}. Suppose we are given n i.i.d. replications of this vector. We propose anew test for testing that Y is independent of (X(i))(i is an element of{1,...,p}\V) conditionally to (X(i))(i is an element of V) against the general alternative that it is not. This procedure does not depend on any prior information on the covariance of X or the variance of Y and applies in a high-dimensional setting. It straightforwardly extends to test the neighborhood of a Gaussian graphical model. The procedure is based on a model of Gaussian regression with random Gaussian covariates. We give nonasymptotic properties of the test and we prove that it is rate optimal [up to a possible log(n) factor] over various classes of alternatives under some additional assumptions. Moreover, it allows us to derive nonasymptotic minimax rates of testing in this random design setting. Finally, we carry out a simulation study in order to evaluate the performance of our procedure."
"10.1214/09-AOS718","2010","Asymptotic inference for high-dimensional data","0","In this paper, we study inference for high-dimensional data characterized by small sample sizes relative to the dimension of the data. In particular, we provide an infinite-dimensional framework to study statistical models that involve situations in which (i) the number of parameters increase with the sample size (that is, allowed to be random) and (ii) there is a possibility of missing data. Under a variety of tail conditions on the components of the data, we provide precise conditions for the joint consistency of the estimators of the mean. In the process, we clarify and improve some of the recent consistency results that appeared in the literature. An important aspect of the work presented is the development of asymptotic normality results for these models. As a consequence, we construct different test statistics for one-sample and two-sample problems concerning the mean vector and obtain their asymptotic distributions as a corollary of the infinite-dimensional results. Finally, we use these theoretical results to develop an asymptotically justifiable methodology for data analyses. Simulation results presented here describe situations where the methodology can be successfully applied. They also evaluate its robustness under a variety of conditions, some of which are substantially different from the technical conditions. Comparisons to other methods used in the literature are provided. Analyses of real-life data is also included."
"10.1214/09-AOS716","2010","A two-sample test for high-dimensional data with applications to gene-set testing","4","We propose a two-sample test for the means of high-dimensional data when the data dimension is much larger than the sample size. Hotelling's classical T(2) test does not work for this ""large p, small n"" situation. The proposed test does not require explicit conditions in the relationship between the data dimension and sample size. This offers much flexibility in analyzing high-dimensional data. An application of the proposed test is in testing significance for sets of genes which we demonstrate in an empirical study on a leukemia data set."
"10.1214/09-AOS702","2010","Inference for stochastic volatility models using time change transformations","0","We address the problem of parameter estimation for diffusion driven stochastic volatility models through Markov chain Monte Carlo (MCMC). To avoid degeneracy issues we introduce an innovative reparametrization defined through transformations that operate on the time scale of the diffusion. A novel MCMC scheme which overcomes the inherent difficulties of time change transformations is also presented. The algorithm is fast to implement and applies to models with stochastic volatility. The methodology is tested through simulation based experiments and illustrated on data consisting of US treasury bill rates."
"10.1214/09-AOS687","2010","Maximum {${\rm L}q$}-likelihood estimation","1","In this paper, the maximum L-q-likelihood estimator (MLqE), a new parameter estimator based on nonextensive entropy [Kibernetika 3 (1967) 30-35] is introduced. The properties of the MLqE are studied via asymptotic analysis and computer simulations. The behavior of the MLqE is characterized by the degree of distortion q applied to the assumed model. When q is properly chosen for small and moderate sample sizes, the MLqE can successfully trade bias for precision, resulting in a substantial reduction of the mean squared error. When the sample size is large and q tends to 1, a necessary and sufficient condition to ensure a proper asymptotic normality and efficiency of MLqE is established."
"10.1214/09-AOS723REJ","2010","Rejoinder [MR2604670; MR2604671; MR2604672; MR2604673]","1",""
"10.1214/09-AOS723","2010","Multivariate quantiles and multiple-output regression quantiles: from {$L_1$} optimization to halfspace depth","2","A new multivariate concept of quantile, based on a directional version of Koenker and Bassett's traditional regression quantiles, is introduced for multivariate location and multiple-output regression problems. In their empirical version, those quantiles can be Computed efficiently via linear programming techniques. Consistency, Bahadur representation and asymptotic normality results are established. Most importantly, the contours generated by those quantiles are shown to coincide with the classical halfspace depth contours associated with the name of Tukey. This relation does not only allow for efficient depth contour computations by means of parametric linear programming, but also for transferring from the quantile to the depth universe such asymptotic results as Bahadur representations. Finally, linear programming duality opens the way to promising developments in depth-related multivariate rank-based inference."
"10.1214/09-AOS734","2010","Balanced control of generalized error rates","1","Consider the problem of testing s hypotheses simultaneously. In this paper, we derive methods which control the generalized family-wise error rate given by the probability of k or more false rejections, abbreviated k-FWER. We derive both single-step and step-down procedures that control the k-FWER in finite samples or asymptotically, depending on the situation. Moreover, the procedures are asymptotically balanced in an appropriate sense. We briefly consider control of the average number of false rejections. Additionally, we consider the false discovery proportion (FDP), defined as the number of false rejections divided by the total number of rejections (and defined to be 0 if there are no rejections). Here, the goal is to construct methods which satisfy, for given gamma and alpha, P{FDP > gamma} <= alpha, at least asymptotically. Special attention is paid to the construction of methods which implicitly take into account the dependence structure of the individual test statistics in order to further increase the ability to detect false null hypotheses. A general resampling and subsampling approach is presented which achieves these objectives, at least asymptotically."
"10.1214/09-AOS733","2010","Conditional least squares estimation in nonstationary nonlinear stochastic regression models","0","Let {Z(n)} be a real nonstationary stochastic process such that E(Z(n)vertical bar F(n-1)) <(a.s.) infinity and E(Z(n)(2)vertical bar F(n-1)) <(a.s.) infinity, where {F(n)} is an increasing sequence of sigma-algebras. Assuming that E(Z(n)vertical bar F(n-1)) = gn(theta(0), nu(0)) = g(n)((1))(theta(0)) + g(n)((2))(theta(0), nu(0)), theta(0) is an element of R(p), p < infinity, nu(0) is an element of R(q) and q <= infinity, we study the symptotic properties of <(theta)over cap>(n) := arg min(theta) Sigma(n)(k=1) (Z(k) = g(k)(theta, (nu) over cap))(2)lambda(-1)(k), where lambda(k) is F(k-1)-measurable, (nu) over cap = {(nu) over cap (k)} is a sequence of estimations of nu(0), g(n)(theta, (nu) over cap) is Lipschits in theta and g(n)((2))(theta(0), (nu) over cap) - g(n)((2))(theta, (nu) over cap) is asymptotically negligible relative to g(n)((1))(theta(0)) - g(n)((1)) (theta). We first generalize to this nonlinear stochastic model the necessary and sufficient condition obtained for the strong consistency of {(theta) over cap (n)} in the linear model. For that, we prove a strong law of large numbers for a class of submartingales. Again using this strong law, we derive the general conditions leading to the asymptotic distribution of (theta) over cap (n). We illustrate the theoretical results with examples of branching processes, and extension to quasi-likelihood estimators is also considered."
"10.1214/09-AOS728","2010","Regularization in kernel learning","1","Under mild assumptions on the kernel, we obtain the best known error rates in a regularized learning scenario taking place in the corresponding reproducing kernel Hilbert space (RKHS). The main novelty in the analysis is a proof that one can use a regularization term that grows significantly slower than the standard quadratic growth in the RKHS norm."
"10.1214/09-AOS727","2010","Invariant {$P$}-values for model checking","0","P-values have been the focus of considerable criticism based on various considerations. Still, the P-value represents one of the most commonly used statistical tools. When assessing the suitability of a single hypothesized distribution, it is not clear that there is a better choice for a measure of surprise. This paper is concerned with the definition of appropriate model-based P-values for model checking."
"10.1214/09-AOS726","2010","Empirical risk minimization in inverse problems","0","We study estimation of a multivariate function f : R(d) -> R when the observations are available from the function Af. where A is a known linear operator. Both the Gaussian white noise model and density estimation are studied. We define an L(2)-empirical risk functional which is used to define a delta-net minimizer and a dense empirical risk minimizer. Upper bounds for the mean integrated squared error of the estimators are given. The upper bounds show how the difficulty of the estimation depends on the operator through the norm of the adjoint of the inverse of the operator and on the underlying function class through the entropy of the class. Corresponding lower bounds are also derived. As examples, we consider convolution operators and the Radon transform. In these examples, the estimators achieve the optimal rates of convergence. Furthermore, a new type of oracle inequality is given for inverse problems in additive models."
"10.1214/09-AOS724","2010","Asymptotic efficiency and finite-sample properties of the generalized profiling estimation of parameters in ordinary differential equations","0","Ordinary differential equations (ODEs) are commonly used to model dynamic behavior of a system. Because many parameters are unknown and have to be estimated from the observed data, there is growing interest in statistics to develop efficient estimation procedures for these parameters. Among the proposed methods in the literature, the generalized profiling estimation method developed by Ramsay and colleagues is particularly promising for its computational efficiency and good performance. In this approach, the ODE solution is approximated with a linear combination of basis functions. The coefficients of the basis functions are estimated by a penalized smoothing procedure with an ODE-defined penalty. However, the statistical properties of this procedure are not known. In this paper, we first give an upper bound on the uniform norm of the difference between the true solutions and their approximations. Then we use this bound to prove the consistency and asymptotic normality of this estimation procedure. We show that the asymptotic covariance matrix is the same as that of the maximum likelihood estimation. Therefore, this procedure is asymptotically efficient. For a fixed sample and fixed basis functions, we study the limiting behavior of the approximation when the smoothing parameter tends to infinity. We propose an algorithm to choose the smoothing parameters and a method to compute the deviation of the spline approximation from solution without Solving the ODEs."
"10.1214/09-AOS722","2010","Edgeworth expansions for {S}tudentized statistics under weak dependence","0","In this paper, we derive valid Edgeworth expansions for studentized versions of a large class of statistics when the data are generated by a strongly mixing process. Under dependence, the asymptotic variance of such a statistic is given by an infinite series of lag-covariances, and therefore, studentizing factors (i.e., estimators of the asymptotic standard error) typically involve an increasing number, say, l of lag-covariance estimators, which are themselves quadratic functions of the observations. The unboundedness of the dimension e of these quadratic functions makes the derivation and the form of the expansions nonstandard. It is shown that in contrast to the case of the studentized means under independence, the derived Edgeworth expansion is a superposition of three distinct series, respectively, given by one in powers of n(-1/2), one in powers of [n/l](-1/2) (resulting from the standard error of the studentizing factor) and one in powers of the bias of the studentizing factor, where n denotes the sample size."
"10.1214/09-AOS721","2010","Maximum smoothed likelihood estimation and smoothed maximum likelihood estimation in the current status model","0","We consider the problem of estimating the distribution function, the density and the hazard rate of the (unobservable.) event time in the current status model. A well studied and natural nonparametric estimator for the distribution function in this model is the nonparametric maximum likelihood estimator (MILE). We study two alternative methods for the estimation of the distribution function, assuming some smoothness of the event time distribution. The first estimator is based oil a maximum smoothed likelihood approach. The second method is based on smoothing the (discrete) MLE of the distribution function. These estimators can be used to estimate the density and hazard rate of the event time distribution based on the plug-in principle."
"10.1214/09-AOS715","2010","Spectral estimation of the fractional order of a {L}\'evy process","3","We consider the problem of estimating the fractional order of a Levy process from low frequency historical and options data. An estimation methodology is developed which allows us to treat both estimation and calibration problems in a unified way. The corresponding procedure consists of two steps: the estimation of a conditional characteristic function and the weighted least squares estimation of the fractional order in spectral domain. While the second step is identical for both calibration and estimation, the first one depends on the problem at hand. Minimax rates of convergence for the fractional order estimate are derived, the asymptotic normality is proved and a data-driven algorithm based on aggregation is proposed. The performance of the estimator in both estimation and calibration setups is illustrated by a simulation study."
"10.1214/09-AOS714","2010","Bayesian analysis in moment inequality models","0","This paper presents a study of the large-sample behavior of the posterior distribution of a structural parameter which is partially identified by moment inequalities. The posterior density is derived based on the limited information likelihood. The posterior distribution converges to zero exponentially fast on any delta-contraction Outside the identified region. Inside, if is bounded below by a positive constant if the identified region is assumed to have a nonempty interior. Our simulation evidence indicates that the Bayesian approach has advantages over frequentist methods, in the sense that, with a proper choice of the prior, the posterior provides more information about the true parameter inside the identified region. We also address the problem of moment and model selection. Our optimality criterion is the maximum posterior procedure and we show that, asymptotically, it selects the true moment/model combination with the most moment inequalities and the simplest model."
"10.1214/09-AOS712","2010","Estimation for a partial-linear single-index model","2","In this paper, we study the estimation for a partial-linear single-index model. A two-stage estimation procedure is proposed to estimate the link function for the single index and the parameters in the single index, as well as the parameters in the linear component of the model. Asymptotic normality is established for both parametric components. For the index, a constrained estimating equation leads to an asymptotically more efficient estimator than existing estimators in the sense that it is of a smaller limiting variance. The estimator of the nonparametric link function achieves optimal convergence rates, and the structural error variance is obtained. In addition, the results facilitate the construction of confidence regions and hypothesis testing for the unknown parameters. A simulation study is performed and an application to a real dataset is illustrated. The extension to multiple indices is briefly sketched."
"10.1214/09-AOS710","2010","Globally optimal parameter estimates for nonlinear diffusions","0","This paper studies an approximation method for the log-likelihood function of a nonlinear diffusion process using the bridge of the diffusion. The main result (Theorem 1) shows that this approximation converges uniformly to the unknown likelihood function and can therefore be used efficiently with any algorithm for sampling from the law of the bridge. We also introduce an expected maximum likelihood (EML) algorithm for inferring the parameters of discretely observed diffusion processes. The approach is applicable to a subclass of nonlinear SDEs with constant volatility and drift that is linear in the model parameters. In this setting, globally optimal parameters are obtained in a single step by solving a linear system. Simulation Studies to test the EML algorithm show that it performs well when compared with algorithms based on the exact maximum likelihood as well its closed-form likelihood expansions."
"10.1214/09-AOS705","2010","Asymptotic equivalence of spectral density estimation and {G}aussian white noise","0","We consider the statistical experiment given by a sample y(l),...,y(n) of a stationary Gaussian process with an unknown smooth spectral density f. Asymptotic equivalence, in the sense of Le Cam's deficiency Delta-distance, to two Gaussian experiments with simpler structure is established. The first one is given by independent zero mean Gaussians with variance approximately f(omega(i)), where omega(i) is a uniform grid of points in (-pi, pi) (nonparametric Gaussian scale regression). This approximation is closely related to well-known asymptotic independence results for the periodogram and corresponding inference methods. The second asymptotic equivalence is to a Gaussian white noise model where the drift function is the log-spectral density. This represents the step from a Gaussian scale model to a location model, and also has a counterpart in established inference methods, that is, log-periodogram regression. The problem of simple explicit equivalence maps (Markov kernels), allowing to directly carry over inference, appears in this context but is not solved here."
"10.1214/09-AOS703","2010","Rates of convergence for the posterior distributions of mixtures of betas and adaptive nonparametric estimation of the density","1","In this paper, we investigate the asymptotic properties of nonparametric Bayesian mixtures of Betas for estimating a smooth density on [0, 1]. We consider a parametrization of Beta distributions in terms of mean and scale parameters and construct a mixture of these Betas in the mean parameter, while putting a prior on this scaling parameter. We prove that such Bayesian nonparametric models have good frequentist asymptotic properties. We determine the posterior rate of concentration around the true density and prove that it is the minimax rate of concentration when the true density belongs to a Holder class with regularity beta, for all positive beta, leading to a minimax adaptive estimating procedure of the density. We also believe that the approximating results obtained on these mixtures of Beta densities can be of interest in a frequentist framework."
"10.1214/09-AOS696","2010","Optimal rates of convergence for estimating the null density and proportion of nonnull effects in large-scale multiple testing","2","An important estimation problem that is closely related to large-scale multiple testing is that of estimating the null density and the proportion of nonnull effects. A few estimators have been introduced in the literature; however, several important problems, including the evaluation of the minimax rate of convergence and the construction of rate-optimal estimators, remain open.In this paper, we consider optimal estimation of the null density and the proportion of nonnull effects. Both minimax lower and upper bounds are derived. The lower bound is established by a two-point testing argument, where at the core is the novel construction of two least favorable marginal densities f(1) and f(2). The density f(1) is heavy tailed both in the spatial and frequency domains and f(2) is a perturbation of f(1) such that the characteristic functions associated with f(1) and f(2) match each other in low frequencies. The minimax upper bound is obtained by constructing estimators which rely on the empirical characteristic function and Fourier analysis. The estimator is shown to be minimax rate optimal.Compared to existing methods in the literature, the proposed procedure not only provides more precise estimates of the null density and the proportion of the nonnull effects, but also yields more accurate results when used inside some multiple testing procedures which aim at controlling the False Discovery Rate (FDR). The procedure is easy to implement and numerical results are given."
"10.1214/08-AOS668","2010","Some nonasymptotic results on resampling in high dimension. {II}. {M}ultiple tests","2","In the context of correlated Multiple tests, we aim to nonasymptotically control the family-wise error rate (FWER) using resampling-type procedures. We observe repeated realizations of a Gaussian random vector in possibly high dimension and with an unknown covariance matrix, and consider the one- and two-sided multiple testing problem for the mean values of its coordinates. We address this problem by using the confidence regions developed in the companion paper [Ann. Statist. (2009), to appear], which lead directly to single-step procedures, these can then be improved using step-down algorithms, following an established general methodology laid down by Romano and Wolf [J. Amer Statist. Assoc. 100 (2005) 94-108]. This gives rise to several different procedures, whose performances are compared Using simulated data."
"10.1214/08-AOS667","2010","Some nonasymptotic results on resampling in high dimension. {I}. {C}onfidence regions","2","We Study generalized bootstrap confidence regions for the mean of a random vector whose coordinates have an unknown dependency structure. The random vector is supposed to be either Gaussian or to have a symmetric and bounded distribution. The dimensionality of the vector can possibly be much larger than the number of observations and we focus on a nonasymptotic control of the confidence level, following ideas inspired by recent results in learning theory. We consider two approaches, the first based on a concentration principle (valid for a large class of resampling weights) and the second oil a resampled quantile, specifically using Rademacher weights. Several intermediate results established in the approach based on concentration principles are of interest in their own right. We also discuss the question of accuracy when using Monte Carlo approximations of the resampled quantities."
"10.1214/08-AOS648","2010","The spectrum of kernel random matrices","1","We place Ourselves in the setting of high-dimensional statistical inference where the number of variables p in a dataset of interest is of the same order of magnitude as the number of observations n.We consider the spectrum of certain kernel random matrices, in particular n x n matrices whose (i, j)th entry is f(X(i)' X(j)/p) or f(vertical bar vertical bar X(i) - X(j) vertical bar vertical bar(2)/p) where p is the dimension of the data, and X(i) are independent data vectors. Here f is assumed to be a locally smooth function.The study is motivated by questions arising in statistics and computer science where these matrices are used to perform, among other things, nonlinear versions of principal component analysis. Surprisingly, we show that in high dimensions, and for the models we analyze, the problem becomes essentially linear-which is at odds with heuristics sometimes used to justify the usage of these methods. The analysis also highlights certain Peculiarities of models widely studied in random matrix theory and raises some questions about their relevance as tools to model high-dimensional data encountered in practice."
"10.1214/09-AOS706","2009","Nonlinear principal components and long-run implications of multivariate diffusions","0","We investigate a method for extracting nonlinear principal components (NPCs). These NPCs maximize variation subject to smoothness and orthogonality constraints; but we allow for a general class of constraints and multivariate probability densities, including densities without compact support and even densities with algebraic tails. We provide primitive sufficient conditions for the existence of these NPCs. By exploiting the theory of continuous-time, reversible Markov diffusion processes, we give a different interpretation of these NPCs and the smoothness constraints. When the diffusion matrix is used to enforce smoothness, the NPCs maximize long-run variation relative to the overall variation subject to orthogonality constraints. Moreover, the NPCs behave as scalar autoregressions with heteroskedastic innovations; this supports semiparametric identification and estimation of a multivariate reversible diffusion process and tests of the overidentifying restrictions implied by Such a process from low-frequency data. We also explore implications for stationary, possibly nonreversible diffusion processes. Finally, we suggest a sieve method to estimate the NPCs from discretely-sampled data."
"10.1214/09-AOS720","2009","Sparsistency and rates of convergence in large covariance matrix estimation","10","This paper studies the sparsistency and rates of convergence for estimating sparse covariance and precision matrices based on penalized likelihood with nonconvex penalty functions. Here, sparsistency refers to the property that all parameters that are zero are actually estimated as zero with probability tending to one. Depending on the case of applications, sparsity priori may occur on the covariance matrix, its inverse or its Cholesky decomposition. We study these three sparsity exploration problems under a unified framework with a general penalty function. We show that the rates of convergence for these problems under the Frobenius norm are of order (s(n) log p(n)/n)(1/2), where s(n) is the number of nonzero elements, p(n) is the size of the covariance matrix and n is the sample size. This explicitly spells out the contribution of high-dimensionality is merely of a logarithmic factor. The conditions on the rate with which the tuning parameter lambda(n) goes to 0 have been made explicit and compared under different penalties. As a result, for the L(1)-penalty, to guarantee the sparsistency and optimal rate of convergence, the number of nonzero elements should be small: s'(n) = O(p(n)) at most, among O(p(n)(2)) parameters, for estimating sparse covariance or correlation matrix, sparse precision or inverse correlation matrix or sparse Cholesky factor, where s'(n) is the number of the nonzero elements on the off-diagonal entries. On the other hand, using the SCAD or hard-thresholding penalty functions, there is no such a restriction."
"10.1214/09-AOS719","2009","Efficient estimation of copula-based semiparametric {M}arkov models","0","This paper considers the efficient estimation of copula-based semiparametric strictly stationary Markov models. These models are characterized by nonparametric invariant (one-dimensional marginal) distributions and parametric bivariate copula functions where the copulas capture temporal dependence and tail dependence of the processes. The Markov processes generated via tail dependent copulas may look highly persistent and are useful for financial and economic applications. We first show that Markov processes generated via Clayton, Gumbel and Student's t copulas and their Survival copulas are all geometrically ergodic. We then propose a sieve maximum likelihood estimation (MLE) for the copula parameter, the invariant distribution and the conditional quantiles. We show that the sieve MLEs of any smooth functional is root-n consistent, asymptotically normal and efficient and that their sieve likelihood ratio statistics are asymptotically chi-square distributed. Monte Carlo studies indicate that, even for Markov models generated via tail dependent copulas and fat-tailed marginals, our sieve MLEs perform very well."
"10.1214/09-AOS717","2009","Decomposition tables for experiments. {I}. {A} chain of randomizations","1","One aspect of evaluating the design for an experiment is the discovery of the relationships between subspaces of the data space. Initially we establish the notation and methods for evaluating an experiment with a single randomization. Starting with two structures, or orthogonal decompositions of the data space, we describe how to combine them to form the overall decomposition for a single-randomization experiment that is ""structure balanced."" The relationships between the two structures are characterized using efficiency factors. The decomposition is encapsulated in a decomposition table. Then, for experiments that involve multiple randomizations forming a chain, we take several structures that pairwise are structure balanced and combine them to establish the form of the orthogonal decomposition for the experiment. In particular, it is proven that the properties of the design for Such an experiment are derived in a straightforward manner from those of the individual designs. We show how to formulate an extended decomposition table giving the sources of variation, their relationships and their degrees of freedom, so that competing designs can be evaluated."
"10.1214/09-AOS713","2009","Local quasi-likelihood with a parametric guide","0","Generalized linear models and the quasi-likelihood method extend the ordinary regression models to accommodate more general conditional distributions of the response. Nonparametric methods need no explicit parametric specification, and the resulting model is completely determined by the data themselves. However, nonparametric estimation schemes generally have a slower convergence rate such as the local polynomial smoothing estimation of nonparametric generalized linear models studied in Fan, Heckman and Wand [J. Amer Statist. Assoc. 90 (1995) 141-150]. In this work, we propose a unified family of parametrically-guided nonparametric estimation schemes. This combines the merits of both parametric and nonparametric approaches and enables us to incorporate prior knowledge. Asymptotic results and numerical simulations demonstrate the improvement of our new estimation schemes over the original nonparametric counterpart."
"10.1214/09-AOS711","2009","Subspace estimation and prediction methods for hidden {M}arkov models","0","Hidden Markov models (HMMs) are probabilistic functions of finite Markov chains, or, put in other words, state space models with finite state space. In this paper, we examine subspace estimation methods for HMMs whose output lies a finite set as well. In particular, we study the geometric structure arising from the nonminimality of the linear state space representation of HMMs, and consistency of a subspace algorithm arising from a certain factorization of the singular value decomposition of the estimated linear prediction matrix, For this algorithm, we show that the estimates of the transition and emission probability matrices are consistent up to a similarity transformation, and that the in-step linear predictor Computed from the estimated system matrices is consistent, i.e., converges to the true optimal linear m-step predictor."
"10.1214/09-AOS709","2009","P{CA} consistency in high dimension, low sample size context","3","Principal Component Analysis (PCA) is an important tool of dimension reduction especially when the dimension (or the number of variables) is very high. Asymptotic studies where the sample size is fixed, and the dimension grows [i.e., High Dimension, Low Sample Size (HDLSS)] are becoming increasingly relevant. We investigate the asymptotic behavior of the Principal Component (PC) directions. HDLSS asymptotics are used to study consistency, strong inconsistency and subspace consistency. We show that if the first few eigenvalues of a population covariance matrix are large enough compared to the others, then the corresponding estimated PC directions are consistent or converge to the appropriate subspace (subspace consistency) and most other PC directions are strongly inconsistent. Broad sets of sufficient conditions for each of these cases are specified and the main theorem gives a catalogue of possible combinations. In preparation for these results, we show that the geometric representation of HDLSS data holds under general conditions, which includes a rho-mixing condition and a broad range of sphericity measures of the covariance matrix."
"10.1214/09-AOS708","2009","A geometric characterization of {$c$}-optimal designs for heteroscedastic regression","1","We consider the common nonlinear regression model where the variance, as well as the mean, is a parametric function of the explanatory variables. The c-optimal design problem is investigated in the case when the parameters of both the mean and the variance function are of interest. A geometric characterization of c-optimal designs in this context is presented, which generalizes the classical result of Elfving [Ann. Math. Statist. 23 (1952) 255-262] for c-optimal designs. As in Elfving's famous characterization, c-optimal designs can be described as representations of boundary points of a convex set. However, in the case where there appear parameters of interest in the variance, the structure of the Elfving set is different. Roughly speaking, the Elfving set corresponding to a heteroscedastic regression model is the convex hull of a set of ellipsoids induced by the underlying model and indexed by the design space. The c-optimal designs are characterized as representations of the points where the line in direction of the vector c intersects the boundary of the new Elfving set. The theory is illustrated in several examples including pharmacokinetic models with random effects."
"10.1214/09-AOS707","2009","Break detection in the covariance structure of multivariate time series models","3","In this paper, we introduce an asymptotic test procedure to assess the stability of volatilities and cross-volatilites of linear and nonlinear multivariate time series models, The test is very flexible as it can be applied, for example, to many of the multivariate GARCH models established in the literature, and also works well in the case of high dimensionality of the underlying data. Since it is nonparametric, the procedure avoids the difficulties associated with parametric model selection, model fitting and parameter estimation. We provide the theoretical foundation for the test and demonstrate its applicability via a Simulation study and an analysis of financial data. Extensions to multiple changes and the case of infinite fourth moments are also discussed."
"10.1214/09-AOS704","2009","Testing conditional independence via {R}osenblatt transforms","0","This paper proposes new tests of conditional independence of two random variables given a single-index involving an unknown finite-dimensional parameter. The tests employ Rosenblatt transforms and are shown to be distribution-free while retaining computational convenience. Some results from Monte Carlo simulations are presented and discussed."
"10.1214/09-AOS701","2009","Detection of spatial clustering with average likelihood ratio test statistics","0","Generalized likelihood ratio (GLR) test statistics are often used in the detection of spatial clustering in case-control and case-population datasets to check for a significantly large proportion of cases within some scanning window. The traditional spatial scan test statistic takes the supremum GLR value over all windows, whereas the average likelihood ratio (ALR) test statistic that we consider here takes an average of the GLR values. Numerical experiments in the literature and in this paper show that the ALR test statistic has more power compared to the spatial scan statistic. We develop in this paper accurate tail probability approximations of the ALR test statistic that allow us to by-pass computer intensive Monte Carlo procedures to estimate p-values. In models that adjust for covariates, these Monte Carlo evaluations require an initial fitting of parameters that can result in very biased p-value estimates."
"10.1214/09-AOS700","2009","Data spectroscopy: eigenspaces of convolution operators and clustering","1","This paper focuses on obtaining clustering information about a distribution from its i.i.d. samples. We develop theoretical results to understand and use clustering information contained in the eigenvectors of data adjacency matrices based on a radial kernel function with a sufficiently fast tail decay. In particular, we provide population analyses to gain insights into which eigenvectors should be used and when the clustering information for the distribution can be recovered from the sample. We learn that a fixed number of top eigenvectors might at the same time contain redundant clustering information and miss relevant clustering information. We use this insight to design the data spectroscopic clustering (DaSpec) algorithm that utilizes properly selected eigenvectors to determine the number of clusters automatically and to group the data accordingly. Our findings extend the intuitions underlying existing spectral techniques such as spectral clustering and Kernel Principal Components Analysis, and provide new understanding into their usability and modes of failure. Simulation Studies and experiments on real-world data are conducted to show the potential of our algorithm. In particular, DaSpec is found to handle unbalanced groups and recover clusters of different shapes better than the competing methods."
"10.1214/09-AOS699","2009","Using the bootstrap to quantify the AUTHORity of an empirical ranking","2","The bootstrap is a popular and convenient method for quantifying the authority of an empirical ordering of attributes, for example of a ranking of the performance of institutions or of the influence of genes on a response variable. In the first of these examples, the number, p, of quantities being ordered is sometimes only moderate in sire; in the second it can be very large, often much greater than sample sire. However, we show that in both types of problem the conventional bootstrap can produce inconsistency. Moreover, the standard n-out-of-n bootstrap estimator of the distribution of an empirical rank may not converge in the usual sense; the estimator may converge in distribution, but not in probability. Nevertheless, in many cases the bootstrap correctly identifies the support of the asymptotic distribution of ranks. In some contemporary problems, bootstrap prediction intervals for ranks are particularly long, and in this context, we also quantify the accuracy of bootstrap methods, showing that the standard bootstrap gets the order of magnitude of the interval right, but not the constant multiplier of interval length. The m-out-of-n bootstrap can improve performance and produce statistical consistency, but it requires empirical choice of m; we suggest a tuning solution to this problem. We show that in genomic examples, where it might be expected that the standard, ""synchronous"" bootstrap will successfully accommodate non-independence of vector components, that approach can produce misleading results. An ""independent component"" bootstrap can overcome these difficulties, even in cases where components are not strictly independent."
"10.1214/09-AOS698","2009","Specification testing in nonlinear and nonstationary time series autoregression","1","This paper considers a class of nonparametric autoregressive models with nonstationarity. We propose a nonparametric kernel test for the conditional mean and then establish an asymptotic distribution of the proposed test. Both the setting and the results differ from earlier work on nonparametric autoregression with stationarity. In addition, we develop a new bootstrap simulation scheme for the selection of a suitable bandwidth parameter involved in the kernel test as well as the choice of a simulated critical value. The finite-sample performance of the proposed test is assessed using one simulated example and one real data example."
"10.1214/09-AOS697","2009","Karl {P}earson's meta-analysis revisited","0","This paper revisits a meta-analysis method proposed by Pearson [Biometrika 26 (1934) 425-442] and first used by David [Biometrika 26 (1934) 1-11]. It was thought to be inadmissible for over fifty years, dating back to a paper of Birnbaum [J. Amer Statist. Assoc. 49 (1954) 559-574]. It turns out that the method Birnbaum analyzed is not the one that Pearson proposed. We show that Pearson's proposal is admissible. Because it is admissible, it has better power than the standard test of Fisher [Statistical Methods for Research Workers (1932) Oliver and Boyd] at some alternatives, and worse power at others. Pearson's method has the advantage when all or most of the nonzero parameters share the same sign. Pearson's test has proved useful in a genomic setting, screening for age-related genes. This paper also presents an FFT-based method for getting hard upper and lower bounds on the CDF of a sum of nonnegative random variables."
"10.1214/09-AOS695","2009","Quantile regression in partially linear varying coefficient models","0","Semiparametric models are often considered for analyzing longitudinal data for a good balance between flexibility and parsimony. In this paper, we study a class of marginal partially linear quantile models with possibly varying coefficients. The functional coefficients are estimated by basis function approximations. The estimation procedure is easy to implement, and it requires no specification of the error distributions. The asymptotic properties of the proposed estimators are established for the varying coefficients as well as for the constant coefficients. We develop rank score tests for hypotheses on the coefficients, including the hypotheses on the constancy of a subset of the varying coefficients. Hypothesis testing of this type is theoretically challenging, as the dimensions of the parameter spaces under both the null and the alternative hypotheses are growing with the sample size. We assess the finite sample performance of the proposed method by Monte Carlo simulation studies, and demonstrate its value by the analysis of an AIDS data set, where the modeling of quantiles provides more comprehensive information than the usual least squares approach."
"10.1214/09-AOS694","2009","Corrections to {LRT} on large-dimensional covariance matrix by {RMT}","0","In this paper, we give an explanation to the failure of two likelihood ratio procedures for testing about covariance matrices from Gaussian populations when the dimension p is large compared to the sample size n. Next, using recent central limit theorems for linear spectral statistics of sample covariance matrices and of random F-matrices, we propose necessary corrections for these LR tests to cope with high-dimensional effects. The asymptotic distributions of these corrected tests under the null are given. Simulations demonstrate that the corrected LR tests yield a realized size close to nominal level for both moderate p (around 20) and high dimension, while the traditional LR tests with chi(2) approximation fails.Another contribution from the paper is that for testing the equality between two covariance matrices, the proposed correction applies equally for non-Gaussian populations yielding a valid pseudo-likelihood ratio test."
"10.1214/09-AOS692","2009","High-dimensional additive modeling","7","We propose a new sparsity-smoothness penalty for high-dimensional generalized additive models. The combination of sparsity and smoothness is crucial for mathematical theory as well as performance for finite-sample data. We present a computationally efficient algorithm, with provable numerical convergence properties, for optimizing the penalized likelihood. Furthermore, we provide oracle results which yield asymptotic optimality of our estimator for high dimensional but sparse additive models. Finally, an adaptive version of Our sparsity-smoothness penalized approach yields large additional performance gains."
"10.1214/08-AOS679","2009","Contour projected dimension reduction","1","In regression analysis, we employ contour projection (CP) to develop a new dimension reduction theory. Accordingly, we introduce the notions of the central contour subspace and generalized contour subspace. We show that both of their structural dimensions are no larger than that of the central subspace Cook [Regression Graphics (1998b) Wiley]. Furthermore, we employ CP-sliced inverse regression, CP-sliced average variance estimation and CP-directional regression to estimate the generalized contour,subspace, and we subsequently obtain their theoretical properties. Monte Carlo studies demonstrate that the three CP-based dimension reduction methods outperform their corresponding non-CP approaches when the predictors have heavy-tailed elliptical distributions. An empirical example is also presented to illustrate the usefulness of the CP method."
"10.1214/08-AOS675","2009","Estimation of trend in state-space models: asymptotic mean square error and rate of convergence","0","The focus of this paper is on trend estimation for a general state-space model Y-t = mu(t) + epsilon(t), where the dth difference of the trend {mu(t)} is assumed to be i.i.d., and the error sequence {epsilon(t)} is assumed to be a mean zero stationary process. A fairly precise asymptotic expression of the mean square error is derived for the estimator obtained by penalizing the dth order differences. Optimal rate of convergence is obtained, and it is shown to be ""asymptotically equivalent"" to a nonparametric estimator of a fixed trend model of smoothness of order d - 0.5. The results of this paper show that the optimal rate of convergence for the stochastic and nonstochastic cases are different. A criterion for selecting the penalty parameter and degree of difference d is given, along with an application to the global temperature data, which shows that a longer term history has nonlinearities that are important to take into consideration."
"10.1214/08-AOS663","2009","Estimating the {G}umbel scale parameter for local alignment of random sequences by importance sampling with stopping times","0","The gapped local alignment score of two random sequences follows a Gumbel distribution. If computers could estimate the parameters of the Gumbel distribution within one second, the use of arbitrary alignment scoring schemes could increase the sensitivity of searching biological sequence databases over the web. Accordingly, this article gives a novel equation for the scale parameter of the relevant Gumbel distribution. We speculate that the equation is exact, although present numerical evidence is limited. The equation involves ascending ladder variates in the global alignment of random sequences. In global alignment simulations, the ladder variates yield stopping times specifying random sequence lengths. Because of the random lengths. and because our trial distribution for importance sampling occurs on a different sample space from our target distribution, our study led to a mapping theorem, which led naturally in turn to an efficient dynamic programming algorithm for the importance sampling weights. Numerical studies using several popular alignment scoring schemes then examined the efficiency and accuracy of the resulting simulations."
"10.1214/09-AOS688","2009","A maximum likelihood method for the incidental parameter problem","0","This paper uses the invariance principle to solve the incidental parameter problem of [Econometrica 16 (1948) 1-32]. We seek group actions that preserve the structural parameter and yield a maximal invariant in the parameter space with fixed dimension. M-estimation from the likelihood of the maximal invariant statistic yields the maximum invariant likelihood estimator (MILE). Consistency of MILE for cases in which the likelihood of the maximal invariant is the product of marginal likelihoods is straightforward. We illustrate this result with a stationary autoregressive model with fixed effects and an agent-specific monotonic transformation model.Asymptotic properties of MILE, when the likelihood of the maximal invariant does not factorize, remain an open question. We are able to provide consistent, asymptotically normal and efficient results of MILE when invariance yields Wishart distributions. Two examples are an instrumental variable (IV) model and a dynamic panel data model with fixed effects."
"10.1214/08-AOS603","2009","Building and using semiparametric tolerance regions for parametric multinomial models","0","We introduce a semiparametric ""tubular neighborhood"" of a parametric model in the multinomial setting. It consists of all multinomial distributions lying in a distance-based neighborhood of the parametric model of interest. Fitting such a tubular model allows one to use a parametric model while treating it as an approximation to the true distribution. In this paper, the Kullback-Leibler distance is used to build the tubular region. Based on this idea one can define the distance between the true multinomial distribution and the parametric model to be the index of fit. The paper develops a likelihood ratio test procedure for testing the magnitude of the index. A semiparametric bootstrap method is implemented to better approximate the distribution of the LRT statistic. The approximation permits more accurate construction of a lower confidence limit for the model fitting index."
"10.1214/09-AOS690","2009","Construction of nested space-filling designs","2","New types of designs called nested space-filling designs have been proposed for conducting multiple computer experiments with different levels of accuracy. In this article, we develop several approaches to constructing Such designs. The development of these methods also leads to the introduction of several new discrete mathematics concepts, including nested orthogonal arrays and nested difference matrices."
"10.1214/08-AOS670","2009","Some results on {$2^{n-m}$} designs of resolution {IV} with (weak) minimum aberration","0","It is known that all resolution IV regular 2(n-m) designs of run size N = 2(n-m) where 5N/16 < n < N/2 must be projections of the maximal even design with N/2 factors and, therefore, are even designs. This paper derives a general and explicit relationship between the wordlength pattern of any even 2(n-m) design and that of its complement in the maximal even design. Using these identities, we identify some (weak) minimum aberration 2(n-m) designs of resolution IV and the structures of their complementary designs. Based on these results, several families of minimum aberration 2(n-m) designs of resolution IV are constructed."
"10.1214/08-AOS644","2009","Existence and construction of randomization defining contrast subspaces for regular factorial designs","0","Regular factorial designs with randomization restrictions are widely used in practice. This paper provides a unified approach to the construction of such designs using randomization defining contrast subspaces for the representation of randomization restrictions. We use finite projective geometry to determine the existence of designs with the required structure and develop a systematic approach for their construction. An attractive feature is that commonly used factorial designs with randomization restrictions are special cases of this general representation. Issues related to the use of these designs for particular factorial experiments are also addressed."
"10.1214/09-AOS693","2009","Asymptotic theory of semiparametric {$Z$}-estimators for stochastic processes with applications to ergodic diffusions and time series","0","This paper generalizes a part of the theory of Z-estimation which has been developed mainly in the context of modern empirical processes to the case of stochastic processes, typically, semimartingales. We present a general theorem to derive the asymptotic behavior of the solution to an estimating equation theta (sic) Psi(n) (theta, (h) over cap (n)) = 0 with an abstract nuisance parameter h when the compensator of Psi(n) is random. As its application, we consider the estimation problem in an ergodic diffusion process model where the drift coefficient contains an unknown, finite-dimensional parameter theta and the diffusion coefficient is indexed by a nuisance parameter h from an infinite-dimensional space. An example for the nuisance parameter space is a class of smooth functions. We establish the asymptotic normality and efficiency of a Z-estimator for the drift coefficient. As another application, we present a similar result also in an ergodic time series model."
"10.1214/08-AOS610","2009","On nonparametric and semiparametric testing for multivariate linear time series","0","We formulate nonparametric and semiparametric hypothesis testing of multivariate stationary linear time series in a unified fashion and propose new test statistics based on estimators of the spectral density matrix. The limiting distributions of these test statistics under null hypotheses are always normal distributions, and they can be implemented easily for practical use. If null hypotheses are false, as the sample size goes to infinity, they diverge to infinity and consequently are consistent tests for any alternative. The approach can be applied to various null hypotheses such as the independence between the component series, the equality of the autocovariance functions or the autocorrelation functions of the component series, the separability of the covariance matrix function and the time reversibility. Furthermore, a null hypothesis with a nonlinear constraint like the conditional independence between the two series can be tested in the same way."
"10.1214/09-AOS683","2009","A unified approach to model selection and sparse recovery using regularized least squares","6","Model selection and sparse recovery are two important problems for which many regularization methods have been proposed. We study the properties of regularization methods in both problems under the unified framework of regularized least squares with concave penalties. For model selection, we establish conditions under which a regularized least squares estimator enjoys a nonasymptotic property, called the weak oracle property, where the dimensionality can grow exponentially with sample size. For sparse recovery, we present a sufficient condition that ensures the recoverability of the sparsest solution. In particular, we approach both problems by considering a family of penalties that give a smooth homotopy between L(0) and L(1) penalties. We also propose the sequentially and iteratively reweighted squares (SIRS) algorithm for sparse recovery. Numerical studies support our theoretical results and demonstrate the advantage of our new methods for model selection and sparse recovery."
"10.1214/07-AOS584","2009","The composite absolute penalties family for grouped and hierarchical variable selection","5","Extracting useful information from high-dimensional data is an important focus of today's statistical research and practice. Penalized loss function minimization has been shown to be effective for this task both theoretically and empirically. With the virtues of both regularization and sparsity, the L-1-penalized squared error minimization method Lasso has been popular in regression models and beyond.In this paper, we combine different norms including L-1 to form an intelligent penalty in order to add side information to the fitting of a regression or classification model to obtain reasonable estimates. Specifically, we introduce the Composite Absolute Penalties (CAP) family, which allows given grouping and hierarchical relationships between the predictors to be expressed. CAP penalties are built by defining groups and combining the properties of norm penalties at the across-group and within-group levels. Grouped selection occurs for nonoverlapping groups. Hierarchical variable selection is reached by defining groups with particular overlapping patterns. We propose using the BLASSO and cross-validation to compute CAP estimates in general. For a subfamily of CAP estimates involving only the L-1 and L-proportional to norms, we introduce the iCAP algorithm to trace the entire regularization path for the grouped selection problem. Within this subfamily, unbiased estimates of the degrees of freedom (df) are derived so that the regularization parameter is selected without cross-validation. CAP is shown to improve on the predictive performance of the LASSO in a series of simulated experiments, including cases with p >> n and possibly mis-specified groupings. When the complexity of a model is properly calculated, iCAP is seen to be parsimonious in the experiments."
"10.1214/08-AOS669","2009","A conjugate prior for discrete hierarchical log-linear models","3","In Bayesian analysis of multi-way contingency tables, the selection of a prior distribution for either the log-linear parameters or the cell probabilities parameters is a major challenge. In this paper, we define a flexible family of conjugate priors for the wide class of discrete hierarchical log-linear models, which includes the class of graphical models. These priors are defined as the Diaconis-Ylvisaker conjugate priors on the log-linear parameters subject to ""baseline constraints"" under multinomial sampling. We also derive the induced prior on the cell probabilities and show that the induced prior is a generalization of the hyper Dirichlet prior. We show that this prior has several desirable properties and illustrate its Usefulness by identifying the most probable decomposable, graphical and hierarchical log-linear models for a six-way contingency table."
"10.1214/09-AOS684","2009","Wavelet regression in random design with heteroscedastic dependent errors","0","We investigate function estimation in nonparametric regression models with random design and heteroscedastic correlated noise. Adaptive properties of warped wavelet nonlinear approximations are studied over a wide range of Besov scales, f is an element of B(pi,r)(S), and for a variety of L(P) error measures. We consider error distributions with Long-Range-Dependence parameter alpha, 0 < alpha <= 1; heteroscedasticity is modeled with a design dependent function a. We prescribe a tuning paradigm, under which warped wavelet estimation achieves partial or full adaptivity results with the rates that are shown to be the minimax rates of convergence. For p > 2, it is seen that there are three rate phases, namely the dense, sparse and long range dependence phase, depending on the relative values of s, p, pi and alpha. Furthermore, we show that long range dependence does not come into play for shape estimation f - integral f. The theory is illustrated with some numerical examples."
"10.1214/09-AOS682","2009","Adaptive density estimation for directional data using needlets","1","This paper is concerned with density estimation of directional data on the sphere. We introduce a procedure based on thresholding on a new type of spherical wavelets called needlets. We establish a minimax result and prove its optimality. We are motivated by astrophysical applications, in particular in connection with the analysis of ultra high-energy cosmic rays."
"10.1214/08-AOS676","2009","Fixed-domain asymptotic properties of tapered maximum likelihood estimators","4","When the spatial sample size is extremely large, which occurs in many environmental and ecological studies, operations on the large covariance matrix are a numerical challenge. Covariance tapering is a technique to alleviate the numerical challenges. Under the assumption that data are collected along a line in a bounded region, we investigate how the tapering affects the asymptotic efficiency of the maximum likelihood estimator (MLE) for the microergodic parameter in the Matern covariance function by establishing the fixed-domain asymptotic distribution of the exact MLE and that of the tapered MLE. Our results imply that, under some conditions on the taper, the tapered MLE is asymptotically as efficient as the true MLE for the microergodic parameter in the Matern model."
"10.1214/09-AOS686","2009","Estimation of functional derivatives","2","Situations of a functional predictor paired with a scalar response are increasingly encountered in data analysis. Predictors are often appropriately modeled as square integrable smooth random functions. Imposing minimal assumptions on the nature of the functional relationship, we aim to estimate the directional derivatives and gradients of the response with respect to the predictor functions. In statistical applications and data analysis, functional derivatives provide a quantitative measure of the often intricate relationship between changes in predictor trajectories and those in scalar responses. This approach provides a natural extension of classical gradient fields in vector space and provides directions of steepest descent. We suggest a kernel-based method for the nonparametric estimation of functional derivatives that utilizes the decomposition of the random predictor functions into their eigenfunctions. These eigenfunctions define a canonical set of directions into which the gradient field is expanded. The proposed method is shown to lead to asymptotically consistent estimates of functional derivatives and is illustrated in an application to growth curves."
"10.1214/08-AOS673","2009","On random tomography with unobservable projection angles","0","We formulate and investigate a statistical inverse problem of a random tomographic nature, where a probability density function on R(3) is to be recovered from observation of finitely many of its two-dimensional projections in random and unobservable directions. Such a problem is distinct from the classic problem of tomography where both the projections and the unit vectors normal to the projection plane are observable. The problem arises in single particle electron microscopy, a powerful method that biophysicists employ to learn the structure of biological macromolecules. Strictly speaking, the problem is unidentifiable and an appropriate reformulation is suggested hinging on ideas from Kendall's theory of shape. Within this setup, we demonstrate that a consistent Solution to the problem may be derived, without attempting to estimate the unknown angles, if the density is assumed to admit a mixture representation."
"10.1214/08-AOS671","2009","On the path density of a gradient field","0","We consider the problem of reliably finding filaments in point clouds. Realistic data sets often have numerous filaments of various sizes and shapes. Statistical techniques exist for finding one (or a few) filaments but these methods do not handle noisy data sets with many filaments. Other methods can be found in the astronomy literature but they do not have rigorous statistical guarantees. We propose the following method. Starting at each data point we construct the steepest ascent path along a kernel density estimator. We locate filaments by finding regions where these paths are highly concentrated. Formally, we define the density of these paths and we construct a consistent estimator of this path density."
"10.1214/08-AOS681","2009","Asymptotic equivalence and adaptive estimation for robust nonparametric regression","0","Asymptotic equivalence theory developed in the literature so far are only for bounded loss functions. This limits the potential applications of the theory because many commonly used loss functions in statistical inference are unbounded. In this paper we develop asymptotic equivalence results for robust nonparametric regression with unbounded loss functions. The results imply that all the Gaussian nonparametric regression procedures can be robustified in a unified way. A key step in our equivalence argument is to bin the data and then take the median of each bin.The asymptotic equivalence results have significant practical implications. To illustrate the general principles of the equivalence argument we consider two important nonparametric inference problems: robust estimation of the reagression function and the estimation of a quadratic functional. In both cases easily implementable procedures are constructed and are shown to enjoy simultaneously a high degree of robustness and adaptivity. Other problems such as construction of confidence sets and nonparametric hypothesis testing can be handled in a similar fashion."
"10.1214/08-AOS591","2009","Robust nearest-neighbor methods for classifying high-dimensional data","0","We suggest a robust nearest-neighbor approach to classifying high-dimensional data. The method enhances sensitivity by employing a threshold and truncates to a sequence of zeros and ones in order to reduce the deleterious impact of heavy-tailed data. Empirical rules are suggested for choosing the threshold. They require the bare minimum of data only one data vector is needed from each population. Theoretical and numerical aspects of performance are explored, paying particular attention to the impacts of correlation and heterogeneity among data components. On the theoretical side, it is shown that our truncated, thresholded, nearest-neighbor classifier enjoys the same classification boundary as more conventional, nonrobust approaches, which require finite moments in order to achieve good performance. In particular, the greater robustness of our approach does not come at the price of reduced effectiveness. Moreover, when both training sample sizes equal 1, our new method can have performance equal to that of optimal classifiers that require independent and identically distributed data with known marginal distributions; yet, our classifier does not itself need conditions of this type."
"10.1214/08-AOS680","2009","Goodness-of-fit problem for errors in nonparametric regression: distribution free approach","0","This paper discusses asymptotically distribution free tests for the classical goodness-of-fit hypothesis of all error distribution in nonparametric regression models. These tests are based on the same martingale transform of the residual empirical process as used in the one sample location model. This transformation eliminates extra randomization due to covariates but not due the errors, which is intrinsically present in the estimators of the regression function. Thus, tests based on the transformed process have, generally, better power. The results of this paper are applicable as soon as asymptotic uniform linearity of nonparametric residual empirical process is available. In particular they are applicable under the conditions Stipulated in recent papers of Akritas and Van Keilegom and Muller, Schick and Wefelmeyer."
"10.1214/09-AOS685","2009","Estimating high-dimensional intervention effects from observational data","0","We assume that we have observational data generated from an unknown underlying directed acyclic graph (DAG) model. A DAG is typically not identifiable from observational data, but it is possible to consistently estimate the equivalence class of a DAG. Moreover, for any given DAG, causal effects can be estimated using intervention calculus. In this paper, we combine these two parts. For each DAG in the estimated equivalence class, we use intervention calculus to estimate the causal effects of the covariates on the response. This yields a collection of estimated causal effects for each covariate. We show that the distinct values in this set can be consistently estimated by an algorithm that uses only local information of the graph. This local approach is computationally fast and feasible in high-dimensional problems. We propose to use summary measures of the set of possible causal effects to determine variable importance. In particular, we use the minimum absolute value of this set, since that is a lower bound on the size of the causal effect. We demonstrate the merits of our methods in a simulation study and on a data set about riboflavin production."
"10.1214/09-AOS689","2009","Identifiability of parameters in latent structure models with many observed variables","3","While hidden class models of various types arise in many statistical applications, it is often difficult to establish the identifiability of their parameters. Focusing on models in which there is some structure of independence of some of the observed variables conditioned on hidden ones, we demonstrate,I general approach for establishing identifiability utilizing algebraic arguments. A theorem of J. Kruskal for a simple latent-class model with finite state space lies at the core of our results, though we apply it to a diverse set of models. These include mixtures of both finite and nonparametric product distributions, hidden Markov models and random graph mixture models, and lead to a number of new results and improvements to old ones.In the parametric setting, this approach indicates that for such models, the classical definition of identifiability is typically too strong. Instead generic identifiability holds, which implies that the set of nonidentifiable parameters has measure zero, so that parameter inference is still meaningful. In particular, this sheds light on the properties of finite mixtures of Bernoulli products, which have been used for decades despite being known to have nonidentifiable parameters. In the nonparametric setting, we again obtain identifiability only when certain restrictions are placed on the distributions that are mixed, but we explicitly describe the conditions."
"10.1214/08-AOS658","2009","Asymptotic normality of a nonparametric estimator of sample coverage","0","This paper establishes a necessary and sufficient condition for the asymptotic normality of the nonparametric estimator of sample coverage proposed by Good [Biometrica 40 (1953) 237-264]. This new necessary and sufficient condition extends the validity of the asymptotic normality beyond the previously proven cases."
"10.1214/08-AOS656","2009","Quarter-fraction factorial designs constructed via quaternary codes","1","The research of developing a general methodology for the construction of good nonregular designs has been very active in the last decade. Recent research by Xu and Wong [Statist. Sinica 17 (2007) 1191-1213] suggested a new class of nonregular designs Constructed from quaternary codes. This paper explores the properties and uses Of quaternary codes toward the construction of quarter-fraction nonregular designs. Some theoretical results are obtained regarding the aliasing structure Of Such designs. Optimal designs are constructed under the maximum resolution, minimum aberration and maximum projectivity criteria. These designs often have larger generalized resolution and larger projectivity than regular designs of the same size. It is further shown that some of these designs have generalized minimum aberration and maximum projectivity among all possible designs."
"10.1214/08-AOS655","2009","Efficient randomized-adaptive designs","3","Response-adaptive randomization hits recently attracted a lot of attention in the literature. In this paper, we propose a new and simple family of response-adaptive randomization procedures that attain the cramer-Rao lower bounds on the allocation variances for any allocation proportions including optimal allocation proportions. The allocation Probability functions of proposed procedures are discontinuous. The existing large sample theory for adaptive designs relies on Taylor expansions of the allocation probability functions. which do not apply to nondifferentiable cases. In the present paper, we Study stopping times of stochastic processes to establish the asymptotic efficiency results. Furthermore, we demonstrate our proposal through examples, simulations and a discussion on the relationship with earlier works, including Efron's biased coin design."
"10.1214/08-AOS651","2009","Hypothesis test for normal mixture models: the {EM} approach","1","Normal mixture distributions are arguably the most important mixture models. and also the most technically challenging. The likelihood function of the normal mixture model is unbounded based oil a set of random samples, unless an artificial bound is placed oil its component variance parameter. Moreover, the model is not strongly identifiable so it is hard to differentiate between over dispersion caused by the presence of a mixture and that caused by a large variance, and it has infinite Fisher information with respect to mixing proportions. There has been extensive research oil finite normal mixture models, but much of it addresses merely consistency of the point estimation or useful practical procedures, and many, results require undesirable restrictions oil the parameter space. We show that an EM-test for homogeneity is effective at overcoming many challenges in the context of finite normal mixtures. We find that the limiting, distribution of the EM-test is a simple function of the 0.5 chi(2)(0) + 0.5 chi(1)(2) and chi(2)(1) distributions when the mixing variances are equal but unknown and the chi(2)(2) when variances are unequal and unknown. Simulations unknown and the show that the limiting distributions approximate the finite sample distribution satisfactorily. Two genetic examples are used to illustrate the application of the EM-test."
"10.1214/08-AOS639","2009","Consistency of a recursive estimate of mixing distributions","1","Mixture models have received considerable attention recently and Newton [Sankhya Ser A 64 (2002) 306-322] proposed a fast recursive algorithm for estimating a mixing distribution. We prove almost sure consistency of this recursive estimate in the weak topology under mild conditions on the family of densities being mixed. This recursive estimate depends on the data ordering and a permutation-invariant modification is proposed, which is an average of the original over permutations of the data sequence. A Rao-Blackwell argument is used to prove consistency in probability of this alternative estimate. Several Simulations are presented, comparing the finite-sample performance of the recursive estimate and a Monte Carlo approximation to the permutation-invariant alternative along with that of the nonparametric maximum likelihood estimate and a nonparametric Bayes estimate."
"10.1214/08-AOS649","2009","Bayesian frequentist hybrid inference","0","Bayesian and frequentist methods differ in many aspects, but share some basic optimality properties. In practice, there are situations in which one of the methods is more preferred by some criteria. We consider the case of inference about a Set Of Multiple parameters, which can be divided into two disjoint subsets. On one set, a frequentist method may be favored and on the other, the Bayesian. This motivates a joint estimation procedure in which some of the parameters are estimated Bayesian, and the rest by the maximum-likelihood estimator in the same parametric model, and thus keep the strengths of both the methods and avoid their weaknesses. Such a hybrid procedure gives us more flexibility in achieving overall inference advantages. We study the consistency and high-order asymptotic behavior of the proposed estimator, and illustrate its application. Also, the results imply a new method for constructing objective prior."
"10.1214/08-AOS645","2009","Asymptotic equivalence of empirical likelihood and {B}ayesian map","0","In this paper we are interested in empirical likelihood (EL)as a method of estimation, and we address the following two problems: (1) selecting among Various empirical discrepancies in an EL framework and (2) demonstrating that El. has a well-defined probabilistic interpretation that would justify its use in a Bayesian context. Using the large deviations approach, a Bayesian law of large numbers is developed that implies that EL and the Bayesian maximum a posteriori probability (MAP) estimators are consistent under mis-specification and that EL can be viewed as an asymptotic form of MAP. Estimators based on other empirical discrepancies are, in general. inconsistent under misspecification."
"10.1214/08-AOS643","2009","On asymptotically optimal tests under loss of identifiability in semiparametric models","1","We consider tests of hypotheses when the parameters are not identifiable under the null in semiparametric models, where regularity conditions for profile likelihood theory fail. Exponential average tests based on integrated profile likelihood are Constructed and shown to be asymptotically optimal under a weighted average power criterion with respect to a prior oil the nonidentifiable aspect of the model. These results extend existing results for parametric models, which involve more restrictive assumptions on the form of the alternative than do our results. Moreover, the proposed tests accommodate models with infinite dimensional nuisance parameters which either may not be identifiable or may not be estimable at the usual parametric rate. Examples include tests of the presence of a change-point in the Cox model With Current status data and tests of regression parameters in odds-rate models with right censored data. Optimal tests have not previously been Studied for these scenarios. We study the asymptotic distribution of the proposed tests Under the null, fixed Contiguous alternatives and random contiguous alternatives. We also propose a weighted bootstrap procedure for computing the critical values of the test statistics. The optimal tests perform well ill simulation Studies, where they may exhibit improved power over alternative tests."
"10.1214/08-AOS662","2009","A semiparametric model for cluster data","2","In the analysis of cluster data, the regression coefficients are frequently assumed to be the same across all clusters. This hampers the ability to Study the varying impacts of factors on each cluster. In this paper, a semiparametric model is introduced to account for varying impacts of factors over clusters by using cluster-level covariates. It achieves the parsimony of parametrization and allows the explorations of nonlinear interactions. The random effect ill the semiparametric model also accounts for within-cluster correlation. Local. linear-based estimation procedure is proposed for estimating functional coefficients, residual variance and within-cluster correlation matrix. The asymptotic properties of the proposed estimators are established, and the method for constructing Simultaneous confidence bands are proposed and studied. In addition, relevant hypothesis testing problems ire addressed. Simulation studies are carried out to demonstrate the methodological power of the proposed methods in the finite sample. The proposed model and methods are used to analyse the second birth interval in Bangladesh, leading to some interesting findings."
"10.1214/08-AOS657","2009","Asymptotic theory for the semiparametric accelerated failure time model with missing data","1","We consider a class of doubly weighted rank-based estimating methods for the transformation (or accelerated failure time) model with missing. data as arise, for example, in case-cohort studies. The weights considered may not be predictable its required in a martingale stochastic process formulation. We treat the general problem as a semi parametric estimating equation problem and provide proofs of asymptotic properties for the weighted estimators, with either true weights or estimated Weights. by using empirical process theory where martingale theory may fail. Simulations show that the outcome-dependent weighted method works well for finite samples in case-cohort studies and improves efficiency compared to methods based oil predictable weights. Further. it is seen that the method is even more efficient when estimated Weights are used, as is commonly the case in the missing data literature. The Gehan censored data Wilcoxon weights are found to lie surprisingly efficient in a wide class of problems."
"10.1214/08-AOS647","2009","Consistent estimates of deformed isotropic {G}aussian random fields on the plane","1","This paper proves fixed domain asymptotic results for estimating a smooth invertible transformation f:R(2) -> R(2) when observint, the deformed oil a dense,rid in a bounded, simply connected random field Z o f on a dense grid in a bounded, simply connected domain Omega, where Z is assumed to be an isotropic Gaussian random field on R(2). The estimate f is constructed on a simply connected domain U, such that (U) over bar subset of Omega and is defined using kernel smoothed quadratic variations, Bergman projections and results from quasi con formal theory. We show, under mild assumptions oil the random field Z and the deformation f, that (f) over cap -> R(theta) f + c uniformly oil compact subsets of U with probability one as the grid spacing goes to zero. where R(theta) is an unidentifiable rotation and c is all unidentifiable translation."
"10.1214/08-AOS652","2009","Deconvolution with unknown error distribution","2","We consider the problem of estimating a density f(X) using a sample Y(l),....Y(n) from f(Y) = f(X) star f(is an element of), where f(is an element of) is an unknown density. We assume that all additional sample is an element of(l),...,is an element of(m) from f(is an element of) is observed. Estimators of f(X) and its derivatives are constructed by using nonparametric estimators of f(X) and f(is an element of) and by applying a spectral cut-off in the Fourier domain. We derive the rate of convergence of the estimators ill case of a known and unknown error density Where it is assumed that f(X) satisfies a polynomial, logarithmic or general source condition. It is shown that the proposed estimators are asymptotically optimal ill a minimax sense ill the models with known or unknown error density, if the density f(X) belongs to a Sobolev space H(p) and f(is an element of) is ordinary smooth or supersmooth."
"10.1214/08-AOS654","2009","Nonparametric estimation by convex programming","1","The problem we concentrate on is as follows: given (1) a convex compact set X in R(n), an affine mapping x bar right arrow A(x), a parametric family {p(mu)(.)} of probability densities and (2) N i.i.d. observations of the random variable omega, distributed with the density p(A(x)) (.) for some (unknown) x is an element of X, estimate the value g(T)x of a given linear form at x.For several families {p(mu)(.)} with no additional assumptions on X and A, we develop computationally efficient estimation routines which are minimax optimal, within an absolute constant factor. We then apply these routines to recovering x itself in the Euclidean norm."
"10.1214/08-AOS642","2009","Estimating linear functionals in nonlinear regression with responses missing at random","1","We consider regression models with parametric (linear or nonlinear) regression function and allow responses to be ""missing at random."" We assume that the errors have mean zero and are independent of the covariates. In order to estimate expectations of functions of covariate and response we use a fully imputed estimator, namely all empirical estimator based oil estimators of conditional expectations given the covariate. We exploit the independence of covariates and errors by writing the conditional expectations as unconditional expectations, which call now be estimated by empirical plug-in estimators. The mean zero constraint oil the error distribution is exploited by adding Suitable residual-based weights. We prove that the estimator is efficient (in the sense of Hajek and Le Cam) if an efficient estimator of the parameter is used. Our results give rise to new efficient estimators of smooth transformations of expectations. Estimation of the mean response is discussed as a special (degenerate) case."
"10.1214/08-AOS640","2009","Estimating the degree of activity of jumps in high frequency data","8","We define a generalized index of jump activity, propose estimators of that index for a discretely sampled process and derive the estimators' properties. These estimators are applicable despite the presence of Brownian volatility in the process, which makes it more challenging to infer the characteristics of the small, infinite activity jumps. When the method is applied to high frequency stock returns, we find evidence of infinitely active jumps in the data and estimate their index of activity."
"10.1214/08-AOS646","2009","High-dimensional variable selection","3","This paper explores the following question: what kind of statistical guarantees can be given when doing variable selection in high-dimensional models? In particular, we look at the error rates and power of some multi-stage regression methods. In the first stage we fit a set of candidate models. In the second stage we select one model by cross-validation. In the third stage we use hypothesis testing to eliminate some variables. We refer to the first two stages as ""screening"" and the last stage as ""cleaning."" We consider three screening methods: the lasso, marginal regression, and forward stepwise regression. Our method gives consistent variable selection under certain conditions."
"10.1214/08-AOS653","2009","Near-ideal model selection by {$\ell_1$} minimization","3","We consider the fundamental problem of estimating the mean of a vector y = X beta + 7, where X is an n x p design matrix in which one can have far more variables than observations, and z is a stochastic error term-the so-called ""p > n"" setup. When beta is sparse, or, more generally, when there is a sparse subset of covariates providing a close approximation to the unknown mean vector, we ask whether or not it is possible to accurately, estimate X beta using a computationally tractable algorithm.We show that, in a Surprisingly wide range of situations. the lasso happens to nearly select the best Subset of variables. Quantitatively speaking, we prove that solving a simple quadratic program achieves a squared error within a logarithmic factor of the ideal mean squared error that one Would achieve with an oracle Supplying perfect information about which variables should and should not be included in the model. Interestingly, our results describe the average performance of the lasso; that is, the performance one can expect in an vast majority of cases where X beta is a sparse or nearly sparse superposition of variables, but not in all cases.Our results are nonasymptotic and widely applicable. since they simply require that pairs of predictor variables are not too collinear."
"10.1214/08-AOS659","2009","Some sharp performance bounds for least squares regression with {$L_1$} regularization","0","We derive sharp performance bounds for least squares regression With L-1 regularization front parameter estimation accuracy and feature selection quality perspectives. The main result proved for L-1 regularization extends it similar result in [Ann. Statist. 35 (2007) 2313-2351] for the Dantzig selector. It gives an affirmative answer to an open question in [Ann. Statist. 35 (2007) 2358-2364]. Moreover, the result leads to an extended view of feature selection that allows less restrictive conditions than some recent work. Based on the theoretical insights, a novel two-stage L-1-regularization procedure with selective penalization is analyzed. It is shown that if the target parameter vector can be decomposed as the sum of a sparse parameter vector with large coefficients and another less sparse vector with relatively small coefficients, then the two-stage procedure can lead to improved performance."
"10.1214/08-AOS641","2009","Functional linear regression that's interpretable","1","Regression models to relate a scalar Y to a functional predictor X(t) are becoming increasingly common. Work in this area has concentrated on estimating a coefficient function, beta(t), with Y related to X(t) through integral beta(t)X(t)dt. Regions where beta (t) not equal 0 correspond to places where there is a relationship between X (t) and Y. Alternatively, points where beta(t) = 0 indicate no relationship. Hence, for interpretation put-poses, it is desirable for a regression procedure to be capable of producing estimates of beta(t) thin are exactly zero over regions with no apparent relationship and have simple structures over the remaining regions. Unfortunately, most fitting procedures result in an estimate for beta(t) that is rarely exactly zero and has unnatural wiggles making the curve hard to interpret. In this article we introduce a new approach which uses variable selection ideas, applied to various derivatives of beta(t), to produce estimates that Lire both interpretable, flexible and accurate. We call Our method ""Functional Linear Regression That's Interpretable"" (FLiRTI) and demonstrate it on simulated and real-world data sets. In addition, non-asymptotic theoretical bounds on the estimation error are presented. The bounds provide strong theoretical motivation for our approach."
"10.1214/07-AOS556","2009","Multivariate {A}rchimedean copulas, {$d$}-monotone functions and {$l_1$}-norm symmetric distributions","1","It is shown that a necessary and sufficient condition for an Archimedean copula generator to generate a d-dimensional copula is that the generator is a d-monotone function. The class of d-dimensional Archimedean copulas is shown to coincide with the class of survival copulas of d-dimensional l(1)-norm symmetric distributions that place no point mass at the origin, The d-monotone Archimedean copula generators may be characterized using a little-known integral transform of Williamson [Duke Math. J. 23 (1956) 189207] in an analogous manner to the well-known Bernstein-Widder characterization of completely monotone generators in terms of the Laplace transform. These insights allow the construction of new Archimedean Copula families and provide a general solution to the problem of sampling multivariate Archimedean copulas. They also yield useful expressions for the d-dimensional Kendall function and Kendall's rank correlation coefficients and facilitate the derivation of results On the existence of densities and the description of singular components for Archimedean copulas. The existence of a sharp lower bound for Archimedean copulas with respect to the positive tower orthant dependence ordering is shown."
"10.1214/08-AOS666","2009","Improved kernel estimation of copulas: weak convergence and goodness-of-fit testing","0","We reconsider the existing kernel estimators for a copula function. as proposed in Gijbels and Mielniczuk [Comm. Statist. Theory, Methods 19 (1990) 445-464] Fermaman, Radulovic and Wegkamp [Bernoulli 10 (2004) 847-860] and Chen and Huang [Canad. J. Statist. 35 (2007) 265-282]. All of these estimators have as a drawback that they can suffer from a corner bias problem. A way to deal with this is to impose rather stringent conditions on the copula, outruling as such many classical families of copulas. In this paper, we propose improved estimators that take care of the typical corner bias problem. For Gijbels and Mielniczuk [Comm. Statist. Theory Methods 19 (1990) 445-464] and Chen and Huang [Canad. J. Statist. 35 (2007) 265-282], the improvement involves shrinking the bandwidth with ail appropriate functional factor; for Fermanian, Radulovic and Wegkamp [Bernoulli 10 (2004) 847-860], this is done by using a transformation. The theoretical contribution of the paper is a weak convergence result for the three improved estimators under conditions that are met for Most copula families. We also discuss the choice of bandwidth parameters, theoretically and practically, and illustrate the finite-sample behaviour of the estimators in a simulation Study. The improved estimators are applied to goodness-of-fit testing for copulas."
"10.1214/08-AOS672","2009","Rank-based inference for bivariate extreme-value copulas","1","Consider a continuous random pair (X, Y) whose dependence is characterized by an extreme-value copula with Pickands dependence function A. When the marginal distributions of X and Y are known, several consistent estimators of A are available. Most of them are variants of the estimators due to Pickands [Bull. Inst. Internat. Statist. 49 (1981) 859-878.] and Caperaa, Fougeres and Genest [Biometrika 84 (1997) 567-577]. In this paper, rank-based versions of these estimators are proposed for the more common case where the margins of X and Y are unknown. Results on the limit behavior of a class of weighted bivariate empirical processes are used to show the consistency and asymptotic normality of these rank-based estimators. Their finite- and large-sample performance is then compared to that of their known-margin analogues, as well as with endpoint-corrected versions thereof. Explicit formulas and consistent estimates for their asymptotic variances are also given."
"10.1214/08-AOS677","2009","Maximum empirical likelihood estimation of the spectral measure of an extreme-value distribution","0","Consider a random sample from a bivariate distribution function F in the max-domain of attraction of all extreme-value distribution function G. This G is characterized by two extreme-value indices and a spectral measure, the latter determining the tail dependence structure of F. A major issue in multivariate extreme-value theory is the estimation of the spectral measure (1)p with respect to the L(p) norm. For every p is an element of [1, infinity], a nonparametric maximum empirical likelihood estimator is proposed for Phi(p). The main novelty is that these estimators are guaranteed to satisfy the moment constraints by which spectral measures are characterized. Asymptotic normality of the estimators is proved under conditions that allow for tail independence. Moreover, the conditions are easily verifiable as we demonstrate through a number of theoretical examples. A simulation study shows a substantially improved performance of the new estimators. Two case Studies illustrate how to implement the methods in practice."
"10.1214/08-AOS665","2009","Properties and refinements of the fused lasso","0","We consider estimating an unknown signal, both blocky and sparse, which is corrupted by additive noise. We study three interrelated least squares procedures and their asymptotic properties. The first procedure is the fused lasso, put forward by Friedman et al. [Ann. Appl. Statist. 1 (2007) 302-332], which we modify into a different estimator, called the fused adaptive lasso, with better properties. The other two estimators we discuss solve least squares problems on sieves; one constrains the maximal e I norm and the maximal total variation seminorm, and the other restricts the number of blocks and the number of nonzero coordinates of the signal. We derive conditions for the recovery of the true block partition and the true sparsity patterns by the fused lasso and the fused adaptive lasso, and we derive convergence rates for the sieve estimators, explicitly in terms of the constraining parameters."
"10.1214/08-AOS664","2009","High-dimensional analysis of semidefinite relaxations for sparse principal components","2","Principal component analysis (PCA) is a classical method for dimensionality reduction based oil extracting the dominant eigenvectors of the Sample covariance matrix. However, PCA is well known to behave poorly inw the ""large p, small n"" setting, in which the problem dimension p is comparable to or larger than the sample size n. This paper studies PICA ill this high-dimensional regime, but under the additional assumption that the maximal eigenvector is sparse, say, with at most k nonzero components. We consider a spiked covariance model in which a base matrix is perturbed by adding a k-sparse maximal eigenvector, and we analyze two computationally tractable methods for recovering the Support set of this maximal eigenvector, as follows: (a) a simple diagonal thresholding method. which transitions from success to failure as a function of the resealed sample size theta(dia)(n, p, k) = n/[k(2) log(p - k)]; and (b) a more sophisticated semidefinite programming (SDP) relaxation, which succeeds once the resealed sample Size theta(sdp)(n, p, k) = n/[k log(p - k)] is larger than a critical threshold. In addition, we prove that no method, including the best method which has exponential-time complexity, can Succeed in recovering the Support if the order parameter theta(sdp)(n, p, k) is below a threshold. Our results thus highlight,in interesting trade-oft between computational and statistical efficiency in high-dimensional inference."
"10.1214/08-AOS660","2009","Conditional predictive inference post model selection","0","We give a finite-sample analysis of predictive inference procedures after model selection in regression with random design. The analysis is focused on a statistically challenging scenario where the number of potentially important explanatory variables can be infinite, where no regularity conditions are imposed on unknown parameters, where the number of explanatory variables in a ""good"" model can be of the same order as sample size and where the number of candidate models can be of larger order than sample size. The performance of inference procedures is evaluated conditional on the training sample. Under weak conditions on only the number of candidate models and on their complexity, and uniformly over all data-generating processes under consideration, we show that a certain prediction interval is approximately valid and short with high probability in finite samples, in the sense that its actual coverage probability is close to the nominal one and in the sense that its length is close to the length of an infeasible interval that is constructed by actually knowing the ""best"" candidate model. Similar results are shown to hold for predictive inference procedures other than prediction intervals like, for example, tests of whether a future response will lie above of below a given threshold."
"10.1214/08-AOS626","2009","Markov equivalence for ancestral graphs","0","Ancestral graphs can encode conditional independence relations that arise in directed acyclic graph (DAG) models with latent and selection variables. However, for any ancestral graph, there may be several other graphs to which it is Markov equivalent. We state and prove conditions under which two maximal ancestral graphs are Markov equivalent to each other, thereby extending analogous results for DAGs given by other authors. These conditions lead to an algorithm for determining Markov equivalence that runs in time that is polynomial in the number of vertices in the graph."
"10.1214/08-AOS607","2009","Parameter tuning in pointwise adaptation using a propagation approach","0","This paper discusses the problem of adaptive estimation Of a univariate object like the value of a regression function at a given point or a linear functional in a linear inverse problem. We consider an adaptive procedure originated from Lepski [Theory Probab. Appl. 35 (1990) 454-466.] that selects in a data-driven way one estimate Out of a given class of estimates ordered by their variability. A serious problem with using this and similar procedures is the choice of some tuning parameters like thresholds. Numerical results show that the theoretically recommended proposals appear to be too conservative and lead to a strong oversmoothing effect. A careful choice of the parameters of the procedure is extremely important for getting the reasonable (quality of estimation. The main contribution of this paper is the new approach for choosing the parameters of the procedure by providing the prescribed behavior of the resulting estimate in the simple parametric situation. We establish a non-asymptotical ""oracle"" bound, which shows that the estimation risk is, Lip to a logarithmic multiplier, equal to the risk of the ""oracle"" estimate that is optimally selected from the given family. A numerical study demonstrates a good performance of the resulting procedure in a number of simulated examples."
"10.1214/08-AOS661","2009","Adaptive {H}ausdorff estimation of density level sets","1","Consider the problem of estimating the gamma-level set G(gamma)* = {x : f(x) > gamma} of an unknown d-dimensional density function f based on n independent observations X(1), ..., X(n) from the density. This problem has been addressed under global error criteria related to the symmetric set difference. However, in certain applications a spatially uniform mode of convergence is desirable to ensure that the estimated set is close to the target set everywhere. The Hausdorff error criterion provides this degree of uniformity and, hence, is more appropriate in such situations. It is known that the minimax optimal rate of error convergence for the Hausdorff metric is (n/logn)(-1/(d+2 alpha)) for level sets with boundaries that have a Lipschitz functional form, where the parameter alpha characterizes the regularity of the density around the level of interest. However, the estimators proposed in previous work are nonadaptive to the density regularity and require knowledge of the parameter alpha. Furthermore, previously developed estimators achieve the minimax optimal rate for rather restricted classes of sets (e.g., the boundary fragment and star-shaped sets) that effectively reduce the set estimation problem to a function estimation problem. This characterization precludes level sets with multiple connected components, which are fundamental to many applications. This paper presents a fully data-driven procedure that is adaptive to unknown regularity conditions and achieves near minimax optimal Hausdorff error control for a class of density level sets with very general shapes and multiple connected Components."
"10.1214/08-AOS674","2009","Asymptotic normality of the quasi-maximum likelihood estimator for multidimensional causal processes","0","Strong consistency and asymptotic normality of the quasi-maximum likelihood estimator are given for a general class of multidimensional causal processes. For particular cases already studied in the literature [for instance univariate or multivariate ARCH(infinity) processes], the assumptions required for establishing these results are often weaker than existing conditions. The QMLE asymptotic behavior is also given for numerous new examples of univariate or multivariate processes (for instance TARCH or NLARCH processes)."
"10.1214/08-AOS636","2009","Local linear quantile estimation for nonstationary time series","2","We consider estimation of quantile curves for a general class of nonstationary processes. Consistency and central limit results are obtained for local linear quantile estimates under a mild short-range dependence condition. Our results are applied to environmental data sets. In particular, our results can be used to address the problem of whether climate variability has changed, an important problem raised by IPCC (Intergovernmental Panel on Climate Change) in 2001."
"10.1214/08-AOS590","2009","On maxima of periodograms of stationary processes","1","We consider the limit distribution of maxima of periodograms for stationary processes. Our method is based on m-dependent approximation for stationary processes and a moderate deviation result,"
"10.1214/08-AOS678","2009","Adaptive {B}ayesian estimation using a {G}aussian random field with inverse gamma bandwidth","3","We consider nonparametric Bayesian estimation inference using a rescaled smooth Gaussian field as a prior for a multidimensional function. The rescaling is achieved using a Gamma variable and the procedure can be viewed as choosing all inverse Gamma bandwidth. The procedure is studied from a frequentist perspective in three statistical settings involving replicated observations (density estimation, regression and classification). We prove that the resulting posterior distribution shrinks to the distribution that generates the data at a speed which is minimax-optimal up to a logarithmic factor, whatever the regularity level of the data-generating distribution. Thus the hierachical Bayesian procedure, with a fixed prior, is shown to be fully adaptive."
"10.1214/07-AOS577","2009","Improving {SAMC} using smoothing methods: theory and applications to {B}ayesian model selection problems","1","Stochastic approximation Monte Carlo (SAMC) has recently been proposed by Liang, Liu and Carroll [J. Amer Statist. Assoc. 102 (2007) 305-320] as a general simulation and optimization algorithm. In this paper, we propose to improve its convergence using smoothing methods and discuss the application of the new algorithm to Bayesian model selection problems. The new algorithm is tested through a change-point identification example. The numerical results indicate that the new algorithm can outperform SAMC and reversible jump MCMC significantly for the model selection problems. The new algorithm represents a general form of the stochastic approximation Markov chain Monte Carlo algorithm. It allows multiple samples to be generated at each iteration, and a bias term to be included in the parameter updating step. A rigorous proof for the convergence of the general algorithm is established under verifiable conditions. This paper also provides a framework oil how to improve efficiency of Monte Carlo simulations by incorporating some nonparametric techniques."
"10.1214/07-AOS575","2009","Nonparametric regression, confidence regions and regularization","2","In this paper we offer a unified approach to the problem of nonparametric regression on the unit interval. It is based on a universal, honest and nonasymptotic confidence region A(n) which is defined by a set of linear in-equalities involving the values of the functions at the design points. Interest will typically center on certain simplest functions in A(n) where simplicity can be defined in terms of shape (number of local extremes, intervals of convexity/concavity) or smoothness (bounds on derivatives) or a combination of both. Once some form of regularization has been decided upon the confidence region can be used to provide honest nonasymptotic confidence bounds which are less informative but conceptually much simpler."
"10.1214/08-AOS635","2009","Optimal discrimination designs","0","We consider the problem of constructing optimal designs for model discrimination between competing regression models. Various new properties of optimal designs with respect to the popular T-optimality criterion are derived, which in many circumstances allow an explicit determination of T-optimal designs. It is also demonstrated, that in nested linear models the number of support points of T-optimal designs is usually too small to estimate all parameters in the extended model. In many cases T-optimal designs are usually not unique, and in this situation we give a characterization of all T-optimal designs. Finally, T-optimal designs are compared with optimal discriminating designs with respect to alternative criteria by means of a small simulation study."
"10.1214/08-AOS634","2009","On the computational complexity of {MCMC}-based estimators in large samples","1","In this paper we examine the implications of the statistical large sample theory for the computational complexity of Bayesian and quasi-Bayesian estimation carried out using Metropolis random walks. Our analysis is motivated by the Laplace-Bernstein-Von Mises central limit theorem, which states that in large samples the posterior or quasi-posterior approaches a. normal density. Using the conditions required for the central limit theorem to hold, we establish polynomial bounds on the computational complexity of general Metropolis random walks methods in large samples. Our analysis covers cases where the underlying log-likelihood or extremum criterion function is possibly nonconcave, discontinuous, and with increasing parameter dimension. However, the central limit theorem restricts the deviations from continuity and log-concavity of the log-likelihood or extremum criterion function in a very specific manner.Under minimal assumptions required for the central limit theorem to hold under the increasing parameter dimension, we show that the Metropolis algorithm is theoretically efficient even for the canonical Gaussian walk which is studied in detail. Specifically, we show that the running time of the algorithm in large samples is bounded in probability by a polynomial in the parameter dimension d and, in particular, is of stochastic order d(2) in the leading cases after the bum-in period. We then give applications to exponential families, curved exponential families and Z-estimation of increasing dimension."
"10.1214/08-AOS633","2009","A {F}ourier transform method for nonparametric estimation of multivariate volatility","0","We provide a nonparametric method for the computation of instantaneous multivariate volatility for continuous serni-martingales, which is based on Fourier analysis. The co-volatility is reconstructed as a stochastic function of time by establishing a connection between the Fourier transform of the prices process and the Fourier transform of the co-volatility process. A nonparametric estimator is derived given a discrete unevenly spaced and asynchronously sampled observations of the asset price processes. The asymptotic properties of the random estimator are studied: namely, consistency in probability uniformly in time and convergence in law to a mixture of Gaussian distributions."
"10.1214/08-AOS632","2009","Maximum likelihood estimation for {$\alpha$}-stable autoregressive processes","1","We consider maximum likelihood estimation for both causal and noncausal autoregressive time series processes with non-Gaussian alpha-stable noise. A nondegenerate limiting distribution is given for maximum likelihood estimators of the parameters of the autoregressive model equation and the parameters of the stable noise distribution. The estimators for the autoregressive parameters are n(1/alpha)-consistent and converge in distribution to the maximizer of a random function. The form of this limiting distribution is intractable, but the shape of the distribution for these estimators can be examined using the bootstrap procedure. The bootstrap is asymptotically valid under general Conditions. The estimators for the parameters of the stable noise distribution have the traditional n(1/2) rate of convergence and are asymptotically normal. The behavior of the estimators for finite samples is studied via simulation, and we use maximum likelihood estimation to fit a noncausal autoregressive model to the natural logarithms of volumes of Wal-Mart stock traded daily on the New York Stock Exchange."
"10.1214/08-AOS631","2009","Asymptotics for posterior hazards","0","An important issue in survival analysis is the investigation and the modeling of hazard rates. Within a Bayesian nonparametric framework, a natural and popular approach is to model hazard rates as kernel mixtures with respect to a completely random measure. In this paper we provide a comprehensive analysis of the asymptotic behavior of such models. We investigate consistency of the posterior distribution and derive fixed sample size central limit theorems for both linear and quadratic functionals of the posterior hazard rate. The general results are then specialized to various specific kernels and mixing measures yielding consistency under minimal conditions and neat central limit theorems for the distribution of functionals."
"10.1214/08-AOS637","2009","Kernel dimension reduction in regression","0","We present a new methodology for sufficient dimension reduction (SDR). Our methodology derives directly from the formulation of SDR in terms of the conditional independence of the covariate X from the response Y, given the projection of X on the central subspace [cf. J. Amer Statist. Assoc. 86 (1991) 316-342 and Regression Graphics (1998) Wiley]. We show that this conditional independence assertion can be characterized in terms of conditional covariance operators on reproducing kernel Hilbert spaces and we show how this characterization leads to an M-estimator for the central subspace. The resulting estimator is shown to be consistent under weak conditions; in particular, we do not have to impose linearity or ellipticity conditions of the kinds that are generally invoked for SDR methods. We also present empirical results showing that the new methodology is competitive in practice."
"10.1214/08-AOS627","2009","Covariate-adjusted nonlinear regression","0","In this paper, we propose a covariate-adjusted nonlinear regression model. In this model, both the response and predictors can only be observed after being distorted by some multiplicative factors. Because of nonlinearity, existing methods for the linear setting cannot be directly employed. To attack this problem, we propose estimating the distorting functions by nonparametrically regressing the predictors and response on the distorting covariate; then, nonlinear least squares estimators for the parameters are obtained using the estimated response and predictors. Root n-consistency and asymptotic normality are established. However, the limiting variance has a very complex structure with several unknown components, and confidence regions based on normal approximation are not efficient. Empirical likelihood-based confidence regions are proposed, and their accuracy is also verified due to its self-scale invariance. Furthermore, unlike the common results derived from the profile methods, even when plug-in estimates are used for the infinite-dimensional nuisance parameters (distorting functions), the limit of empirical likelihood ratio is still chi-squared distributed. This property cases the construction of the empirical likelihood-based confidence regions. A simulation study is carried out to assess the finite sample performance of the proposed estimators and confidence regions. We apply our method to study the relationship between glomerular filtration rate and serum creatinine."
"10.1214/08-AOS624","2009","Testing for common arrivals of jumps for discretely observed multidimensional processes","0","We consider a bivariate process X(t) = (X(t)(1), X(t)(2)), which is observed on a finite time interval [0, T] at discrete times 0, Delta(n), 2 Delta(n), .... Assuming that its two components X(1) and X(2) have jumps on [0, T], we derive tests to decide whether they have at least one jump occurring at the same time (""common jumps"") or not (""disjoint jumps""). There are two different tests for the two possible null hypotheses (common jumps or disjoint jumps). Those tests have a prescribed asymptotic level, as the mesh Delta(n) goes to 0. We show on some simulations that these tests perform reasonably well even in the finite sample case, and we also put them in use for some exchange rates data."
"10.1214/08-AOS602","2009","Change-point estimation under adaptive sampling","2","We consider the problem of locating a jump discontinuity (chan-e-point) in a smooth parametric regression model with a bounded covariate. It is assumed that one can sample the covariate at different values and measure the corresponding responses. Budget constraints dictate that a total of n such measurements can be obtained. A multistage adaptive procedure is proposed, where at each stage an estimate of the change point is obtained and new points are sampled from its appropriately chosen neighborhood. It is shown that such procedures accelerate the rate of convergence of the least squares estimate of the change-point. Further, the asymptotic distribution of the estimate is derived using empirical processes techniques. The latter result provides guidelines on how to choose the tuning parameters of the multistage procedure in practice. The improved efficiency of the procedure is demonstrated using real and synthetic data. This problem is primarily motivated by applications in engineering systems."
"10.1214/08-AOS625","2009","On the adaptive elastic-net with a diverging number of parameters","9","We consider the problem of model selection and estimation in situations where the number of parameters diverges with the sample size. When the dimension is high, an ideal method should have the oracle property [J Amer. Statist. Assoc. 96 (2001) 1348-1360] and [Ann. Statist. 32 (2004) 928-961] which ensures the optimal large sample performance. Furthermore, the high-dimensionality often induces the collinearity problem, which should be properly handled by the ideal method. Many existing variable selection methods fail to achieve both goals simultaneously. In this paper, we propose the adaptive elastic-net that combines the strengths of the quadratic regularization and the adaptively weighted lasso shrinkage. Under weak regularity conditions, we establish the oracle property of the adaptive elastic-net. We show by simulations that the adaptive elastic-net deals with the collinearity problem better than the other oracle-like methods, thus enjoying much improved finite sample performance."
"10.1214/08-AOS620","2009","Simultaneous analysis of lasso and {D}antzig selector","25","We show that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior. Forboth methods, wederive, inparallel, oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the e(p) estimation loss for 1 <= p <= 2 in the linear model when the number of variables can be Much larger than the sample size."
"10.1214/08-AOS630","2009","Nonparametric empirical {B}ayes and compound decision approaches to estimation of a high-dimensional vector of normal means","0","We consider the classical problem of estimating a vector mu = (mu(1,) ..., mu(n)) based on independent observations Yi similar to N(mu(i), 1), i = 1, ..., n.Suppose mu(i), i = 1, ..., n are independent realizations from a completely unknown G. We suggest an easily computed estimator (mu) over cap, such that the ratio of its risk E((mu) over cap - mu)(2) with that of the Bayes procedure approaches 1. A related compound decision result is also obtained.Our asymptotics is of a triangular array; that is, we allow the distribution G to depend on n. Thus, our theoretical asymptotic results are also meaningful in situations where the vector mu is sparse and the proportion of zero coordinates approaches 1.We demonstrate the performance of our estimator in simulations, emphasizing sparse setups. In ""moderately-sparse"" situations, our procedure performs very well compared to known procedures tailored for sparse setups. It also adapts well to nonsparse situations."
"10.1214/08-AOS638","2009","General maximum likelihood empirical {B}ayes estimation of normal means","0","We propose a general maximum likelihood empirical Bayes (GMLEB) method for the estimation of a mean vector based on observations with i.i.d. normal errors. We prove that under mild moment conditions on the unknown means, the average mean squared error (MSE) of the GMLEB is within an infinitesimal fraction of the minimum average MSE among all separable estimators which use a single deterministic estimating function on individual observations, provided that the risk is of greater order than (logn)(5)/n. We also prove that the GMLEB is uniformly approximately minimax in regular and weak l(P) balls when the order of the length-normalized norm of the unknown means is between (log n)(k1/)n(1)/((p boolean AND 2)) and n/(logn)(k2). Simulation experiments demonstrate that the GMLEB outperforms the James-Stein and several state-of-the-art threshold estimators in a wide range of settings without much down side."
"10.1214/08-AOS623","2009","Fast learning rates in statistical inference through aggregation","0","We develop minimax optimal risk bounds for the general learning task consisting in predicting as well as the best function in a reference set g up to the smallest possible additive term, called the convergence rate. When the reference set is finite and when n denotes the size of the training data, we provide minimax convergence rates of the form C(log|g|/n)(nu) with tight evaluation of the positive constant C and with exact 0 < nu <= 1, the latter value depending on the convexity of the loss function and on the level of noise in the output distribution.The risk upper bounds are based on a sequential randomized algorithm, which at each step concentrates on functions having both low risk and low variance with respect to the previous step prediction function. Our analysis puts forward the links between the probabilistic and worst-case viewpoints, and allows to obtain risk bounds unachievable with the standard statistical learning approach. One of the key ideas of this work is to use probabilistic inequalities with respect to appropriate (Gibbs) distributions on the prediction function space instead of using them with respect to the distribution generating the data.The risk lower bounds are based on refinements of the Assouad lemma taking particularly into account the properties of the loss function. Our key example to illustrate the upper and lower bounds is to consider the L-q-regression setting for which an exhaustive analysis of the convergence rates is given while q ranges in [1; + infinity]."
"10.1214/08-AOS622","2009","On-line predictive linear regression","0","We consider the on-line predictive version of the standard problem of linear regression; the goal is to predict each consecutive response given the corresponding explanatory variables and all the previous observations. The standard treatment of prediction in linear regression analysis has two drawbacks: (1) the classical prediction intervals guarantee that the probability of error is equal to the nominal significance level epsilon, but this property per se does not imply that the long-run frequency of error is close to epsilon; (2) it is not suitable for prediction of complex systems as it assumes that the number of observations exceeds the number of parameters. We state a general result showing that in the on-line protocol the frequency of error for the classical prediction intervals does equal the nominal significance level, up to statistical fluctuations. We also describe alternative regression models in which informative prediction intervals can be found before the number of observations exceeds the number of parameters. One of these models, which only assumes that the observations are independent and identically distributed, is popular in machine learning but greatly underused in the statistical theory of regression."
"10.1214/08-AOS617","2009","On a generalized false discovery rate","0","The concept of k-FWER has received much attention lately as an appropriate error rate for multiple testing when one seeks to control at least k false rejections, for some fixed k >= 1. A less conservative notion, the k-FDR, has been introduced very recently by Sarkar [Ann. Statist. 34 (2006) 394-415], generalizing the false discovery rate of Benjamini and Hochberg [J. Roy. Statist. Soc. Ser. B 57 (1995) 289-300]. In this article, we bring newer insight to the k-FDR considering a mixture model involving independent p-values before motivating the developments of some new procedures that control it. We prove the k-FDR control of the proposed methods under a slightly weaker condition than in the mixture model. We provide numerical evidence of the proposed methods' superior power performance over some k-FWER and k-FDR methods. Finally, we apply our methods to a real data set."
"10.1214/08-AOS616","2009","A new multiple testing method in the dependent case","1","The most popular multiple testing procedures are stepwise procedures based on P-values for individual test statistics. Included among these are the false discovery rate (FDR) controlling procedures of Benjamini-Hochberg [J. Roy. Statist. Soc. Ser B 57 (1995) 289-300] and their offsprings. Even for models that entail dependent data, P-values based on marginal distributions are used. Unlike such methods, the new method takes dependency into account at all stages. Furthermore, the P-value procedures often lack an intuitive convexity property, which is needed for admissibility. Still further, the new methodology is computationally feasible. If the number of tests is large and the proportion of true alternatives is less than say 25 percent, simulations demonstrate a clear preference for the new methodology. Applications are detailed for models such as testing treatments against control (or any intraclass correlation model), testing for change points and testing means when correlation is successive."
"10.1214/08-AOS615","2009","Case-control survival analysis with a general semiparametric shared frailty model: a pseudo full likelihood approach","0","In this work we deal with correlated failure time (age at onset) data arising from population-based, case-control studies, where case and control probands are selected by population-based sampling and all array of risk factor measures is collected for both cases and controls and their relatives. Parameters of interest are effects of risk factors on the failure time hazard function and within-family dependencies among failure times after adjusting for the risk factors. Due to the retrospective sampling scheme, large sample theory for existing methods has not been established. We develop a novel technique for estimating the parameters of interest under a general semiparametric shared frailty model. We also present a simple, easily computed, and noniterative nonparametric estimator for the cumulative baseline hazard function. We provide rigorous large sample theory for the proposed method. We also present simulation results and a real data example for illustrating the utility of the proposed method."
"10.1214/08-AOS614","2009","Nonlinear sequential designs for logistic item response theory models with applications to computerized adaptive tests","0","Computerized adaptive testing is becoming increasingly popular due to advancement of modern computer technology. It differs from the conventional standardized testing in that the selection of test items is tailored to individual examinee's ability level. Arising from this selection strategy is a nonlinear sequential design problem. We study, in this paper, the sequential design problem in the context of the logistic item response theory models. We show that the adaptive design obtained by maximizing the item information leads to I consistent and asymptotically normal ability estimator in the case of the Rasch model. Modifications to the maximum information approach are proposed for the two- and three-parameter logistic models. Similar asymptotic properties are established for the modified designs and the resulting estimator. Examples are also given in the case of the two-parameter logistic model to show that without such modifications, the maximum likelihood estimator of the ability parameter may not be consistent."
"10.1214/08-AOS613","2009","Minimal sufficient causation and directed acyclic graphs","0","Notions of minimal sufficient causation are incorporated within the directed acyclic graph causal framework. Doing so allows for the graphical representation of sufficient causes and minimal sufficient causes on causal directed acyclic graphs while maintaining all of the properties of causal directed acyclic graphs. This in turn provides a clear theoretical link between two major conceptualizations of causality: one counterfactual-based and the other based on a more mechanistic understanding of causation. The theory developed can be used to draw conclusions about the sign of the conditional covariances among variables."
"10.1214/08-AOS612","2009","Multiscale local change point detection with applications to value-at-risk","0","This paper offers a new approach to modeling and forecasting of nonstationary time series with applications to volatility modeling for financial data. The approach is based on the assumption of local homogeneity: for every time point, there exists a historical interval of homogeneity, in which the volatility parameter can be well approximated by a constant. The proposed procedure recovers this interval from the data using the local change point (LCP) analysis. Afterward, the estimate of the volatility can be simply obtained by local averaging. The approach carefully addresses the question of choosing the tuning parameters of the procedure using the so-called ""propagation"" condition. The main result claims a new ""oracle"" inequality in terms of the modeling bias which measures the quality of the local constant approximation. This result yields the optimal rate of estimation for smooth and piecewise constant volatility functions. Then, the new procedure is applied to some data sets and a comparison with a standard GARCH model is also provided. Finally, we discuss applications of the new method to the Value at Risk problem. The numerical results demonstrate a very reasonable performance of the new method."
"10.1214/08-AOS611","2009","Nonparametric estimation of composite functions","1","We study the problem of nonparametric estimation of a multivariate function g: R(d) -> R that can be represented as a composition of two unknown smooth functions f : R -> R and G :R(d) -> R. We suppose that f and G belong to known smoothness classes of functions, with smoothness gamma and beta, respectively. We obtain the full description of minimax rates of estimation of g in terms of gamma and beta, and propose rate-optimal estimators for the sup-norm loss. For the construction of such estimators, we first prove an approximation result for composite functions that may have an independent interest, and then a result on adaptation to the local structure. Interestingly, the construction of rate-optimal estimators for composite functions (with given, fixed smoothness) needs adaptation, but not in the traditional sense: it is now adaptation to the local structure. We prove that composition models generate only two types of local structures: the local single-index model and the local model with roughness isolated to a single dimension (i.e., a model containing elements of both additive and single-index structure). We also find the zones of (gamma,beta) where no local structure is generated, as well as the zones where the composition modeling leads to faster rates, as compared to the classical nonparametric rates that depend only to the overall smoothness of g."
"10.1214/08-AOS621","2009","Sparse recovery in convex hulls via entropy penalization","2","Let (X. Y) be a random couple in S x T with unknown distribution P and (X(1), Y(1)),..., (X(n), Y(n),) be i.i.d. copies of (X, Y). Denote P(n) the empirical distribution of (X(1), Y(1)),..., (X(n), Y(n)). Let h(1),..., h(N): S bar right arrow [-1, 1] be a dictionary that consists of N functions. For lambda is an element of R(N), denote f(lambda) := Sigma(N)(j=1) lambda(j)h(j). Let l: T x R bar right arrow R be a given loss function and suppose it is convex with respect to the second variable. Let (l center dot f)(x, y) := l(y; f (x)). Finally, let &ULAMBDA subset of R(N) be the simplex of all probability distributions on {1,..., N}. Consider the following penalized empirical risk minimization problem(lambda) over cap (epsilon) := argmin(lambda is an element of Lambda)[ P(n)(l center dot f(lambda)) + epsilon Sigma(N)(j=1)lambda(j)log lambda(j)]along with its distribution dependent versionlambda(epsilon) := argmin(lambda is an element of Lambda)[ P(l center dot f(lambda)) + epsilon Sigma(N)(j=1)lambda(j)log lambda(j)],where epsilon >= 0 is a regularization parameter. It is proved that the ""approximate sparsity"" of lambda(epsilon) implies the ""approximate sparsity"" of (lambda) over cap (epsilon) and the impact of ""sparsity"" oil bounding the excess risk of the empirical solution is explored. Similar results are also discussed in the case of entropy penalized density estimation."
"10.1214/08-AOS609","2009","Limit distribution theory for maximum likelihood estimation of a log-concave density","4","We find limiting distributions of the nonparametric maximum likelihood estimator (MLE) of a log-concave density, that is, a density of the form f(0) = exp phi(0) where phi(0) is a concave function on R. The pointwise limiting distributions depend on the second and third derivatives at 0 of H-k, the ""lower invelope"" of an integrated Brownian motion process minus a drift term depending on the number of vanishing derivatives of phi(0) = log f(0) at the point of interest. We also establish the limiting distribution of the resulting estimator of the mode M(f(0)) and establish a new local asymptotic minimax lower bound which shows the optimality Of Our mode estimator in terms of both rate of convergence and dependence of constants on Population values."
"10.1214/08-AOS598","2009","Dimension reduction for nonelliptically distributed predictors","3","Sufficient dimension reduction methods often require stringent conditions on the joint distribution of the predictor, or, when such conditions are not satisfied, rely on marginal transformation or reweighting to fulfill them approximately. For example, a typical dimension reduction method would require the predictor to have elliptical or even multivariate normal distribution. In this paper, we reformulate the commonly used dimension reduction methods, via the notion of ""central solution space,"" so as to circumvent the requirements of such strong assumptions, while at the same time preserve the desirable properties of the classical methods, such as root n-consistency and asymptotic normality. Imposing elliptical distributions or even stronger assumptions on predictors is often considered as the necessary tradeoff for overcoming the ""curse of dimensionality,"" but the development of this paper shows that this need not be the case. The new methods will be compared with existing methods by simulation and applied to a data set."
"10.1214/08-AOS608","2009","Consistency of restricted maximum likelihood estimators of principal components","1","[it this paper we consider two closely related problems: estimation of eigenvalues and eigenfunctions of the covariance kernel of functional data based on (possibly) irregular measurements, and the problem of estimating the eigenvalues and eigenvectors of the covariance matrix for high-dimensional Gaussian vectors. In [A geometric approach to maximum likelihood estimation of covariance kernel from sparse irregular longitudinal data (2007)], a restricted maximum likelihood (REML) approach has been developed to deal with the first problem. In this paper, we establish consistency and derive rate of convergence of the REML estimator for the functional data case, under appropriate smoothness conditions. Moreover, we prove that when the number of measurements per sample curve is bounded, under squared-error loss, the rate of convergence of the REML estimators of eigenfunctions is near-optimal. In the case of Gaussian vectors, asymptotic consistency and;in efficient score representation of the estimators are obtained under the assumption that the effective dimension grows at a rate slower than the sample size. These results are derived through an explicit utilization of the intrinsic geometry of the parameter space, which is non-Euclidean. Moreover, the results derived in this paper Suggest all asymptotic equivalence between the inference on functional data with dense measurements and that of the high-dimensional Gaussian vectors."
"10.1214/08-AOS606","2009","Consistency of {B}ayesian procedures for variable selection","1","It has long been known that for the comparison of pairwise nested models, a decision based on the Bayes factor produces I consistent model selector (in the frequentist sense). Here we go beyond the usual consistency for nested pairwise models, and show that for a wide class of prior distributions, including intrinsic priors, the corresponding Bayesian procedure for variable selection in normal regression is consistent in the entire class of normal linear models. We find that the asymptotics of the Bayes factors for intrinsic priors are equivalent to those of the Schwarz (BIC) criterion. Also, recall that the Jeffreys-Lindley paradox refers to the well-known fact that a point null hypothesis on the normal mean parameter is always accepted when the variance of the conjugate prior goes to infinity. This implies that some limiting forms of proper prior distributions are not necessarily Suitable for testing problems. Intrinsic priors are limits of proper prior distributions, and for finite sample sizes they have been proved to behave extremely well for variable selection in regression;,I consequence of our results is that for intrinsic priors Lindley's paradox does not arise."
"10.1214/08-AOS604","2009","Does median filtering truly preserve edges better than linear filtering?","0","Image processing researchers commonly assert that ""median filtering is better than linear filtering for removing noise in the presence of edges."" Using a straightforward large-n decision-theory framework, this folk-theorem is seen to be false in general. We show that median filtering and linear filtering have similar asymptotic worst-case mean-squared error (MSE) when the signal-to-noise ratio (SNR) is of order 1, which corresponds to the case of constant per-pixel noise level in a digital signal. To see dramatic benefits of median smoothing in an asymptotic setting, the per-pixel noise level should tend to zero (i.e., SNR should grow very large).We show that a two-stage median filtering using two very different window widths can dramatically outperform traditional linear and median filtering in settings where the underlying object has edges. In this two-stage procedure, the first pass, at a fine scale, aims at increasing the SNR. The second pass, at a coarser scale, correctly exploits the nonlinearity of the median.Image processing methods based on nonlinear partial differential equations (PDEs) are often said to improve on linear filtering in the presence of edges. Such methods seem difficult to analyze rigorously in a decision-theoretic framework. A popular example is mean curvature motion (MCM), which is formally a kind of iterated median filtering. Our results on iterated median filtering suggest that some PDE-based methods are candidates to rigorously outperform linear filtering in an asymptotic framework."
"10.1214/08-AOS601","2009","Asymptotics for spherical needlets","1","We investigate invariant random fields on the sphere using a new type of spherical wavelets, called needlets. These are compactly supported in frequency and enjoy excellent localization properties in real space, with quasi-exponentially decaying tails. We show that, for random fields on the sphere, the needlet coefficients are asymptotically uncorrelated for any fixed angular distance. This property is used to derive CLT and functional CLT converaence results for polynomial functionals of the needlet coefficients: here the asymptotic theory is considered in the high-frequency sense. Our proposals emerge from strong empirical motivations, especially in connection with the analysis of cosmological data sets."
"10.1214/08-AOS599","2009","New multi-sample nonparametric tests for panel count data","0","This paper considers the problem of multi-sample nonparametric comparison of counting processes with panel count data, which arise naturally when recurrent events are considered. Such data frequently occur in medical follow-up studies and reliability experiments, for example. For the problem considered, we construct two new classes of nonparametric test statistics based on the accumulated weighted differences between the rates of increase of the estimated mean functions of the counting processes over observation times, wherein the nonparametric maximum likelihood approach is used to estimate the mean function instead of the nonparametric maximum pseudo-likelihood. The asymptotic distributions of the proposed statistics are derived and their finite-sample properties are examined through Monte Carlo simulations. The simulation results show that the proposed methods work quite well and are more powerful than the existing test procedures. Two real data sets are analyzed and presented as illustrative examples."
"10.1214/07-AOS555","2009","Extending the scope of empirical likelihood","2","This article extends the scope of empirical likelihood methodology ill three directions: to allow for plug-in estimates Of nuisance parameters in estimating equations, slower than root n-rates of convergence, and settings in which there are a relatively large number of estimating equations compared to the sample size. Calibrating empirical likelihood confidence regions with plug-in is sometimes intractable due to the complexity of the asymptotics, so we introduce a bootstrap approximation that call be used in such situations. We provide a range of examples from survival analysis and nonparametric statistics to illustrate the main results."
"10.1214/08-AOS596","2009","Asymptotics in response-adaptive designs generated by a two-color, randomly reinforced urn","1","This paper illustrates asymptotic properties for a response-adaptive design generated by a two-color, randomly reinforced urn model. The design considered is optimal in the sense that it assigns patients to the best treatment, with probability converging to one. An approach to show the joint asymptotic normality of the estimators of the mean responses to the treatments is provided in spite of the fact that allocation proportions converge to zero and one. Results on the rate of convergence of the number of patients assigned to each treatment are also obtained. Finally, we study the asymptotic behavior of a suitable test statistic."
"10.1214/08-AOS593","2009","The {C}hernoff lower bound for symmetric quantum hypothesis testing","0","We consider symmetric hypothesis testing in quantum statistics, where the hypotheses are density operators on a finite-dimensional complex Hilbert space, representing states of a finite quantum system. We prove a lower bound on the asymptotic rate exponents of Bayesian error probabilities. The bound represents a quantum extension of the Chernoff bound, which gives the best asymptotically achievable error exponent in classical discrimination between two probability measures on a finite set. In our framework, the classical result is reproduced if the two hypothetic density operators commute. Recently, it has been shown elsewhere [Phys. Rev. Lett. 98 (2007) 1605041 that the lower bound is achievable also in the generic quantum (noncommutative) case. This implies that our result is one part of the definitive quantum Chernoff bound."
"10.1214/08-AOS597","2009","A nonmanipulable test","0","A test is said to control for type I error if it is unlikely to reject the data-generating process. However, if it is possible to produce stochastic processes at random such that, for all possible future realizations of the data, the selected process is unlikely to be rejected, then the test is said to be manipulable. So, a manipulable test has essentially no capacity to reject a strategic expert.Many tests proposed in the existing literature, including calibration tests, control for type I error but are manipulable. We construct a test that controls for type I error and is nonmanipulable."
"10.1214/07-AOS571","2009","Likelihood ratio tests and singularities","3","Many statistical hypotheses can be formulated in terms of polynomial equalities and inequalities in the unknown parameters and thus correspond to semi-algebraic Subsets of the parameter space. We consider large sample asymptotics for the likelihood ratio test of such hypotheses in models that satisfy standard probabilistic regularity conditions. We show that the assumptions of Chernoff's theorem hold for semi-algebraic sets such that the asymptotics are determined by the tangent cone at the true parameter point. At boundary points or singularities, the tangent cone need not be I linear space and limiting distributions Other than chi-square distributions may arise. While boundary points often lead to mixtures of chi-square distributions, Singularities give rise to nonstandard limits. We demonstrate that minima of chisquare random variables are important for locally identifiable models, and in a study of the factor analysis model with one factor, we reveal connections to eigenvalues of Wishart matrices."
"10.1214/08-AOS594","2009","Matrix representations and independencies in directed acyclic graphs","0","For a directed acyclic graph, there are two known criteria to decide whether any specific conditional independence statement is implied for all distributions factorized according to the given graph. Both criteria are based on special types of path in graphs. They are called separation criteria because independence holds whenever the conditioning set is a separating set in a graph theoretical sense. We introduce and discuss an alternative approach using binary matrix representations of graphs in which zeros indicate independence statements. A matrix condition is shown to give a new path criterion for separation and to be equivalent to each of the previous two path criteria."
"10.1214/08-AOS592","2009","Differentiability of {$t$}-functionals of location and scatter","0","The paper aims at finding widely and smoothly defined nonparametric location and scatter functionals. As a convenient vehicle, maximum likelihood estimation of the location vector mu and scatter matrix Sigma of an elliptically symmetric t distribution off R(d) with degrees of freedom nu > 1 extends to an M-functional defined off all probability distributions P in a weakly open, weakly dense domain U. Here U consists of P Putting not too much mass in hyperplanes of dimension < d, as shown for empirical measures by Kent and Tyler [Ann. Statist. 19 (1991) 2102-2119]. It will be seen here that (mu, Sigma) is analytic on U for the bounded Lipschitz norm, of for d = 1 for the sup norm on distribution functions. For k = 1, 2,..., and other norms, depending on k and more directly adapted to t functionals, one has continuous differentiability of order k, allowing the delta-method to be applied to (mu, Sigma) for any P in U, which can be arbitrarily heavy-tailed. These results imply asymptotic normality of the corresponding M-estimators (mu(n), Sigma(n)). In dimension d = 1 only, the t(nu) functional (mu, sigma) extends to be defined and weakly continuous at all P."
"10.1214/07-AOS587","2009","The formal definition of reference priors","1","Reference analysis produces objective Bayesian inference, in the sense that inferential statements depend only on the assumed model and the available data, and the prior distribution used to make an inference is least informative in a certain information-theoretic sense. Reference priors have been rigorously defined in specific contexts and heuristically defined in general, but a rigorous general definition has been lacking. We produce a rigorous general definition here and then show how an explicit expression for the reference prior can be obtained under very weak regularity conditions. The explicit expression can be used to derive new reference priors both analytically and numerically."
"10.1214/08-AOS595","2009","On surrogate loss functions and {$f$}-divergences","0","The goal of binary classification is to estimate a discriminant function gamma from observations of covariate vectors and corresponding binary labels. We consider an elaboration of this problem in which the covariates are not available directly but are transformed by a dimensionality-reducing quantizer Q. We present conditions on loss functions such that empirical risk minimization yields Bayes consistency when both the discriminant function and the quantizer are estimated. These conditions are stated in terms of a general correspondence between loss functions and a class of functionals known as Ali-Silvey or f-divergence functionals. Whereas this correspondence was established by Blackwell [Proc. 2nd Berkeley Symp. Probab. Statist. 1 (1951) 93-102. Univ. California Press, Berkeley] for the 0-1 loss, we extend the correspondence to the broader class of surrogate loss functions that play a key role in the general theory of Bayes consistency for binary classification. Our result makes it possible to pick out the (strict) subset of surrogate loss functions that yield Bayes consistency for joint estimation of the discriminant function and the quantizer."
"10.1214/07-AOS562","2009","Consistency of support vector machines for forecasting the evolution of an unknown ergodic dynamical system from observations with unknown noise","0","We consider the problem of forecasting the next (observable) state of an unknown ergodic dynamical system from a noisy observation of the present state. Our main result shows, for example, that Support vector machines (SVMs) using Gaussian RBF kernels can learn the best forecaster from a sequence of noisy observations if (a) the unknown observational noise process is bounded and has a summable alpha-mixing rate and (b) the unknown ergodic dynamical system is defined by a Lipschitz continuous function on some compact subset of R-d and has a summable decay of correlations for Lipschitz continuous functions. In order to prove this result we first establish a general consistency result for SVMs and all stochastic processes that satisfy a mixing notion that is substantially weaker than alpha-mixing."
"10.1214/07-AOS570","2009","Robust estimation for {ARMA} models","0","This paper introduces a new class of robust estimates for ARMA models. They are M-estimates, but the residuals are computed so the effect of one outlier is limited to the period where it occurs. These estimates are closely related to those based on a robust filter, but they have two important advantages: they are consistent and the asymptotic theory is tractable. We perform a Monte Carlo where we show that these estimates compare favorably with respect to standard M-estimates and to estimates based on a diagnostic procedure."
"10.1214/07-AOS579","2009","Estimating a concave distribution function from data corrupted with additive noise","0","We consider two nonparametric procedures for estimating a concave distribution function based on data corrupted with additive noise generated by a bounded decreasing density on (0, infinity). For the maximum likelihood (ML) estimator and least squares (LS) estimator, we state qualitative properties, prove consistency and propose a computational algorithm. For the LS estimator and its derivative, we also derive the pointwise asymptotic distribution. Moreover, the rate n(-2/5) achieved by the LS estimator is shown to be minimax for estimating the distribution function at a fixed point."
"10.1214/07-AOS564","2009","Inference for censored quantile regression models in longitudinal studies","0","We develop inference procedures for longitudinal data where some of the measurements are censored by fixed constants. We consider a semi-parametric quantile regression model that makes no distributional assumptions. Our research is motivated by the lack of proper inference procedures for data from biomedical studies where measurements are censored due to a fixed quantification limit. In such studies the focus is often on testing hypotheses about treatment equality. To this end, we propose a rank score test for large sample inference on a subset of the covariates. We demonstrate the importance of accounting for both censoring and intra-subject dependency and evaluate the performance of our proposed methodology in a simulation study. We then apply the proposed inference procedures to data from an AIDS-related clinical trial. We conclude that our framework and proposed methodology is very valuable for differentiating the influences of predictors at different locations in the conditional distribution of a response variable."
"10.1214/07-AOS589","2009","An {RKHS} formulation of the inverse regression dimension-reduction problem","1","Suppose that Y is a scalar and X is a second-order stochastic process, where Y and X are conditionally independent given the random variables xi(1), ..., xi(p) which belong to the closed span L-X(2) of X. This paper investigates a unified framework for the inverse regression dimension-reduction problem. It is found that the identification of L-X(2) with the reproducing kernel Hilbert space of X provides a platform for a seamless extension from the finite- to infinite-dimensional settings. It also facilitates convenient computational algorithms that can be applied to a variety of models."
"10.1214/07-AOS574","2009","The pseudo-marginal approach for efficient {M}onte {C}arlo computations","1","We introduce a powerful and flexible MCMC algorithm for stochastic simulation. The method builds on a pseudo-marginal method originally introduced in [Genetics 164 (2003) 1139-1160], showing how algorithms which are approximations to an idealized marginal algorithm, can share the same marginal stationary distribution as the idealized method. Theoretical results are given describing the convergence properties of the proposed method, and simple numerical examples are given to illustrate the promising empirical characteristics of the technique. Interesting comparisons with a more obvious, but inexact, Monte Carlo approximation to the marginal algorithm, are also given."
"10.1214/07-AOS580","2009","S{CAD}-penalized regression in high-dimensional partially linear models","0","We consider the problem of simultaneous variable selection and estimation in partially linear models with a divergent number of covariates in the linear part, under the assumption that the vector of regression coefficients is sparse. We apply the SCAD penalty to achieve sparsity in the linear part and use polynomial splines to estimate the nonparametric component. Under reasonable conditions, it is shown that consistency in terms of variable selection and estimation can be achieved simultaneously for the linear and nonparametric components. Furthermore, the SCAD-penalized estimators of the nonzero coefficients are shown to have the asymptotic oracle property, in the sense that it is asymptotically normal with the same means and covariances that they would have if the zero coefficients were known in advance. The finite sample behavior of the SCAD-penalized estimators is evaluated with simulation and illustrated with a data set."
"10.1214/07-AOS573","2009","Gaussian model selection with an unknown variance","0","Let Y be a Gaussian vector whose components are independent with a common unknown variance. We consider the problem of estimating the mean p of Y by model selection. More precisely, we start with a collection S = {S(m), m is an element of M} of linear subspaces of R(n) and associate to each of these the least-squares estimator of mu on S(m). Then, we use a data driven penalized criterion in order to select one estimator among these. Our first objective is to analyze the performance of estimators associated to classical criteria such as FPE, AIC, BIC and AMDL. Our second objective is to propose better penalties that are versatile enough to take into account both the complexity of the collection S and the sample size. Then we apply those to solve various statistical problems such as variable selection, change point detections and signal estimation among others. Our results are based on a nonasymptotic risk bound with respect to the Euclidean loss for the selected estimator. Some analogous results are also established for the Kullback loss."
"10.1214/07-AOS586","2009","An adaptive step-down procedure with proven {FDR} control under independence","4","In this work we study an adaptive step-down procedure for testing m hypotheses. It stems from the repeated use of the false discovery rate controlling the linear step-up procedure (sometimes called BH), and makes use of the critical constants iq/[(m + 1 - i (1 - q)], i = 1,..., m. Motivated by its success as a model selection procedure, as well as by its asymptotic optimality, we are interested in its false discovery rate (FDR) controlling properties for a finite number of hypotheses. We prove this step-down procedure controls the FDR at level q for independent test statistics. We then numerically compare it with two other procedures with proven FDR control under independence, both in terms of power under independence and FDR control under positive dependence."
"10.1214/07-AOS569","2009","On the false discovery rate and an asymptotically optimal rejection curve","4","In this paper we introduce and investigate a new rejection curve for asymptotic control of the false discovery rate (FDR) in multiple hypotheses testing problems. We first give a heuristic motivation for this new curve and propose some procedures related to it. Then we introduce a set of possible assumptions and give a unifying short proof of FDR control for procedures based on Simes' critical values, whereby certain types of dependency are allowed. This methodology of proof is then applied to other fixed rejection curves including the proposed new curve. Among others, we investigate the problem of finding least favorable parameter configurations such that the FDR becomes largest. We then derive a series of results concerning asymptotic FDR control for procedures based on the new curve and discuss several example procedures in more detail. A main result will be an asymptotic optimality statement for various procedures based on the new curve in the class of fixed rejection curves. Finally, we briefly discuss strict FDR control for a finite number of hypotheses."
"10.1214/07-AOS538","2009","A data-driven block thresholding approach to wavelet estimation","0","A data-driven block thresholding procedure for wavelet regression is proposed and its theoretical and numerical properties are investigated. The procedure empirically chooses the block size and threshold level at each resolution level by minimizing Stein's unbiased risk estimate. The estimator is sharp adaptive over a class of Besov bodies and achieves simultaneously within a small constant factor of the minimax risk over a wide collection of Besov Bodies including both the ""dense"" and ""sparse"" cases. The procedure is easy to implement. Numerical results show that it has, superior finite sample performance in comparison to the other leading wavelet thresholding estimators."
"10.1214/00-AOS576","2009","A universal procedure for aggregating estimators","0","In this paper we study the aggregation problem that can be formulated as follows. Assume that we have a family of estimators F built on the basis of available observations. The goal is to construct a new estimator whose risk is as close as possible to that of the best estimator in the family. We propose a general aggregation scheme that is universal in the following sense: it applies for families of arbitrary estimators and a wide variety of models and global risk measures. The procedure is based on comparison of empirical estimates of certain linear functionals with estimates induced by the family F. We derive oracle inequalities and show that they are unimprovable in some sense. Numerical results demonstrate good practical behavior of the procedure."
"10.1214/07-AOS560","2009","Support points of locally optimal designs for nonlinear models with two parameters","2","We propose a new approach for identifying the support points of a locally optimal design when the model is a nonlinear model. In contrast to the commonly used geometric approach, we use an approach based on algebraic tools. Considerations are restricted to models with two parameters, and the general results are applied to often used special cases, including logistic, probit, double exponential and double reciprocal models for binary data, a loglinear Poisson regression model for count data, and the Michaelis-Men ten model. The approach, which is also of value for multi-stage experiments, works both with constrained and unconstrained design regions and is relatively easy to implement."
"10.1214/07-AOS585","2009","Empirical likelihood for estimating equations with missing values","1","We consider an empirical likelihood inference for parameters defined by general estimating equations when some components of the random observations are subject to missingness. As the nature of the estimating equations is wide-ranging, we propose a nonparametric imputation of the missing values from a kernel estimator of the conditional distribution of the missing variable given the always observable variable. The empirical likelihood is used to construct a profile likelihood for the parameter of interest. We demonstrate that the proposed nonparametric imputation can remove the selection bias in the missingness and the empirical likelihood leads to more efficient parameter estimation. The proposed method is further evaluated by simulation and an empirical study on a genetic dataset on recombinant inbred mice."
"10.1214/07-AOS572","2009","Asymptotic inference for semiparametric association models","1","Association models for a pair of random elements X and Y (e.g., vectors) are considered which specify the odds ratio function up to an unknown parameter theta. These models are shown to be semiparametric in the sense that they do not restrict the marginal distributions of X and Y. Inference for the odds ratio parameter theta may be obtained from sampling either Y conditionally on X or vice versa. Generalizing results from Prentice and Pyke, Weinberg and Wacholder and Scott and Wild, we show that asymptotic inference for theta under sampling conditional oil Y is the same as if sampling had been conditional on X. Common regression models, for example, generalized linear models with canonical link or multivariate linear, respectively, logistic models, are association models where the regression parameter beta is closely related to the odds ratio parameter theta. Hence inference for beta may be drawn from samples conditional oil Y using an association model."
"10.1214/07-AOS561","2009","Statistical inference for semiparametric varying-coefficient partially linear models with error-prone linear covariates","1","We study semiparametric varying-coefficient partially linear models when some linear covariates are not observed, but ancillary variables are available. Semiparametric profile least-square based estimation procedures are developed for parametric and nonparametric components after we calibrate the error-prone covariates. Asymptotic properties of the proposed estimators are established. We also propose the profile least-square based ratio test and Wald test to identify significant parametric and nonparametric components. To improve accuracy of the proposed tests for small or moderate sample sizes, a wild bootstrap version is also proposed to calculate the critical values. Intensive simulation experiments are conducted to illustrate the proposed approaches."
"10.1214/07-AOS554","2009","Proportional hazards models with continuous marks","0","For time-to-event data with finitely many competing risks, the proportional hazards model has been a popular tool for relating the cause-specific outcomes to covariates [Prentice et al. Biometrics 34 (1978) 541-554]. This article studies an extension of this approach to allow a continuum of competing risks, in which the cause of failure is replaced by a continuous mark only observed at the failure time. We develop inference for the proportional hazards model in which the regression parameters depend nonparametrically on the mark and the baseline hazard depends nonparametrically on both time and mark. This work is motivated by the need to assess HIV vaccine efficacy, while taking into account the genetic divergence of infecting HIV viruses in trial participants from the HIV strain that is contained in the vaccine, and adjusting for covariate effects. Mark-specific vaccine efficacy is expressed in terms of one of the regression functions in the mark-specific proportional hazards model. The new approach is evaluated in simulations and applied to the first HIV vaccine efficacy trial."
"10.1214/07-AOS578","2009","A pseudo empirical likelihood approach for stratified samples with nonresponse","0","Nonresponse is common in Surveys. When the response probability of a survey variable Y depends on Y through ail observed auxiliary categorical variable Z (i.e., the response probability of Y is conditionally independent of Y given Z), a simple method often used in practice is to use Z categories as imputation cells and construct estimators by imputing nonrespondents or reweighting respondents within each imputation cell. This simple method, however, is inefficient when some Z categories have small sizes Laid ad hoc methods are often applied to collapse small imputation cells. Assuming a parametric model on the conditional probability of Z given Y and a nonparametric model oil the distribution of Y, we develop a pseudo empirical likelihood method to provide more efficient survey estimators. Our method avoids any ad hoc collapsing small Z categories, since reweighting or imputation is done across Z categories. Asymptotic distributions for estimators of population means based on the pseudo empirical likelihood method are derived. For variance estimation, we consider a bootstrap procedure and its consistency is established. Some simulation results are provided to assess the finite sample performance of the proposed estimators."
"10.1214/07-AOS567","2009","A note on the stationary bootstrap's variance","1","Because the stationary bootstrap resamples data blocks of random length, this method has been thought to have the largest asymptotic variance among block bootstraps Lahiri [Ann. Statist. 27 (1999) 386-404]. It is shown here that the variance of the stationary bootstrap surprisingly matches that of a block bootstrap based on nonrandom, nonoverlapping blocks. This argument translates the variance expansion into the frequency domain and provides a unified way of determining variances for other block bootstraps. Some previous results on the stationary bootstrap, related to asymptotic relative efficiency and optimal block size, are also updated."
"10.1214/07-AOS557","2009","Robustness of multiple testing procedures against dependence","3","An important aspect of multiple hypothesis testing is controlling the significance level, or the level of Type I error. When the test statistics are not independent it can be particularly challenging to deal with this problem, without resorting to very conservative procedures. In this paper we show that, in the context of contemporary multiple testing problems, where the number of tests is often very large, the difficulties caused by dependence are less serious than in classical cases. This is particularly true when the null distributions of test statistics are relatively light-tailed, for example, when they can be based on Normal or Student's t approximations. There, if the test statistics can fairly be viewed as being generated by a linear process, an analysis founded on the incorrect assumption of independence is asymptotically correct as the number of hypotheses diverges. In particular, the point process representing the null distribution of the indices at which statistically significant test results occur is approximately Poisson, just as in the case of independence. The Poisson process also has the same mean as in the independence case, and of course exhibits no clustering of false discoveries. However, this result can fail if the null distributions are particularly heavy-tailed. There clusters of statistically significant results can occur, even when the null hypothesis is correct. We give an intuitive explanation for these disparate properties in light- and heavy-tailed cases, and provide rigorous theory underpinning the intuition."
"10.1214/07-AOS588","2009","Propagation of outliers in multivariate data","1","We investigate the performance of robust estimates of multivariate location under nonstandard data contamination models such as componentwise outliers (i.e., contamination in each variable is independent from the other variables). This model brings up a possible new source of statistical error that we call ""propagation of outliers."" This source of error is Unusual in the sense that it is generated by the data processing itself and takes place after the data has been collected. We define and derive the influence function of robust multivariate location estimates under flexible contamination models and use it to investigate the effect of propagation of outliers. Furthermore, we show that standard high-breakdown affine equivariant estimators propagate outliers and therefore show poor breakdown behavior under componentwise contamination when the dimension d is high."
"10.1214/07-AOS551","2009","Inference for the limiting cluster size distribution of extreme values","0","Any limiting point process for the time normalized exceedances of high levels by a stationary sequence is necessarily compound Poisson under appropriate long range dependence conditions. Typically exceedances appear in clusters. The underlying Poisson points represent the cluster positions and the multiplicities correspond to the cluster sizes. In the present paper we introduce estimators of the limiting Cluster size probabilities, which are constructed through a recursive algorithm. We derive estimators of the extremal index which plays a key role in determining the intensity of cluster positions. We study the asymptotic properties of the estimators and investigate their finite sample behavior on simulated data."
"10.1214/07-AOS582","2009","Lasso-type recovery of sparse representations for high-dimensional data","11","The Lasso is an attractive technique for regularization and variable selection for high-dimensional data, where the number of predictor variables p(n) is potentially much larger than the number of samples n. However, it was recently discovered that the sparsity pattern of the Lasso estimator can only be asymptotically identical to the true sparsity pattern if the design matrix satisfies the so-called irrepresentable condition. The latter condition can easily be violated in the presence of highly correlated variables.Here we examine the behavior of the Lasso estimators if the irrepresentable condition is relaxed. Even though the Lasso cannot recover the correct sparsity pattern, we show that the estimator is still consistent in the l(2)-norm sense for fixed designs under conditions on (a) the number s(n) Of nonzero components of the vector beta(n) and (b) the minimal singular values of design matrices that are induced by selecting small subsets of variables. Furthermore, a rate of convergence result is obtained on the l(2) error with an appropriate choice of the smoothing parameter. The rate is shown to be optimal under the condition of bounded maximal and minimal sparse eigenvalues. Our results imply that, with high probability, all important variables are selected. The set of selected variables is a meaningful reduction on the original set of variables. Finally, our results are illustrated with the detection of closely adjacent frequencies, a problem encountered in astrophysics."
"10.1214/07-AOS550","2009","Monte {C}arlo maximum likelihood estimation for discretely observed diffusion processes","1","This paper introduces a Monte Carlo method for maximum likelihood inference in the context of discretely observed diffusion processes. The method gives unbiased and a.s. continuous estimators of the likelihood function for a family of diffusion models aid its performance in numerical examples is computationally efficient. It uses a recently developed technique for the exact simulation of diffusions, and involves no discretization error. We show that, under regularity conditions, the Monte Carlo MLE converges a.s. to the true MLE. For datasize n -> infinity, we show that the number of Monte Carlo iterations should be tuned as O (n(1/2)) and we demonstrate the consistency properties of the Monte Carlo MLE as an estimator of the true parameter value."
"10.1214/07-AOS568","2009","Testing for jumps in a discretely observed process","4","We propose a new test to determine whether jumps are present in asset returns or other discretely sampled processes. As the sampling interval tends to 0, our test statistic converges to I if there are jumps, and to another deterministic and known value (such as 2) if there are no jumps. The test is valid 4 for all Ito semi martingales, depends neither on the law of the process nor on the coefficients of the equation which it solves, does not require a preliminary estimation of these coefficients, and when there are jumps the test is applicable whether jumps have finite or infinite-activity and for an arbitrary Blumenthal-Getoor index. We finally implement the test on simulations and asset returns data."
"10.1214/07-AOS558","2009","Consistencies and rates of convergence of jump-penalized least squares estimators","2","We study the asymptotics for jump-penalized least squares regression aiming at approximating a regression function by piecewise constant functions. Besides conventional consistency and convergence rates of the estimates in L(2)([0, 1)) our results cover other metrics like Skorokhod metric on the space of cadlag functions and uniform metrics on C([0, 1]). We will show that these estimators are in an adaptive sense rate optimal over certain classes of ""approximation spaces."" Special cases are the class of functions of bounded variation (piecewise) Holder continuous functions of order 0 < alpha <= 1 and the class of step functions with a finite but arbitrary number of jumps. In the latter setting, we will also deduce the rates known from change-point analysis for detecting the jumps. Finally, the issue of fully automatic selection of the smoothing parameter is addressed."
"10.1214/07-AOS565","2009","Minimum distance regression model checking with {B}erkson measurement errors","0","Lack-of-fit testing of a regression model with Berkson measurement error has not been discussed in the literature to date. To fill this void, we propose a class of tests based on minimized integrated square distances between nonparametric regression function estimator and the parametric model being fitted. We prove asymptotic normality of these test statistics under the null hypothesis and that of the corresponding minimum distance estimators under minimal conditions on the model being fitted. We also prove consistency of the proposed tests against a class of fixed alternatives and obtain their asymptotic power against a class of local alternatives orthogonal to the null hypothesis. These latter results are new even when there is no measurement error. A simulation that is included shows very desirable finite sample behavior of the proposed inference procedures."
"10.1214/07-AOS553","2009","Quantile pyramids for {B}ayesian nonparametrics","1","Polya trees fix partitions and use random probabilities in order to construct random probability measures. With quantile pyramids we instead fix probabilities and use random partitions. For nonparametric Bayesian inference we use a prior which supports piecewise linear quantile functions, based on the need to work with a finite set of partitions, yet we show that the limiting version of the prior exists. We also discuss and investigate an alternative model based on the so-called substitute likelihood, Both approaches factorize in a convenient way leading to relatively straightforward analysis via MCMC, since analytic summaries of posterior distributions are too complicated. We give conditions securing the existence of an absolute continuous quantile process, and discuss consistency and approximate normality for the sequence of posterior distributions. Illustrations are included."
"10.1214/07-AOS552","2009","Functional deconvolution in a periodic setting: uniform case","5","We extend deconvolution in a periodic setting to deal with functional data. The resulting functional deconvolution model can be viewed as a generalization of a multitude of inverse problems in mathematical physics where one needs to recover initial or boundary conditions on the basis of observations from a noisy solution of a partial differential equation. In the case when it is observed at a finite number of distinct points, the proposed functional deconvolution model can also be viewed as a multichannel deconvolution model.We derive minimax lower bounds for the L-2-risk in the proposed functional deconvolution model when f(.) is assumed to belong to a Besov ball and the blurring function is assumed to possess some smoothness properties, including both regular-smooth and super-smooth convolutions. Furthermore, we propose an adaptive wavelet estimator of f (.) that is asymptotically optimal (in the minimax sense), or near-optimal within a logarithmic factor, in a wide range of Besov balls.In addition, we consider a discretization of the proposed functional deconvolution model and investigate when the availability of continuous data gives advantages over observations at the asymptotically large number of points. As an illustration, we discuss particular examples for both continuous and discrete settings."
"10.1214/07-AOS563","2009","Smoothing splines estimators for functional linear regression","6","The paper considers functional linear regression, where scalar responses Y(1), ... , Y(n) are modeled in dependence of random functions X(1), ... , X(n). We propose a smoothing splines estimator for the functional slope parameter based on a slight modification of the usual penalty. Theoretical analysis concentrates on the error in all out-of-sample prediction of the response for a new random function X(n+1). It is shown that rates of convergence of the prediction error depend on the smoothness of the slope function and on the Structure Of the predictors. We then prove that these rates are optimal in the sense that they are minimax over large classes of possible slope functions and distributions of the predictive curves. For the case of models with errors-in-variables the smoothing spline estimator is modified by using a denoising correction of the covariance matrix of discretized curves. The methodology is then applied to a real case study where the aim is to predict the maximum of the concentration of ozone by using the curve of this concentration measured the preceding day."
"10.1214/07-AOS516","2009","Common functional principal components","5","Functional principal component analysis (FPCA) based on the Karhunen-Loeve decomposition has been successfully applied in many applications, mainly for one sample problems. In this paper we consider common functional principal components for two sample problems. Our research is motivated not only by the theoretical challenge of this data situation, but also by the actual question of dynamics of implied volatility (IV) functions. For different maturities the log-returns of IVs are samples of (smooth) random functions and the methods proposed here study the similarities of their stochastic behavior. First we present a new method for estimation of functional principal components from discrete noisy data. Next we present the two sample inference for FPCA and develop the two sample theory. We propose bootstrap tests for testing the equality of eigenvalues, eigenfunctions, and mean functions of two functional samples, illustrate the test-properties by simulation study and apply the method to the TV analysis."
"10.1214/08-AOS628","2008","Inference for eigenvalues and eigenvectors of {G}aussian symmetric matrices","1","This article presents maximum likelihood estimators (MLEs) and log-likelihood ratio (LLR) tests for the eigenvalues and eigenvectors of Gaussian random symmetric matrices of arbitrary dimension, where the observations are independent repeated samples from one or two populations. These inference problems are relevant in the analysis of diffusion tensor imaging data and polarized cosmic background radiation data, where the observations are, respectively, 3 x 3 and 2 x 2 symmetric positive definite matrices. The parameter sets involved in the inference problems for eigenvalues and eigenvectors are subsets of Euclidean space that are either affine subspaces, embedded submanifolds that are invariant under orthogonal transformations or polyhedral convex cones. We show that for a class of sets that includes the ones considered in this paper, the MLEs of the mean parameter do not depend on the covariance parameters if and only if the covariance structure is orthogonally invariant. Closed-form expressions for the MLEs and the associated LLRs are derived for this covariance structure."
"10.1214/07-AOS583","2008","Statistical eigen-inference from large {W}ishart matrices","2","We consider settings where the observations are drawn from a zero-mean multivariate (real or complex) normal distribution with the population covariance matrix having eigenvalues of arbitrary multiplicity. We assume that the eigenvectors of the population covariance matrix are unknown and focus on inferential procedures that are based on the sample eigenvalues alone (i.e., ""eigen-inference"").Results found in the literature establish the asymptotic normality of the fluctuation in the trace of powers of the sample covariance matrix. We develop concrete algorithms for analytically computing the limiting quantities and the covariance of the fluctuations. We exploit the asymptotic normality of the trace of powers of the sample covariance matrix to develop eigenvalue-based procedures for testing and estimation. Specifically. we formulate a Simple test of hypotheses for the population eigenvalues and a technique for estimating the population eigenvalues in settings where the cumulative distribution function of the (nonrandom) population eigenvalues has a staircase structure.Monte Carlo simulations are used to demonstrate the superiority of the proposed methodologies over classical techniques and the robustness of the proposed techniques in high-dimensional, (relatively) small sample size settings. The improved performance results from the fact that the proposed inference procedures are ""global"" (in a sense that we describe) and exploit ""global"" information thereby overcoming the inherent biases that cripple classical inference procedures which are ""local"" and rely on ""local"" information."
"10.1214/08-AOS619","2008","Flexible covariance estimation in graphical {G}aussian models","3","In this paper, we propose a class of Bayes estimators for the covariance matrix of graphical Gaussian models Markov with respect to a decomposable graph G. Working with the W(PG) family defined by Letac and Massam [Ann. Statist. 35 (2007) 1278-1323] we derive closed-form expressions for Bayes estimators under the entropy and squared-error losses. The W(PG) family includes the classical inverse of the hyper inverse Wishart but has many more shape parameters, thus allowing for flexibility in differentially shrinking various parts of the covariance matrix. Moreover, using this family avoids recourse to MCMC, often infeasible in high-dimensional problems. We illustrate the performance of our estimators through a collection of numerical examples where we explore frequentist risk properties and the efficacy of graphs in the estimation of high-dimensional covariance structures."
"10.1214/08-AOS618","2008","Finite sample approximation results for principal component analysis: a matrix perturbation approach","3","Principal component analysis (PCA) is a standard tool for dimensional reduction of a set of n observations (samples), each with p variables. In this paper, using a matrix perturbation approach, we study the nonasymptotic relation between the eigenvalues and eigenvectors of PCA computed on a finite sample of size n, and those of the limiting population PCA as n -> infinity. As in machine learning, we present a finite sample theorem which holds with high probability for the closeness between the leading eigenvalue and eigenvector of sample PCA and population PCA under a spiked covariance model. In addition, we also consider the relation between finite sample PCA and the asymptotic results in the joint limit p, n -> infinity, with p/n = c. We present a matrix perturbation view of the ""phase transition phenomenon,"" and a simple linear-algebra based derivation of the eigenvalue and eigenvector overlap in this asymptotic limit. Moreover, our analysis also applies for finite p, n where we show that although there is no sharp phase transition as in the infinite case, either as a function of noise level or as a function of sample size n, the eigenvector of sample PICA may exhibit a sharp ""loss of tracking,"" suddenly losing its relation to the (true) eigenvector of the population PCA matrix. This occurs due to a crossover between the eigenvalue due to the signal and the largest eigenvalue due to noise, whose eigenvector points in a random direction."
"10.1214/07-AOS581","2008","Spectrum estimation for large dimensional covariance matrices using random matrix theory","3","Estimating the eigenvalues of a population covariance matrix from a sample covariance matrix is a problem of fundamental importance in multivariate statistics; the eigenvalues of covariance matrices play a key role in many widely used techniques, in particular in principal component analysis (PCA). In many modern data analysis problems, statisticians are faced with large datasets where the sample size, n, is of the same order of magnitude as the number of variables p. Random matrix theory predicts that in this context, the eigenvalues of the sample covariance matrix are not good estimators of the eigenvalues of the population covariance.We propose to use a fundamental result in random matrix theory, the Marcenko-Pastur equation, to better estimate the eigenvalues of large dimensional covariance matrices. The Marcenko-Pastur equation holds in very wide generality and under weak assumptions. The estimator we obtain can be thought of as ""shrinking"" in a nonlinear fashion the eigenvalues of the sample covariance matrix to estimate the population eigenvalues. Inspired by ideas of random matrix theory, we also suggest a change of point of view when thinking about estimation of high-dimensional vectors: we do not try to estimate directly the vectors but rather a probability measure that describes them. We think this is a theoretically more fruitful way to think about these problems.Our estimator gives fast and good or very good results in extended simulations. Our algorithmic approach is based on convex optimization. We also show that the proposed estimator is consistent."
"10.1214/07-AOS559","2008","Operator norm consistent estimation of large-dimensional sparse covariance matrices","15","Estimating covariance matrices is a problem of fundamental importance in multivariate statistics. In practice it is increasingly frequent to work with data matrices X of dimension if x p, where p and n are both large. Results from random matrix theory show very clearly that in this setting, standard estimators like the sample covariance matrix perform in general very poorly.In this ""large n, large p"" setting, it is sometimes the case that practitioners are willing to assume that many elements of the population covariance matrix are equal to 0, and hence this matrix is sparse. We develop an estimator to handle this situation. The estimator is shown to be consistent in operator norm, when, for instance, we have p asymptotic to n as n -> infinity. In other words the largest singular value of the difference between the estimator and the population covariance matrix goes to zero. This implies consistency of all the eigenvalues and consistency of eigenspaces associated to isolated eigenvalues.We also propose a notion of sparsity for matrices, that is, ""compatible"" with spectral analysis and is independent of the ordering of the variables."
"10.1214/08-AOS605","2008","Multivariate analysis and {J}acobi ensembles: largest eigenvalue, {T}racy-{W}idom limits and rates of convergence","1","Let A and B be independent, central Wishart matrices in p variables with common covariance and having in and n degrees of freedom, respectively. The distribution of the largest eigenvalue of (A + B)(-1) B has numerous applications in multivariate statistics, but is difficult to calculate exactly. Suppose that in and n grow in proportion to p. We show that after centering and scaling, the distribution is approximated to second-order, O(p(-2/3)), by the Tracy-Widom law. The results are obtained for both complex and then real-valued data by using methods of random matrix theory to study the largest eigenvalue of the Jacobi unitary and orthogonal ensembles. Asymptotic approximations of Jacobi polynomials near the largest zero play a central role."
"10.1214/07-AOS504","2008","High-dimensional classification using features annealed independence rules","11","Classification using high-dimensional features arises frequently in many contemporary statistical studies such as tumor classification using microarray or other high-throughput data. The impact of dimensionality on classifications is poorly understood. In a seminal paper, Bickel and Levina [Bernoulli 10 (2004) 989-1010] show that the Fisher discriminant performs poorly due to diverging spectra and they propose to use the independence rule to overcome the problem. We first demonstrate that even for the independence classification rule, classification using all the features can be as poor as the random guessing due to noise accumulation in estimating population centroids in high-dimensional feature space. In fact, we demonstrate further that almost all linear discriminants can perform as poorly as the random guessing. Thus, it is important to select a subset of important features for high-dimensional classification, resulting in Features Annealed Independence Rules (FAIR). The conditions under which all the important features can be selected by the two-sample t-statistic are established. The choice of the optimal number of features, or equivalently, the threshold value of the test statistics are proposed based on an upper bound of the classification error. Simulation studies and real data analysis support our theoretical results and demonstrate convincingly the advantage of our new classification procedure."
"10.1214/08-AOS600","2008","Covariance regularization by thresholding","19","This paper considers regularizing a covariance matrix of p variables estimated from it observations, by hard thresholding. We show that the thresholded estimate is consistent in the operator norm as long as the true covariance matrix is sparse in a suitable sense, the variables are Gaussian or sub-Gaussian, and (log p)/n -> 0, and obtain explicit rates. The results are uniform over families of covariance matrices which satisfy a fairly natural notion of sparsity. We discuss an intuitive resampling scheme for threshold selection and prove a general cross-validation result that justifies this approach. We also compare thresholding to other covariance estimators in simulations and on an example from climate data."
"10.1214/07-AOS503","2008","A {CLT} for regularized sample covariance matrices","0","We consider the spectral properties of a class of regularized estimators of (large) empirical covariance matrices corresponding to stationary (but not necessarily Gaussian) sequences, obtained by banding. We prove a law of large numbers (similar to that proved in the Gaussian case by Bickel and Levina), which implies that the spectrum of a banded empirical covariance matrix is an efficient estimator. Our main result is a central limit theorem in the same regime, which to our knowledge is new, even in the Gaussian setup."
"10.1214/07-AOS540","2008","Stein estimation for the drift of {G}aussian processes using the {M}alliavin calculus","0","We consider the nonparametric functional estimation of the drift of a Gaussian process via minimax and Bayes estimators. In this context, we construct superefficient estimators of Stein type for such drifts using the Malliavin integration by parts formula and superharmonic functionals on Gaussian space. Our results are illustrated by numerical simulations and extend the construction of James-Stein type estimators for Gaussian processes by Berger and Wolpert [J. Multivariate Anal. 13 (1983) 401-424]."
"10.1214/07-AOS545","2008","Multiple local {W}hittle estimation in stationary systems","0","Moving from univariate to bivariate jointly dependent long-memory time series introduces a phase parameter (gamma), at the frequency of principal interest. zeros for short-memory series gamma = 0 automatically. The latter case has also been stressed under long memory, along with the ""fractional differencing"" case gamma = (delta(2) - delta(1))pi/2, where delta(1), delta(2) are the memory parameters of the two series. We develop time domain conditions under which these are and are not relevant, and relate the consequent properties of cross-autocovariances to ones of the (possibly bilateral) moving average representation which. with martingale difference innovations of arbitrary dimension, is used in asymptotic theory for local Whittle parameter estimates depending on a single smoothing, number. Incorporating also a regression parameter (beta) which, when nonzero, indicates cointegration, the consistency proof of these implicitly defined estimates is nonstandard due to the beta estimate converging faster than the others. We also establish joint asymptotic normality of the estimates, and indicate how this Outcome can apply in statistical inference on several questions of interest. Issues of implemention are discussed, along with implications of knowing beta and of correct or incorrect specification of gamma, and possible extensions to higher-dimensional systems and nonstationary series."
"10.1214/07-AOS549","2008","Tilted {E}uler characteristic densities for central limit random fields, with application to ``bubbles''","0","Local increases in the mean of a random field are detected (conservatively) by thresholding a field of test statistics at a level u chosen to control the tail probability or p-value of its maximum. This p-value is approximated by the expected Euler characteristic (EC) of the excursion set of the test statistic field above u, denoted E phi (A(u)). Under isotropy, one can use the expansion E phi(A(u)) = Sigma(k) V(k rho k()u), where V-k is an intrinsic volume of the parameter space and rho(k) is an EC density of the field. EC densities are available for a number of processes, mainly those constructed from (multivariate) Gaussian fields via smooth functions. Using saddlepoint methods, we derive an expansion for (rho k)(u) for fields which are only approximately Gaussian, but for which higher-order cumulants are available. We focus on linear combinations of n independent non-Gaussian fields, whence a Central Limit theorem is in force. The threshold u is allowed to grow with the sample size n, in which case our expression has a smaller relative asymptotic error than the Gaussian EC density. Several illustrative examples including an application to ""bubbles"" data accompany the theory."
"10.1214/07-AOS543","2008","Residual empirical processes for long and short memory time series","0","This paper studies the residual empirical process of long- and short-memory time series regression models and establishes its uniform expansion under a general framework. The results are applied to the stochastic regression models and unstable autoregressive models. For the long-memory noise, it is shown that the limit distribution of the Kolmogorov-Smimov test statistic studied in Ho and Hsing [Ann. Statist. 24 (1996) 992-1024] does not hold when the stochastic regression model includes an unknown intercept or when the characteristic polynomial of the unstable autoregressive model has a unit root. To this end, two new statistics are proposed to test for the distribution of the long-memory noises of stochastic regression models and unstable autoregressive models."
"10.1214/07-AOS542","2008","Evaluation of formal posterior distributions via {M}arkov chain arguments","0","We consider evaluation of proper posterior distributions obtained from improper prior distributions. Our context is estimating a bounded function phi of a parameter when the loss is quadratic. If the posterior mean of 0 is admissible for all bounded phi, the posterior is strongly admissible. We give sufficient conditions for strong admissibility. These conditions involve the recurrence of a Markov chain associated with the estimation problem. We develop general sufficient conditions for recurrence of general state space Markov chains that are also of independent interest. Our main example concerns the p-dimensional multivariate normal distribution with mean vector 0 when the prior distribution has the form g(parallel to theta parallel to(2)) d theta on the parameter space RP. Conditions on g for strong admissibility of the posterior are provided."
"10.1214/07-AOS532","2008","An algorithmic and a geometric characterization of coarsening at random","0","We show that the class of conditional distributions satisfying the coarsening at random (CAR) property for discrete data has a simple and robust algorithmic description based oil randomized uniform multicovers: combinatorial objects generalizing the notion of partition of a set. However, the complexity of a given CAR mechanism can be large: the maximal ""height"" of the needed multicovers can be exponential in the number of points, in the sample space. The results stein from a geometric interpretation of the set of CAR distributions as a convex polytope and a characterization of its extreme points. The hierarchy of CAR models defined in this way could be useful in parsimonious statistical modeling of CAR mechanisms, though the results also raise doubts in applied work as to the meaningfulness of the CAR assumption in its full generality."
"10.1214/07-AOS528","2008","On the {B}ehrens-{F}isher problem: a globally convergent algorithm and a finite-sample study of the {W}ald, {LR} and {LM} tests","0","In this paper we provide a provably convergent algorithm for the multi-variate Gaussian Maximum Likelihood version of the Behrens-Fisher Problem. Our work builds upon a formulation of the log-likelihood function proposed by Buot and Richards [5]. Instead of focusing on the first order optimality conditions, the algorithm aims directly for the maximization of the log-likelihood function itself to achieve a global solution. Convergence proof and complexity estimates are provided for the algorithm. Computational experiments illustrate the applicability of such methods to high-dimensional data. We also discuss how to extend the proposed methodology to a broader class of problems.We establish a systematic algebraic relation between the Wald, Likelihood Ratio and Lagrangian Multiplier Test (W >= LR >= LM) in the context of the Behrens-Fisher Problem. Moreover, we use our algorithm to computationally investigate the finite-sample size and power of the Wald, Likelihood Ratio and Lagrange Multiplier Tests, which previously were only available through asymptotic results. The methods developed here are applicable to much higher dimensional settings than the ones available in the literature. This allows us to better capture the role of high dimensionality oil the actual size and power of the tests for finite samples."
"10.1214/07-AOS514","2008","Limit theorems for weighted samples with applications to sequential {M}onte {C}arlo methods","1","In the last decade, sequential Monte Carlo methods (SMC) emerged as a key tool in computational statistics [see, e.g., Sequential Monte Carlo Methods in Practice (2001) Springer, New York, Monte Carlo Stratergies in Scientific Computing (2001) Springer, New York, Complex Stochastic systems (2001) 109-173]. These algorithms approximate a sequence of distributions by a sequence of weighted empirical measures associated to a weighted population of particles, which are generated recursively.Despite many theoretical advantages [see, e.g., J. Roy. Statist. Soc. Ser. B 63 (2001) 127-146. Ann. Statist. 33 (2005) 1983-2021, Feymann-Kac Formulae.Genealogical and interacting Particle Systems with Applications (2004) Springer, Ann. Statist. 32 (2004) 2385-2411], the large-sample theory of these approximations remains a question of central interest. In this paper we establish a law of large numbers and a central limit theorem as the number of particles gets large. We introduce the concepts of weighted sample consistency and asymptotic normality, and derive conditions under which the transformations of the weighted sample used in the SMC algorithm preserve these properties. To illustrate our findings, we analyze SMC algorithms to approximate the filtering distribution in state-space models. We show how our techniques allow to relax restrictive technical conditions used in previously reported works and provide grounds to analyze more sophisticated sequential sampling strategies, including branching, resampling at randomly selected times. and so on."
"10.1214/07-AOS548","2008","Local antithetic sampling with scrambled nets","1","We consider the problem of computing an approximation to the integral I = integral([0, 1])d f (x) dx. Monte Carlo (MC) sampling typically attains a root mean squared error (RMSE) of O(n(-1/2)) from n independent random function evaluations. By contrast, quasi-Monte Carlo (QMC) sampling using carefully equispaced evaluation points can attain the rate O(n(-1+epsilon)) for any epsilon > 0 and randomized QMC (RQMC) can attain the RMSE O(n(-3/2+epsilon)), both under mild conditions on f.Classical variance reduction methods for MC call be adapted to QMC. Published results combining QMC with importance sampling and with control variates have found worthwhile improvements, but no change in the error rate. This paper extends the classical variance reduction method of antithetic sampling and combines it with RQMC. One Such method is shown to bring a modest improvement in the RMSE rate, attaining O(n(-3/2-1/d+epsilon)) for any epsilon > 0, for smooth enough f."
"10.1214/07-AOS541","2008","Trimming and likelihood: robust location and dispersion estimation in the elliptical model","0","Robust estimators of location and dispersion are Often used in the elliptical model to obtain an uncontaminated and highly representative subsample by trimming the data Outside an ellipsoid based in the associated Mahalanobis distance. Here we analyze some one (or k)-step Maximum Likelihood Estimators computed on a subsample obtained with Such a procedure. We introduce different models which arise naturally from the ways in which the discarded data can be treated, leading to truncated or censored likelihoods. as well as to a likelihood based on an only outliers gross errors model. Results on existence, uniqueness, robustness and asymptotic properties of the proposed estimators are included. A remarkable fact is that the proposed estimators generally keep the breakdown point of the initial (robust) estimators, but they Could improve the rate of convergence of the initial estimator because our estimators always converge at rate n(1/2). independently of the rate of convergence of the initial estimator."
"10.1214/07-AOS522","2008","Moments of minors of {W}ishart matrices","1","For a random matrix following a Wishart distribution, we derive formulas for the expectation and the covariance matrix of compound matrices. The compound matrix of order m is populated by all m x m-minors of the Wishart matrix. Our results yield first and second moments of the minors of the sample covariance matrix for multivariate normal observations. This work is motivated by the fact that such minors arise in the expression of constraints on the covariance matrix in many classical multivariate problems."
"10.1214/07-AOS544","2008","Profile-kernel likelihood inference with diverging number of parameters","8","The generalized varying coefficient partially linear model with a growing number of predictors arises in many contemporary scientific endeavor. In this paper we set foot on both theoretical and practical sides of profile likelihood estimation and inference. When the number of parameters grows with sample size, the existence and asymptotic normality of the profile likelihood estimator are established under some regularity conditions. Profile likelihood ratio inference for the growing number of parameters is proposed and Wilk's phenomenon is demonstrated. A new algorithm, called the accelerated profile-kernel algorithm, for computing profile-kernel estimator is proposed and investigated. Simulation studies show that the resulting estimates are as efficient as the fully iterative profile-kernel estimates. For moderate sample sizes, our proposed procedure saves much computational time over the fully iterative profile-kernel one and gives stabler estimates. A set of real data is analyzed using our proposed algorithm."
"10.1214/07-AOS547","2008","Gibbs posterior for variable selection in high-dimensional classification and data mining","0","In the popular approach of ""Bayesian variable selection"" (BVS), one uses prior and posterior distributions to select a subset of candidate variables to enter the model. A completely new direction will be considered here to study BVS with I Gibbs posterior originating in statistical mechanics. The Gibbs posterior is constructed from a risk function of practical interest (Such as the classification error) and aims at minimizing a risk function without modeling the data probabilistically. This can improve the performance over the Usual Bayesian approach, which depends on a probability model which may be misspecified. Conditions will be provided to achieve good risk performance, even in the presence of high dimensionality, when the number of candidate variables ""K"" can be much larger than the sample size ""n."" In addition, we develop a convenient Markov chain Monte Carlo algorithm to implement BVS with the Gibbs posterior."
"10.1214/07-AOS546","2008","Learning by mirror averaging","3","Given a finite collection of estimators or classifiers, we study the problem of model selection type aggregation, that is, we construct a new estimator or classifier, called aggregate, which is nearly as good as the best among them with respect to a given risk criterion. We define our aggregate by a simple recursive procedure which solves an auxiliary stochastic linear programming problem related to the original nonlinear one and constitutes a special case of the mirror averaging algorithm. We show that the aggregate satisfies sharp oracle inequalities under some general assumptions. The results are applied to several problems including regression, classification and density estimation."
"10.1214/07-AOS539","2008","A class of {R}\'enyi information estimators for multidimensional densities","0","A class of estimators of the Renyi and Tsallis entropies of an unknown distribution f in R-m is presented. These estimators are based on the kth nearest-neighbor distances computed from a sample of N i.i.d. vectors with distribution f. We show that entropies of any order q, including Shannon's entropy, can be estimated consistently with minimal assumptions on f. Moreover, we show that it is straightforward to extend the nearest-neighbor method to estimate the statistical distance between two distributions using one i.i.d. sample from each."
"10.1214/07-AOS537","2008","Choice of neighbor order in nearest-neighbor classification","1","The kth-nearest neighbor rule is arguably the simplest and most intuitively appealing nonparametric classification procedure. However, application of this method is inhibited by lack of knowledge about its properties, in particular, about the manner in which it is influenced by the value of k; and by the absence of techniques for empirical choice of k. In the present paper we detail the way in which the value of k determines the misclassification error. We consider two models, Poisson and Binomial, for the training samples. Under the first model, data are recorded in a Poisson stream and are ""assigned"" to one or other of the two populations in accordance with the prior probabilities. In particular, the total number of data in both training samples is a Poisson-distributed random variable. Under the Binomial model, however, the total number of data in the training samples is fixed, although again each data value is assigned in a random way. Although the values of risk and regret associated with the Poisson and Binomial models are different, they are asymptotically equivalent to first order, and also to the risks associated with kernel-based classifiers that are tailored to the case of two derivatives. These properties motivate new methods for choosing the value of k."
"10.1214/07-AOS534","2008","Estimation of distributions, moments and quantiles in deconvolution problems","2","When using the bootstrap in the presence of measurement error, we must first estimate the target distribution function; we cannot directly resample since we don not have a sample from the target. These and other considerations motivate the development of estimators of distributions, and of related quantities such as moments and quantiles, in errors-in-variables settings. We show that such estimators have curious and unexpected properties. For example, if the distributions of the variable of interest, W, say, and of the observation error are both centered at zero, then the rate of convergence of an estimator of the distribution function of W can be slower at the origin than away from the origin. This is an intrinsic characteristic of the problem, not a quirk of particular estimators: the property holds true for optimal estimators."
"10.1214/07-AOS531","2008","Analysis of variance, coefficient of determination and {$F$}-test for local polynomial regression","1","This paper provides ANOVA inference for nonparametric local polynomial regression (LPR) in analogy with ANOVA tools for the classical linear regression model. A surprisingly simple and exact local ANOVA decomposition is established, and a local R-squared quantity is defined to measure the proportion of local variation explained by fitting LPR. A global ANOVA decomposition is obtained by integrating local counterparts, and a global R-squared and a symmetric projection matrix are defined. We show that the proposed projection matrix is asymptotically idempotent and asymptotically orthogonal to its complement, naturally leading to an F-test for testing for no effect. A by-product result is that the asymptotic bias of the ""projected"" response based on local linear regression is of quartic order of the bandwidth. Numerical results illustrate the behaviors of the proposed R-squared and F-test. The ANOVA methodology is also extended to varying coefficient models."
"10.1214/07-AOS513","2008","Robust nonparametric estimation via wavelet median regression","2","In this paper we develop a nonparametric regression method that is simultaneously adaptive over a wide range of function classes for the regression function and robust over a large collection of error distributions, including those that are heavy-tailed, and may not even possess variances or means. Our approach is to first use local medians to turn the problem of nonparametric regression with unknown noise distribution into a standard Gaussian regression problem and then apply a wavelet block thresholding procedure to construct an estimator of the regression function. It is shown that the estimator simultaneously attains the optimal rate of convergence over a wide range of the Besov classes, without prior knowledge of the smoothness of the underlying functions or prior knowledge of the error distribution. The estimator also automatically adapts to the local smoothness of the underlying function, and attains the local adaptive minimax rate for estimating functions at a point.A key technical result in our development is a quantile coupling theorem which gives a tight bound for the quantile coupling between the sample medians and a normal variable. This median Coupling inequality may be of independent interest."
"10.1214/07-AOS509","2008","Adaptive variance function estimation in heteroscedastic nonparametric regression","1","We consider a wavelet thresholding approach to adaptive variance function estimation in heteroscedastic nonparametric regression. A data-driven estimator is constructed by applying wavelet thresholding to the squared first-order differences of the observations. We show that the variance function estimator is nearly optimally adaptive to the smoothness of both the mean and variance functions. The estimator is shown to achieve the optimal adaptive rate of convergence under the pointwise squared error simultaneously over a range of smoothness classes. The estimator is also adaptively within a logarithmic factor of the minimax risk under the global mean integrated squared error over a collection of spatially inhomogeneous function classes. Numerical implementation and simulation results are also discussed."
"10.1214/07-AOS530","2008","A multivariate central limit theorem for randomized orthogonal array sampling designs in computer experiments","4","Let f : [0, 1)(d) -> R be an integrable function. An objective of many computer experiments is to estimate integral(d)([0, 1)) f (x) dx by evaluating f at a finite number of points in [0, 1)(d). There is a design issue in the choice of these points and a popular choice is via the use of randomized orthogonal arrays. This article proves a multivariate central limit theorem for a class of randomized orthogonal array sampling designs [Owen Statist. Sinica 2 (1992a) 439-452] as well as for a class of OA-based Latin hypercubes."
"10.1214/07-AOS525","2008","Asymptotic equivalence for nonparametric regression with multivariate and random design","3","We show that nonparametric regression is asymptotically equivalent, in Le Cam's sense, to a sequence of Gaussian white noise experiments as the number of observations tends to infinity. We propose a general constructive framework, based on approximation spaces, which allows asymptotic equivalence to be achieved, even in the cases of multivariate and random design."
"10.1214/07-AOS527","2008","A wavelet {W}hittle estimator of the memory parameter of a nonstationary {G}aussian time series","0","We consider a time series X = (X-k, k is an element of Z) with memory parameter d(0) is an element of R. This time series is either stationary or can be made stationary after differencing a finite number of times. We study the ""local Whittle wavelet estimator"" of the memory parameter d(0). This is a wavelet-based semiparametric pseudo-likelihood maximum method estimator. The estimator may depend on a given finite range of scales or on a range which becomes infinite with the sample size. We show that the estimator is consistent and rate optimal if X is a linear process, and is asymptotically normal if X is Gaussian."
"10.1214/07-AOS524","2008","Locally adaptive estimation of evolutionary wavelet spectra","2","We introduce a wavelet-based model of local stationarity. This model enlarges the class of locally stationary, wavelet processes and contains processes whose spectral density function may change very suddenly in time. A notion of time-varying wavelet spectrum is uniquely defined as a wavelet-type transform of the autocovariance function with respect to so-called autocorrelation wavelets. This leads to a natural representation of the autocovariance which is localized on scales. We propose a pointwise adaptive estimator of the time-varying spectrum. The behavior of the estimator studied in homogeneous and inhomogeneous regions of the wavelet spectrum."
"10.1214/07-AOS533","2008","Confidence bands in nonparametric time series regression","2","We consider nonparametric estimation of mean regression and conditional variance (or volatility) functions in nonlinear stochastic regression models. Simultaneous confidence bands are constructed and the coverage probabilities are shown to be asymptotically correct. The imposed dependence structure allows applications in many linear and nonlinear autoregressive processes. The results are applied to the S&P 500 Index data."
"10.1214/07-AOS536","2008","General frequentist properties of the posterior profile distribution","2","In this paper, inference for the parametric component of a semiparametric model based on sampling from the posterior profile distribution is thoroughly investigated from the frequentist viewpoint. The higher-order validity of the profile sampler obtained in Cheng and Kosorok [Ann. Statist. 36 (2008)] is extended to semiparametric models in which the infinite dimensional nuisance parameter may not have a root-n convergence rate. This is a nontrivial extension because it requires a delicate analysis of the entropy of the semiparametric models involved. We find that the accuracy of inferences based on the profile sampler improves as the convergence rate of the nuisance parameter increases. Simulation studies are used to verify this theoretical result. We also establish that an exact frequentist confidence interval obtained by inverting the profile log-likelihood ratio can be estimated with higher-order accuracy by the credible set of the same type obtained from the posterior profile distribution. Our theory is verified for several specific examples."
"10.1214/07-AOS523","2008","Higher order semiparametric frequentist inference with the profile sampler","3","We consider higher order frequentist inference for the parametric component of a semiparametric model based on sampling from the posterior profile distribution. The first order validity of this procedure established by Lee, Kosorok and Fine in [J. American Statist. Assoc. 100 (2005) 960969] is extended to second-order validity in the setting where the infinite-dimensional nuisance parameter achieves the parametric rate. Specifically, we obtain higher order estimates of the maximum profile likelihood estimator and of the efficient Fisher information. Moreover, we prove that an exact frequentist confidence interval for the parametric component at level alpha can be estimated by the alpha-level credible set from the profile sampler with an error of order O p (n(-1)). Simulation studies are used to assess second-order asymptotic validity of the profile sampler. As far as we are aware, these are the first higher order accuracy results for semiparametric frequentist inference."
"10.1214/07-AOS521","2008","Multiscale inference about a density","1","We introduce a multiscale test statistic based on local order statistics and spacings that provides simultaneous confidence statements for the existence and location of local increases and decreases of a density or a failure rate. The procedure provides guaranteed finite-sample significance levels, is easy to implement and possesses certain asymptotic optimality and adaptivity properties."
"10.1214/07-AOS526","2008","Searching for a trail of evidence in a maze","3","Consider a graph with a set of vertices and oriented edges connecting pairs of vertices. Each vertex is associated with a random variable and these are assumed to be independent. In this setting, suppose we wish to solve the following hypothesis testing problem: under the null, the random variables have common distribution N(0, 1) while under the alternative, there is an unknown path along which random variables have distribution N(mu, 1), mu > 0, and distribution N(0, 1) away from it. For which values of the mean shift mu can one reliably detect and for which values is this impossible?Consider, for example, the usual regular lattice with vertices of the form{(i, j) : 0 <= i, -i <= j <= i and j has the parity of i}and oriented edges (i, j) -> (i + 1, j + s), where s = +/- 1. We show that for paths of length m starting at the origin, the hypotheses become distinguishable (in a minimax sense) if mu(m) >> 1/root logm, while they are not if mu(m) << 1/log m. We derive equivalent results in a Bayesian setting where one assumes that all paths are equally likely; there, the asymptotic threshold is mu(m) approximate to m(-14).We obtain corresponding results for trees (where the threshold is of order I and independent of the size of the tree), for distributions other than the Gaussian and for other graphs. The concept of the predictability profile, first introduced by Benjamini, Pemantle and Peres, plays a crucial role in our analysis."
"10.1214/07-AOS519","2008","Semiparametric detection of significant activation for brain f{MRI}","2","Functional magnetic resonance imaging (fMRI) aims to locate activated regions in human brains when specific tasks are performed. The conventional tool for analyzing fMRI data applies some variant of the linear model, which is restrictive in modeling assumptions. To yield more accurate prediction of the time-course behavior of neuronal responses, the semiparametric inference for the underlying hemodynamic response function is developed to identify significantly activated voxels. Under mild regularity conditions, we demonstrate that a class of the proposed semiparametric test statistics, based on the local linear estimation technique, follow chi(2) distributions under null hypotheses for a number of useful hypotheses. Furthermore, the asymptotic power functions of the constructed tests are derived under the fixed and contiguous alternatives. Simulation evaluations and real fMRI data application suggest that the semiparametric inference procedure provides more efficient detection of activated brain areas than the popular imaging analysis tools AFNI and FSL."
"10.1214/07-AOS517","2008","Fence methods for mixed model selection","1","Many model search strategies involve trading off model fit with model complexity in a penalized goodness of fit measure. Asymptotic properties for these types of procedures in settings like linear regression and ARMA time series have been studied, but these do not naturally extend to nonstandard situations such as mixed effects models, where simple definition of the sample size is not meaningful. This paper introduces a new class of strategies, known as fence methods, for mixed model selection, which includes linear and generalized linear mixed models. The idea involves a procedure to isolate a subgroup of what are known as correct models (of which the optimal model is a member). This is accomplished by constructing a statistical fence, or barrier, to carefully eliminate incorrect models. Once the fence is constructed, the optimal model is selected from among those within the fence according to a criterion which can be made flexible. In addition, we propose two variations of the fence. The first is a stepwise procedure to handle situations of many predictors; the second is an adaptive approach for choosing a tuning constant. We give sufficient conditions for consistency of fence and its variations, a desirable property for a good model selection procedure. The methods are illustrated through simulation studies and real data analysis."
"10.1214/07-AOS529","2008","Dimension reduction based on constrained canonical correlation and variable filtering","5","The ""curse of dimensionality"" has remained a challenge for high-dimensional data analysis in statistics'. The sliced inverse regression (SIR) and canonical correlation (CANCOR) methods aim to reduce the dimensionality of data by replacing the explanatory variables with a small number of composite directions without losing much information. However, the estimated composite directions generally involve all of the variables, making their interpretation difficult. To simplify the direction estimates, Ni, Cook and Tsai [Biometrika 92 (2005) 242-247] proposed the shrinkage sliced inverse regression (SSIR) based on SIR. In this paper, we propose the constrained canonical correlation (C-3) method based on CANCOR, followed by a simple variable filtering method. As a result, each composite direction consists of a subset of the variables for interpretability as well as predictive power. The proposed method aims to identify simple structures without sacrificing the desirable properties of the unconstrained CANCOR estimates. The simulation studies demonstrate the performance advantage of the proposed C-3 method over the SSIR method. We also use the proposed method in two examples for illustration."
"10.1214/07-AOS535","2008","Statistics of extremes by oracle estimation","1","We use the fitted Pareto law to construct an accompanying approximation of the excess distribution function. A selection rule of the location of the excess distribution function is proposed based on a stagewise lack-of-fit testing procedure. Our main result is an oracle type inequality for the Kullback-Leibler loss."
"10.1214/009053607000000578","2008","``{P}reconditioning'' for feature selection and regression in high-dimensional problems","0","We consider regression problems where the number of predictors greatly exceeds the number of observations. We propose a method for variable selection that first estimates the regression function, yielding a ""preconditioned"" response variable. The primary method used for this initial regression is supervised principal components. Then we apply a standard procedure such as forward stepwise selection or the LASSO to the preconditioned response variable. In a number of simulated and real data examples, this two-step procedure outperforms forward stepwise selection or the usual LASSO (applied directly to the raw outcome). We also show that under a certain Gaussian latent variable model, application of the LASSO to the preconditioned response variable is consistent as the number of predictors and observations increases. Moreover, when the observational noise is rather large, the suggested procedure can give a more accurate estimate than LASSO. We illustrate our method on some real problems, including survival analysis with microarray data."
"10.1214/07-AOS520","2008","The sparsity and bias of the {LASSO} selection in high-dimensional linear regression","35","Meinshausen and Buhlmann [Ann. Statist. 34 (2006) 1436-1462] showed that, for neighborhood selection in Gaussian graphical models, under a neighborhood stability condition, the LASSO is consistent, even when the number of variables is of greater order than the sample size. Zhao and Yu [(2006) J. Machine Learning Research 7 2541-2567] formalized the neighborhood stability condition in the context of linear regression as a strong irrepresentable condition. That paper showed that under this condition, the LASSO selects exactly the set of nonzero regression coefficients, provided that these coefficients are bounded away from zero at a certain rate. In this paper, the regression coefficients outside an ideal model are assumed to be small, but not necessarily zero. Under a sparse Riesz condition on the correlation of design variables, we prove that the LASSO selects a model of the correct order of dimensionality, controls the bias of the selected model at a level determined by the contributions of small regression coefficients and threshold bias, and selects all coefficients of greater order than the bias of the selected model. Moreover, as a consequence of this rate consistency of the LASSO in model selection, it is proved that the sum of error squares for the mean response and the l(alpha)-loss for the regression coefficients converge at the best possible rates under the given conditions. An interesting aspect of our results is that the logarithm of the number of variables can be of the same order as the sample size for certain random dependent designs."
"10.1214/07-AOS0316B","2008","Who cares if it is a white cat or a black cat? {D}iscussion: ``{O}ne-step sparse estimates in nonconcave penalized likelihood models'' [{A}nn. {S}tatist. {\bf 36} (2008), no. 4, 1509--1533; MR2435443] by {H}. {Z}ou and {R}. {L}i","0",""
"10.1214/009053607000000802","2008","One-step sparse estimates in nonconcave penalized likelihood models","26","Fan and Li propose a family of variable selection methods via penalized likelihood using concave penalty functions. The nonconcave penalized likelihood estimators enjoy the oracle properties, but maximizing the penalized likelihood function is computationally challenging, because the objective function is nondifferentiable and nonconcave. In this article, we propose a new unified algorithm based on the local linear approximation (LLA) for maximizing the penalized likelihood for a broad class of concave penalty functions. Convergence and other theoretical properties of the LLA algorithm are established. A distinguished feature of the LLA algorithm is that at each LLA step, the LLA estimator can naturally adopt a sparse representation. Thus, we suggest using the one-step LLA estimator from the LLA algorithm as the final estimates. Statistically, we show that if the regularization parameter is appropriately chosen, the one-step LLA estimates enjoy the oracle properties with good initial estimators. Computationally, the one-step LLA estimation methods dramatically reduce the computational cost in maximizing the nonconcave penalized likelihood. We conduct some Monte Carlo simulation to assess the finite sample performance of the one-step sparse estimation methods. The results are very encouraging."
"10.1214/009053607000000820","2008","Statistical modeling of causal effects in continuous time","1","This article studies the estimation of the causal effect of a time-varying treatment on time-to-an-event or on some other continuously distributed outcome. The paper applies to the situation where treatment is repeatedly adapted to time-dependent patient characteristics. The treatment effect cannot be estimated by simply conditioning on these time-dependent patient characteristics, as they may themselves be indications of the treatment effect. This time-dependent confounding is common in observational studies. Robins [(1992) Biometrika 79 321-334, (1998b) Encyclopedia of Biostatistics 6 4372-4389] has proposed the so-called structural nested models to estimate treatment effects in the presence of time-dependent confounding. In this article we provide a conceptual framework and formalization for structural nested models in continuous time. We show that the resulting estimators are consistent and asymptotically normal. Moreover, as conjectured in Robins [(1998b) Encyclopedia of Biostatistics 6 4372-4389], a test for whether treatment affects the outcome of interest can be performed without specifying a model for treatment effect. We illustrate the ideas in this article with an example."
"10.1214/009053607000000613","2008","Rates of contraction of posterior distributions based on {G}aussian process priors","2","We derive rates of contraction of posterior distributions on nonparametric or semiparametric models based on Gaussian processes. The rate of contraction is shown to depend on the position of the true parameter relative to the reproducing kernel Hilbert space of the Gaussian process and the small ball probabilities of the Gaussian process. We determine these quantities for a range of examples of Gaussian priors and in several statistical settings. For instance, we consider the rate of contraction of the posterior distribution based on sampling from a smooth density model when the prior models the log density as a (fractionally integrated) Brownian motion. We also consider regression with Gaussian errors and smooth classification under a logistic or probit link function combined with various priors."
"10.1214/009053607000000587","2008","Hurst exponent estimation of locally self-similar {G}aussian processes using sample quantiles","1","This paper is devoted to the introduction of a new class of consistent estimators of the fractal dimension of locally self-similar Gaussian processes. These estimators are based on convex combinations of sample quantiles of discrete variations of a sample path over a discrete grid of the interval [0, 1]. We derive the almost sure convergence and the asymptotic normality for these estimators. The key-ingredient is a Bahadur representation for sample quantiles of nonlinear functions of Gaussian sequences with correlation function decreasing as k(-alpha) L(k) for some alpha > 0 and some slowly varying function L(.)."
"10.1214/07-AOS511","2008","The distribution of maxima of approximately {G}aussian random fields","0","Motivated by the problem of testing for the existence of a signal of known parametric structure and unknown ""location"" (as explained below) against a noisy background, we obtain for the maximum of a centered, smooth random field an approximation for the tail of the distribution. For the motivating class of problems this gives approximately the significance level of the maximum score test. The method is based on an application of a likelihood-ratio-identity followed by approximations of local fields. Numerical examples illustrate the accuracy of the approximations."
"10.1214/009053607000000992","2008","Adaptive goodness-of-fit tests based on signed ranks","0","Within the nonparametric regression model with unknown regression function l and independent, symmetric errors, a new multiscale signed rank statistic is introduced and a conditional multiple test of the simple hypothesis l = 0 against a nonparametric alternative is proposed. This test is distribution-free and exact for finite samples even in the heteroscedastic case. It adapts in a certain sense to the unknown smoothness of the regression function under the alternative, and it is uniformly consistent against alternatives whose sup-norm tends to zero at the fastest possible rate. The test is shown to be asymptotically optimal in two senses: It is rate-optimal adaptive against Holder classes. Furthermore, its relative asymptotic efficiency with respect to an asymptotically minimax optimal test under sup-norm loss is close to I in case of homoscedastic Gaussian errors within a broad range of Holder classes simultaneously."
"10.1214/07-AOS515","2008","A general trimming approach to robust cluster analysis","1","We introduce a new method for performing clustering with the aim of fitting clusters with different scatters and weights. It is designed by allowing to handle a proportion alpha of contaminating data to guarantee the robustness of the method. As a characteristic feature, restrictions on the ratio between the maximum and the minimum eigenvalues of the groups scatter matrices are introduced. This makes the problem to be well defined and guarantees the consistency of the sample solutions to the population ones.The method covers a wide range of clustering approaches depending on the strength of the chosen restrictions. Our proposal includes an algorithm for approximately solving the sample problem."
"10.1214/07-AOS505","2008","Multivariate spacings based on data depth. {I}. {C}onstruction of nonparametric multivariate tolerance regions","0","This paper introduces and studies multivariate spacings. The spacings are developed using the order statistics derived from data depth. Specifically, the spacing between two consecutive order statistics is the region which bridges the two order statistics, in the sense that the region contains all the points whose depth values fall between the depth values of the two consecutive order statistics. These multivariate spacings can be viewed as a data-driven realization of the so-called ""statistically equivalent blocks."" These spacings assume a form of center-outward layers of ""shells"" (""rings"" in the two-dimensional case), where the shapes of the shells follow closely the underlying probabilistic geometry. The properties and applications of these spacings are studied. In particular, the spacings are used to construct tolerance regions. The construction of tolerance regions is nonparametric and completely data driven, and the resulting tolerance region reflects the true geometry of the underlying distribution. This is different from most existing approaches which require that the shape of the tolerance region be specified in advance. The proposed tolerance regions are shown to meet the prescribed specifications, in terms of beta-content and beta-expectation. They are also asymptotically minimal under elliptical distributions. Finally, a simulation and comparison study on the proposed tolerance regions is presented."
"10.1214/07-AOS508","2008","Optimal rank-based tests for homogeneity of scatter","2","We propose a class of locally and asymptotically optimal tests, based on multivariate ranks and signs for the homogeneity of scatter matrices in M. elliptical populations. Contrary to the existing parametric procedures, these tests remain valid without any moment assumptions, and thus are perfectly robust against heavy-tailed distributions (validity robustness). Nevertheless, they reach semiparametric efficiency bounds at correctly specified elliptical densities and maintain high powers under all (efficiency robustness). In particular, their normal-score version outperforms traditional Gaussian likelihood ratio tests and their pseudo-Gaussian robustifications under a very broad range of non-Gaussian densities including, for instance, all multivariate Student and power-exponential distributions."
"10.1214/009053607000000541","2008","Data-driven {S}obolev tests of uniformity on compact {R}iemannian manifolds","1","Data-driven versions of Sobolev tests of uniformity on compact Riemannian manifolds are proposed. These tests are invariant under isometries; and are consistent against all alternatives. The large-sample asymptotic null distributions are given."
"10.1214/07-AOS512","2008","Parametric bootstrap approximation to the distribution of {EBLUP} and related prediction intervals in linear mixed models","1","Empirical best linear unbiased prediction (EBLUP) method uses a linear mixed model in combining information from different sources of information. This method is particularly useful in small area problems. The variability of an EBLUP is traditionally measured by the mean squared prediction error (MSPE), and interval estimates are generally constructed using estimates of the MSPE. Such methods have shortcomings like under-coverage or over-coverage, excessive length and lack of interpretability. We propose a parametric bootstrap approach to estimate the entire distribution of a suitably centered and scaled EBLUP. The bootstrap histogram is highly accurate, and differs from the true EBLUP distribution by only O(d(3)n(-3/2)), where d is the number of parameters and n the number of observations. This result is used to obtain highly accurate prediction intervals. Simulation results demonstrate the superiority of this method over existing techniques of constructing prediction intervals in linear mixed models."
"10.1214/009053607000000677","2008","Kernel methods in machine learning","0","We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data.We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data."
"10.1214/07-AOS506","2008","Admissible predictive density estimation","0","Let X vertical bar mu similar to N-p (mu, upsilon I-x) and Y vertical bar mu similar to N-p (mu, upsilon I-y) be independent p-dimensional multivariate normal vectors with common unknown mean A. Based on observing X = x, we consider the problem of estimating the true predictive density p(y vertical bar mu) of Y under expected Kullback-Leibler loss. Our focus here is the characterization of admissible procedures for this problem. We show that the class of all generalized Bayes rules is a complete class, and that the easily interpretable conditions of Brown and Hwang [Statistical Decision Theory and Related Topics (1982) III 205-230] are sufficient for a formal Bayes rule to be admissible."
"10.1214/009053607000000965","2008","Adaptive estimation of and oracle inequalities for probability densities and characteristic functions","0","The theory of adaptive estimation and oracle inequalities for the case of Gaussian-shift-finite-interval experiments has made significant progress in recent years. In particular, sharp-minimax adaptive estimators and exact exponential-type oracle inequalities have been suggested for a vast set of functions including analytic and Sobolev with any positive index as well as for Efromovich-Pinsker and Stein blockwise-shrinkage estimators. Is it possible to obtain similar results for a more interesting applied problem of density estimation and/or the dual problem of characteristic function estimation? The answer is ""yes."" In particular, the obtained results include exact exponential-type oracle inequalities which allow to consider, for the first time in the literature, a simultaneous sharp-minimax estimation of Sobolev densities with any positive index (not necessarily larger than 1/2), infinitely differentiable densities (including analytic, entire and stable), as well as of not absolutely integrable characteristic functions. The same adaptive estimator is also rate minimax over a familiar class of distributions with bounded spectrum where the density and the characteristic function can be estimated with the parametric rate."
"10.1214/07-AOS507","2008","Composite quantile regression and the oracle model selection theory","5","Coefficient estimation and variable selection in multiple linear regression is routinely done in the (penalized) least squares (LS) framework. The concept of model selection oracle introduced by Fan and Li [J. Amer. Statist. Assoc. 96 (2001) 1348-1360] characterizes the optimal behavior of a model selection procedure. However, the least-squares oracle theory breaks down if the error variance is infinite. In the current paper we propose a new regression method called composite quantile regression (CQR). We show that the oracle model selection theory using the CQR oracle works beautifully even when the error variance is infinite. We develop a new oracular procedure to achieve the optimal properties of the CQR oracle. When the error variance is finite, CQR still enjoys great advantages in terms of estimation efficiency. We show that the relative efficiency of CQR compared to the least squares is greater than 70% regardless the error distribution. Moreover, CQR could be much more efficient and sometimes arbitrarily more efficient than the least squares. The same conclusions hold when comparing a CQR-oracular estimator with a LS-oracular estimator."
"10.1214/07-AOS518","2008","Optimal designs for mixed models in experiments based on ordered units","0","We consider experiments for comparing treatments using units that are ordered linearly over time or space within blocks. In addition to the block effect, we assume that a trend effect influences the response. The latter is modeled as a smooth component plus a random term that captures departures from the smooth trend. The model is flexible enough to cover a variety of situations; for instance, most of the effects may be either random or fixed. The information matrix for a design will be a function of several variance parameters. While data will shed light on the values of these parameters, at the design stage, they are unlikely to be known, so we suggest a maximin approach, in which a minimal information matrix is maximized. We derive maximin universally optimal designs and study their robustness. These designs are based on semibalanced arrays. Special cases correspond to results available in the literature."
"10.1214/009053607000000983","2008","Current status data with competing risks: limiting distribution of the {MLE}","3","We study nonparametric estimation for current status data with competing risks. Our main interest is in the nonparametric maximum likelihood estimator (MLE), and for comparison we also consider a simpler ""naive estimator."" Groeneboom, Maathuis and Wellner [Ann. Statist. (2008) 36 10311063] proved that both types of estimators converge globally and locally at rate n(1/3). We use these results to derive the local limiting distributions of the estimators. The limiting distribution of the naive estimator is given by the slopes of the convex minorants of correlated Brownian motion processes with parabolic drifts. The limiting distribution of the MLE involves a new self-induced limiting process. Finally, we present a simulation study showing that the MLE is superior to the naive estimator in terms of mean squared error, both for small sample sizes and asymptotically."
"10.1214/009053607000000974","2008","Current status data with competing risks: consistency and rates of convergence of the {MLE}","2","We study nonparametric estimation of the sub-distribution functions for current status data with competing risks. Our main interest is in the nonparametric maximum likelihood estimator (MLE), and for comparison we also consider a simpler ""naive estimator."" Both types of estimators were studied by Jewell, van der Laan and Henneman [Biometrika (2003) 90 183-197], but little was known about their large sample properties. We have started to fill this gap, by proving that the estimators are consistent and converge globally and locally at rate n(1/3). We also show that this local rate of convergence is optimal in a minimax sense. The proof of the local rate of convergence of the MLE uses new methods, and relies on a rate result for the sum of the MLEs of the sub-distribution functions which holds uniformly on a fixed neighborhood of a point. Our results are used in Groeneboom, Maathuis and Wellner [Ann. Statist. (2008) 36 1064-1089] to obtain the local limiting distributions of the estimators."
"10.1214/009053607000000866","2008","Quotient correlation: a simple based alternative to {P}earson's correlation","1","The quotient correlation is defined here as an alternative to Pearson's correlation that is more intuitive and flexible in cases where the tail behavior of data is important. It measures nonlinear dependence where the regular correlation coefficient is generally not applicable. One of its most useful features is a test statistic that has high power when testing nonlinear dependence in cases where the Fisher's Z-transformation test may fail to reach a right conclusion. Unlike most asymptotic test statistics, which are either normal or chi(2), this test statistic has a limiting gamma distribution (henceforth, the gamma test statistic). More than the common usages of correlation, the quotient correlation can easily and intuitively be adjusted to values at tails. This adjustment generates two new concepts-the tail quotient correlation and the tail independence test statistics, which are also gamma statistics. Due to the fact that there is no analogue of the correlation coefficient in extreme value theory, and there does not exist an efficient tail independence test statistic, these two new concepts may open up a new field of study. In addition, an alternative to Spearman's rank correlation, a rank based quotient correlation, is also defined. The advantages of using these new concepts are illustrated with simulated data and a real data analysis of internet traffic."
"10.1214/009053607000000956","2008","Quadratic distances on probabilities: a unified foundation","0","This work builds a unified framework for the study of quadratic form distance measures as they are used in assessing the goodness of fit of models. Many important procedures have this structure, but the theory for these methods is dispersed and incomplete. Central to the statistical analysis of these distances is the spectral decomposition of the kernel that generates the distance. We show how this determines the limiting distribution of natural goodness-of-fit tests. Additionally, we develop a new notion, the spectral degrees of freedom of the test, based on this decomposition. The degrees of freedom are easy to compute and estimate, and can be used as a guide in the construction of useful procedures in this class."
"10.1214/07-AOS501","2008","Objective priors for the bivariate normal model","0","Study of the bivariate normal distribution raises the full range of issues involving objective Bayesian inference, including the different types of objective priors (e.g., Jeffreys, invariant, reference, matching), the different modes of inference (e.g., Bayesian, frequentist, fiducial) and the criteria involved in deciding on optimal objective priors (e.g., ease of computation, frequentist performance, marginalization paradoxes). Summary recommendations as to optimal objective priors are made for a variety of inferences involving the bivariate normal distribution.In the course of the investigation, a variety of surprising results were found, including the availability of objective priors that yield exact frequentist inferences for many functions of the bivariate normal parameters, including the correlation coefficient."
"10.1214/009053607000000857","2008","Bounds for {B}ayesian order identification with application to mixtures","1","The efficiency of two Bayesian order estimators is studied. By using nonparametric techniques, we prove new underestimation and overestimation bounds. The results apply to various models, including mixture models. In this case, the errors are shown to be O(e(-an)) and O((log n)(b) / root n) (a, b > 0), respectively."
"10.1214/009053607000000622","2008","Closed-form likelihood expansions for multivariate diffusions","4","This paper provides closed-form expansions for the log-likelihood function of multivariate diffusions sampled at discrete time intervals. The coefficients of the expansion are calculated explicitly by exploiting the special structure afforded by the diffusion model. Examples of interest in financial statistics and Monte Carlo evidence are included, along with the convergence of the expansion to the true likelihood function."
"10.1214/07-AOS500","2008","Adaptive confidence bands","5","We show that there do not exist adaptive confidence bands for curve estimation except under very restrictive assumptions. We propose instead to construct adaptive bands that cover a surrogate function f* which is close to, but simpler than, f. The surrogate captures the significant features in f. We establish lower bounds on the width for any confidence band for f* and construct a procedure that comes within a small constant factor of attaining the lower bound for finite-samples."
"10.1214/009052607000000910","2008","Ranking and empirical minimization of {$U$}-statistics","0","The problem of ranking/ordering instances, instead of simply classifying them, has recently gained much attention in machine learning. In this paper we formulate the ranking problem in a rigorous statistical framework. The goal is to learn a ranking rule for deciding, among two instances, which one is ""better,"" with minimum ranking risk. Since the natural estimates of the risk are of the form of a U-statistic, results of the theory of U-processes are required for investigating the consistency of empirical risk minimizers. We establish, in particular, a tail inequality for degenerate U-processes, and apply it for showing that fast rates of convergence may be achieved under specific noise assumptions, just like in classification. Convex risk minimization methods are also studied."
"10.1214/009053607000000947","2008","Semiparametric efficiency in {GMM} models with auxiliary data","3","We study semiparametric efficiency bounds and efficient estimation of parameters defined through general moment restrictions with missing data. Identification relies on auxiliary data containing information about the distribution of the missing variables conditional on proxy variables that are observed in both the primary and the auxiliary database, when such distribution is common to the two data sets. The auxiliary sample can be independent of the primary sample, or can be a subset of it. For both cases, we derive bounds when the probability of missing data given the proxy variables is unknown, or known, or belongs to a correctly specified parametric family. We find that the conditional probability is not ancillary when the two samples are independent. For all cases, we discuss efficient semiparametric estimators. An estimator based on a conditional expectation projection is shown to require milder regularity conditions than one based on inverse probability weighting."
"10.1214/009053607000000938","2008","Sequential change detection revisited","0","In sequential change detection, existing performance measures differ significantly in the way they treat the time of change. By modeling this quantity as a random time, we introduce a general framework capable of capturing and better understanding most well-known criteria and also propose new ones. For a specific new criterion that constitutes an extension to Lorden's performance measure, we offer the optimum structure for detecting a change in the constant drift of a Brownian motion and a formula for the corresponding optimum performance."
"10.1214/07-AOS510","2008","Normalized least-squares estimation in time-varying {ARCH} models","1","We investigate the time-varying ARCH (tvARCH) process. It is shown that it can be used to describe the slow decay of the sample autocorrelations of the squared returns often observed in financial time series, which warrants the further study of parameter estimation methods for the model.Since the parameters are changing over time, a successful estimator needs to perform well for small samples. We propose a kernel normalized-least-squares (kernel-NLS) estimator which has a closed form, and thus outperforms the previously proposed kernel quasi-maximum likelihood (kernel-QML) estimator for small samples. The kernel-NLS estimator is simple, works under mild moment assumptions and avoids some of the parameter space restrictions imposed by the kernel-QML estimator. Theoretical evidence shows that the kernel-NLS estimator has the same rate of convergence as the kernel-QML estimator. Due to the kernel-NLS estimator's ease of computation, computationally intensive procedures can be used. A prediction-based cross-validation method is proposed for selecting the bandwidth of the kernel-NLS estimator. Also, we use a residual-based bootstrap scheme to bootstrap the tvARCH process. The bootstrap sample is used to obtain pointwise confidence intervals for the kernel-NLS estimator. It is shown that distributions of the estimator using the bootstrap and the ""true"" tvARCH estimator asymptotically coincide.We illustrate our estimation method on a variety of currency exchange and stock index data for which we obtain both good fits to the data and accurate forecasts."
"10.1214/009053607000000893","2008","Estimating deformations of isotropic {G}aussian random fields on the plane","1","This paper presents a new approach to the estimation of the deformation of an isotropic Gaussian random field on R-2 based on dense observations of a single realization of the deformed random field. Under this framework we investigate the identification and estimation of deformations. We then present a complete methodological package-from model assumptions to algorithmic recovery of the deformation-for the class of nonstationary processes obtained by deforming isotropic Gaussian random fields."
"10.1214/009053607000000848","2008","Estimation of a semiparametric transformation model","0","This paper proposes consistent estimators for transformation parameters in semiparametric models. The problem is to find the optimal transformation into the space of models with a predetermined regression structure like additive or multiplicative separability. We give results for the estimation of the transformation when the rest of the model is estimated non- or semi-parametrically and fulfills some consistency conditions. We propose two methods for the estimation of the transformation parameter maximizing a profile likelihood function or minimizing the mean squared distance from independence. First the problem of identification of such models is discussed. We then state asymptotic results for a general class of nonparametric estimators. Finally, we give some particular examples of nonparametric estimators of transformed separable models. The small sample performance is studied in several simulations."
"10.1214/009053607000000884","2008","On deconvolution with repeated measurements","8","In a large class of statistical inverse problems it is necessary to suppose that the transformation that is inverted is known. Although, in many applications, it is unrealistic to make this assumption, the problem is often insoluble without it. However, if additional data are available, then it is possible to estimate consistently the unknown error density. Data are seldom available directly on the transformation, but repeated, or replicated, measurements increasingly are becoming available. Such data consist of ""intrinsic"" values that are measured several times, with errors that are generally independent. Working in this setting we treat the nonparametric deconvolution problems of density estimation with observation errors, and regression with errors in variables. We show that, even if the number of repeated measurements is quite small, it is possible for modified kernel estimators to achieve the same level of performance they would if the error distribution were known. Indeed, density and regression estimators can be constructed from replicated data so that they have the same first-order properties as conventional estimators in the known-error case, without any replication, but with sample size equal to the sum of the numbers of replicates. Practical methods for constructing estimators with these properties are suggested, involving empirical rules for smoothing-parameter choice."
"10.1214/009053607000000901","2008","Effect of mean on variance function estimation in nonparametric regression","2","Variance function estimation in nonparametric regression is considered and the minimax rate of convergence is derived. We are particularly interested in the effect of the unknown mean on the estimation of the variance function. Our results indicate that, contrary to the common practice, it is not desirable to base the estimator of the variance function on the residuals from an optimal estimator of the mean when the mean function is not smooth. Instead it is more desirable to use estimators of the mean with minimal bias. On the other hand, when the mean function is very smooth, our numerical results show that the residual-based method performs better, but not substantial better than the first-order-difference-based estimator. In addition our asymptotic results also correct the optimal rate claimed in Hall and Carroll [J. Roy. Statist. Soc. Ser. B 51 (1989) 3-14]."
"10.1214/009053607000000929","2008","High-dimensional generalized linear models and the lasso","17","We consider high-dimensional generalized linear models with Lipschitz loss functions, and prove a nonasymptotic oracle inequality for the empirical risk minimizer with Lasso penalty. The penalty is based on the coefficients in the linear predictor, after normalization with the empirical norm. The examples include logistic regression, density estimation and classification with hinge loss. Least squares regression is also discussed."
"10.1214/009053607000000875","2008","Asymptotic properties of bridge estimators in sparse high-dimensional regression models","17","We Study the asymptotic properties of bridge estimators in sparse, high-dimensional, linear regression models when the number of covariates may increase to infinity with the sample size. We are particularly interested in the use of bridge estimators to distinguish between covariates whose coefficients are zero and covariates whose coefficients are nonzero. We show that under appropriate conditions, bridge estimators correctly select covariates with nonzero coefficients with probability converging to one and that the estimators of nonzero coefficients have the same asymptotic distribution that they would have if the zero coefficients were known in advance. Thus, bridge estimators have an oracle property in the sense of Fan and Li [J. Amer. Statist. Assoc. 96 (2001) 1348-1360] and Fan and Peng [Ann. Statist. 32 (2004) 928-961]. In general, the oracle property holds only if the number of covariates is smaller than the sample size. However, under a partial orthogonality condition in which the covariates of the zero coefficients are uncorrelated or weakly correlated with the covariates of nonzero coefficients, we show that marginal bridge estimators can correctly distinguish between covariates with nonzero and zero coefficients with probability converging to one even when the number of covariates is greater than the sample size."
"10.1214/009053607000000640","2008","Consistency of spectral clustering","5","Consistency is a key property of all statistical procedures analyzing randomly sampled data. Surprisingly, despite decades of work, little is known about consistency of most clustering algorithms. In this paper we investigate consistency of the popular family of spectral clustering algorithms, which clusters the data with the help of eigenvectors of graph Laplacian matrices. We develop new methods to establish that, for increasing sample size, those eigenvectors converge to the eigenvectors of certain limit operators. As a result, we can prove that one of the two major classes of spectral clustering (normalized clustering) converges under very general conditions, while the other (unnormalized clustering) is only consistent under strong additional assumptions, which are not always satisfied in real data. We conclude that our analysis provides strong evidence for the superiority of normalized spectral clustering."
"10.1214/009053607000000569","2008","A theoretical comparison of the data augmentation, marginal augmentation and {PX}-{DA} algorithms","3","The data augmentation (DA) algorithm is a widely used Markov chain Monte Carlo (MCMC) algorithm that is based on a Markov transition density of the form p(x vertical bar x') = integral y fx vertical bar y (x vertical bar y)fY vertical bar X (y vertical bar x') dy, where fX vertical bar Y and fY vertical bar X are conditional densities. The PX-DA and marginal augmentation algorithms of Liu and Wu [J. Amer. Statist. Assoc. 94 (1999) 1264-1274] and Meng and van Dyk [Biometrika 86 (1999) 301-320] are alternatives to DA that often converge much faster and are only slightly more computationally demanding. The transition densities of these alternative algorithms can be written in the form PR (x vertical bar x') = integral Y integral y fX vertical bar Y (x vertical bar y') R(y, dy')fY vertical bar X (y vertical bar x') dy, where R is a Markov transition function on Y. We prove that when R satisfies certain conditions, the MCMC algorithm driven by PR is at least as good as that driven by p in terms of performance in the central limit theorem and in the operator norm sense. These results are brought to bear on a theoretical comparison of the DA, PX-DA and marginal augmentation algorithms. Our focus is on situations where the group structure exploited by Liu and Wu is available. We show that the PX-DA algorithm based on Haar measure is at least as good as any PX-DA algorithm constructed using a proper prior on the group."
"10.1214/009053607000000839","2008","Statistical performance of support vector machines","2","The support vector machine (SVM) algorithm is well known to the computer learning community for its very good practical results. The goal of the present paper is to study this algorithm from a statistical perspective, using tools of concentration theory and empirical processes.Our main result builds on the observation made by other authors that the SVM can be viewed as a statistical regularization procedure. From this point of view, it can also be interpreted as a model selection principle using a penalized criterion. It is then possible to adapt general methods related to model selection in this framework to Study two important points: (1) what is the minimum penalty and how does it compare to the penalty actually used in the SVM algorithm; (2) is it possible to obtain ""oracle inequalities"" in that setting, for the specific loss function used in the SVM algorithm? We show that the answer to the latter question is positive and provides relevant insight to the former. Our result shows that it is possible to obtain fast rates of convergence for SVMs."
"10.1214/009053607000000686","2008","Asymptotic inference in some heteroscedastic regression models with long memory design and errors","1","This paper discusses asymptotic distributions of various estimators of the underlying parameters in some regression models with long memory (LM) Gaussian design and nonparametric heteroscedastic LM moving average errors. In the simple linear regression model, the first-order asymptotic distribution of the least square estimator of the slope parameter is observed to be degenerate. However, in the second order, this estimator is n(1/2)-consistent and asymptotically normal for h + H < 3/2; nonnormal otherwise, where h and H are LM parameters of design and error processes, respectively. The finite-dimensional asymptotic distributions of a class of kernel type estimators of the conditional variance function sigma(2)(x) in a more general heteroscedastic regression model are found to be normal whenever H < (1 + h)/2, and nonnormal otherwise. In addition, in this general model, log (n)-consistency of the local Whittle estimator of H based on pseudo residuals and consistency of a cross validation type estimator of sigma(2)(x) are established. All of these findings are then used to propose a lack-of-fit test of a parametric regression model, with an application to some currency exchange rate data which exhibit LM."
"10.1214/009005360700000712","2008","A complementary design theory for doubling","1","Chen and Cheng [Ann. Statist. 34 (2006) 546-558] discussed the method of doubling for constructing two-level fractional factorial designs. They showed that for 9N/32 <= n <= 5N/16, all minimum aberration designs with N runs and n factors are projections of the maximal design with 5N/16 factors which is constructed by repeatedly doubling the 2(5-1) design defined by I = ABCDE. This paper develops a general complementary design theory for doubling. For any design obtained by repeated doubling, general identities are established to link the wordlength patterns of each pair of complementary projection designs. A rule is developed for choosing minimum aberration projection designs from the maximal design with 5N/16 factors. It is further shown that for 17N/64 <= n <= 5N/16, all minimum aberration designs with N runs and n factors are projections of the maximal design with N runs and 5N/16 factors."
"10.1214/009053607000000776","2008","Locally {$D$}-optimal designs based on a class of composed models resulted from blending {$E_{\max}$} and one-compartment models","2","A class of nonlinear models combining a pharmacokinetic compartmental model and a pharmacodynamic Emax model is introduced. The locally D-optimal (LD) design for a four-parameter composed model is found to be a saturated four-point uniform LD design with the two boundary points of the design space in the LD design support. For a five-parameter composed model, a sufficient condition for the LD design to require the minimum number of sampling time points is derived. Robust LD designs are also investigated for both models. It is found that an LD design with k parameters is equivalent to an LD design with k - 1 parameters if the linear parameter in the two composed models is a nuisance parameter. Assorted examples of LD designs are presented."
"10.1214/009053607000000703","2008","Endogenous post-stratification in surveys: classifying with a sample-fitted model","2","Post-stratification is frequently used to improve the precision of survey estimators when categorical auxiliary information is available from sources outside the survey. In natural resource surveys, such information is often obtained from remote sensing data, classified into categories and displayed as pixel-based maps. These maps may be constructed based on classification models fitted to the sample data. Post-stratification of the sample data based on categories derived from the sample data (""endogenous post-stratification"") violates the standard post-stratification assumptions that observations are classified without error into post-strata, and post-stratum population counts are known. Properties of the endogenous post-stratification estimator are derived for the case of a sample-fitted generalized linear model, from which the post-strata are constructed by dividing the range of the model predictions into predetermined intervals. Design consistency of the endogenous post-stratification estimator is established under mild conditions. Under a super-population model, consistency and asymptotic normality of the endogenous post-stratification estimator are established, showing that it has the same asymptotic variance as the traditional post-stratified estimator with fixed strata. Simulation experiments demonstrate that the practical effect of first fitting a model to the survey data before post-stratifying is small, even for relatively small sample sizes."
"10.1214/009053607000000767","2008","Properties of higher criticism under strong dependence","7","The problem of signal detection using sparse, faint information is closely related to a variety of contemporary statistical problems, including the control of false-discovery rate, and classification using very high-dimensional data. Each problem can be solved by conducting a large number of simultaneous hypothesis tests, the properties of which are readily accessed under the assumption of independence. In this paper we address the case of dependent data, in the context of higher criticism methods for signal detection. Short-range dependence has no first-order impact on performance, but the situation changes dramatically under strong dependence. There, although higher criticism can continue to perform well, it can be bettered using methods based on differences of signal values or on the maximum of the data. The relatively inferior performance of higher criticism in such cases can be explained in terms of the fact that, under strong dependence, the higher criticism statistic behaves as though the data were partitioned into very large blocks, with all but a single representative of each block being eliminated from the dataset."
"10.1214/009053607000000730","2008","On false discovery control under dependence","5","A popular framework for false discovery control is the random effects model in which the null hypotheses are assumed to be independent. This paper generalizes the random effects model to a conditional dependence model which allows dependence between null hypotheses. The dependence can be useful to characterize the spatial structure of the null hypotheses. Asymptotic properties of false discovery proportions and numbers of rejected hypotheses are explored and a large-sample distributional theory is obtained."
"10.1214/009053607000000550","2008","Generalizing {S}imes' test and {H}ochberg's stepup procedure","2","In a multiple testing problem where one is willing to tolerate a few false rejections, procedure controlling the familywise error rate (FWER) can potentially be improved in terms of its ability to detect false null hypotheses by generalizing it to control the k-FWER, the probability of falsely rejecting at least k null hypotheses, for some fixed k > 1. Simes' test for testing the intersection null hypothesis is generalized to control the k-FWER weakly, that is, under the intersection null hypothesis, and Hochberg's stepup procedure for simultaneous testing of the individual null hypotheses is generalized to control the k-FWER strongly, that is, under any configuration of the true and false null hypotheses. The proposed generalizations are developed utilizing joint null distributions of the k-dimensional subsets of the p-values, assumed to be identical. The generalized Simes' test is proved to control the k-FWER weakly under the multivariate totally positive of order two (MTP2) condition V. Multivariate Analysis 10 (1980) 467-498] of the joint null distribution of the P-values by generalizing the original Simes' inequality. It is more powerful to detect k or more false null hypotheses than the original Simes' test when the p-values are independent. A stepdown procedure strongly controlling the k-FWER, a version of generalized Holm's procedure that is different from and more powerful than [Ann. Statist. 33 (2005) 1138-1154] with independent p-values, is derived before proposing the generalized Hochberg's procedure. The strong control of the k-FWER for the generalized Hochberg's procedure is established in situations where the generalized Simes' test is known to control its k-FWER weakly."
"10.1214/009053607000000721","2008","Nonlinear estimation for linear inverse problems with error in the operator","3","We study two nonlinear methods for statistical linear inverse problems when the operator is not known. The two constructions combine Galerkin regularization and wavelet thresholding. Their performances depend on the underlying structure of the operator, quantified by an index of sparsity. We prove their rate-optimality and adaptivity properties over Besov classes."
"10.1214/009053607000000668","2008","Mixed-rates asymptotics","1","A general method is presented for deriving the limiting behavior of estimators that are defined as the values of parameters optimizing an empirical criterion function. The asymptotic behavior of such estimators is typically deduced from uniform limit theorems for rescaled and reparametrized criterion functions. The new method can handle cases where the standard approach does not yield the complete limiting behavior of the estimator. The asymptotic analysis depends on a decomposition of criterion functions into sums of components with different rescalings. The method is explained by examples from Lasso-type estimation, k-means clustering, Shorth estimation and partial linear models."
"10.1214/009053607000000604","2008","Variable selection in semiparametric regression modeling","11","In this paper, we are concerned with how to select significant variables in semiparametric modeling. Variable selection for semiparametric regression models consists of two components: model selection for nonparametric components and selection of significant variables for the parametric portion. Thus, semiparametric variable selection is much more challenging than parametric variable selection (e.g., linear and generalized linear models) because traditional variable selection procedures including stepwise regression and the best subset selection now require separate model selection for the nonparametric components for each submodel. This leads to a very heavy computational burden. In this paper, we propose a class of variable selection procedures for semiparametric regression models using nonconcave penalized likelihood. We establish the rate of convergence of the resulting estimate. With proper choices of penalty functions and regularization parameters, we show the asymptotic normality of the resulting estimate and further demonstrate that the proposed procedures perform as well as an oracle procedure. A semiparametric generalized likelihood ratio test is proposed to select significant variables in the nonparametric component. We investigate the asymptotic behavior of the proposed test and demonstrate that its limiting null distribution follows a chi-square distribution which is independent of the nuisance parameters. Extensive Monte Carlo simulation studies are conducted to examine the finite sample performance of the proposed variable selection procedures."
"10.1214/009053607000000596","2008","Smooth backfitting in generalized additive models","4","Generalized additive models have been popular among statisticians and data analysts in multivariate nonparametric regression with non-Gaussian responses including binary and count data. In this paper, a new likelihood approach for fitting generalized additive models is proposed. It aims to maximize a smoothed likelihood. The additive functions are estimated by solving a system of nonlinear integral equations. An iterative algorithm based on smooth backfitting is developed from the Newton-Kantorovich theorem. Asymptotic properties of the estimator and convergence of the algorithm are discussed. It is shown that our proposal based on local linear fit achieves the same bias and variance as the oracle estimator that uses knowledge of the other components. Numerical comparison with the recently proposed two-stage estimator [Ann. Statist. 32 (2004) 2412-2443] is also made."
"10.1214/009053607000000758","2008","Regularized estimation of large covariance matrices","32","This paper considers estimating a covariance matrix of p variables from n observations by either banding or tapering the sample covariance matrix, or estimating a banded version of the inverse of the covariance. We show that these estimates are consistent in the operator norm as long as (log p)/n -> 0, and obtain explicit rates. The results are uniform over some fairly natural well-conditioned families of covariance matrices. We also introduce an analogue of the Gaussian white noise model and show that if the population covariance is embeddable in that model and well-conditioned, then the banded approximations produce consistent estimates of the eigenvalues and associated eigenvectors of the covariance matrix. The results can be extended to smooth versions of banding and to non-Gaussian distributions with sufficiently short tails. A resampling approach is proposed for choosing the banding parameter in practice. This approach is illustrated numerically on both simulated and real data."
"10.1214/009053607000000659","2008","A test for model specification of diffusion processes","2","We propose a test for model specification of a parametric diffusion process based on a kernel estimation of the transitional density of the process. The empirical likelihood is used to formulate a statistic, for each kernel smoothing bandwidth, which is effectively a Studentized L-2-distance between the kernel transitional density estimator and the parametric transitional density implied by the parametric process. To reduce the sensitivity of the test on smoothing bandwidth choice, the final test statistic is constructed by combining the empirical likelihood statistics over a set of smoothing bandwidths. TO better capture the finite sample distribution of the test statistic and data dependence, the critical value of the test is obtained by a parametric bootstrap procedure. Properties of the test are evaluated asymptotically and numerically by simulation and by a real data example."
"10.1214/009053607000000695","2008","Weighted empirical likelihood in some two-sample semiparametric models with various types of censored data","0","In this article, the weighted empirical likelihood is applied to a general setting of two-sample semiparametric models, which includes biased sampling models and case-control logistic regression models as special cases. For various types of censored data, such as right censored data, doubly censored data, interval censored data and partly interval-censored data, the weighted empirical likelihood-based semiparametric maximum likelihood estimator ((theta) over tilde (n), (F) over tilde (n)) for the underlying parameter theta(0) and distribution F(0) is derived, and the strong consistency of ((theta) over tilde (n), (F) over tilde (n)) and the asymptotic normality of (theta) over tilde (n) are established. Under biased sampling models, the weighted empirical log-likelihood ratio is shown to have an asymptotic scaled chi-squared distribution for censored data aforementioned. For right censored data, doubly censored data and partly interval-censored data, it is shown that root n((F) over tilde (n) - F(0)) weakly converges to a centered Gaussian process, which leads to a consistent goodness-of-fit test for the case-control logistic regression models."
"10.1214/009053607000000794","2008","High breakdown point robust regression with censored data","0","In this paper, we propose a class of high breakdown point estimators for the linear regression model when the response variable contains censored observations. These estimators are robust against high-leverage outliers and they generalize the LMS (least median of squares), S, MM and tau-estimators for linear regression. An important contribution of this paper is that we can define consistent estimators using a bounded loss function (or equivalently, a re-descending score function). Since the calculation of these estimators can be computationally costly, we propose an efficient algorithm to compute them. We illustrate their use on an example and present simulation studies that show that these estimators also have good finite sample properties."
"10.1214/009053607000000749","2008","Stability of the {G}ibbs sampler for {B}ayesian hierarchical models","1","We characterize the convergence of the Gibbs sampler which samples from the joint posterior distribution of parameters and missing data in hierarchical linear models with arbitrary symmetric error distributions. We show that the convergence can be uniform, geometric or subgeometric depending on the relative tail behavior of the error distributions, and on the parametrization chosen. Our theory is applied to characterize the convergence of the Gibbs sampler on latent Gaussian process models. We indicate how the theoretical framework we introduce will be useful in analyzing more complex models."
"10.1214/009053607000000631","2008","Approximation and learning by greedy algorithms","4","We consider the problem of approximating a given element f from a Hilbert space H by means of greedy algorithms and the application of such procedures to the regression problem in statistical learning theory. We improve on the existing theory of convergence rates for both the orthogonal greedy algorithm and the relaxed greedy algorithm, as well as for the forward stepwise projection algorithm. For all these algorithms, we prove convergence results for a variety of function classes and not simply those that are related to the convex hull of the dictionary. We then show how these bounds for convergence rates lead to a new theory for the performance of greedy algorithms in learning. In particular, we build upon the results in [IEEE Trans. Inform. Theory 42 (1996) 2118-2132] to construct learning algorithms based on greedy approximations which are universally consistent and provide provable convergence rates for large classes of functions. The use of greedy algorithms in the context of learning is very appealing since it greatly reduces the computational burden when compared with standard model selection using general dictionaries."
"10.1214/009053607000000811","2008","Rodeo: sparse, greedy nonparametric regression","0","We present a greedy method for simultaneously performing local bandwidth selection and variable selection in nonparametric regression. The method starts with a local linear estimator with large bandwidths, and incrementally decreases the bandwidth of variables for which the gradient of the estimator with respect to bandwidth is large. The method-called rodeo (regularization of derivative expectation operator)-conducts a sequence of hypothesis tests to threshold derivatives, and is easy to implement. Under certain assumptions on the regression function and sampling density, it is shown that the rodeo applied to local linear smoothing avoids the curse of dimensionality, achieving near optimal minimax rates of convergence in the number of relevant variables, as if these variables were isolated in advance."
"10.1214/009053607000000406","2008","Random fields of multivariate test statistics, with applications to shape analysis","1","Our data are random fields of multivariate Gaussian observations, and we fit a multivariate linear model with common design matrix at each point. We are interested in detecting those points where some of the coefficients are nonzero using classical multivariate statistics evaluated at each point. The problem is to find the P-value of the maximum of such a random field of test statistics. We approximate this by the expected Euler characteristic of the excursion set. Our main result is a very simple method for calculating this, which not only gives us the previous result of Cao and Worsley [Ann. Statist. 27 (1999) 925-942] for Hotelling's T-2, but also random fields of Roy's maximum root, maximum canonical correlations [Ann. Appl. Probab. 9 (1999) 1021-1057], multilinear forms [Ann. Statist. 29 (2001) 328-371], chi(-2) [Statist. Probab. Lett 32 (1997) 367-376, Ann. Statist. 25 (1997) 2368-2387] and chi(2) scale space [Adv. in Appl. Probab. 33 (2001) 773-793]. The trick involves approaching the problem from the point of view of Roy's union-intersection principle. The results are applied to a problem in shape analysis where we look for brain damage due to nonmissile trauma."
"10.1214/009053607000000523","2007","Estimation of the covariance matrix of random effects in longitudinal studies","2","Longitudinal studies are often conducted to explore the cohort and age effects in many scientific areas. The within cluster correlation structure plays a very important role in longitudinal data analysis. This is because not only can an estimator be improved by incorporating the within cluster correlation structure into the estimation procedure, but also the within cluster correlation structure can sometimes provide valuable insights in practical problems. For example, it can reveal the correlation strengths among the impacts of various factors. Motivated by data typified by a set from Bangladesh pertinent to the use of contraceptives, we propose a random effect varying-coefficient model, and an estimation procedure for the within cluster correlation structure of the proposed model. The estimation procedure is optimization-free and the proposed estimators enjoy asymptotic normality under mild conditions. Simulations suggest that the proposed estimation is practicable for finite samples and resistent against mild forms of model m is specification. Finally, we analyze the data mentioned above with the new random effect varying-coefficient model together with the proposed estimation procedure, which reveals some interesting sociological dynamics."
"10.1214/009053607000000505","2007","Measuring and testing dependence by correlation of distances","0","Distance correlation is a new measure of dependence between random vectors. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but unlike the classical definition of correlation, distance correlation is zero only if the random vectors are independent. The empirical distance dependence measures are based on certain Euclidean distances between sample elements rather than sample moments, yet have a compact representation analogous to the classical covariance and correlation. Asymptotic properties and applications in testing independence are discussed. Implementation of the test and Monte Carlo results are also presented."
"10.1214/009053607000000785","2007","Analysis of boosting algorithms using the smooth margin function","0","We introduce a useful tool for analyzing boosting algorithms called the ""smooth margin function,"" a differentiable approximation of the usual margin for boosting algorithms. We present two boosting algorithms based on this smooth margin, ""coordinate ascent boosting"" and ""approximate coordinate ascent boosting,"" which are similar to Freund and Schapire's AdaBoost algorithm and Breiman's arc-gv algorithm. We give convergence rates to the maximum margin solution for both of our algorithms and for arc-gv. We then study AdaBoost's convergence properties using the smooth margin function. We precisely bound the margin attained by AdaBoost when the edges of the weak classifiers fall within a specified range. This shows that a previous bound proved by Ratsch and Warmuth is exactly tight. Furthermore, we use the smooth margin to capture explicit properties of AdaBoost in cases where cyclic behavior occurs."
"10.1214/009053607000000280","2007","Some theoretical results on neural spike train probability models","0","This article contains two main theoretical results on neural spike train models, using the counting or point process on the real line as a model for the spike train. The first part of this article considers template matching of multiple spike trains. P-values for the occurrences of a given template or pattern in a set of spike trains are computed using a general scoring system. By identifying the pattern with an experimental stimulus, multiple spike trains can be deciphered to provide useful information.The second part of the article assumes that the counting process has a conditional intensity function that is a product of a free firing rate function s, which depends only on the stimulus, and a recovery function r, which depends only on the time since the last spike. If s and r belong to a q-smooth class of functions, it is proved that sieve maximum likelihood estimators for s and r achieve the optimal convergence rate (except for a logarithmic factor) under L-1 loss."
"10.1214/009053607000000352","2007","A constructive approach to the estimation of dimension reduction directions","9","In this paper we propose two new methods to estimate the dimension-reduction directions of the central subspace (CS) by constructing a regression model such that the directions are all captured in the regression mean. Compared with the inverse regression estimation methods [e.g., J. Amer Statist. Assoc. 86 (1991) 328-332, J Amer Statist. Assoc. 86 (1991) 316-342, J Amer Statist. Assoc. 87 (1992) 1025-1039], the new methods require no strong assumptions on the design of covariates or the functional relation between regressors and the response variable, and have better perforrnance than the inverse regression estimation methods for finite samples. Compared with the direct regression estimation methods [e.g., J. Amer. Statist. Assoc. 84 (1989) 986-995, Ann. Statist. 29 (2001) 1537-1566, J R. Stat. Soc. Ser B Stat. Methodol. 64 (2002) 363-410], which can only estimate the directions of CS in the regression mean, the new methods can detect the directions of CS exhaustively. Consistency of the estimators and the convergence of corresponding algorithms are proved."
"10.1214/009053607000000497","2007","Accelerated convergence for nonparametric regression with coarsened predictors","1","We consider nonparametric estimation of a regression function for a situation where precisely measured predictors are used to estimate the regression curve for coarsened, that is, less precise or contaminated predictors. Specifically, while one has available a sample (W(1), Y(1)),..., (W(n) ,Y(n)) of independent and identically distributed data, representing observations with precisely measured predictors, where E(Y(i)/W(i)) = g (W(i)), instead of the smooth regression function g, the target of interest is another smooth regression function m that pertains to predictors Xi that are noisy versions of the Wi. Our target is then the regression function m(x) = E(Y/X = x), where X is a contaminated version of W, that is, X = W + delta. It is assumed that either the density of the errors is known, or replicated data are available resembling, but not necessarily the same as, the variables X. In either case, and under suitable conditions, we obtain root n-rates of convergence of the proposed estimator and its derivatives, and establish a functional limit theorem. Weak convergence to a Gaussian limit process implies pointwise and uniform confidence intervals and /i-i-consistent estimators of extrema and zeros of m. It is shown that these results are preserved under more general models in which X is determined by an explanatory variable. Finite sample performance is investigated in simulations and illustrated by a real data example."
"10.1214/009053607000000361","2007","Testing the suitability of polynomial models in errors-in-variables problems","1","A low-degree polynomial model for a response curve is used commonly in practice. It generally incorporates a linear or quadratic function of the covariate. In this paper we suggest methods for testing the goodness of fit of a general polynomial model when there are errors in the covariates. There, the true covariates are not directly observed, and conventional bootstrap methods for testing are not applicable. We develop a new approach, in which deconvolution methods are used to estimate the distribution of the covariates under the null hypothesis, and a ""wild"" or moment-matching bootstrap argument is employed to estimate the distribution of the experimental errors (distinct from the distribution of the errors in covariates). Most of our attention is directed at the case where the distribution of the errors in covariates is known, although we also discuss methods for estimation and testing when the covariate error distribution is estimated. No assumptions are made about the distribution of experimental error, and, in particular, we depart substantially from conventional parametric models for errors-in-variables problems."
"10.1214/009053607000000415","2007","Rate-optimal estimation for a general class of nonparametric regression models with unknown link functions","3","This paper discusses a nonparametric regression model that naturally generalizes neural network models. The model is based on a finite number of one-dimensional transformations and can be estimated with a one-dimensional rate of convergence. The model contains the generalized additive model with unknown link function as a special case. For this case, it is shown that the additive components and link function can be estimated with the optimal rate by a smoothing spline that is the solution of a penalized least squares criterion."
"10.1214/009053607000000343","2007","Perturbation selection and influence measures in local influence analysis","2","Cook's [J. Roy. Statist. Soc. Ser. B 48 (1986) 133-169] local influence approach based on normal curvature is an important diagnostic tool for assessing local influence of minor perturbations to a statistical model. However, no rigorous approach has been developed to address two fundamental issues: the selection of an appropriate perturbation and the development of influence measures for objective functions at a point with a nonzero first derivative. The aim of this paper is to develop a differential-geometrical framework of a perturbation model (called the perturbation manifold) and utilize associated metric tensor and affine curvatures to resolve these issues. We will show that the metric tensor of the perturbation manifold provides important information about selecting an appropriate perturbation of a model. Moreover, we will introduce new influence measures that are applicable to objective functions at any point. Examples including linear regression models and linear mixed models are examined to demonstrate the effectiveness of using new influence measures for the identification of influential observations."
"10.1214/009053607000000262","2007","Estimation of a {$k$}-monotone density: limit distribution theory and the spline connection","1","We study the asymptotic behavior of the Maximum Likelihood and Least Squares Estimators of a k-monotone density go at a fixed point x(0) when k > 2. We find that the jib derivative of the estimators at x(0) converges at the rate n(-(k-j)/(2k+1)) for j = 0.... k - 1. The limiting distribution depends on an almost surely uniquely defined stochastic process H-k that stays above (below) the k-fold integral of Brownian motion plus a deterministic drift when k is even (odd). Both the MLE and LSE are known to be splines of degree k - I with simple knots. Establishing the order of the random gap tau(+)(n) - tau(-)(n) where tau(+/-)(n) denote two successive knots, is a key ingredient of the proof of the main results. We show that this ""gap problem"" can be solved if a conjecture about the upper bound on the error in a particular Hermite interpolation via odd-degree splines holds."
"10.1214/009053607000000253","2007","Conditional density estimation in a regression setting","1","Regression problems are traditionally analyzed via univariate characteristics like the regression function, scale function and marginal density of regression errors. These characteristics are useful and informative whenever the association between the predictor and the response is relatively simple. More detailed information about the association can be provided by the conditional density of the response given the predictor. For the first time in the literature, this article develops the theory of minimax estimation of the conditional density for regression settings with fixed and random designs of predictors, bounded and unbounded responses and a vast set of anisotropic classes of conditional densities. The study of fixed design regression is of special interest and novelty because the known literature is devoted to the case of random predictors. For the aforementioned models, the paper suggests a universal adaptive estimator which (i) matches performance of an oracle that knows both an underlying model and an estimated conditional density; (ii) is sharp minimax over a vast class of anisotropic conditional densities; (iii) is at least rate minimax when the response is independent of the predictor and thus a bivariate conditional density becomes a univariate density; (iv) is adaptive to an underlying design (fixed or random) of predictors."
"10.1214/009053607000000488","2007","Spline-backfitted kernel smoothing of nonlinear additive autoregression model","1","Application of nonparametric and semiparametric regression techniques to high-dimensional time series data has been hampered due to the lack of effective tools to address the ""curse of dimensionality."" Under rather weak conditions, we propose spline-backfitted kernel estimators of the component functions for the nonlinear additive time series data that are both computationally expedient so they are usable for analyzing very high-dimensional time series, and theoretically reliable so inference can be made on the component functions with confidence. Simulation experiments have provided strong evidence that corroborates the asymptotic theory."
"10.1214/009053607000000514","2007","Consistency of cross validation for comparing regression procedures","3","Theoretical developments on cross validation (CV) have mainly focused on selecting one among a list of finite-dimensional models (e.g., subset or order selection in linear regression) or selecting a smoothing parameter (e.g., bandwidth for kernel smoothing). However, little is known about consistency of cross validation when applied to compare between parametric and nonparametric methods or within nonparametric methods. We show that under some conditions, with an appropriate choice of data splitting ratio, cross validation is consistent in the sense of selecting the better procedure with probability approaching 1.Our results reveal interesting behavior of cross validation. When comparing two models (procedures) converging at the same nonparametric rate, in contrast to the parametric case, it turns out that the proportion of data used for evaluation in CV does not need to be dominating in size. Furthermore, it can even be of a smaller order than the proportion for estimation while not affecting the consistency property."
"10.1214/009053607000000334","2007","Estimation and confidence sets for sparse normal mixtures","5","For high dimensional statistical models, researchers have begun to focus on situations which can be described as having relatively few moderately large coefficients. Such situations lead to some very subtle statistical problems. In particular, Ingster and Donoho and Jin have considered a sparse normal means testing problem, in which they described the precise demarcation or detection boundary. Meinshausen and Rice have shown that it is even possible to estimate consistently the fraction of nonzero coordinates on a subset of the detectable region, but leave unanswered the question of exactly in which parts of the detectable region consistent estimation is possible.In the present paper we develop a new approach for estimating the fraction of nonzero means for problems where the nonzero means are moderately large. We show that the detection region described by Ingster and Donoho and Jin turns out to be the region where it is possible to consistently estimate the expected fraction of nonzero coordinates. This theory is developed further and minimax rates of convergence are derived. A procedure is constructed which attains the optimal rate of convergence in this setting. Furthermore, the procedure also provides an honest lower bound for confidence intervals while minimizing the expected length of such an interval. Simulations are used to enable comparison with the work of Meinshausen and Rice, where a procedure is given but where rates of convergence have not been discussed. Extensions to more general Gaussian mixture models are also given."
"10.1214/009053607000000398","2007","Stepup procedures controlling generalized {FWER} and generalized {FDR}","2","In many applications of multiple hypothesis testing where more than one false rejection can be tolerated, procedures controlling error rates measuring at least k false rejections, instead of at least one, for some fixed k >= 1 can potentially increase the ability of a procedure to detect false null hypotheses. The k-FWER, a generalized version of the usual familywise error rate (FWER), is such an error rate that has recently been introduced in the literature and procedures controlling it have been proposed. A further generalization of a result on the k-FWER is provided in this article. In addition, an alternative and less conservative notion of error rate, the k-FDR, is introduced in the same spirit as the k-FWER by generalizing the usual false discovery rate (FDR). A k-FWER procedure is constructed given any set of increasing constants by utilizing the kth order joint null distributions of the p-values without assuming any specific form of dependence among all the p-values. Procedures controlling the k-FDR are also developed by using the kth order joint null distributions of the p-values, first assuming that the sets of null and nonnull p-values are mutually independent or they are jointly positively dependent in the sense of being multivariate totally positive of order two (MTP2) and then discarding that assumption about the overall dependence among the p-values."
"10.1214/009053607000000460","2007","A tale of three cousins: {L}asso, {$L_2$}{B}oosting and {D}antzig. {D}iscussion: ``{T}he {D}antzig selector: statistical estimation when {$p$} is much larger than {$n$}'' [{A}nn. {S}tatist. {\bf 35} (2007), no. 6, 2313--2351; \refcno 2382644] by {E}. {C}andes and {T}. {T}ao","4",""
"10.1214/009053606000001523","2007","The {D}antzig selector: statistical estimation when {$p$} is much larger than {$n$}","49","In many important statistical applications, the number of variables or parameters p is much larger than the number of observations n. Suppose then that we have observations y = X beta + z, where beta epsilon R-p is a parameter vector of interest, X is a data matrix with possibly far fewer rows than columns, n << p, and the z(i)'s are i.i.d. N(0, sigma(2)). Is it possible to estimate beta reliably based on the noisy data y?To estimate beta, we introduce a new estimator-we call it the Dantzig selector-which is a solution to the l(1)-regularization problem(min) ((beta) over tilde epsilon Rp) parallel to(beta) over tilde parallel to l(1) subject to parallel to X*r parallel to l(infinity) <= (1 + t(-1)) root 2 log p.sigma,where r is the residual vector y - X (beta) over tilde and t is a positive scalar. We show that if X obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector beta is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability,parallel to(beta) over cap-beta parallel to(2)(l2) <= C-2 . 2 log p . (sigma(2) + Sigma(i) min(beta(2)(i), sigma(2))).Our results are nonasymptotic and we give values for the constant C. Even though n may be much smaller than p, our estimator achieves a loss within a logarithmic factor of the ideal mean squared error one would achieve with an oracle which would supply perfect information about which coordinates are nonzero, and which were above the noise level.In multivariate regression and from a model selection viewpoint, our result says that it is possible nearly to select the best subset of variables by solving a very simple convex program, which, in fact, can easily be recast as a convenient linear program (LP)."
"10.1214/009053607000000271","2007","Spatial aggregation of local likelihood estimates with applications to classification","4","This paper presents a new method for spatially adaptive local (constant) likelihood estimation which applies to a broad class of nonparametric models, including the Gaussian, Poisson and binary response models. The main idea of the method is, given a sequence of local likelihood estimates (""weak"" estimates), to construct a new aggregated estimate whose pointwise risk is of order of the smallest risk among all ""weak"" estimates. We also propose a new approach toward selecting the parameters of the procedure by providing the prescribed behavior of the resulting estimate in the simple parametric situation. We establish a number of important theoretical results concerning the optimality of the aggregated estimate. In particular, our ""oracle"" result claims that its risk is, up to some logarithmic multiplier, equal to the smallest risk for the given family of estimates. The performance of the procedure is illustrated by application to the classification problem. A numerical study demonstrates its reasonable performance in simulated and real-life examples."
"10.1214/009053607000000226","2007","On optimality of {B}ayesian testimation in the normal means problem","1","We consider a problem of recovering a high-dimensional vector mu observed in white noise, where the unknown vector g is assumed to be sparse. The objective of the paper is to develop a Bayesian formalism which gives rise to a family of l(0)-type penalties. The penalties are associated with various choices of the prior distributions pi(n)(center dot) on the number of nonzero entries of mu and, hence, are easy to interpret. The resulting Bayesian estimators lead to a general thresholding rule which accommodates many of the known thresholding and model selection procedures as particular cases corresponding to specific choices of pi(n)(center dot). Furthermore, they achieve optimality in a rather general setting under very mild conditions on the prior. We also specify the class of priors pi(n)(center dot) for which the resulting estimator is adaptively optimal (in the minimax sense) for a wide range of sparse sequences and consider several examples of such priors."
"10.1214/009053607000000208","2007","Iterative estimating equations: linear convergence and asymptotic properties","0","We propose an iterative estimating equations procedure for analysis of longitudinal data. We show that, under very mild conditions, the probability that the procedure converges at an exponential rate tends to one as the sample size increases to infinity. Furthermore, we show that the limiting estimator is consistent and asymptotically efficient, as expected. The method applies to semiparametric regression models with unspecified covariances among the observations. In the special case of linear models, the procedure reduces to iterative reweighted least squares. Finite sample performance of the procedure is studied by simulations, and compared with other methods. A numerical example from a medical study is considered to illustrate the application of the method."
"10.1214/009053607000000145","2007","Variance estimation in nonparametric regression via the difference sequence method","2","Consider a Gaussian nonparametric regression problem having both an unknown mean function and unknown variance function. This article presents a class of difference-based kernel estimators for the variance function. Optimal convergence rates that are uniform over broad functional classes and bandwidths are fully characterized, and asymptotic normality is also established. We also show that for suitable asymptotic formulations our estimators achieve the minimax rate."
"10.1214/009053607000000325","2007","Optimal third root asymptotic bounds in the statistical estimation of thresholds","0","This paper is concerned with estimating the intersection point of two densities, given a sample of both of the densities. This problem arises in classification theory. The main results provide lower bounds for the probability of the estimation errors to be large on a scale determined by the inverse cube root of the sample size. As corollaries, we obtain probabilistic bounds for the prediction error in a classification problem. The key to the proof is an entropy estimate. The lower bounds are based on bounds for general estimators, which are applicable in other contexts as well. Furthermore, we introduce a class of optimal estimators whose errors asymptotically meet the border permitted by the lower bounds."
"10.1214/009053607000000127","2007","On the ``degrees of freedom'' of the lasso","16","We study the effective degrees of freedom of the lasso in the framework of Stein's unbiased risk estimation (SURE). We show that the number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso-a conclusion that requires no special assumption on the predictors. In addition, the unbiased estimator is shown to be asymptotically consistent. With these results on hand, various model selection criteria-C-p, AIC and BIC-are available, which, along with the LARS algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit."
"10.1214/009053607000000172","2007","On surrogate dimension reduction for measurement error regression: an invariance law","0","We consider a general nonlinear regression problem where the predictors contain measurement error. It has been recently discovered that several well-known dimension reduction methods, such as OLS, SIR and pHd, can be performed on the surrogate regression problem to produce consistent estimates for the original regression problem involving the unobserved true predictor. In this paper we establish a general invariance law between the surrogate and the original dimension reduction spaces, which implies that, at least at the population level, the two dimension reduction problems are in fact equivalent. Consequently we can apply all existing dimension reduction methods to measurement error regression problems. The equivalence holds exactly for multivariate normal predictors, and approximately for arbitrary predictors. We also characterize the rate of convergence for the surrogate dimension reduction estimators. Finally, we apply several dimension reduction methods to real and simulated data sets involving measurement error to compare their performances."
"10.1214/009053607000000181","2007","Two likelihood-based semiparametric estimation methods for panel count data with covariates","5","We consider estimation in a particular semiparametric regression model for the mean of a counting process with ""panel count"" data. The basic model assumption is that the conditional mean function of the counting process is of the form E{N(t)vertical bar Z} = exp(beta(T)(0)Z)Lambda(0)(t) where Z is a vector of covariates and Lambda(0) is the baseline mean function. The ""panel count"" observation scheme involves observation of the counting process N for an individual at a random number K of random time points; both the number and the locations of these time points may differ across individuals.We study semiparametric maximum pseudo-likelihood and maximum likelihood estimators of the unknown parameters (beta(0), Lambda(0)) derived on the basis of a nonhomogeneous Poisson process assumption. The pseudo-likelihood estimator is fairly easy to compute, while the maximum likelihood estimator poses more challenges from the computational perspective. We study asymptotic properties of both estimators assuming that the proportional mean model holds, but dropping the Poisson process assumption used to derive the estimators. In particular we establish asymptotic normality for the estimators of the regression parameter beta(0) under appropriate hypotheses. The results show that our estimation procedures are robust in the sense that the estimators converge to the truth regardless of the underlying counting process."
"10.1214/009053607000000235","2007","Asymptotically optimal multistage tests of simple hypotheses","0","A family of variable stage size multistage tests of simple hypotheses is described, based on efficient multistage sampling procedures. Using a loss function that is a linear combination of sampling costs and error probabilities, these tests are shown to minimize the integrated risk to second order as the costs per stage and per observation approach zero. A numerical study shows significant improvement over group sequential tests in a binomial testing problem."
"10.1214/009053607000000307","2007","Higher-order asymptotic normality of approximations to the modified signed likelihood ratio statistic for regular models","0","Approximations to the modified signed likelihood ratio statistic are asymptotically standard normal with error of order n(-1), where n is the sample size. Proofs of this fact generally require that the sufficient statistic of the model be written as ((theta) over cap, a), where theta is the maximum likelihood estimator of the parameter theta of the model and a is an ancillary statistic. This condition is very difficult or impossible to verify for many models. However, calculation of the statistics themselves does not require this condition. The goal of this paper is to provide conditions under which these statistics are asymptotically normally distributed to order n(-1) without making any assumption about the sufficient statistic of the model."
"10.1214/0009053607000000244","2007","Goodness-of-fit tests via phi-divergences","2","A unified family of goodness-of-fit tests based on phi-divergences is introduced and studied. The new family of test statistics S-n(s) includes both the supremum version of the Anderson-Darling statistic and the test statistic of Berk and Jones [Z. Wahrsch. Verw. Gebiete 47 (1979) 47-59] as special cases (s = 2 and s = 1, resp.). We also introduce integral versions of the new statistics.We show that the asymptotic null distribution theory of Berk and Jones [Z. Wahrsch. Verw. Gebiete 47 (1979) 47-59] and Wellner and Koltchinskii [High Dimensional Probability 111 (2003) 321-332. Birkhauser, Basel] for the Berk-Jones statistic applies to the whole family of statistics S, (s) with s c[-1, 2]. On the side of power behavior, we study the test statistics under fixed alternatives and give extensions of the ""Poisson boundary"" phenomena noted by Berk and Jones for their statistic. We also extend the results of Donoho and Jin [Ann. Statist. 32 (2004) 962-994] by showing that all our new tests for s is an element of [-1, 2] have the same ""optimal detection boundary"" for normal shift mixture alternatives as Tukey's ""higher-criticism"" statistic and the Berk-Jones statistic."
"10.1214/009053607000000136","2007","Asymptotic theory of least squares estimators for nearly unstable processes under strong dependence","1","This paper considers the effect of least squares procedures for nearly unstable linear time series with strongly dependent innovations. Under a general framework and appropriate scaling, it is shown that ordinary least squares procedures converge to functionals of fractional Ornstein-Uhlenbeck processes. We use fractional integrated noise as an example to illustrate the important ideas. In this case, the functionals bear only formal analogy to those in the classical framework with uncorrelated innovations, with Wiener processes being replaced by fractional Brownian motions. It is also shown that limit theorems for the functionals involve nonstandard scaling and nonstandard limiting distributions. Results of this paper shed light on the asymptotic behavior of nearly unstable long-memory processes."
"10.1214/009053607000000299","2007","Transform martingale estimating functions","0","An estimation method is proposed for a wide variety of discrete time stochastic processes that have an intractable likelihood function but are otherwise conveniently specified by an integral transform such as the characteristic function, the Laplace transform or the probability generating function. This method involves the construction of classes of transform-based martingale estimating functions that fit into the general framework of quasi-likelihood. In the parametric setting of a discrete time stochastic process, we obtain transform quasi-score functions by projecting the unavailable score function onto the special linear spaces formed by these classes. The specification of the process by any of the main integral transforms makes possible an arbitrarily close approximation of the score function in an infinite-dimensional Hilbert space by optimally combining transform martingale quasi-score functions. It also allows an extension of the domain of application of quasi-likelihood methodology to processes with infinite conditional second moment."
"10.1214/009053607000000316","2007","Estimation of the {H}urst parameter from discrete noisy data","0","We estimate the Hurst parameter H of a fractional Brownian motion from discrete noisy data observed along a high frequency sampling scheme. The presence of systematic experimental noise makes recovery of H more difficult since relevant information is mostly contained in the high frequencies of the signal.We quantify the difficulty of the statistical problem in a min-max sense: we prove that the rate n(-1/(4H+2)) is optimal for estimating H and propose rate optimal estimators based on adaptive estimation of quadratic functionals."
"10.1214/009053607000000109","2007","Estimation in spin glasses: a first step","0","The Shenington-Kirkpatrick model of spin glasses, the Hopfield model of neural networks and the Ising spin glass are all models of binary data belonging to the one-parameter exponential family with quadratic sufficient statistic. Under bare minimal conditions, we establish the root N-consistency of the maximum pseudolikelihood estimate of the natural parameter in this family, even at critical temperatures. Since very little is known about the low and critical temperature regimes of these extremely difficult models, the proof requires several new ideas. The author's version of Stein's method is a particularly useful tool. We aim to introduce these techniques into the realm of mathematical statistics through an example and present some open questions."
"10.1214/009053607000000118","2007","Goodness-of-fit testing and quadratic functional estimation from indirect observations","0","We consider the convolution model where i.i.d. random variables Xi having unknown density f are observed with additive i.i.d. noise, independent of the X's. We assume that the density f belongs to either a Sobolev class or a class of supersmooth functions. The noise distribution is known and its characteristic function decays either polynornially or exponentially asymptotically.We consider the problem of goodness-of-fit testing in the convolution model. We prove upper bounds for the risk of a test statistic derived from a kernel estimator of the quadratic functional f f 2 based on indirect observations. When the unknown density is smoother enough than the noise density, we prove that this estimator is n(-1/2) consistent, asymptotically normal and efficient (for the variance we compute). Otherwise, we give nonparametric upper bounds for the risk of the same estimator.We give an approach unifying the proof of nonparametric minimax lower bounds for both problems. We establish them for Sobolev densities and for supersmooth densities less smooth than exponential noise. In the two setups we obtain exact testing constants associated with the asymptotic minimax rates."
"10.1214/009053607000000163","2007","Computer model validation with functional output","3","A key question in evaluation of computer models is Does the computer model adequately represent reality? A six-step process for computer model validation is set out in Bayarri et al. [Technometrics 49 (2007) 138-154] (and briefly summarized below), based on comparison of computer model runs with field data of the process being modeled. The methodology is particularly suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models; combining multiple sources of information; and being able to adapt to different, but related scenarios.Two complications that frequently arise in practice are the need to deal with highly irregular functional data and the need to acknowledge and incorporate uncertainty in the inputs. We develop methodology to deal with both complications. A key part of the approach utilizes a wavelet representation of the functional data, applies a hierarchical version of the scalar validation methodology to the wavelet coefficients, and transforms back, to ultimately compare computer model output with field output. The generality of the methodology is only limited by the capability of a combination of computational tools and the appropriateness of decompositions of the sort (wavelets) employed here.The methods and analyses we present are illustrated with a test bed dynamic stress analysis for a particular engineering system."
"10.1214/009053607000000217","2007","Object oriented data analysis: sets of trees","2","Object oriented data analysis is the statistical analysis of populations of complex objects. In the special case of functional data analysis, these data objects are curves, where standard Euclidean approaches, such as principal component analysis, have been very successful. Recent developments in medical image analysis motivate the statistical analysis of populations of more complex data objects which are elements of mildly non-Euclidean spaces, such as Lie groups and symmetric spaces, or of strongly non-Euclidean spaces, such as spaces of tree-structured data objects. These new contexts for object oriented data analysis create several potentially large new interfaces between mathematics and statistics. This point is illustrated through the careful development of a novel mathematical framework for statistical analysis of populations of tree-structured objects."
"10.1214/009053607000000064","2007","Computer-intensive rate estimation, diverging statistics and scanning","0","A general rate estimation method is proposed that is based on studying the in-sample evolution of appropriately chosen diverging/converging statistics. The proposed rate estimators are based on simple least squares arguments, and are shown to be accurate in a very general setting without requiring the choice of a tuning parameter. The notion of scanning is introduced with the purpose of extracting useful subsamples of the data series; the proposed rate estimation method is applied to different scans, and the resulting estimators are then combined to improve accuracy. Applications to heavy tail index estimation as well as to the problem of estimating the long memory parameter are discussed; a small simulation study complements our theoretical results."
"10.1214/009053606000001596","2007","Optimal rate of convergence for nonparametric change-point estimators for nonstationary sequences","0","Let (X(i))(i)=1,..., n be a possibly nonstationary sequence such that L(X(i)) = P(n), if i <= n theta and L(X(i)) = Q(n), if i > n theta, where 0 < theta < 1 is the location of the change-point to be estimated. We construct a class of estimators based on the empirical measures and a seminorm on the space of measures defined through a family of functions F. We prove the consistency of the estimator and give rates of convergence under very general conditions. In particular, the 1/n rate is achieved for a wide class of processes including long-range dependent sequences and even nonstationary ones. The approach unifies, generalizes and improves on the existing results for both parametric and nonparametric change-point estimation, applied to independent, Short-range dependent and as well long-range dependent sequences."
"10.1214/009053606000001479","2007","Asymptotic spectral theory for nonlinear time series","6","We consider asymptotic problems in spectral analysis of stationary causal processes. Limiting distributions of periodograms and smoothed periodograrn spectral density estimates are obtained and applications to the spectral domain bootstrap are given. Instead of the commonly used strong mixing conditions, in our asymptotic spectral theory we impose conditions only involving (conditional) moments, which are easily verifiable for a variety of nonlinear time series."
"10.1214/009053606000001451","2007","A companion for the {K}iefer-{W}olfowitz-{B}lum stochastic approximation algorithm","1","A stochastic algorithm for the recursive approximation of the location theta of a maximum of a regression function was introduced by Kiefer and Wolfowitz [Ann. Math. Statist. 23 (1952) 462-466] in the univariate framework, and by Blum [Ann. Math. Statist. 25 (1954) 737-744] in the multivariate case. The aim of this paper is to provide a companion algorithm to the Kiefer-Wolfowitz-Blum algorithm, which allows one to simultaneously recursively approximate the size p of the maximum of the regression function. A precise study of the joint weak convergence rate of both algorithms is given; it turns out that, unlike the location of the maximum, the size of the maximum can be approximated by an algorithm which converges at the parametric rate. Moreover, averaging leads to an asymptotically efficient algorithm for the approximation of the couple (theta, mu)."
"10.1214/009053606000001569","2007","Optimal strategies for a class of sequential control problems with precedence relations","0","Consider the following multi-phase project management problem. Each project is divided into several phases. All projects enter the next phase at the same point chosen by the decision maker based on observations up to that point. Within each phase, one can pursue the projects in any order. When pursuing the project with one unit of resource, the project state changes according to a Markov chain. The probability distribution of the Markov chain is known up to an unknown parameter. When pursued, the project generates a random reward depending on the phase and the state of the project and the unknown parameter. The decision maker faces two problems: (a) how to allocate resources to projects within each phase, and (b) when to enter the next phase, so that the total expected reward is as large as possible. In this paper we formulate the preceding problem as a stochastic scheduling problem and propose asymptotic optimal strategies, which minimize the shortfall from perfect information payoff. Concrete examples are given to illustrate our method."
"10.1214/009053607000000055","2007","Simultaneous adaptation to the margin and to complexity in classification","4","We consider the problem of adaptation to the margin and to complexity in binary classification. We suggest an exponential weighting aggregation scheme. We use this aggregation procedure to construct classifiers which adapt automatically to margin and complexity. Two main examples are worked out in which adaptivity is achieved in frameworks proposed by Steinwart and Scovel [Learning Theory. Lecture Notes in Comput. Sci. 3559 (2005) 279-294. Springer, Berlin; Ann. Statist. 35 (2007) 575-607] and Tsybakov [Ann. Statist. 32 (2004) 135-166]. Adaptive schemes, like ERM or penalized ERM, usually involve a minimization step. This is not the case for our procedure."
"10.1214/009053606000001587","2007","Aggregation for {G}aussian regression","16","This paper studies statistical aggregation procedures in the regression setting. A motivating factor is the existence of many different methods of estimation, leading to possibly competing estimators. We consider here three different types of aggregation: model selection (MS) aggregation, convex (C) aggregation and linear (L) aggregation. The objective of (MS) is to select the optimal single estimator from the list; that of (C) is to select the optimal convex combination of the given estimators; and that of (L) is to select the optimal linear combination of the given estimators. we are interested in evaluating the rates of convergence of the excess risks of the estimators obtained by these procedures. Our approach is motivated by recently published minimax results [Nemirovski, A. (2000). Topics in non-parametric statistics. Lectures on Probability Theory and Statistics (Saint-Flour, 1998). Lecture Notes in Math. 1738 85-277. Springer, Berlin; Tsybakov, A. B. (2003). Optimal rates of aggregation. Learning Theory and Kernel Machines. Lecture Notes in Artificial Intelligence 2777 303-313. Springer, Heidelberg]. There exist competing aggregation procedures achieving optimal convergence rates for each of the (MS), (C) and (L) cases separately. Since these procedures are not directly comparable with each other, we suggest an alternative solution. We prove that all three optimal rates, as well as those for the newly introduced (S) aggregation (subset selection), are nearly achieved via a single ""universal"" aggregation procedure. The procedure consists of mixing the initial estimators with weights obtained by penalized least squares. Two different penalties are considered: one of them is of the BIC type, the second one is a data-dependent l(1)-type penalty."
"10.1214/009053606000001613","2007","Asymptotic approximation of nonparametric regression experiments with unknown variances","0","Asymptotic equivalence results for nonparametric regression experiments have always assumed that the variances of the observations are known. In practice, however the variance of each observation is generally considered to be an unknown nuisance parameter. We establish an asymptotic approximation to the nonparametric regression experiment when the value of the variance is an additional parameter to be estimated or tested. This asymptotically equivalent experiment has two components: the first contains all the information about the variance and the second has all the information about the mean. The result can be extended to regression problems where the variance varies slowly from observation to observation."
"10.1214/009053607000000082","2007","Nonparametric estimation of correlation functions in longitudinal and spatial data, with applications to colon carcinogenesis experiments","4","In longitudinal and spatial studies, observations often demonstrate strong correlations that are stationary in time or distance lags, and the times or locations of these data being sampled may not be homogeneous. We propose a nonparametric estimator of the correlation function in such data, using kernel methods. We develop a pointwise asymptotic normal distribution for the proposed estimator, when the number of subjects is fixed and the number of vectors or functions within each subject goes to infinity. Based on the asymptotic theory, we propose a weighted block bootstrapping method for making inferences about the correlation function, where the weights account for the inhomogeneity of the distribution of the times or locations. The method is applied to a data set from a colon carcinogenesis study, in which colonic crypts were sampled from a piece of colon segment from each of the 12 rats in the experiment and the expression level of p27, an important cell cycle protein, was then measured for each cell within the sampled crypts. A simulation study is also provided to illustrate the numerical performance of the proposed method."
"10.1214/009053607000000073","2007","Integral curves of noisy vector fields and statistical problems in diffusion tensor imaging: nonparametric kernel estimation and hypotheses testing","1","Let nu be a vector field in a bounded open set G subset of R-d. Suppose that nu is observed with a random noise at random points X-i, i = 1,..., n, that are independent and uniformly distributed in G. The problem is to estimate the integral curve of the differential equationdx(t)/dt = nu(x(t)), t >= 0, x(0) = x(0) epsilon G,starting at a given point x(0) = x0 epsilon G and to develop statistical tests for the hypothesis that the integral curve reaches a specified set Gamma subset of G. We develop an estimation procedure based on a Nadaraya-Watson type kernel regression estimator, show the asymptotic normality of the estimated integral curve and derive differential and integral equations for the mean and covariance function of the limit Gaussian process. This provides a method of tracking not only the integral curve, but also the covariance matrix of its estimate. We also study the asymptotic distribution of the squared minimal distance from the integral curve to a smooth enough surface Gamma subset of G. Building upon this, we develop testing procedures for the hypothesis that the integral curve reaches Gamma.The problems of this nature are of interest in diffusion tensor imaging, a brain imaging technique based on measuring the diffusion tensor at discrete locations in the cerebral white matter, where the diffusion of water molecules is typically anisotropic. The diffusion tensor data is used to estimate the dominant orientations of the diffusion and to track white matter fibers from the initial location following these orientations. Our approach brings more rigorous statistical tools to the analysis of this problem providing, in particular, hypothesis testing procedures that might be useful in the study of axonal connectivity of the white matter."
"10.1214/009053606000001604","2007","A {K}iefer-{W}olfowitz comparison theorem for {W}icksell's problem","2","We extend the isotonic analysis for Wicksell's problem to estimate a regression function, which is motivated by the problem of estimating dark matter distribution in astronomy. The main result is a version of the Kiefer-Wolfowitz theorem comparing the empirical distribution to its least concave majorant, but with a convergence rate n(-1) log n faster than n(-2/3) log n. The main result is useful in obtaining asymptotic distributions for estimators, such as isotonic and smooth estimators."
"10.1214/009053607000000028","2007","A ridge-parameter approach to deconvolution","4","Kernel methods for deconvolution have attractive features, and prevail in the literature. However, they have disadvantages, which include the fact that they are usually suitable only for cases where the error distribution is infinitely supported and its characteristic function does not ever vanish. Even in these settings, optimal convergence rates are achieved by kernel estimators only when the kernel is chosen to adapt to the unknown smoothness of the target distribution. In this paper we suggest alternative ridge methods, not involving kernels in any way. We show that ridge methods (a) do not require the assumption that the error-distribution characteristic function is nonvanishing; (b) adapt themselves remarkably well to the smoothness of the target density, with the result that the degree of smoothness does not need to be directly estimated; and (c) give optimal convergence rates in a broad range of settings."
"10.1214/009053606000001442","2007","Nonparametric estimation of a point-spread function in multivariate problems","0","The removal of blur from a signal, in the presence of noise, is readily accomplished if the blur can be described in precise mathematical terms. However, there is growing interest in problems where the extent of blur is known only approximately, for example in terms of a blur function which depends on unknown parameters that must be computed from data. More challenging still is the case where no parametric assumptions are made about the blur function. There has been a limited amount of work in this setting, but it invariably relies on iterative methods, sometimes under assumptions that are mathematically convenient but physically unrealistic (e.g., that the operator defined by the blur function has an integrable inverse). In this paper we suggest a direct, noniterative approach to nonparametric, blind restoration of a signal. Our method is based on a new, ridge-based method for deconvolution, and requires only mild restrictions on the blur function. We show that the convergence rate of the method is close to optimal, from some viewpoints, and demonstrate its practical performance by applying it to real images."
"10.1214/009053607000000019","2007","Bayesian variable selection for high dimensional generalized linear models: convergence rates of the fitted densities","3","Bayesian variable selection has gained much empirical success recently in a variety of applications when the number K of explanatory variables (x(1),..., x(K)) is possibly much larger than the sample size a. For generalized linear models, if most of the x(j)'s have very small effects on the response y, we show that it is possible to use Bayesian variable selection to reduce overtitting caused by the curse of dimensionality K >> n. In this approach a suitable prior can be used to choose a few out of the many xj's to model y, so that the posterior will propose probability densities p that are ""often close"" to the true density p* in some sense. The closeness can be described by a Hellinger distance between p and p* that scales at a power very close to n(-1/2), which is the ""finite-dimensional rate"" corresponding to a low-dimensional situation. These findings extend some recent work of Jiang [Technical Report 05-02 (2005) Dept. Statistics, Northwestern Univ.] on consistency of Bayesian variable selection for binary classification."
"10.1214/009053606000001433","2007","Marginal asymptotics for the ``large {$p$}, small {$n$}'' paradigm: with applications to microarray data","6","The ""large p, small n"" paradigm arises in microarray studies, image analysis, high throughput molecular screening, astronomy, and in many other high dimensional applications. False discovery rate (FDR) methods are useful for resolving the accompanying multiple testing problems. In cDNA microarray studies, for example, p-values may be computed for each of p genes using data from n arrays, where typically p is in the thousands and n is less than 30. For FDR methods to be valid in identifying differentially expressed genes, the p-values for the nondifferentially expressed genes must simultaneously have uniform distributions marginally. While feasible for permutation p-values, this uniformity is problematic for asymptotic based p-values since the number of p-values involved goes to infinity and intuition suggests that at least some of the p-values should behave erratically. We examine this neglected issue when n is moderately large but p is almost exponentially large relative to n. We show the somewhat surprising result that, under very general dependence structures and for both mean and median tests., the p-values are simultaneously valid. A small simulation study and data analysis are used for illustration."
"10.1214/009053607000000046","2007","Dependency and false discovery rate: asymptotics","4","Some effort has been undertaken over the last decade to provide conditions for the control of the false discovery rate by the linear step-up procedure (LSU) for testing n hypotheses when test statistics are dependent. In this paper we investigate the expected error rate (EER) and the false discovery rate (FDR) in some extreme parameter configurations when n tends to infinity for test statistics being exchangeable under null hypotheses. All results are derived in terms of p-values. In a general setup we present a series of results concerning the interrelation of Simes' rejection curve and the (limiting) empirical distribution function of the p-values. Main objects under investigation are largest (limiting) crossing points between these functions, which play a key role in deriving explicit formulas for EER and FDR. As specific examples we investigate equi-correlated normal and t-variables in more detail and compute the limiting EER and FDR theoretically and numerically. A surprising limit behavior occurs if these models tend to independence."
"10.1214/009053607000000037","2007","On the performance of {FDR} control: constraints and a partial solution","3","The False Discovery Rate (FDR) paradigm aims to attain certain control on Type I errors with relatively high power for multiple hypothesis testing. The Benjamini-Hochberg (BH) procedure is a well-known FDR controlling procedure. Under a random effects model, we show that, in general, unlike the FDR, the positive FDR (pFDR) of the BH procedure cannot be controlled at an arbitrarily low level due to the limited evidence provided by the observations to separate false and true nulls. This results in a criticality phenomenon, which is characterized by a transition of the procedure's power from being positive to asymptotically 0 without any reduction in the pFDR, once the target FDR control level is below a positive critical value. To address the constraints on the power and pFDR control imposed by the criticality phenomenon, we propose a procedure which applies BH-type procedures at multiple locations in the domain of p-values. Both analysis and simulations show that the proposed procedure can attain substantially improved power and pFDR control."
"10.1214/009053606000001622","2007","Control of generalized error rates in multiple testing","3","Consider the problem of testing s hypotheses simultaneously. The usual approach restricts attention to procedures that control the probability of even one false rejection, the familywise error rate (FWER). If s is large, one might be willing to tolerate more than one false rejection, thereby increasing the ability of the procedure to correctly reject false null hypotheses. One possibility is to replace control of the FWER by control of the probability of k or more false rejections, which is called the k-FWER. We derive both single-step and step-down procedures that control the k-FWER in finite samples or asymptotically, depending on the situation. We also consider the false discovery proportion (FDP) defined as the number of false rejections divided by the total number of rejections (and defined to be 0 if there are no rejections). The false discovery rate proposed by Benjamini and Hochberg [J Roy. Statist. Soc. Ser B 57 (1995) 289-300] controls E(FDP). Here, the goal is to construct methods which satisfy, for a given gamma and alpha, P{FDP > gamma} <= alpha, at least asymptotically. In contrast to the proposals of Lehmann and Romano [Ann. Statist. 33 (2005) 1138-1154], we construct methods that implicitly take into account the dependence structure of the individual test statistics in order to further increase the ability to detect false null hypotheses. This feature is also shared by related work of van der Laan, Dudoit and Pollard [Stat. Appl. Genet. Mol. Biol. 3 (2004) article 15], but our methodology is quite different. Like the work of Pollard and van der Laan [Proc. 2003 International Multi-Conference in Computer Science and Engineering, METMBS'03 Conference (2003) 3-9] and Dudoit, van der Laan and Pollard [Stat. Appl. Genet. Mol. Biol. 3 (2004) article 13], we employ resampling methods to achieve our goals. Some simulations compare finite sample performance to currently available methods."
"10.1214/009053606000001460","2007","Size, power and false discovery rates","10","Modern scientific technology has provided a new class of large-scale simultaneous inference problems, with thousands of hypothesis tests to consider at the same time. Microarrays epitomize this type of technology, but similar situations arise in proteomics, spectroscopy, imaging, and social science surveys. This paper uses false discovery rate methods to carry out both size and power calculations on large-scale problems. A simple empirical Bayes approach allows the false discovery rate (fdr) analysis to proceed with a minimum of frequentist or Bayesian modeling assumptions. Closed-form accuracy formulas are derived for estimated false discovery rates, and used to compare different methodologies: local or tail-area fdr's, theoretical, permutation, or empirical null hypothesis estimates. Two microarray data sets as well as simulations are used to evaluate the methodology, the power diagnostics showing why nonnull cases might easily fail to appear on a list of ""significant"" discoveries."
"10.1214/009053607000000190","2007","Asymptotic expansions for sums of block-variables under weak dependence","3","Let {X-i}(i)(infinity)=-infinity be a sequence of random vectors and Y-in= f(in)(X-i,X-l) be zero mean block-variables where X-i,X-l = (X-i,X-...,X- Xi+l- 1), i >= 1, are overlapping blocks of length e and where fin are Borel measurable functions. This paper establishes valid joint asymptotic expansions of general orders for the joint distribution of the sums Sigma(n)(i=1) X-i and Sigma(n)(i-1) Y-in under weak dependence conditions on the sequence {X-i}(i=-infinity)(infinity) when the block length l grows to infinity. In contrast to the classical Edgeworth expansion results where the terms in the expansions are given by powers of n(-1/2), the expansions derived here are mixtures of two series, one in powers of n(-1/2) and the other in powers of [n/l](-1/2). Applications of the main results to (i) expansions for 7 Studentized statistics of time series data and (ii) second order correctness of the blocks of blocks bootstrap method are given."
"10.1214/009053606000001235","2007","Wishart distributions for decomposable graphs","5","When considering a graphical Gaussian model N-G Markov with respect to a decomposable graph G, the parameter space of interest for the precision parameter is the cone PG of positive definite matrices with fixed zeros corresponding to the missing edges of G. The parameter space for the scale parameter of N-G is the cone Q(G), dual to P-G, of incomplete matrices with submatrices corresponding to the cliques of G being positive definite. In this paper we construct on the cones Q(G) and P-G two families of Wishart distributions, namely the Type I and Type II Wisharts. They can be viewed as generalizations of the hyper Wishart and the inverse of the hyper inverse Wishart as defined by Dawid and Lauritzen [Ann. Statist. 21 (1993) 1272-1317]. We show that the Type I and II Wisharts have properties similar to those of the hyper and hyper inverse Wishart. Indeed, the inverse of the Type II Wishart forms a conjugate family of priors for the covariance parameter of the graphical Gaussian model and is strong directed hyper Markov for every direction given to the graph by a perfect order of its cliques, while the Type I Wishart is weak hyper Markov. Moreover, the inverse Type II Wishart as a conjugate family presents the advantage of having a multidimensional shape parameter, thus offering flexibility for the choice of a prior. Both Type I and II Wishart distributions depend on multivariate shape parameters. A shape parameter is acceptable if and only if it satisfies a certain eigenvalue property. We show that the sets of acceptable shape parameters for a noncomplete G have dimension equal to at least one plus the number of cliques in G. These families, as conjugate families, are richer than the traditional Diaconis-Ylvisaker conjugate families which all have a shape parameter set of dimension one. A decomposable graph which does not contain a three-link chain as an induced subgraph is said to be homogeneous. In this case, our Wisharts are particular cases of the Wisharts on homogeneous cones as defined by Andersson and Wojnar [J. Theoret. Probab. 17 (2004) 781-818] and the dimension of the shape parameter set is even larger than in the nonhomogeneous case: it is indeed equal to the number of cliques plus the number of distinct minimal separators. Using the model where G is a three-link chain, we show by computing a 7-tuple integral that in general we cannot expect the shape parameter sets to have dimension larger than the number of cliques plus one."
"10.1214/009053606000001550","2007","Accumulated prediction errors, information criteria and optimal forecasting for autoregressive time series","3","The predictive capability of a modification of Rissanen's accumulated prediction error (APE) criterion, APES,, is investigated in infinite-order autoregressive (AR(infinity)) models. Instead of accumulating squares of sequential prediction errors from the beginning, APES, is obtained by summing these squared errors from stage n delta(n), where n is the sample size and 1/n <= delta(n) <= 1 - (1/1n) may depend on n. Under certain regularity conditions, an asymptotic expression is derived for the mean-squared prediction error (MSPE) of an AR predictor with order determined by APES,. This expression shows that the prediction performance of APE delta(n), can vary dramatically depending on the choice of delta(n). Another interesting finding is that when delta(n) approaches 1 at a certain rate, APE delta(n), can achieve asymptotic efficiency in most practical situations. An asymptotic equivalence between APES, and an information criterion with a suitable penalty term is also established from the MSPE point of view. This offers new perspectives for understanding the information and prediction-based model selection criteria. Finally, we provide the first asymptotic efficiency result for the case when the underlying AR(infinity) model is allowed to degenerate to a finite autoregression."
"10.1214/009053606000001514","2007","Testing for change points in time series models and limiting theorems for {NED} sequences","1","This paper first establishes a strong law of large numbers and a strong invariance principle for forward and backward sums of near-epoch dependent sequences. Using these limiting theorems, we develop a general asymptotic theory on the Wald test for change points in a general class of time series models under the no change-point hypothesis. As an application, we verify our assumptions for the long-memory fractional ARIMA model."
"10.1214/009053606000001541","2007","Statistical aspects of the fractional stochastic calculus","1","We apply the techniques of stochastic integration with respect to fractional Brownian motion and the theory of regularity and supremum estimation for stochastic processes to study the maximum likelihood estimator (MLE) for the drift parameter of stochastic processes satisfying stochastic equations driven by a fractional Brownian motion with arty level of Holder-regularity (any Hurst parameter). We prove existence and strong consistency of the MLE for linear and nonlinear equations. We also prove that a version of the MLE using only discrete observations is still a strongly consistent estimator."
"10.1214/009053606000001424","2007","Asymptotic properties of covariate-adjusted response-adaptive designs","3","Response-adaptive designs have been extensively studied and used in clinical trials. However, there is a lack of a comprehensive study of responseadaptive designs that include covariates, despite their importance in clinical trials. Because the allocation scheme and the estimation of parameters are affected by both the responses and the covariates, covariate-adjusted responseadaptive (CARA) designs are very complex to formulate. In this paper, we overcome the technical hurdles and lay out a framework for general CARA designs for the allocation of subjects to K (>= 2) treatments. The asymptotic properties are studied under certain widely satisfied conditions. The proposed CARA designs can be applied to generalized linear models. Two important special cases, the linear model and the logistic regression model, are considered in detail."
"10.1214/009053607000000091","2007","A complement to {L}e {C}am's theorem","1","This paper examines asymptotic equivalence in the sense of Le Cam between density estimation experiments and the accompanying Poisson experiments. The significance of asymptotic equivalence is that all asymptotically optimal statistical procedures can be carried over from one experiment to the other. The equivalence given here is established under a weak assumption on the parameter space T. In particular, a sharp Besov smoothness condition is given on T which is sufficient for Poissonization, namely, if F is in a Besov ball B-p,q(alpha) (M) with alpha p > 1/2. Examples show Poissonization is not possible whenever up < 1/2. In addition, asymptotic equivalence of the density estimation model and the accompanying Poisson experiment is established for all compact subsets of C([0, 1](m)), a condition which includes all Holder balls with smoothness alpha > 0."
"10.1214/009053607000000154","2007","On local {$U$}-statistic processes and the estimation of densities of functions of several sample variables","0","A notion of local U-statistic process is introduced and central limit theorems in various norms are obtained for it. This involves the development of several inequalities for U-processes that may be useful in other contexts. This local U-statistic process is based on an estimator of the density of a function of several sample variables proposed by Frees [J. Amer Statist. Assoc. 89 (1994) 517-525] and, as a consequence, uniform in bandwidth central limit theorems in the sup and in the L-p norms are obtained for these estimators."
"10.1214/009053606000001497","2007","On the {$\Bbb L_p$}-error of monotonicity constrained estimators","1","We aim at estimating a function lambda : [0, 1] -> R, subject to the constraint that it is decreasing (or increasing). We provide a unified approach for studying the L-p-loss of an estimator defined as the slope of a concave (or convex) approximation of an estimator of a primitive of., based on n observations. Our main task is to prove that the Lp-loss is asymptotically Gaussian with explicit (though unknown) asymptotic mean and variance. We also prove that the local L-p-risk at a fixed point and the global Lp-risk are of order n(-p/3). Applying the results to the density and regression models, we recover and generalize known results about Grenander and Brunk estimators. Also, we obtain new results for the Huang-Wellner estimator of a monotone failure rate in the random censorship model, and for an estimator of the monotone intensity function of an inhomogeneous Poisson process."
"10.1214/009053606000001505","2007","Statistical inferences for functional data","5","With modem technology development, functional data are being observed frequently in many scientific fields. A popular method for analyzing such functional data is '' smoothing first, then estimation.'' That is, statistical inference such as estimation and hypothesis testing about functional data is conducted based on the substitution of the underlying individual functions by their reconstructions obtained by one smoothing technique or another. However, little is known about this substitution effect on functional data analysis. In this paper this problem is investigated when the local polynomial kernel (LPK) smoothing technique is used for individual function reconstructions. We find that under some mild conditions, the substitution effect can be ignored asymptotically. Based on this, we construct LPK reconstruction-based estimators for the mean, covariance and noise variance functions of a functional data set and derive their asymptotics. We also propose a GCV rule for selecting good bandwidths for the LPK reconstructions. When the mean function also depends on some time-independent covariates, we consider a functional linear model where the mean function is linearly related to the covariates but the covariate effects are functions of time. The LPK reconstruction-based estimators for the covariate effects and the covariance function are also constructed and their asymptotics are derived. Moreover, we propose a L-2- norm-based global test statistic for a general hypothesis testing problem about the covariate effects and derive its asymptotic random expression. The effect of the bandwidths selected by the proposed GCV rule on the accuracy of the LPK reconstructions and the mean function estimator is investigated via a simulation study. The proposed methodologies are illustrated via an application to a real functional data set collected in climatology."
"10.1214/009053606000001532","2007","A nonparametric approach to the estimation of lengths and surface areas","1","The Minkowski content L-0(G) of a body G C R-d represents the bound ary length (for d = 2) or the surface area (for d = 3) of G. A method for estimating L-0(G) is proposed. It relies on a nonparametric estimator based on the information provided by a random sample (taken on a rectangle containing G) in which we are able to identify whether every point is inside or outside G. Some theoretical properties concerning strong consistency, L-1-error and convergence rates are obtained. A practical application to a problem of image analysis in cardiology is discussed in some detail. A brief simulation study is provided."
"10.1214/009053606000001370","2007","Piecewise linear regularized solution paths","15","We consider the generic regularized optimization problem (beta) over cap(lambda) = arg min(beta) L (y, X beta) + lambda J (beta). Efron, Hastie, Johnstone and Tibshirani [Ann. Statist. 32 (2004) 407-499] have shown that for the LASSO-that is, if L is squared error loss and J(beta) = vertical bar vertical bar beta vertical bar vertical bar(1) is the if l(1) norm of beta-the optimal coefficient path is piecewise linear, that is, is piecewise constant. We derive a general characterization of the properties of (loss L, penalty J) pairs which give piecewise linear coefficient paths. Such pairs allow for efficient generation of the full regularized coefficient paths. We investigate the nature of efficient path following algorithms which arise. We use our results to suggest robust versions of the LASSO for regression and classification, and to develop new, efficient algorithms for existing problems in the literature, including Mammen and van de Geer's locally adaptive regression splines."
"10.1214/009053606000001389","2007","Monte {C}arlo likelihood inference for missing data models","1","We describe a Monte Carlo method to approximate the maximum likelihood estimate (MLE), when there are missing data and the observed data likelihood is not available in closed form. This method uses simulated missing data that are independent and identically distributed and independent of the observed data. Our Monte Carlo approximation to the MLE is a consistent and asymptotically normal estimate of the minimizer theta* of the Kullback-Leibler information, as both Monte Carlo and observed data sample sizes go to infinity simultaneously. Plug-in estimates of the asymptotic variance are provided for constructing confidence regions for theta*. We give Logit-Normal generalized linear mixed model examples, calculated using an R package."
"10.1214/009053606000001244","2007","Inference under right censoring for transformation models with a change-point based on a covariate threshold","6","We consider linear transformation models applied to right censored survival data with a change-point in the regression coefficient based on a covariate threshold. We establish consistency and weak convergence of the nonparametric maximum likelihood estimators. The change-point parameter is shown to be n-consistent, while the remaining parameters are shown to have the expected root-n consistency. We show that the procedure is adaptive in the sense that the nonthreshold parameters are estimable with the same precision as if the true threshold value were known. We also develop Monte Carlo methods of inference for model parameters and score tests for the existence of a change-point. A key difficulty here is that some of the model parameters are not identifiable under the null hypothesis of no change-point. Simulation studies establish the validity of the proposed score tests for finite sample sizes."
"10.1214/009053606000001578","2007","Likelihood based inference for monotone response models","1","The behavior of maximum likelihood estimates (MLEs) and the likelihood ratio statistic in a family of problems involving pointwise nonparametric estimation of a monotone function is studied. This class of problems differs radically from the usual parametric or semiparametric situations in that the MLE of the monotone function at a point converges to the truth at rate n(1/3) (slower than the usual root n rate) with a non-Gaussian limit distribution. A framework for likelihood based estimation of monotone functions is developed and limit theorems describing the behavior of the MLES and the likelihood ratio statistic are established. In particular, the likelihood ratio statistic is found to be asymptotically pivotal with a limit distribution that is no longer X-2 but can be explicitly characterized in terms of a functional of Brownian motion. Applications of the main results are presented and potential extensions discussed."
"10.1214/009053606000001280","2007","Estimating the number of classes","0","Estimating the unknown number of classes in a population has numerous important applications. In a Poisson mixture model, the problem is reduced to estimating the odds that a class is undetected in a sample. The discontinuity of the odds prevents the existence of locally unbiased and informative estimators and restricts confidence intervals to be one-sided. Confidence intervals for the number of classes are also necessarily one-sided. A sequence of lower bounds to the odds is developed and used to define pseudo maximum likelihood estimators for the number of classes."
"10.1214/009053606000001299","2007","Local partial likelihood estimation in proportional hazards regression","1","Fan, Gijbels and King [Ann. Statist. 25 (1997) 1661-1690] considered the estimation of the risk function psi(x) in the proportional hazards model. Their proposed estimator is based on integrating the estimated derivative function obtained through a local version of the partial likelihood. They proved the large sample properties of the derivative function, but the large sample properties of the estimator for the risk function itself were not established. In this paper, we consider direct estimation of the relative risk function Psi(x(2)) - Psi(x(1)) for any location normalization point x(1). The main novelty in our approach is that we select observations in shrinking neighborhoods of both x(1) and x(2) when constructing a local version of the partial likelihood, whereas Fan, Gijbels and King [Ann. Statist. 25 (1997) 1661-1690] only concentrated on a single neighborhood, resulting in the cancellation of the risk function in the local likelihood function. The asymptotic properties of our estimator are rigorously established and the variance of the estimator is easily estimated. The idea behind our approach is extended to estimate the differences between groups. A simulation study is carried out."
"10.1214/009053606000001334","2007","When do stepwise algorithms meet subset selection criteria?","0","Recent results in homotopy and solution paths demonstrate that certain well-designed greedy algorithms, with a range of values of the algorithmic parameter, can provide solution paths to a sequence of convex optimization problems. On the other hand, in regression many existing criteria in subset selection (including C-p, AIC, BIC, MDL, RIC, etc.) involve optimizing an objective function that contains a counting measure. The two optimization problems are formulated as (P1) and (P0) in the present paper. The latter is generally combinatoric and has been proven to be NP-hard. We study the conditions under which the two optimization problems have common solutions. Hence, in these situations a stepwise algorithm can be used to solve the seemingly unsolvable problem. Our main result is motivated by recent work in sparse representation, while two others emerge from different angles: a direct analysis of sufficiency and necessity and a condition on the mostly correlated covariates. An extreme example connected with least angle regression is of independent interest."
"10.1214/009053606000001316","2007","Rank-based estimation for all-pass time series models","1","An autoregressive-moving average model in which all roots of the autoregressive polynomial are reciprocals of roots of the moving average polynomial and vice versa is called an all-pass time series model. All-pass models are useful for identifying and modeling noncausal and noninvertible autoregressive-moving average processes. We establish asymptotic normality and consistency for rank-based estimators of all-pass model parameters. The estimators are obtained by minimizing the rank-based residual dispersion function given by Jaeckel [Ann. Math. Statist. 43 (1972) 1449-1458]. These estimators can have the same asymptotic efficiency as maximum likelihood estimators and are robust. The behavior of the estimators for finite samples is studied via simulation and rank estimation is used in the deconvolution of a simulated water gun seismogram."
"10.1214/009053606000001352","2007","Uniformly root-{$N$} consistent density estimators for weakly dependent invertible linear processes","0","Convergence rates of kernel density estimators for stationary time series are well studied. For invertible linear processes, we construct a new density estimator that converges, in the supremum norm, at the better, parametric, rate n(-1/2). Our estimator is a convolution of two different residual-based kernel estimators. We obtain in particular convergence rates for such residual-based kernel estimators; these results are of independent interest."
"10.1214/009053606000001325","2007","Complete enumeration of two-level orthogonal arrays of strength {$d$} with {$d+2$} constraints","2","Enumerating nonisomorphic orthogonal arrays is an important, yet very difficult, problem. Although orthogonal arrays with a specified set of parameters have been enumerated in a number of cases, general results are extremely rare. In this paper, we provide a complete solution to enumerating nonisomorphic two-level orthogonal arrays of strength d with d + 2 constraints for any d and any run size n = lambda 2(d). Our results not only give the number of nonisomorphic orthogonal arrays for given d and n, but also provide a systematic way of explicitly constructing these arrays. Our approach to the problem is to make use of the recently developed theory of J-characteristics for fractional factorial designs. Besides the general theoretical results, the paper presents some results from applications of the theory to orthogonal arrays of strength two, three and four."
"10.1214/009053606000001307","2007","On the number of support points of maximin and {B}ayesian optimal designs","0","We consider maximin and Bayesian D-optimal designs for nonlinear regression models. The maximin criterion requires the specification of a region for the nonlinear parameters in the model, while the Bayesian optimality criterion assumes that a prior for these parameters is available. On interval parameter spaces, it was observed empirically by many authors that an increase of uncertainty in the prior information (i.e., a larger range for the parameter space in the maximin criterion or a larger variance of the prior in the Bayesian criterion) yields a larger number of support points of the corresponding optimal designs. In this paper, we present analytic tools which are used to prove this phenomenon in concrete situations. The proposed methodology can be used to explain many empirically observed results in the literature. Moreover, it explains why maximin D-optimal designs are usually supported at more points than Bayesian D-optimal designs."
"10.1214/009053606000001253","2007","Resolvable designs with large blocks","0","Resolvable designs with two blocks per replicate are studied from an optimality perspective. Because in practice the number of replicates is typically less than the number of treatments, arguments can be based on the dual of the information matrix and consequently given in terms of block concurrences. Equalizing block concurrences for given block sizes is often, but not always, the best strategy. Sufficient conditions are established for various strong optimalities and a detailed study of E-optimality is offered, including a characterization of the E-optimal class. Optimal designs are found to correspond to balanced arrays and an affine-like generalization."
"10.1214/009053606000001361","2007","On rates of convergence for posterior distributions in infinite-dimensional models","0","This paper introduces a new approach to the study of rates of convergence for posterior distributions. It is a natural extension of a recent approach to the study of Bayesian consistency. In particular, we improve on current rates of convergence for models including the mixture of Dirichlet process model and the random Bernstein polynomial model."
"10.1214/009053606000001262","2007","On the design-consistency property of hierarchical {B}ayes estimators in finite population sampling","0","We obtain a limit of a hierarchical Bayes estimator of a finite population mean when the sample size is large. The limit is in the sense of ordinary calculus, where the sample observations are treated as fixed quantities. Our result suggests a simple way to correct the hierarchical Bayes estimator to achieve design-consistency, a well-known property in the traditional randomization approach to finite population sampling. We also suggest three different measures of uncertainty of our proposed estimator."
"10.1214/009053606000001271","2007","Posterior convergence rates of {D}irichlet mixtures at smooth densities","5","We study the rates of convergence of the posterior distribution for Bayesian density estimation with Dirichlet mixtures of normal distributions as the prior. The true density is assumed to be twice continuously differentiable. The bandwidth is given a sequence of priors which is obtained by scaling a single prior by an appropriate order. In order to handle this problem, we derive a new general rate theorem by considering a countable covering of the parameter space whose prior probabilities satisfy a summability condition together with certain individual bounds on the Hellinger metric entropy. We apply this new general theorem on posterior convergence rates by computing bounds for Hellinger (bracketing) entropy numbers for the involved class of densities, the error in the approximation of a smooth density by normal mixtures and the concentration rate of the prior. The best obtainable rate of convergence of the posterior turns out to be equivalent to the well-known frequentist rate for integrated mean squared error n(-2/5) up to a logarithmic factor."
"10.1214/009053606000001343","2007","Cram\'er-type large deviations for samples from a finite population","0","Cramer-type large deviations for means of samples from a finite population are established under weak conditions. The results are comparable to results for the so-called self-normalized large deviation for independent random variables. Cramer-type large deviations for the finite population Student t-statistic are also investigated."
"10.1214/009053606000001208","2007","Point estimation with exponentially tilted empirical likelihood","4","Parameters defined via general estimating equations (GEE) can be estimated by maximizing the empirical likelihood (EL). Newey and Smith [Econometrica 72 (2004) 219-255] have recently shown that this EL estimator exhibits desirable higher-order asymptotic properties, namely, that its O(n(-1)) bias is small and that bias-corrected EL is higher-order efficient. Although EL possesses these properties when the model is correctly specified, this paper shows that, in the presence of model misspecification, EL may cease to be root n convergent when the functions defining the moment conditions are unbounded (even when their expectations are bounded). In contrast, the related exponential tilting (ET) estimator avoids this problem. This paper shows that the ET and EL estimators can be naturally combined to yield an estimator called exponentially tilted empirical likelihood (ETEL) exhibiting the same O(n(-1)) bias and the same O(n(-2)) variance as EL, while maintaining root n convergence under model misspecification."
"10.1214/009053606000001217","2007","Fast learning rates for plug-in classifiers","4","It has been recently shown that, under the margin (or low noise) assumption, there exist classifiers attaining fast rates of convergence of the excess Bayes risk, that is, rates faster than n(-1/2). The work on this subject has suggested the following two conjectures: (i) the best achievable fast rate is of the order n(-1), and (ii) the plug-in classifiers generally converge more slowly than the classifiers based on empirical risk minimization. We show that both conjectures are not correct. In particular, we construct plug-in classifiers that can achieve not only fast, but also super-fast rates, that is, rates faster than n-1. We establish minimax lower bounds showing that the obtained rates cannot be improved."
"10.1214/009053606000001226","2007","Fast rates for support vector machines using {G}aussian kernels","5","For binary classification we establish learning rates up to the order of n(-1) for support vector machines (SVMs) with hinge loss and Gaussian RBF kernels. These rates are in terms of two assumptions on the considered distributions: Tsybakov's noise assumption to establish a small estimation error, and a new geometric noise condition which is used to bound the approximation error. Unlike previously proposed concepts for bounding the approximation error, the geometric noise assumption does not employ any smoothness assumption."
"10.1214/009053606000001415","2007","Confidence sets for split points in decision trees","1","We investigate the problem of finding confidence sets for split points in decision trees (CART). Our main results establish the asymptotic distribution of the least squares estimators and some associated residual sum of squares statistics in a binary decision tree approximation to a smooth regression curve. Cube-root asymptotics with nonnormal limit distributions are involved. We study various confidence sets for the split point, one calibrated using the subsampling bootstrap, and others calibrated using plug-in estimates of some nuisance parameters. The performance of the confidence sets is assessed in a simulation study. A motivation for developing such confidence sets comes from the problem of phosphorus pollution in the Everglades. Ecologists have suggested that split points provide a phosphorus threshold at which biological imbalance occurs, and the lower endpoint of the confidence set may be interpreted as a level that is protective of the ecosystem. This is illustrated using data from a Duke University Wetlands Center phosphorus dosing study in the Everglades."
"10.1214/009053606000001398","2007","Reducing variance in univariate smoothing","1","A variance reduction technique in nonparametric smoothing is proposed: at each point of estimation, form a linear combination of a preliminary estimator evaluated at nearby points with the coefficients specified so that the asymptotic bias remains unchanged. The nearby points are chosen to maximize the variance reduction. We study in detail the case of univariate local linear regression. While the new estimator retains many advantages of the local linear estimator, it has appealing asymptotic relative efficiencies. Bandwidth selection rules are available by a simple constant factor adjustment of those for local linear estimation. A simulation study indicates that the finite sample relative efficiency often matches the asymptotic relative efficiency for moderate sample sizes. This technique is very general and has a wide range of applications."
"10.1214/009053606000001406","2007","{$M$}-estimation of linear models with dependent errors","3","We study asymptotic properties of M-estimates of regression parameters in linear models in which errors are dependent. Weak and strong Bahadur representations of the M-estimates are derived and a central limit theorem is established. The results are applied to linear models with errors being short-range dependent linear processes, heavy-tailed linear processes and some widely used nonlinear time series."
"10.1214/009053606000001488","2007","Minimax and adaptive estimation of the {W}igner function in quantum homodyne tomography with noisy data","0","We estimate the quantum state of a light beam from results of quantum homodyne measurements performed on identically prepared quantum Systems. The state is represented through the Wigner function, a generalized probability density on R(2) which may take negative values and must respect intrinsic positivity constraints imposed by quantum physics. The effect of the losses due to detection inefficiencies, which are always present in a real experiment, is the addition to the tomographic data of independent Gaussian noise.We construct a kernel estimator for the Wigner function, prove that it is minimax efficient for the pointwise risk over a class of infinitely differentiable functions, and implement it for numerical results. We construct adaptive estimators, that is, which do not depend on the smoothness parameters, and prove that in some setups they attain the minimax rates for the corresponding smoothness class."
"10.1214/009053606000001136","2007","Step-up simultaneous tests for identifying active effects in orthogonal saturated designs","0","A sequence of null hypotheses regarding the number of negligible effects (zero effects) in orthogonal saturated designs is formulated. Two step-up simultaneous testing procedures are proposed to identify active effects (nonzero effects) under the commonly used assumption of effect sparsity. It is shown that each procedure controls the experimentwise error rate at a given alpha level in the strong sense."
"10.1214/009053606000001154","2007","Convergence of adaptive mixtures of importance sampling schemes","1","In the design of efficient simulation algorithms, one is often beset with a poor choice of proposal distributions. Although the performance of a given simulation kernel can clarify a posteriori how adequate this kernel is for the problem at hand, a permanent on-line modification of kernels causes concerns about the validity of the resulting algorithm. While the issue is most often intractable for MCMC algorithms, the equivalent version for importance sampling algorithms can be validated quite precisely. We derive sufficient convergence conditions for adaptive mixtures of population Monte Carlo algorithms and show that Rao-Blackwellized versions asymptotically achieve an optimum in terms of a Kullback divergence criterion, while more rudimentary versions do not benefit from repeated updating."
"10.1214/009053606000001163","2007","Stable marked point processes","1","In many contexts such as queuing theory, spatial statistics, geostatistics and meteorology, data are observed at irregular spatial positions. One model of this situation involves considering the observation points as generated by a Poisson process. Under this assumption, we study the limit behavior of the partial sums of the marked point process {(t(i), X (t(i)))}, where X(t) is a stationary random field and the points t(i) are generated from an independent Poisson random measure N on R-d. We define the sample mean and sample variance statistics and determine their joint asymptotic behavior in a heavy-tailed setting, thus extending some finite variance results of Karr [Adv. in Appl. Probab. 18 (1986) 406-422]. New results on subsampling in the context of a marked point process are also presented, with the application of forming a confidence interval for the unknown mean under an unknown degree of heavy tails."
"10.1214/009053606000001190","2007","Volatility estimators for discretely sampled {L}\'evy processes","4","This paper studies the estimation of the volatility parameter in a model where the driving process is a Brownian motion or a more general symmetric stable process that is perturbed by another Levy process. We distinguish between a parametric case, where the law of the perturbing process is known, and a semiparametric case, where it is not. In the parametric case, we construct estimators which are asymptotically efficient. In the semiparametric case, we can obtain asymptotically efficient estimators by sampling at a sufficiently high frequency, and these estimators are efficient uniformly in the law of the perturbing process."
"10.1214/009053606000001145","2007","Hazard models with varying coefficients for multivariate failure time data","1","Statistical estimation and inference for marginal hazard models with varying coefficients for multivariate failure time data are important subjects in survival analysis. A local pseudo-partial likelihood procedure is proposed for estimating the unknown coefficient functions. A weighted average estimator is also proposed in an attempt to improve the efficiency of the estimator. The consistency and asymptotic normality of the proposed estimators are established and standard error formulas for the estimated coefficients are derived and empirically tested. To reduce the computational burden of the maximum local pseudo-partial likelihood estimator, a simple and useful one-step estimator is proposed. Statistical properties of the one-step estimator are established and simulation studies are conducted to compare the performance of the one-step estimator to that of the maximum local pseudo-partial likelihood estimator. The results show that the one-step estimator can save computational cost without compromising performance both asymptotically and empirically and that an optimal weighted average estimator is more efficient than the maximum local pseudo-partial likelihood estimator. A data set from the Busselton Population Health Surveys is analyzed to illustrate our proposed methodology."
"10.1214/009053606000001127","2007","Nonparametric estimation when data on derivatives are available","0","We consider settings where data are available on a nonparametric function and various partial derivatives. Such circumstances arise in practice, for example in the joint estimation of cost and input functions in economics. We show that when derivative data are available, local averages can be replaced in certain dimensions by nonlocal averages, thus reducing the nonparametric dimension of the problem. We derive optimal rates of convergence and conditions under which dimension reduction is achieved. Kernel estimators and their properties are analyzed, although other estimators, such as local polynomial, spline and nonparametric least squares, may also be used. Simulations and an application to the estimation of electricity distribution costs are included."
"10.1214/009053606000001181","2007","Nonparametric estimation in a nonlinear cointegration type model","2","We derive an asymptotic theory of nonparametric estimation for a time series regression model Z(t) = f (X-t) + W-t, where {X-t) and {Z(t)} are observed nonstationary processes and {W-t} is an unobserved stationary process. In econometrics, this can be interpreted as a nonlinear cointegration type relationship, but we believe that our results are of wider interest. The class of nonstationary processes allowed for {Xt} is a subclass of the class of null recurrent Markov chains. This subclass contains random walk, unit root processes and nonlinear processes. We derive the asymptotics of a nonparametric estimate of f (x) under the assumption that {W-t} is a Markov chain satisfying some mixing conditions. The finite-sample properties of (f) over cap (x) are studied by means of simulation experiments."
"10.1214/009053606000001118","2007","Inference for mixtures of symmetric distributions","2","This article discusses the problem of estimation of parameters in finite mixtures when the mixture components are assumed to be symmetric and to come from the same location family. We refer to these mixtures as semi-parametric because no additional assumptions other than symmetry are made regarding the parametric form of the component distributions. Because the class of symmetric distributions is so broad, identifiability of parameters is a major issue in these mixtures. We develop a notion of identifiability of finite mixture models, which we call k-identifiability, where k denotes the number of components in the mixture. We give sufficient conditions for k-identifiability of location mixtures of symmetric components when k = 2 or 3. We propose a novel distance-based method for estimating the (location and mixing) parameters from a k-identifiable model and establish the strong consistency and asymptotic normality of the estimator. In the specific case of L-2-distance, we show that our estimator generalizes the Hodges-Lehmann estimator. We discuss the numerical implementation of these procedures, along with an empirical estimate of the component distribution, in the two-component case. In comparisons with maximum likelihood estimation assuming normal components, our method produces somewhat higher standard error estimates in the case where the components are truly normal, but dramatically outperforms the normal method when the components are heavy-tailed."
"10.1214/009053606000001172","2007","Convergence rates of posterior distributions for non-i.i.d. observations","7","We consider the asymptotic behavior of posterior distributions and Bayes estimators based on observations which are required to be neither independent nor identically distributed. We give general results on the rate of convergence of the posterior measure relative to distances derived from a testing criterion. We then specialize our results to independent, nonidentically distributed observations, Markov processes, stationary Gaussian time series and the white noise model. We apply our general results to several examples of infinite-dimensional statistical models including nonparametric regression with normal errors, binary regression, Poisson regression, an interval censoring model, Whittle estimation of the spectral density of a time series and a nonlinear autoregressive model."
"10.1214/009053606000000984","2007","Asymptotic local efficiency of {C}ram\'er-von {M}ises tests for multivariate independence","0","Deheuvels [J. Multivariate Anal. 11 (1981) 102-113] and Genest and Remillard [Test 13 (2004) 335-369] have shown that powerful rank tests of multivariate independence can be based on combinations of asymptotically independent Cramer-von Mises statistics derived from a Mobius decomposition of the empirical copula process. A result on the large-sample behavior of this process under contiguous sequences of alternatives is used here to give a representation of the limiting distribution of such test statistics and to compute their relative local asymptotic efficiency. Local power curves and asymptotic relative efficiencies are compared under familiar classes of copula alternatives."
"10.1214/009053606000001109","2007","Outlier robust corner-preserving methods for reconstructing noisy images","0","The ability to remove a large amount of noise and the ability to preserve most structure are desirable properties of an image smoother. Unfortunately, they usually seem to be at odds with each other; one can only improve one property at the cost of the other. By combining M-smoothing and least-squares-trimming, the TM-smoother is introduced as a means to unify corner-preserving properties and outlier robustness. To identify edge- and corner-preserving properties, a new theory based on differential geometry is developed. Further, robustness concepts are transferred to image processing. In two examples, the TM-smoother outperforms other comer-preserving smoothers. A software package containing both the TM- and the M-smoother can be downloaded from the Internet."
"10.1214/009053606000000993","2007","Asymptotic data analysis on manifolds","0","Given an m-dimensional compact submanifold M of Euclidean space R-s, the concept of mean location of a distribution, related to mean or expected vector, is generalized to more general R-s-valued functionals including median location, which is derived from the spatial median. The asymptotic statistical inference for general functionals of distributions on such submanifolds is elaborated. Convergence properties are studied in relation to the behavior of the underlying distributions with respect to the cutlocus. An application is given in the context of independent, but not identically distributed, samples, in particular, to a multisample setup."
"10.1214/009053606000000966","2007","Quantile regression with varying coefficients","6","Quantile regression provides a framework for modeling statistical quantities of interest other than the conditional mean. The regression methodology is well developed for linear models, but less so for nonparametric models. We consider conditional quantiles with varying coefficients and propose a methodology for their estimation and assessment using polynomial splines. The proposed estimators are easy to compute via standard quantile regression algorithms and a stepwise knot selection algorithm. The proposed Rao-score-type test that assesses the model against a linear model is also easy to implement. We provide asymptotic results on the convergence of the estimators and the null distribution of the test statistic. Empirical results are also provided, including an application of the methodology to forced expiratory volume (FEV) data."
"10.1214/009053606000000957","2007","Methodology and convergence rates for functional linear regression","7","In functional linear regression, the slope ""parameter"" is a function. Therefore, in a nonparametric context, it is determined by an infinite number of unknowns. Its estimation involves solving an ill-posed problem and has points of contact with a range of methodologies, including statistical smoothing and deconvolution. The standard approach to estimating the slope function is based explicitly on functional principal components analysis and, consequently, on spectral decomposition in terms of eigenvalues and eigenfunctions. We discuss this approach in detail and show that in certain circumstances, optimal convergence rates are achieved by the PCA technique. An alternative approach based on quadratic regularisation is suggested and shown to have advantages from some points of view."
"10.1214/009053606000001091","2007","Asymptotics for sliced average variance estimation","5","In this paper, we systematically study the consistency of sliced average variance estimation (SAVE). The findings reveal that when the response is continuous, the asymptotic behavior of SAVE is rather different from that of sliced inverse regression (SIR). SIR can achieve root n, consistency even when each slice contains only two data points. However, SAVE cannot be root n consistent and it even turns out to be not consistent when each slice contains a fixed number of data points that do not depend on n, where n is the sample size. These results theoretically confirm the notion that SAVE is more sensitive to the number of slices than SIR. Taking this into account, a bias correction is recommended in order to allow SAVE to be root n consistent. In contrast, when the response is discrete and takes finite values, root n consistency can be achieved. Therefore, an approximation through discretization, which is commonly used in practice, is studied. A simulation study is carried out for the purposes of illustration."
"10.1214/009053606000000975","2007","On the maximal bias functions of {$MM$}-estimates and constrained {$M$}-estimates of regression","0","We derive the maximum bias functions of the MM-estimates and the constrained M-estimates or CM-estimates of regression and compare them to the maximum bias functions of the S-estimates and the tau-estimates of regression. In these comparisons, the CM-estimates tend to exhibit the most favorable bias-robustness properties. Also, under the Gaussian model, it is shown how one can construct a CM-estimate which has a smaller maximum bias function than a given S-estimate, that is, the resulting CM-estimate dominates the S-estimate in terms of maxbias and, at the same time, is considerably more efficient."
"10.1214/009053606000001000","2007","Tree-structured regression and the differentiation of integrals","0","This paper provides answers to questions regarding the almost sure limiting behavior of rooted, binary tree-structured rules for regression. Examples show that questions raised by Gordon and Olshen in 1984 have negative answers. For these examples of regression functions and sequences of their associated binary tree-structured approximations, for all regression functions except those in a set of the first category, almost sure consistency fails dramatically on events of full probability. One consequence is that almost sure consistency of binary tree-structured rules such as CART requires conditions beyond requiring that (1) the regression function be in L-1, (2) partitions of a Euclidean feature space be into polytopes with sides parallel to coordinate axes, (3) the mesh of the partitions becomes arbitrarily fine almost surely and (4) the empirical learning sample content of each polytope be ""large enough."" The material in this paper includes the solution to a problem raised by Dudley in discussions. The main results have a corollary regarding the lack of almost sure consistency of certain Bayes-risk consistent rules for classification."
"10.1214/009053606000000902","2006","A frequency domain empirical likelihood for short- and long-range dependence","2","This paper introduces a version of empirical likelihood based on the periodogram and spectral estimating equations. This formulation handles dependent data through a data transformation (i.e., a Fourier transform) and is developed in terms of the spectral distribution rather than a time domain probability distribution. The asymptotic properties of frequency domain empirical likelihood are studied for linear time processes exhibiting both short- and long-range dependence. The method results in likelihood ratios which can be used to build nonparametric, asymptotically correct confidence regions for a class of normalized (or ratio) spectral parameters, including autocorrelations. Maximum empirical likelihood estimators are possible, as well as tests of spectral moment conditions. The methodology can be applied to several inference problems such as Whittle estimation and goodness-of-fit testing."
"10.1214/009053606000000920","2006","Asymptotic minimaxity of false discovery rate thresholding for sparse exponential data","7","We apply FDR thresholding to a non-Gaussian vector whose coordinates X-i, i = 1,..., n, are independent exponential with individual means mu(i). The vector mu = (mu(i)) is thought to be sparse, with most coordinates 1 but a small fraction significantly larger than 1; roughly, most coordinates are simply 'noise,' but a small fraction contain 'signal.' We measure risk by percoordinate mean-squared error in recovering log(mu(i)), and study minimax estimation over parameter spaces defined by constraints on the per-coordinate p-norm of log(mu(i)), 1/n Sigma(n)(i=1) log(p) (mu(i)) <= eta(p) .We show for large n and small q that FDR thresholding can be nearly minimax. The FDR control parameter 0 < q < 1 plays an important role: when q <= 1/2, the FDR estimator is nearly minimax, while choosing a fixed q > 1/2 prevents near minimaxity.These conclusions mirror those found in the Gaussian case in Abramovich et al. [Ann. Statist. 34 (2006) 584-653]. The techniques developed here seem applicable to a wide range of other distributional assumptions, other loss measures and non-i.i.d. dependency structures."
"10.1214/009053606000000894","2006","Semiparametric estimation of fractional cointegrating subspaces","2","We consider a common-components model for multivariate fractional cointegration, in which the s >= 1 components have different memory parameters. The cointegrating rank may exceed 1. We decompose the true cointegrating vectors into orthogonal fractional cointegrating subspaces such that vectors from distinct subspaces yield cointegrating errors with distinct memory parameters. We estimate each cointegrating subspace separately, using appropriate sets of eigenvectors of an averaged periodogram matrix of tapered, differenced observations, based on the first m Fourier frequencies, with m fixed. The angle between the true and estimated cointegrating subspaces is o(p)(l). We use the cointegrating residuals corresponding to an estimated cointegrating vector to obtain a consistent and asymptotically normal estimate of the memory parameter for the given cointegrating subspace, using a univariate Gaussian serniparametric estimator with a bandwidth that tends to infinity more slowly than n. We use these estimates to test for fractional cointegration and to consistently identify the cointegrating subspaces."
"10.1214/009053606000000885","2006","Asymptotically minimax {B}ayes predictive densities","0","Given a random sample from a distribution with density function that depends on an unknown parameter 0, we are interested in accurately estimating the true parametric density function at a future observation from the same distribution. The asymptotic risk of Bayes predictive density estimates with Kullback-Leibler loss function D(f(theta)parallel to(f) over cap) = integral (f) over cap (theta) log (f(theta)/(f) over cap) is used to examine various ways of choosing prior distributions; the principal type of choice studied is minimax. We seek asymptotically least favorable predictive densities for which the corresponding asymptotic risk is minimax. A result resembling Stein's paradox for estimating normal means by maximum likelihood holds for the uniform prior in the multivariate location family case: when the dimensionality of the model is at least three, the Jeffreys prior is minimax, though inadmissible. The Jeffreys prior is both admissible and minimax for one- and two-dimensional location problems."
"10.1214/009053606000000911","2006","Convergence rates for {B}ayesian density estimation of infinite-dimensional exponential families","0","We study the rate of convergence of posterior distributions in density estimation problems for log-densities in periodic Sobolev classes characterized by a smoothness parameter p, The posterior expected density provides a nonparametric estimation procedure attaining the optimal minimax rate of convergence under Hellinger loss if the posterior distribution achieves the optimal rate over certain uniformity classes. A prior on the density class of interest is induced by a prior on the coefficients of the trigonometric series expansion of the log-density. We show that when p is known, the posterior distribution of a Gaussian prior achieves the optimal rate provided the prior variances die off sufficiently rapidly. For a mixture of normal distributions, the mixing weights on the dimension of the exponential family are assumed to be bounded below by an exponentially decreasing sequence. To avoid the use of infinite bases, we develop priors that cut off the series at a sample-size-dependent truncation point. When the degree of smoothness is unknown, a finite mixture of normal priors indexed by the smoothness parameter, which is also assigned a prior, produces the best rate. A rate-adaptive estimator is derived."
"10.1214/009053606000000876","2006","On the limiting distributions of multivariate depth-based rank sum statistics and related tests","0","A depth-based rank sum statistic for multivariate data introduced by Liu and Singh [J. Amer. Statist. Assoc. 88 (1993) 252-260] as an extension of the Wilcoxon rank sum statistic for univariate data has been used in multivariate rank tests in quality control and in experimental studies. Those applications, however, are based on a conjectured limiting distribution, provided by Liu and Singh [J. Amer Statist. Assoc. 88 (1993) 252-260]. The present paper proves the conjecture under general regularity conditions and, therefore, validates various applications of the rank sum statistic in the literature. The paper also shows that the corresponding rank sum tests can be more powerful than Hotelling's T-2 test and some commonly used multivariate rank tests in detecting location-scale changes in multivariate distributions."
"10.1214/009053606000000858","2006","Robust estimates in generalized partially linear models","0","In this paper, we introduce a family of robust estimates for the parametric and nonparametric components under a generalized partially linear model, where the data are modeled by y(i)vertical bar(x(i), t(i)) similar to F (center dot, mu(i)) with mu(i) = H(eta(t(i)) +X-i(t) beta), for some known distribution function F and link function H. It is shown that the estimates of fi are root-n consistent and asymptotically normal. Through a Monte Carlo study, the performance of these estimators is compared with that of the classical ones."
"10.1214/009053606000000939","2006","Efficient independent component analysis","3","Independent component analysis (ICA) has been widely used for blind source separation in many fields, such as brain imaging analysis, signal processing and telecommunication. Many statistical techniques based on M-estimates have been proposed for estimating the mixing matrix. Recently, several nonparametric methods have been developed, but in-depth analysis of asymptotic efficiency has not been available. We analyze ICA using semiparametric theories and propose a straightforward estimate based on the efficient score function by using B-spline approximations. The estimate is asymptotically efficient under moderate conditions and exhibits better performance than standard ICA methods in a variety of simulations."
"10.1214/009053606000000867","2006","Nonparametric quasi-maximum likelihood estimation for {G}aussian locally stationary processes","3","This paper deals with nonparametric maximum likelihood estimation for Gaussian locally stationary processes. Our nonparametric MLE is constructed by minimizing a frequency domain likelihood over a class of functions. The asymptotic behavior of the resulting estimator is studied. The results depend on the richness of the class of functions. Both sieve estimation and global estimation are considered.Our results apply, in particular, to estimation under shape constraints. As an example, autoregressive model fitting with a monotonic variance function is discussed in detail, including algorithmic considerations.A key technical tool is the time-varying empirical spectral process indexed by functions. For this process, a Bernstein-type exponential inequality and a central limit theorem are derived. These results for empirical spectral processes are of independent interest."
"10.1214/009053606000000948","2006","Semiparametrically efficient rank-based inference for shape. {II}. {O}ptimal {$R$}-estimation of shape","7","A class of R-estimators based on the concepts of multivariate signed ranks and the optimal rank-based tests developed in Hallin and Paindaveine [Ann. Statist. 34 (2006) 2707-2756] is proposed for the estimation of the shape matrix of an elliptical distribution. These R-estimators are root-n consistent under any radial density g, without any moment assumptions, and semiparametrically efficient at some prespecified density f. When based on normal scores, they are uniformly more efficient than the traditional normal-theory estimator based on empirical covariance matrices (the asymptotic normality of which, moreover, requires finite moments of order four), irrespective of the actual underlying elliptical density. They rely on an original rank-based version of Le Cam's one-step methodology which avoids the unpleasant nonparametric estimation of cross-information quantities that is generally required in the context of R-estimation. Although they are not strictly affine-equivariant, they are shown to be equivariant in a weak asymptotic sense. Simulations confirm their feasibility and excellent finite-sample performance."
"10.1214/009053606000000731","2006","Semiparametrically efficient rank-based inference for shape. {I}. {O}ptimal rank-based tests for sphericity","5","We propose a class of rank-based procedures for testing that the shape matrix V of an elliptical distribution (with unspecified center of symmetry, scale and radial density) has some fixed value V-0; this includes, for V-0 = I-k, the problem of testing for sphericity as an important particular case. The proposed tests are invariant under translations, monotone radial transformations, rotations and reflections with respect to the estimated center of symmetry. They are valid without any moment assumption. For adequately chosen scores, they are locally asymptotically maximin (in the Le Cam sense) at given radial densities. They are strictly distribution-free when the center of symmetry is specified, and asymptotically so when it must be estimated. The multivariate ranks used throughout are those of the distances-in the metric associated with the null value V-0 of the shape matrix-between the observations and the (estimated) center of the distribution. Local powers (against elliptical alternatives) and asymptotic relative efficiencies (AREs) are derived with respect to the adjusted Mauchly test (a modified version of the Gaussian likelihood ratio procedure proposed by Muirhead and Waternaux [Biometrika 67 (1980) 31-43]) or, equivalently, with respect to (an extension of) the test for sphericity introduced by John [Biometrika 59 (1972) 169-173]. For Gaussian scores, these AREs are uniformly larger than one, irrespective of the actual radial density. Necessary and/or sufficient conditions for consistency under nonlocal, possibly nonelliptical alternatives are given. Finite sample performance is investigated via a Monte Carlo study."
"10.1214/009053606000001019","2006","Local {R}ademacher complexities and oracle inequalities in risk minimization","12","Let F be a class of measurable functions f : S -> [0, 1] defined on a probability space (S, A, P). Given a sample (X(1,...,) X(n)) of i.i.d. random variables taking values in S with common distribution P, let P(n) denote the empirical measure based on (X(1,...,) X(n)). We study an empirical risk minimization problem P(n) f -> min, f is an element of F. Given a solution (f) over cap (n) of this problem, the goal is to obtain very general upper bounds on its excess risk E(p)((f) over cap (n)) ;= P (f) over cap (n) - inf/f is an element of F p f, expressed in terms of relevant geometric parameters of the class F. Using concentration inequalities and other empirical processes tools, we obtain both distribution-dependent and data-dependent upper bounds on the excess risk that are of asymptotically correct order in many examples. The bounds involve localized sup-norms of empirical and Rademacher processes indexed by functions from the class. We use these bounds to develop model selection techniques in abstract risk minimization problems that can be applied to more specialized frameworks of regression and classification."
"10.1214/009053606000000821","2006","Can one estimate the conditional distribution of post-model-selection estimators?","4","We consider the problem of estimating the conditional distribution of a post-model-selection estimator where the conditioning is on the selected model. The notion of a post-model-selection estimator here refers to the combined procedure resulting from first selecting a model (e.g., by a model selection criterion such as AIC or by a hypothesis testing procedure) and then estimating the parameters in the selected model (e.g., by least-squares or maximum likelihood), all based on the same data set. We show that it is impossible to estimate this distribution with reasonable accuracy even asymptotically. In particular, we show that no estimator for this distribution can be uniformly consistent (not even locally). This follows as a corollary to (local) minimax lower bounds on the performance of estimators for this distribution. Similar impossibility results are also obtained for the conditional distribution of linear functions (e.g., predictors) of the post-model-selection estimator."
"10.1214/009053606000000777","2006","Blocked regular fractional factorial designs with minimum aberration","2","This paper considers the construction of minimum aberration (MA) blocked factorial designs. Based on coding theory, the concept of minimum moment aberration due to Xu [Statist. Sinica 13 (2003) 691-708] for unblocked designs is extended to blocked designs. The coding theory approach studies designs in a row-wise fashion and therefore links blocked designs with nonregular and supersaturated designs. A lower bound on blocked wordlength pattern is established. It is shown that a blocked design has MA if it originates from an unblocked MA design and achieves the lower bound. It is also shown that a regular design can be partitioned into maximal blocks if and only if it contains a row without zeros. Sufficient conditions are given for constructing MA blocked designs from unblocked MA designs. The theory is then applied to construct MA blocked designs for all 32 runs, 64 runs up to 32 factors, and all 81 runs with respect to four combined wordlength patterns."
"10.1214/009053606000000812","2006","Efficient prediction for linear and nonlinear autoregressive models","0","Conditional expectations given past observations in stationary time series are usually estimated directly by kernel estimators, or by plugging in kernel estimators for transition densities. We show that, for linear and nonlinear autoregressive models driven by independent innovations, appropriate smoothed and weighted von Mises statistics of residuals estimate conditional expectations at better parametric rates and are asymptotically efficient. The proof is based on a uniform stochastic expansion for smoothed and weighted von Mises processes of residuals. We consider, in particular, estimation of conditional distribution functions and of conditional quantile functions."
"10.1214/009053606000000803","2006","Quasi-maximum-likelihood estimation in conditionally heteroscedastic time series: a stochastic recurrence equations approach","1","This paper studies the quasi-maximum-likelihood estimator (QMLE) in a general conditionally heteroscedastic time series model of multiplicative form X-t = sigma(t)Z(t), where the unobservable volatility sigma(t) is a parametric function of (Xt-1,..., Xt-p, sigma(t-1),..., sigma(t-q)) for some p, q >= 0, and (Z(t)) is standardized i.i.d. noise. We assume that these models are solutions to stochastic recurrence equations which satisfy a contraction (random Lipschitz coefficient) property. These assumptions are satisfied for the popular GARCH, asymmetric GARCH and exponential GARCH processes. Exploiting the contraction property, we give conditions for the existence and uniqueness of a strictly stationary solution (X-t) to the stochastic recurrence equation and establish consistency and asymptotic normality of the QMLE. We also discuss the problem of invertibility of such time series models."
"10.1214/009053606000000740","2006","The shape of incomplete preferences","0","Incomplete preferences provide the epistemic foundation for models of imprecise subjective probabilities and utilities that are used in robust Bayesian analysis and in theories of bounded rationality. This paper presents a simple axiomatization of incomplete preferences and characterizes the shape of their representing sets of probabilities and utilities. Deletion of the completeness assumption from the axiom system of Anscombe and Aumann yields preferences represented by a convex set of state-dependent expected utilities, of which at least one must be a probability/utility pair. A strengthening of the state-independence axiom is needed to obtain a representation purely in terms of a set of probability/utility pairs."
"10.1214/009053606000000795","2006","Posterior consistency of {G}aussian process prior for nonparametric binary regression","2","Consider binary observations whose response probability is an unknown smooth function of a set of covariates. Suppose that a prior on the response probability function is induced by a Gaussian process mapped to the unit interval through a link function. In this paper we study consistency of the resulting posterior distribution. If the covariance kernel has derivatives up to a desired order and the bandwidth parameter of the kernel is allowed to take arbitrarily small values, we show that the posterior distribution is consistent in the L-1-distance. As an auxiliary result to our proofs, we show that, under certain conditions, a Gaussian process assigns positive probabilities to the uniform neighborhoods of a continuous function. This result may be of independent interest in the literature for small ball probabilities of Gaussian processes."
"10.1214/009053606000000759","2006","A multivariate empirical {B}ayes statistic for replicated microarray time course data","2","In this paper we derive one- and two-sample multivariate empirical Bayes statistics (the MB-statistics) to rank genes in order of interest from longitudinal replicated developmental microarray time course experiments. We first use conjugate priors to develop our one-sample multivariate empirical Bayes framework for the null hypothesis that the expected temporal profile stays at 0. This leads to our one-sample MB-statistic and a one-sample T-2-statistic, a variant of the one-sample Hotelling T-2-statistic. Both the MB-statistic and T-2-statistic can be used to rank genes in the order of evidence of nonzero mean, incorporating the correlation structure across time points, moderation and replication. We also derive the corresponding MB-statistics and T-2-statistics for the one-sample problem where the null hypothesis states that the expected temporal profile is constant, and for the two-sample problem where the null hypothesis is that two expected temporal profiles are the same."
"10.1214/009053606000000768","2006","Best subset selection, persistence in high-dimensional statistical learning and optimization under {$l_1$} constraint","9","Let (Y, X-1,..., X-m) be a random vector. It is desired to predict Y based on (X-1,..., X-m). Examples of prediction methods are regression, classification using logistic regression or separating hyperplanes, and so on.We consider the problem of best subset selection, and study it in the context m = n(alpha), alpha > 1, where n is the number of observations. We investigate procedures that are based on empirical risk minimization. It is shown, that in common cases, we should aim to find the best subset among those of size which is of order o(n/log(n)). It is also shown, that in some ""asymptotic sense,"" when assuming a certain sparsity condition, there is no loss in letting m be much larger than n, for example, m = n(alpha), alpha > 1. This is in comparison to starting with the ""best"" subset of size smaller than n and regardless of the value of alpha.We then study conditions under which empirical risk minimization subject to l(1) constraint yields nearly the best subset. These results extend some recent results obtained by Greenshtein and Ritov.Finally we present a high-dimensional simulation study of a ""boosting type"" classification procedure."
"10.1214/009053606000000786","2006","Risk bounds for statistical learning","7","We propose a general theorem providing upper bounds for the risk of an empirical risk minimizer (ERM). We essentially focus on the binary classification framework. We extend Tsybakov's analysis of the risk of an ERM under margin type conditions by using concentration inequalities for conveniently weighted empirical processes. This allows us to deal with ways of measuring the ""size"" of a class of classifiers other than entropy with bracketing as in Tsybakov's work. In particular, we derive new risk bounds for the ERM when the classification rules belong to some VC-class under margin conditions and discuss the optimality of these bounds in a minimax sense."
"10.1214/009053606000000849","2006","Optimal adaptive estimation of a quadratic functional","1","Adaptive estimation of a quadratic functional over both Besov and L(p) balls is considered. A collection of nonquadratic estimators are developed which have useful bias and variance properties over individual Besov and L(p) balls. An adaptive procedure is then constructed based on penalized maximization over this collection of nonquadratic estimators. This procedure is shown to be optimally rate adaptive over the entire range of Besov and L(p) balls in the sense that it attains certain constrained risk bounds."
"10.1214/009053606000000722","2006","Component selection and smoothing in multivariate nonparametric regression","9","We propose a new method for model selection and model fitting in multivariate nonparametric regression models, in the framework of smoothing spline ANOVA. The ""COSSO"" is a method of regularization with the penalty functional being the sum of component norms, instead of the squared norm employed in the traditional smoothing spline method. The COSSO provides a unified framework for several recent proposals for model selection in linear models and smoothing spline ANOVA models. Theoretical properties, such as the existence and the rate of convergence of the COSSO estimator, are studied. In the special case of a tensor product design with periodic functions, a detailed analysis reveals that the COSSO does model selection by applying a novel soft thresholding type operation to the function components. We give an equivalent formulation of the COSSO estimator which leads naturally to an iterative algorithm. We compare the COSSO with MARS, a popular method that builds functional ANOVA models, in simulations and real examples. The COSSO method can be extended to classification problems and we compare its performance with those of a number of machine learning algorithms on real datasets. The COSSO gives very competitive performance in these studies."
"10.1214/009053606000000696","2006","A simple smooth backfitting method for additive models","4","In this paper a new smooth backfitting estimate is proposed for additive regression models. The estimate has the simple structure of Nadaraya-Watson smooth backfitting but at the same time achieves the oracle property of local linear smooth backfitting. Each component is estimated with the same asymptotic accuracy as if the other components were known."
"10.1214/009053606000000713","2006","Multidimensional trimming based on projection depth","0","As estimators of location parameters, univariate trimmed means are well known for their robustness and efficiency. They can serve as robust alternatives to the sample mean while possessing high efficiencies at normal as well as heavy-tailed models. This paper introduces multidimensional trimmed means based on projection depth induced regions. Robustness of these depth trimmed means is investigated in terms of the influence function and finite sample breakdown point. The influence function captures the local robustness whereas the breakdown point measures the global robustness of estimators. It is found that the projection depth trimmed means are highly robust locally as well as globally. Asymptotics of the depth trimmed means are investigated via those of the directional radius of the depth induced regions. The strong consistency, asymptotic representation and limiting distribution of the depth trimmed means are obtained. Relative to the mean and other leading competitors, the depth trimmed means are highly efficient at normal or symmetric models and overwhelmingly more efficient when these models are contaminated. Simulation studies confirm the validity of the asymptotic efficiency results at finite samples."
"10.1214/009053606000000704","2006","From {$\epsilon$}-entropy to {KL}-entropy: analysis of minimum information complexity density estimation","4","We consider an extension of E-entropy to a KL-divergence based complexity measure for randomized density estimation methods. Based on this extension, we develop a general information-theoretical inequality that measures the statistical complexity of some deterministic and randomized density estimators. Consequences of the new inequality will be presented. In particular, we show that this technique can lead to improvements of some classical results concerning the convergence of minimum description length and Bayesian posterior distributions. Moreover, we are able to derive clean finite-sample convergence bounds that are not obtainable using previous approaches."
"10.1214/009053606000000830","2006","Prediction in functional linear regression","15","There has been substantial recent work on methods for estimating the slope function in linear regression for functional data analysis. However, as in the case of more conventional finite-dimensional regression, much of the practical interest in the slope centers on its application for the purpose of prediction, rather than on its significance in its own right. We show that the problems of slope-function estimation, and of prediction from an estimator of the slope function, have very different characteristics. While the former is intrinsically nonparametric, the latter can be either nonparametric or semi-parametric. In particular, the optimal mean-square convergence rate of predictors is n(-1), where n denotes sample size, if the predictand is a sufficiently smooth function. In other cases, convergence occurs at a polynomial rate that is strictly slower than n(-1). At the boundary between these two regimes, the mean-square convergence rate is less than n(-1) by only a logarithmic factor. More generally, the rate of convergence of the predicted value of the mean response in the regression model, given a particular value of the explanatory variable, is determined by a subtle interaction among the smoothness of the predictand, of the slope function in the model, and of the autocovariance function for the distribution of explanatory variables."
"10.1214/009053606000000687","2006","Poisson inverse problems","0","In this paper we focus on nonparametric estimators in inverse problems for Poisson processes involving the use of wavelet decompositions. Adopting an adaptive wavelet Galerkin discretization, we find that our method combines the well-known theoretical advantages of wavelet-vaguelette decompositions for inverse problems in terms of optimally adapting to the unknown smoothness of the solution, together with the remarkably simple closed-form expressions of Galerkin inversion methods. Adapting the results of Barron and Sheu [Ann. Statist. 19 (1991) 1347-1369] to the context of log-intensity functions approximated by wavelet series with the use of the Kullback-Leibler distance between two point processes, we also present an asymptotic analysis of convergence rates that justifies our approach. In order to shed some light on the theoretical results obtained and to examine the accuracy of our estimates in finite samples, we illustrate our method by the analysis of some simulated examples."
"10.1214/009053606000000623","2006","Conditional growth charts","9","Growth charts are often more informative when they are customized per subject, taking into account prior measurements and possibly other covariates of the subject. We study a global semiparametric quantile regression model that has the ability to estimate conditional quantiles without the usual distributional assumptions. The model can be estimated from longitudinal reference data with irregular measurement times and with some level of robustness against outliers, and it is also flexible for including covariate information. We propose a rank score test for large sample inference on covariates, and develop a new model assessment tool for longitudinal growth data. Our research indicates that the global model has the potential to be a very useful tool in conditional growth chart analysis."
"10.1214/009053606000000614","2006","Efficient likelihood estimation in state space models","1","Motivated by studying asymptotic properties of the maximum likelihood estimator (MLE) in stochastic volatility (SV) models, in this paper we investigate likelihood estimation in state space models. We first prove, under some regularity conditions, there is a consistent sequence of roots of the likelihood equation that is asymptotically normal with the inverse of the Fisher information as its variance. With an extra assumption that the likelihood equation has a unique root for each n, then there is a consistent sequence of estimators of the unknown parameters. If, in addition, the supremum of the log likelihood function is integrable, the MLE exists and is strongly consistent. Edgeworth expansion of the approximate solution of likelihood equation is also established. Several examples, including Markov switching models, ARMA models, (G)ARCH models and stochastic volatility (SV) models, are given for illustration."
"10.1214/009053606000000597","2006","Optimal designs which are efficient for lack of fit tests","0","Linear regression models are among the models most used in practice, although the practitioners are often not sure whether their assumed linear regression model is at least approximately true. In such situations, only designs for which the linear model can be checked are accepted in practice. For important linear regression models such as polynomial regression, optimal designs do not have this property. To get practically attractive designs, we suggest the following strategy. One part of the design points is used to allow one to carry out a lack of fit test with good power for practically interesting alternatives. The rest of the design points are determined in such a way that the whole design is optimal for inference on the unknown parameter in case the lack of fit test does not reject the linear regression model.To solve this problem, we introduce efficient lack of fit designs. Then we explicitly determine the e(k)-optimal design in the class of efficient lack of fit designs for polynomial regression of degree k-1."
"10.1214/009053606000000434","2006","Weighted approximations of tail copula processes with application to testing the bivariate extreme value condition","2","Consider n i.i.d. random vectors on R-2, with unknown, common distribution function F. Under a sharpening of the extreme value condition on F, we derive a weighted approximation of the corresponding tail copula process. Then we construct a test to check whether the extreme value condition holds by comparing two estimators of the limiting extreme value distribution, one obtained from the tail copula process and the other obtained by first estimating the spectral measure which is then used as a building block for the limiting extreme value distribution. We derive the limiting distribution of the test statistic from the aforementioned weighted approximation. This limiting distribution contains unknown functional parameters. Therefore, we show that a version with estimated parameters converges weakly to the true limiting distribution. Based on this result, the finite sample properties of our testing procedure are investigated through a simulation study. A real data application is also presented."
"10.1214/009053606000000416","2006","Confidence regions for high quantiles of a heavy tailed distribution","0","Estimating high quantiles plays an important role in the context of risk management. This involves extrapolation of an unknown distribution function. In this paper we propose three methods, namely, the normal approximation method, the likelihood ratio method and the data tilting method, to construct confidence regions for high quantiles of a heavy tailed distribution. A simulation study prefers the data tilting method."
"10.1214/009053606000000452","2006","A{NOVA} for diffusions and {I}t\^o processes","1","U processes are the most common form of continuous semimartingales, and include diffusion processes. This paper is concerned with the nonparametric regression relationship between two such U processes. We are interested in the quadratic variation (integrated volatility) of the residual in this regression, over a unit of time (such as a day). A main conceptual finding is that this quadratic variation can be estimated almost as if the residual process were observed, the difference being that there is also a bias which is of the same asymptotic order as the mixed normal error term.The proposed methodology, ""ANOVA for diffusions and Ito processes,"" can be used to measure the statistical quality of a parametric model and, nonparametrically, the appropriateness of a one-regressor model in general. On the other hand, it also helps quantify and characterize the trading (hedging) error in the case of financial applications."
"10.1214/009053606000000443","2006","A general asymptotic scheme for inference under order restrictions","2","Limit distributions for the greatest convex minorant and its derivative are considered for a general class of stochastic processes including partial sum processes and empirical processes, for independent, weakly dependent and long range dependent data. The results are applied to isotonic regression, isotonic regression after kernel smoothing, estimation of convex regression functions, and estimation of monotone and convex density functions. Various pointwise limit distributions are obtained, and the rate of convergence depends on the self similarity properties and on the rate of convergence of the processes considered."
"10.1214/009053606000000461","2006","Stepup procedures for control of generalizations of the familywise error rate","5","Consider the multiple testing problem of testing null hypotheses H-1,..., H-S. A classical approach to dealing with the multiplicity problem is to restrict attention to procedures that control the familywise error rate (FWER), the probability of even one false rejection. But if s is large, control of the FWER is so stringent that the ability of a procedure that controls the FWER to detect false null hypotheses is limited. It is therefore desirable to consider other measures of error control. This article considers two generalizations of the FIVER. The first is the k-FWER, in which one is willing to tolerate k or more false rejections for some fixed k >= 1. The second is based on the false discovery proportion (FDP), defined to be the number of false rejections divided by the total number of rejections (and defined to be 0 if there are no rejections). Benjamini and Hochberg [J. Roy. Statist. Soc. Ser B 57 (1995) 289-300] proposed control of the false discovery rate (FDR), by which they meant that, for fixed alpha, E(FDP) <= alpha. Here, we consider control of the FDP in the sense that, for fixed gamma and alpha, P {FDP > gamma} <= alpha. Beginning with any nondecreasing sequence of constants and p-values for the individual tests, we derive stepup procedures that control each of these two measures of error control without imposing any assumptions on the dependence structure of the p-values. We use our results to point out a few interesting connections with some closely related stepdown procedures. We then compare and contrast two FDP-controlling procedures obtained using our results with the stepup procedure for control of the FDR of Benjamini and Yekutieli."
"10.1214/009053606000000425","2006","On the {B}enjamini-{H}ochberg method","4","We investigate the properties of the Benjamini-Hochberg method for multiple testing and of a variant of Storey's generalization of it, extending and complementing the asymptotic and exact results available in the literature. Results are obtained under two different sets of assumptions and include asymptotic and exact expressions and bounds for the proportion of rejections, the proportion of incorrect rejections out of all rejections and two other proportions used to quantify the efficacy of the method."
"10.1214/009053606000000407","2006","Affinely invariant matching methods with discriminant mixtures of proportional ellipsoidally symmetric distributions","2","In observational studies designed to estimate the effects of interventions or exposures, such as cigarette smoking, it is desirable to try to control background differences between the treated group (e.g., current smokers) and the control group (e.g., never smokers) on covariates X (e.g., age, education). Matched sampling attempts to effect this control by selecting subsets of the treated and control groups with similar distributions of such covariates. This paper examines the consequences of matching using affinely invariant methods when the covariate distributions are ""discriminant mixtures of proportional ellipsoidally symmetric"" (DMPES) distributions, a class herein defined, which generalizes the ellipsoidal symmetry class of Rubin and Thomas [Ann. Statist. 20 (1992) 1079-1093]. The resulting generalized results help indicate why earlier results hold quite well even when the simple assumption of ellipsoidal symmetry is not met [e.g., Biometrics 52 (1996) 249-264]. Extensions to conditionally affinely invariant matching with conditionally DMPES distributions are also discussed."
"10.1214/009053606000000551","2006","Resampling methods for spatial regression models under a class of stochastic designs","1","In this paper we consider the problem of bootstrapping a class of spatial regression models when the sampling sites are generated by a (possibly nonuniform) stochastic design and are irregularly spaced. It is shown that the natural extension of the existing block bootstrap methods for grid spatial data does not work for irregularly spaced spatial data under nonuniform stochastic designs. A variant of the blocking mechanism is proposed. It is shown that the proposed block bootstrap method provides a valid approximation to the distribution of a class of M-estimators; of the spatial regression parameters. Finite sample properties of the method are investigated through a moderately large simulation study and a real data example is given to illustrate the methodology."
"10.1214/009053606000000588","2006","Texture synthesis and nonparametric resampling of random fields","0","This paper introduces a nonparametric algorithm for bootstrapping a stationary random field and proves certain consistency properties of the algorithm for the case of mixing random fields. The motivation for this paper comes from relating a heuristic texture synthesis algorithm popular in computer vision to general nonparametric bootstrapping of stationary random fields. We give a formal resampling scheme for the heuristic texture algorithm and prove that it produces a consistent estimate of the joint distribution of pixels in a window of certain size under mixing and regularity conditions on the random field. The joint distribution of pixels is the quantity of interest here because theories of human perception of texture suggest that two textures with the same joint distribution of pixel values in a suitably chosen window will appear similar to a human. Thus we provide theoretical justification for an algorithm that has already been very successful in practice, and suggest an explanation for its perceptually good results."
"10.1214/009053606000000579","2006","Nonparametric estimation of mean-squared prediction error in nested-error regression models","3","Nested-error regression models are widely used for analyzing clustered data. For example, they are often applied to two-stage sample surveys, and in biology and econometrics. Prediction is usually the main goal of such analyses, and mean-squared prediction error is the main way in which prediction performance is measured. In this paper we suggest a new approach to estimating mean-squared prediction error. We introduce a matched-moment, double-bootstrap algorithm, enabling the notorious underestimation of the naive mean-squared error estimator to be substantially reduced. Our approach does not require specific assumptions about the distributions of errors. Additionally, it is simple and easy to apply. This is achieved through using Monte Carlo simulation to implicitly develop formulae. which, in a more conventional approach, would be derived laboriously by mathematical arguments."
"10.1214/009053606000000560","2006","Asymptotic equivalence of nonparametric autoregression and nonparametric regression","0","It is proved that nonparametric autoregression is asymptotically equivalent in the sense of Le Cam's deficiency distance to nonparametric regression with random design as well as with regular nonrandom design."
"10.1214/009053606000000533","2006","The {B}ernstein-von {M}ises theorem for the proportional hazard model","1","We study large sample properties of Bayesian analysis of the proportional hazard model with neutral to the right process priors on the baseline hazard function. We show that the posterior distribution of the baseline cumulative hazard function and regression coefficients centered at the maximum likelihood estimator is jointly asymptotically equivalent to the sampling distribution of the maximum likelihood estimator."
"10.1214/009053606000000542","2006","Risk hull method and regularization by projections of ill-posed inverse problems","1","We study a standard method of regularization by projections of the linear inverse problem Y = Af + epsilon, where epsilon is a white Gaussian noise, and A is a known compact operator with singular values converging to zero with polynomial decay. The unknown function f is recovered by a projection method using the singular value decomposition of A. The bandwidth choice of this projection regularization is governed by a data-driven procedure which is based on the principle of risk hull minimization. We provide nonasymptotic upper bounds for the mean square risk of this method and we show, in particular, that in numerical simulations this approach may substantially improve the classical method of unbiased risk estimation."
"10.1214/009053606000000515","2006","Equi-energy sampler with applications in statistical inference and statistical mechanics","5","We introduce a new sampling algorithm, the equi-energy sampler, for efficient statistical sampling and estimation. Complementary to the widely used temperature-domain methods, the equi-energy sampler, utilizing the temperature-energy duality, targets the energy directly. The focus on the energy function not only facilitates efficient sampling, but also provides a powerful means for statistical estimation, for example, the calculation of the density of states and microcanonical averages in statistical mechanics. The equi-energy sampler is applied to a variety of problems, including exponential regression in statistics, motif sampling in computational biology and protein folding in biophysics."
"10.1214/009053606000000164","2006","Strong invariance principles for sequential {B}ahadur-{K}iefer and {V}ervaat error processes of long-range dependent sequences","0","In this paper we study strong approximations (invariance principles) of the sequential uniform and general Bahadur-Kiefer processes of long-range dependent sequences. We also investigate the strong and weak asymptotic behavior of the sequential Vervaat process, that is, the integrated sequential Bahadur-Kiefer process, properly normalized, as well as that of its deviation from its limiting process, the so-called Vervaat error process. It is well known that the Bahadur-Kiefer and the Vervaat error processes cannot converge weakly in the i.i.d. case. In contrast to this, we conclude that the Bahadur-Kiefer and Vervaat error processes, as well as their sequential versions, do converge weakly to a Dehling-Taqqu type limit process for certain long-range dependent sequences."
"10.1214/009053606000000191","2006","Fitting an error distribution in some heteroscedastic time series models","1","This paper addresses the problem of fitting a known distribution to the innovation distribution in a class of stationary and ergodic time series models. The asymptotic null distribution of the usual Kolmogorov-Smimov test based on the residuals generally depends on the underlying model parameters and the error distribution. To overcome the dependence on the underlying model parameters, we propose that tests be based on a vector of certain weighted residual empirical processes. Under the null hypothesis and under minimal moment conditions, this vector of processes is shown to converge weakly to a vector of independent copies of a Gaussian process whose covariance function depends only on the fitted distribution and not on the model. Under certain local alternatives, the proposed test is shown to have nontrivial asymptotic power. The Monte Carlo critical values of this test are tabulated when fitting standard normal and double exponential distributions. The results obtained are shown to be applicable to GARCH and ARMA-GARCH models, the often used models in econometrics and finance. A simulation study shows that the test has satisfactory size and power for finite samples at these models. The paper also contains an asymptotic uniform expansion result for a general weighted residual empirical process useful in heteroscedastic models under minimal moment conditions, a result of independent interest."
"10.1214/009053606000000209","2006","Explicit representation of finite predictor coefficients and its applications","0","We consider the finite-past predictor coefficients of stationary time series, and establish an explicit representation for them, in terms of the MA and AR coefficients. The proof is based on the alternate applications of projection operators associated with the infinite past and the infinite future. Applying the result to long memory processes, we give the rate of convergence of the finite predictor coefficients and prove an inequality of Baxter-type."
"10.1214/009053606000000173","2006","Characterizing {M}arkov equivalence classes for {AMP} chain graph models","0","Chain graphs (CG) (= adicyclic graphs) use undirected and directed edges to represent both structural and associative dependences. Like acyclic directed graphs (ADGs), the CG associated with a statistical Markov model may not be unique, so CG(S) fall into Markov equivalence classes, which may be superexponentially large, leading to unidentifiability and computational inefficiency in model search and selection. It is shown here that, under the Andersson-Madigan-Perlman (AMP) interpretation of a CG, each Markovequivalence class can be uniquely represented by a single distinguished CG, the AMP essential graph, that is itself simultaneously Markov equivalent to all CGs in the AMP Markov equivalence class. A complete characterization of AMP essential graphs is obtained. Like the essential graph previously introduced for ADGs, the AMP essential graph will play a fundamental role for inference and model search and selection for AMP CG models."
"10.1214/009053606000000065","2006","Product-limit estimators of the survival function with twice censored data","0","A model for competing (resp. complementary) risks survival data where the failure time can be left (resp. right) censored is proposed. Product-limit estimators for the survival functions of the individual risks are derived. We deduce the strong convergence of our estimators on the whole real half-line without any additional assumptions and their asymptotic normality under conditions concerning only the observed distribution. When the observations are generated according to the double censoring model introduced by Turnbull, the product-limit estimators represent upper and lower bounds for Turnbull's estimator."
"10.1214/009053606000000038","2006","Asymptotic theory for the {C}ox model with missing time-dependent covariate","0","The relationship between a time-dependent covariate and survival times is usually evaluated via the Cox model. Time-dependent covariates are generally available as longitudinal data collected regularly during the course of the study. A frequent problem, however, is the occurence of missing covariate data. A recent approach to estimation in the Cox model in this case jointly models survival and the longitudinal covariate. However, theoretical justification of this approach is still lacking. In this paper we prove existence and consistency of the maximum likelihood estimators in a joint model. The asymptotic distribution of the estimators is given along with a consistent estimator of the asymptotic variance."
"10.1214/009053606000000056","2006","An iterative procedure for general probability measures to obtain {I}-projections onto intersections of convex sets","0","The iterative proportional fitting procedure (IPFP) was introduced formally by Deming and Stephan in 1940. For bivariate densities, this procedure has been investigated by Kullback and Ruschendorf. It is well known that the IPFP is a sequence of successive I-projections onto sets of probability measures with fixed marginals. However, when finding the I-projection onto the intersection of arbitrary closed, convex sets (e.g., marginal stochastic orders), a sequence of successive I-projections onto these sets may not lead to the actual solution. Addressing this situation, we present a new iterative I-projection algorithm. Under reasonable assumptions and using tools from Fenchel duality, convergence of this algorithm to the true solution is shown. The cases of infinite dimensional IPFP and marginal stochastic orders are worked out in this context."
"10.1214/009053606000000029","2006","Misspecification in infinite-dimensional {B}ayesian statistics","0","We consider the asymptotic behavior of posterior distributions if the model is misspecified. Given a prior distribution and a random sample from a distribution P-0, which may not be in the support of the prior, we show that the posterior concentrates its mass near the points in the support of the prior that minimize the Kullback-Leibler divergence with respect to P0. An entropy condition and a prior-mass condition determine the rate of convergence. The method is applied to several examples, with special interest for infinite-dimensional models. These include Gaussian mixtures, nonparametric regression and parametric models."
"10.1214/009053606000000047","2006","A {B}ayes method for a monotone hazard rate via {\bf {S}}-paths","1","A class of random hazard rates, which is defined as a mixture of an indicator kernel convolved with a completely random measure, is of interest. We provide an explicit characterization of the posterior distribution of this mixture hazard rate model via a finite mixture of S-paths. A closed and tractable Bayes estimator for the hazard rate is derived to be a finite sum over S-paths. The path characterization or the estimator is proved to be a Rao-Blackwellization of an existing partition characterization or partition-sum estimator. This accentuates the importance of S-paths in Bayesian modeling of monotone hazard rates. An efficient Markov chain Monte Carlo (MCMC) method is proposed to approximate this class of estimates. It is shown that S-path characterization also exists in modeling with covariates by a proportional hazard model, and the proposed algorithm again applies. Numerical results of the method are given to demonstrate its practicality and effectiveness."
"10.1214/009053606000000010","2006","Shrinkage priors for {B}ayesian prediction","1","We investigate shrinkage priors for constructing Bayesian predictive distributions. It is shown that there exist shrinkage predictive distributions asymptotically dominating Bayesian predictive distributions based on the Jeffreys prior or other vague priors if the model manifold satisfies some differential geometric conditions. Kullback-Leibler divergence from the true distribution to a predictive distribution is adopted as a loss function. Conformal transformations of model manifolds corresponding to vague priors are introduced. We show several examples where shrinkage predictive distributions dominate Bayesian predictive distributions based on vague priors."
"10.1214/009053606000000128","2006","Frequentist optimality of {B}ayesian wavelet shrinkage rules for {G}aussian and non-{G}aussian noise","1","The present paper investigates theoretical performance of various Bayesian wavelet shrinkage rules in a nonparametric regression model with i.i.d. errors which are not necessarily normally distributed. The main purpose is comparison of various Bayesian models in terms of their frequentist asymptotic optimality in Sobolev and Besov spaces.We establish a relationship between hyperparameters, verify that the majority of Bayesian models studied so far achieve theoretical optimality, state which Bayesian models cannot achieve optimal convergence rate and explain why it happens."
"10.1214/009053606000000100","2006","The behavior of the {NPMLE} of a decreasing density near the boundaries of the support","0","We investigate the behavior of the nonparametric maximum likelihood estimator (f) over cap (n), for a decreasing density f near the boundaries of the support of f. We establish the limiting distribution of (f) over capn(n(-alpha)), where we need to distinguish between different values of 0 < alpha < 1. Similar results are obtained for the upper endpoint of the support, in the case it is finite. This yields consistent estimators for the values of f at the boundaries of the support. The limit distribution of these estimators is established and their performance is compared with the penalized maximum likelihood estimator."
"10.1214/009053606000000137","2006","Tailor-made tests for goodness of fit to semiparametric hypotheses","2","We introduce a new framework for constructing tests of general semiparametric hypotheses which have nontrivial power on the n(-1/2) scale in every direction, and can be tailored to put substantial power on alternatives of importance. The approach is based on combining test statistics based on stochastic processes of score statistics with bootstrap critical values."
"10.1214/009053606000000119","2006","Adaptive goodness-of-fit tests in a density model","1","Given an i.i.d. sample drawn from a density f, we propose to test that f equals some prescribed density fo or that f belongs to some translation/scale family. We introduce a multiple testing procedure based on an estimation of the L-2-distance between f and fo or between f and the parametric family that we consider. For each sample size n, our test has level of significance a. In the case of simple hypotheses, we prove that our test is adaptive: it achieves the optimal rates of testing established by Ingster [J. Math. Sci. 99 (2000) 1110-1119] over various classes of smooth functions simultaneously. As for composite hypotheses, we obtain similar results up to a logarithmic factor. We carry out a simulation study to compare our procedures with the Kolmogorov-Smimov tests, or with goodness-of-fit tests proposed by Bickel and Ritov [in Nonparametric Statistics and Related Topics (1992) 51-57] and by Kallenberg and Ledwina [Ann. Statist. 23 (1995) 1594-1608]."
"10.1214/009053606000000083","2006","Inference for covariate adjusted regression via varying coefficient models","2","We consider covariate adjusted regression (CAR), a regression method for situations where predictors and response are observed after being distorted by a multiplicative factor. The distorting factors are unknown functions of an observable covariate, where one specific distorting function is associated with each predictor or response. The dependence of both response and predictors on the same confounding covariate may alter the underlying regression relation between undistorted but unobserved predictors and response. We consider a class of highly flexible adjustment methods for parameter estimation in the underlying regression model, which is the model of interest. Asymptotic normality of the estimates is obtained by establishing a connection to varying coefficient models. These distribution results combined with proposed consistent estimates of the asymptotic variance are used for the construction of asymptotic confidence intervals for the regression coefficients. The proposed approach is illustrated with data on serum creatinine, and finite sample properties of the proposed procedures are investigated through a simulation study."
"10.1214/009053606000000074","2006","Adapting to unknown sparsity by controlling the false discovery rate","22","We attempt to recover an n-dimensional vector observed in white noise, where n is large and the vector is known to be sparse, but the degree of sparsity is unknown. We consider three different ways of defining sparsity of a vector: using the fraction of nonzero terms; imposing power-law decay bounds on the ordered entries; and controlling the l(p) norm for p small. We obtain a procedure which is asymptotically minimax for l(r) loss, simultaneously throughout a range of such sparsity classes.The optimal procedure is a data-adaptive thresholding scheme, driven by control of the false discovery rate (FDR). FDR control is a relatively recent innovation in simultaneous testing, ensuring that at most a certain expected fraction of the rejected null hypotheses will correspond to false rejections.In our treatment, the FDR control parameter q(n) also plays a determining role in asymptotic minimaxity. If q = limq(n) is an element of [0, 1/2] and also q(n) > gamma/log(n), we get sharp asymptotic minimaxity, simultaneously, over a wide range of sparse parameter spaces and loss functions. On the other hand, q = lim q(n) is an element of (1/2, 1] forces the risk to exceed the minimax risk by a factor growing with q.To our knowledge, this relation between ideas in simultaneous inference and asymptotic decision theory is new.Our work provides a new perspective on a class of model selection rules which has been introduced recently by several authors. These new rules impose complexity penalization of the form 2 center dot log(potential model size/actual model sizes). We exhibit a close connection with FDR-controlling procedures under stringent control of the false discovery rate."
"10.1214/009053606000000092","2006","Boosting for high-dimensional linear models","9","We prove that boosting with the squared error loss, L(2)Boosting, is consistent for very high-dimensional linear models, where the number of predictor variables is allowed to grow essentially as fast as O(exp(sample size)), assuming that the true underlying regression function is sparse in terms of the l(1)-norm of the regression coefficients. In the language of signal processing, this means consistency for de-noising using a strongly overcomplete dictionary if the underlying signal is sparse in terms of the l(1)-norm. We also propose here an AIC-based method for tuning, namely for choosing the number of boosting iterations. This makes L(2)Boosting computationally attractive since it is not required to run the algorithm multiple times for cross-validation as commonly used so far. We demonstrate L(2)Boosting for simulated data, in particular where the predictor dimension is large in comparison to sample size, and for a difficult tumor-classification problem with gene expression microarray data."
"10.1214/009053606000000380","2006","Generalized score test of homogeneity for mixed effects models","0","Many important problems in psychology and biomedical studies require testing for overdispersion, correlation and heterogeneity in mixed effects and latent variable models, and score tests are particularly useful for this purpose. But the existing testing procedures depend on restrictive assumptions. In this paper we propose a class of test statistics based on a general mixed effects model to test the homogeneity hypothesis that all of the variance components are zero. Under some mild conditions, not only do we derive asymptotic distributions of the test statistics, but also propose a resampling procedure for approximating their asymptotic distributions conditional on the observed data. To overcome the technical challenge, we establish an invariance principle for random quadratic forms indexed by a parameter. A simulation study is conducted to investigate the empirical performance of the test statistics. A real data set is analyzed to illustrate the application of our theoretical results."
"10.1214/009053606000000371","2006","Assessing extrema of empirical principal component functions","0","The difficulties of estimating and representing the distributions of functional data mean that principal component methods play a substantially greater role in functional data analysis than in more conventional finite-dimensional settings. Local maxima and minima in principal component functions are of direct importance; they indicate places in the domain of a random function where influence on the function value tends to be relatively strong but of opposite sign. We explore statistical properties of the relationship between extrema of empirical principal component functions, and their counterparts for the true principal component functions. It is shown that empirical principal component funcions have relatively little trouble capturing conventional extrema, but can experience difficulty distinguishing a ""shoulder"" in a curve from a small bump. For example, when the true principal component function has a shoulder, the probability that the empirical principal component function has instead a bump is approximately equal to 1/2 We suggest and describe the performance of bootstrap methods for assessing the strength of extrema. It is shown that the subsample bootstrap is more effective than the standard bootstrap in this regard. A ""bootstrap likelihood"" is proposed for measuring extremum strength. Exploratory numerical methods are suggested."
"10.1214/009053606000000272","2006","Properties of principal component methods for functional and longitudinal data analysis","15","The use of principal component methods to analyze functional data is appropriate in a wide range of different settings. In studies of ""functional data analysis,"" it has often been assumed that a sample of random functions is observed precisely, in the continuum and without noise. While this has been the traditional setting for functional data analysis, in the context of longitudinal data analysis a random function typically represents a patient, or subject, who is observed at only a small number of randomly distributed points, with nonnegligible measurement error. Nevertheless, essentially the same methods can be used in both these cases, as well as in the vast number of settings that lie between them. How is performance affected by the sampling plan? In this paper we answer that question. We show that if there is a sample of n functions, or subjects, then estimation of eigenvalues is a semiparametric problem, with root-n consistent estimators, even if only a few observations are made of each function, and if each observation is encumbered by noise. However, estimation of eigenfunctions becomes a nonparametric problem when observations are sparse. The optimal convergence rates in this case are those which pertain to more familiar function-estimation settings. We also describe the effects of sampling at regularly spaced points, as opposed to random points. In particular, it is shown that there are often advantages in sampling randomly. However, even in the case of noisy data there is a threshold sampling rate (depending on the number of functions treated) above which the rate of sampling (either randomly or regularly) has negligible impact on estimator performance, no matter whether eigenfunctions or eigenvectors are being estimated."
"10.1214/009053606000000263","2006","On the toric algebra of graphical models","2","We formulate necessary and sufficient conditions for an arbitrary discrete probability distribution to factor according to an undirected graphical model, or a log-linear model, or other more general exponential models. For decomposable graphical models these conditions are equivalent to a set of conditional independence statements similar to the Hammersley-Clifford theorem; however, we show that for nondecomposable graphical models they are not. We also show that nondecomposable models can have nonrational maximum likelihood estimates. These results are used to give several novel characterizations of decomposable graphical models."
"10.1214/009053606000000281","2006","High-dimensional graphs and variable selection with the lasso","64","The pattern of zero entries in the inverse covariance matrix of a multivariate normal distribution corresponds to conditional independence restrictions between variables. Covariance selection aims at estimating those structural zeros from data. We show that neighborhood selection with the Lasso is a computationally attractive alternative to standard covariance selection for sparse high-dimensional graphs. Neighborhood selection estimates the conditional independence restrictions separately for each node in the graph and is hence equivalent to variable selection for Gaussian linear models. We show that the proposed neighborhood selection scheme is consistent for sparse high-dimensional graphs. Consistency hinges on the choice of the penalty parameter. The oracle value for optimal prediction does not lead to a consistent neighborhood estimate. Controlling instead the probability of falsely joining some distinct connectivity components of the graph, consistent estimation for sparse graphs is achieved (with exponential rates), even when the number of variables grows as the number of observations raised to an arbitrary power."
"10.1214/009053606000000317","2006","Estimation in semiparametric spatial regression","1","Nonparametric methods have been very popular in the last couple of decades in time series and regression, but no such development has taken place for spatial models. A rather obvious reason for this is the curse of dimensionality. For spatial data on a grid evaluating the conditional mean given its closest neighbors requires a four-dimensional nonparametric regression. In this paper a serniparametric spatial regression approach is proposed to avoid this problem. An estimation procedure based on combining the so-called marginal integration technique with local linear kernel estimation is developed in the serniparametric spatial regression setting. Asymptotic distributions are established under some mild conditions. The same convergence rates as in the one-dimensional regression case are established. An application of the methodology to the classical Mercer and Hall wheat data set is given and indicates that one directional component appears to be nonlinear, which has gone unnoticed in earlier analyses."
"10.1214/009053606000000326","2006","Recovering convex boundaries from blurred and noisy observations","0","We consider the problem of estimating convex boundaries from blurred and noisy observations. In our model, the convolution of an intensity function f is observed with additive Gaussian white noise. The function f is assumed to have convex support G whose boundary is to be recovered. Rather than directly estimating the intensity function, we develop a procedure which is based on estimating the support function of the set G. This approach is closely related to the method of geometric hyperplane probing, a well-known technique in computer vision applications. We establish bounds that reveal how the estimation accuracy depends on the ill-posedness of the convolution operator and the behavior of the intensity function near the boundary."
"10.1214/009053606000000335","2006","Convergence of algorithms for reconstructing convex bodies and directional measures","0","We investigate algorithms for reconstructing a convex body K in R-n from noisy measurements of its support function or its brightness function in k directions u(1), (. .) (.) , u(k). The key idea of these algorithms is to construct a convex polytope P-k whose support function (or brightness function) best approximates the given measurements in the directions u(1), (.) (.) (.) , u(k) (in the least squares sense). The measurement errors are assumed to be stochastically independent and Gaussian.It is shown that this procedure is (strongly) consistent, meaning that, almost surely, P-k tends to K in the Hausdorff metric as k -> infinity. Here some mild assumptions on the sequence (u(i)) of directions are needed. Using results from the theory of empirical processes, estimates of rates of convergence are derived, which are first obtained in the L-2 metric and then transferred to the Hausdorff metric. Along the way, a new estimate is obtained for the metric entropy of the class of origin-symmetric zonoids contained in the unit ball.Similar results are obtained for the convergence of an algorithm that reconstructs an approximating measure to the directional measure of a stationary fiber process from noisy measurements of its rose of intersections in k directions u(1), (.) (.) (.) , u(k). Here the Dudley and Prohorov metrics are used. The methods are linked to those employed for the support and brightness function algorithms via the fact that the rose of intersections is the support function of a projection body."
"10.1214/009053606000000308","2006","Closed form expressions for {B}ayesian sample size","0","Sample size criteria are often expressed in terms of the concentration of the posterior density, as controlled by some sort of error bound. Since this is done pre-experimentally, one can regard the posterior density as a function of the data. Thus, when a sample size criterion is formalized in terms of a functional of the posterior, its value is a random variable. Generally, such functionals have means under the true distribution.We give asymptotic expressions for the expected value, under a fixed parameter, for certain types of functionals of the posterior density in a Bayesian analysis. The generality of our treatment permits us to choose functionals that encapsulate a variety of inference criteria and large ranges of error bounds. Consequently, we get simple inequalities which can be solved to give minimal sample sizes needed for various estimation goals. In several parametric examples, we verify that our asymptotic bounds give good approximations to the expected values of the functionals they approximate. Also, our numerical computations suggest our treatment gives reasonable results."
"10.1214/009053606000000290","2006","Bayesian analysis for reversible {M}arkov chains","1","We introduce a natural conjugate prior for the transition matrix of a reversible Markov chain. This allows estimation and testing. The prior arises from random walk with reinforcement in the same way the Dirichlet prior arises from Polya's urn. We give closed form normalizing constants, a simple method of simulation from the posterior and a characterization along the lines of W. E. Johnson's characterization of the Dirichlet prior."
"10.1214/009053606000000236","2006","Consistency of {B}ayes estimators of a binary regression function","0","do nonparametric Bayesian procedures ""overfit""? To shed light on this question, we consider a binary regression problem in detail and establish frequentist consistency for a certain class of Bayes procedures based on hierarchical priors, called uniform mixture priors. These are defined as follows: let v be any probability distribution on the nonnegative integers. To sample a function f from the prior pi(v), first sample m from v and then sample f uniformly from the set of step functions from [0, 1] into [0, 1] that have exactly m jumps (i.e., sample all m jump locations and m + 1 function values independently and uniformly). The main result states that if a data-stream is generated according to any fixed, measurable binary-regression function f(0) not equivalent to 1/2, then frequentist consistency obtains: that is, for any V with infinite support, the posterior of pi(v) concentrates on any L-1 neighborhood of fo. Solution of an associated large-deviations problem is central to the consistency proof."
"10.1214/009053606000000353","2006","Semiparametric estimation of a two-component mixture model","2","Suppose that univariate data are drawn from a mixture of two distributions that are equal up to a shift parameter. Such a model is known to be nonidentifiable from a nonparametric viewpoint. However, if we assume that the unknown mixed distribution is symmetric, we obtain the identifiability of this model, which is then defined by four unknown parameters: the mixing proportion, two location parameters and the cumulative distribution function of the symmetric mixed distribution. We propose estimators for these four parameters when no training data is available. Our estimators are shown to be strongly consistent under mild regularity assumptions and their convergence rates are studied. Their finite-sample properties are illustrated by a Monte Carlo study and our method is applied to real data."
"10.1214/009053606000000344","2006","Testing the order of a model","1","This paper deals with order identification for nested models in the i.i.d. framework. We study the asymptotic efficiency of two generalized likelihood ratio tests of the order. They are based on two estimators which are proved to be strongly consistent. A version of Stein's lemma yields an optimal underestimation error exponent. The lemma also implies that the overestimation error exponent is necessarily trivial. Our tests admit nontrivial underestimation error exponents. The optimal underestimation error exponent is achieved in some situations. The overestimation error can decay exponentially with respect to a positive power of the number of observations.These results are proved under mild assumptions by relating the underestimation (resp. overestimation) error to large (resp. moderate) deviations of the log-likelihood process. In particular, it is not necessary that the classical Cramer condition be satisfied; namely, the log-densities are not required to admit every exponential moment. Three benchmark examples with specific difficulties (location mixture of normal distributions, abrupt changes and various regressions) are detailed so as to illustrate the generality of our results."
"10.1214/009053606000000254","2006","On discriminating between long-range dependence and changes in mean","1","We develop a testing procedure for distinguishing between a long-range dependent time series and a weakly dependent time series with change-points in the mean. In the simplest case, under the null hypothesis the time series is weakly dependent with one change in mean at an unknown point, and under the alternative it is long-range dependent. We compute the CUSUM statistic T-n, which allows us to construct an estimator (k) over cap of a change-point. We then compute the statistic T-n,T-1 based on the observations up to time (k) over cap and the statistic T,2 based on the observations after time (k) over cap. The statistic M-n = max[T-n,T-1, T-n,T-2] converges to a well-known distribution under the null, but diverges to infinity if the observations exhibit long-range dependence. The theory is illustrated by examples and an application to the returns of the Dow Jones index."
"10.1214/009053606000000218","2006","Estimation for almost periodic processes","0","Processes with almost periodic covariance functions have spectral mass on lines parallel to the diagonal in the two-dimensional spectral plane. Methods have been given for estimation of spectral mass on the lines of spectral concentration if the locations of the lines are known. Here methods for estimating the intercepts of the lines of spectral concentration in the Gaussian case are given under appropriate conditions. The methods determine rates of convergence sufficiently fast as the sample size n -> infinity so that the spectral estimation on the estimated lines can then proceed effectively. This task involves bounding the maximum of an interesting class of non-Gaussian possibly nonstationary processes."
"10.1214/009053606000000227","2006","Statistical inference for time-varying {ARCH} processes","2","In this paper the class of ARCH(infinity) models is generalized to the nonstationary class of ARCH(infinity) models with time-varying coefficients. For fixed time points, a stationary approximation is given leading to the notation ""locally stationary ARCH(infinity) process."" The asymptotic properties of weighted quasi-likelihood estimators of time-varying ARCH(p) processes (p < infinity) are studied, including asymptotic normality. In particular, the extra bias due to nonstationarity of the process is investigated. Moreover, a Taylor expansion of the nonstationary ARCH process in terms of stationary processes is given and it is proved that the time-varying ARCH process can be written as a time-varying Volterra series."
"10.1214/009053606000000245","2006","Pseudo-maximum likelihood estimation of {${\rm ARCH}(\infty)$} models","1","Strong consistency and asymptotic normality of the Gaussian pseudomaximum likelihood estimate of the parameters in a wide class of ARCH(infinity) processes are established. The conditions are shown to hold in case of exponential and hyperbolic decay in the ARCH weights, though in the latter case a faster decay rate is required for the central limit theorem than for the law of large numbers. Particular parameterizations are discussed."
"10.1214/009053605000000813","2006","Doubling and projection: a method of constructing two-level designs of resolution {IV}","4","Given a two-level regular fractional factorial design of resolution IV, the method of doubling produces another design of resolution IV which doubles both the run size and the number of factors of the initial design. On the other hand, the projection of a design of resolution IV onto a subset of factors is of resolution IV or higher. Recent work in the literature of projective geometry essentially determines the structures of all regular designs of resolution IV with n >= N/4 + 1 in terms of doubling and projection, where N is the run size and n is the number of factors. These results imply that, for instance, all regular designs of resolution IV with 5N/16 < n < N/2 must be projections of the regular design of resolution IV with N/2 factors. We show that, for 9N/32 <= n <= 5N/16, all minimum aberration designs are projections of the design with 5N/16 factors which is constructed by repeatedly doubling the 2(5-1) design defined by I = ABCDE. To prove this result, we also derive some properties of doubling, including an identity that relates the wordlength pattern of a design to that of its double and a result that does the same for the alias patterns of two-factor interactions."
"10.1214/009053605000000822","2006","Sequential importance sampling for multiway tables","0","We describe an algorithm for the sequential sampling of entries in multiway contingency tables with given constraints. The algorithm can be used for computations in exact conditional inference. To justify the algorithm, a theory relates sampling values at each step to properties of the associated toric ideal using computational commutative algebra. In particular, the property of interval cell counts at each step is related to exponents on lead indeterminates of a lexicographic Grobner basis. Also, the approximation of integer programming by linear programming for sampling is related to initial terms of a toric ideal. We apply the algorithm to examples of contingency tables which appear in the social and medical sciences. The numerical results demonstrate that the theory is applicable and that the algorithm performs well."
"10.1214/009053605000000840","2006","Stable limits of martingale transforms with application to the estimation of {GARCH} parameters","3","In this paper we study the asymptotic behavior of the Gaussian quasi maximum likelihood estimator of a stationary GARCH process with heavy-tailed innovations. This means that the innovations are regularly varying with index alpha is an element of (2, 4). Then, in particular, the marginal distribution of the GARCH process has infinite fourth moment and standard asymptotic theory with normal limits and root n-rates breaks down. This was recently observed by Hall and Yao [Econometrica 71 (2003) 285-317]. It is the aim of this paper to indicate that the limit theory for the parameter estimators in the heavy-tailed case nevertheless very much parallels the normal asymptotic theory. In the light-tailed case, the limit theory is based on the CLT for stationary ergodic finite variance martingale difference sequences. In the heavy-tailed case such a general result does not exist, but an analogous result with infinite variance stable limits can be shown to hold under certain mixing conditions which are satisfied for GARCH processes. It is the aim of the paper to give a general structural result for infinite variance limits which can also be applied in situations more general than GARCH."
"10.1214/009053605000000831","2006","Asymptotic normality of extreme value estimators on {$C[0,1]$}","0","Consider n i.i.d. random elements on C[0, 1]. We show that, under an appropriate strengthening of the domain of attraction condition, natural estimators of the extreme-value index, which is now a continuous function, and the normalizing functions have a Gaussian process as limiting distribution. A key tool is the weak convergence of a weighted tail empirical process, which makes it possible to obtain the results uniformly on [0, 1]. Detailed examples are also presented."
"10.1214/009053605000000804","2006","Nonsubjective priors via predictive relative entropy regret","1","We explore the construction of nonsubjective prior distributions in Bayesian statistics via a posterior predictive relative entropy regret criterion. We carry out a minimax analysis based on a derived asymptotic predictive loss function and show that this approach to prior construction has a number of attractive features. The approach here differs from previous work that uses either prior or posterior relative entropy regret in that we consider predictive performance in relation to alternative nondegenerate prior distributions. The theory is illustrated with an analysis of some specific examples."
"10.1214/009053605000000732","2006","Poisson calculus for spatial neutral to the right processes","1","Neutral to the right (NTR) processes were introduced by Doksum in 1974 as Bayesian priors on the class of distributions on the real line. Since that time there have been numerous applications to models that arise in survival analysis subject to possible right censoring. However, unlike the Dirichlet process, the larger class of NTR processes has not been used in a wider range of more complex statistical applications. Here, to circumvent some of these limitations, we describe a natural extension of NTR processes to arbitrary Polish spaces, which we call spatial neutral to the right processes. Our construction also leads to a new rich class of random probability measures, which we call NTR species sampling models. We show that this class contains the important two parameter extension of the Dirichlet process. We provide a posterior analysis, which yields tractable NTR analogues of the Blackwell-MacQueen distribution. Our analysis turns out to be closely related to the study of regenerative composition structures. A new computational scheme, which is an ordered variant of the general Chinese restaurant processes, is developed. This can be used to approximate complex posterior quantities. We also discuss some relationships to results that appear outside of Bayesian nonparametrics."
"10.1214/009053605000000778","2006","False discovery and false nondiscovery rates in single-step multiple testing procedures","14","Results on the false discovery rate (FDR) and the false nondiscovery rate (FNR) are developed for single-step multiple testing procedures. In addition to verifying desirable properties of FDR and FNR as measures of error rates, these results extend previously known results, providing further insights, particularly under dependence, into the notions of FDR and FNR and related measures. First, considering fixed configurations of true and false null hypotheses, inequalities are obtained to explain how an FDR- or FNR-controlling single-step procedure, such as a Bonferroni or Sidak procedure, can potentially be improved. Two families of procedures are then constructed, one that modifies the FDR-controlling and the other that modifies the FNR-controlling Sidak procedure. These are proved to control FDR or FNR under independence less conservatively than the corresponding families that modify the FDR- or FNR-controlling Bonferroni procedure. Results of numerical investigations of the performance of the modified Sidak FDR procedure over its competitors are presented. Second, considering a mixture model where different configurations of true and false null hypotheses are assumed to have certain probabilities, results are also derived that extend some of Storey's work to the dependence case."
"10.1214/009053605000000741","2006","Estimating the proportion of false null hypotheses among a large number of independently tested hypotheses","13","We consider the problem of estimating the number of false null hypotheses among a very large number of independently tested hypotheses, focusing on the situation in which the proportion of false null hypotheses is very small. We propose a family of methods for establishing lower 100(l - alpha)% confidence bounds for this proportion, based on the empirical distribution of the p-values of the tests. Methods in this family are then compared in terms of ability to consistently estimate the proportion by letting alpha -> 0 as the number of hypothesis tests increases and the proportion decreases. This work is motivated by a signal detection problem that occurs in astronomy."
"10.1214/009053605000000750","2006","Optimal change-point estimation from indirect observations","3","We study nonparametric change-point estimation from indirect noisy observations. Focusing on the white noise convolution model, we consider two classes of functions that are smooth apart from the change-point. We establish lower bounds on the minimax risk in estimating the change-point and develop rate optimal estimation procedures. The results demonstrate that the best achievable rates of convergence are determined both by smoothness of the function away from the change-point and by the degree of ill-posedness of the convolution operator. Optimality is obtained by introducing a new technique that involves, as a key element, detection of zero crossings of an estimate of the properly smoothed second derivative of the underlying function."
"10.1214/009053605000000787","2006","Adaptive multiscale detection of filamentary structures in a background of uniform random points","3","We are given a set of n points that might be uniformly distributed in the unit square [0, 1](2). We wish to test whether the set, although mostly consisting of uniformly scattered points, also contains a small fraction of points sampled from some (a priori unknown) curve with C-alpha-norm bounded by beta. An asymptotic detection threshold exists in this problem; for a constant T_ (alpha, beta) > 0, if the number of points sampled from the curve is smaller than T_(alpha, beta)n(1/(1+alpha)), reliable detection is not possible for large n. We describe a multiscale significant-runs algorithm that can reliably detect concentration of data near a smooth curve, without knowing the smoothness information alpha or 0 in advance, provided that the number of points on the curve exceeds T-*(alpha, beta)n(1/(1+alpha)). This algorithm therefore has an optimal detection threshold, up to a factor T-*/T_.At the heart of our approach is an analysis of the data by counting membership in multiscale multianisotropic strips. The strips will have area 2/n and exhibit a variety of lengths, orientations and anisotropies. The strips are partitioned into anisotropy classes; each class is organized as a directed graph whose vertices all are strips of the same anisotropy and whose edges link such strips to their ""good continuations."" The point-cloud data are reduced to counts that measure membership in strips. Each anisotropy graph is reduced to a subgraph that consist of strips with significant counts. The algorithm rejects Ho whenever some such subgraph contains a path that connects many consecutive significant counts."
"10.1214/009053605000000796","2006","Local partial-likelihood estimation for lifetime data","6","This paper considers a proportional hazards model, which allows one to examine the extent to which covariates interact nonlinearly with an exposure variable, for analysis of lifetime data. A local partial-likelihood technique is proposed to estimate nonlinear interactions. Asymptotic normality of the proposed estimator is established. The baseline hazard function, the bias and the variance of the local likelihood estimator are consistently estimated. In addition, a one-step local partial-likelihood estimator is presented to facilitate the computation of the proposed procedure and is demonstrated to be as efficient as the fully iterated local partial-likelihood estimator. Furthermore, a penalized local likelihood estimator is proposed to select important risk variables in the model. Numerical examples are used to illustrate the effectiveness of the proposed procedures."
"10.1214/009053605000000769","2006","Serial and nonserial sign-and-rank statistics: asymptotic representation and asymptotic normality","3","The classical theory of rank-based inference is entirely based either on ordinary ranks, which do not allow for considering location (intercept) parameters, or on signed ranks, which require an assumption of symmetry. If the median, in the absence of a symmetry assumption, is considered as a location parameter, the maximal invariance property of ordinary ranks is lost to the ranks and the signs. This new maximal invariant thus suggests a new class of statistics, based on ordinary ranks and signs. An asymptotic representation theory A la Hajek is developed here for such statistics, both in the nonserial and in the serial case. The corresponding asymptotic normality results clearly show how the signs add a separate contribution to the asymptotic variance, hence, potentially, to asymptotic efficiency. As shown by Hallin and Werker [Bernoulli 9 (2003) 137-165], conditioning in an appropriate way on the maximal invariant potentially even leads to semiparametrically efficient inference. Applications to semiparametric inference in regression and time series models with median restrictions are treated in detail in an upcoming companion paper."
"10.1214/009053605000000877","2006","Adaptive nonparametric confidence sets","8","We construct honest confidence regions for a Hilbert space-valued parameter in various statistical models. The confidence sets can be centered at arbitrary adaptive estimators, and have diameter which adapts optimally to a given selection of models. The latter adaptation is necessarily limited in scope. We review the notion of adaptive confidence regions, and relate the optimal rates of the diameter of adaptive confidence regions to the minimax rates for testing and estimation. Applications include the finite normal mean model, the white noise model, density estimation and regression with random design."
"10.1214/009053606000000146","2006","Adaptive confidence balls","10","Adaptive confidence balls are constructed for individual resolution levels as well as the entire mean vector in a multiresolution framework. Finite sample lower bounds are given for the minimum expected squared radius for confidence balls with a prespecified confidence level. The confidence balls are centered on adaptive estimators based on special local block thresholding rules. The radius is derived from an analysis of the loss of this adaptive estimator. In addition adaptive honest confidence balls are constructed which have guaranteed coverage probability over all of RN and expected squared radius adapting over a maximum range of Besov bodies."
"10.1214/009053605000000895","2006","Penalized maximum likelihood and semiparametric second-order efficiency","4","We consider the problem of estimation of a shift parameter of an unknown symmetric function in Gaussian white noise. We introduce a notion of semiparametric second-order efficiency and propose estimators that are semiparametrically efficient and second-order efficient in our model. These estimators are of a penalized maximum likelihood type with an appropriately chosen penalty. We argue that second-order efficiency is crucial in semiparametric problems since only the second-order terms in asymptotic expansion for the risk account for the behavior of the ""nonparametric component"" of a semiparametric procedure, and they are not dramatically smaller than the first-order terms."
"10.1214/009053605000000886","2006","Spatial extremes: models for the stationary case","5","The aim of this paper is to provide models for spatial extremes in the case of stationarity. The spatial dependence at extreme levels of a stationary process is modeled using an extension of the theory of max-stable processes of de Haan and Pickands [Probab. Theory Related Fields 72 (1986) 477-492]. We propose three one-dimensional and three two-dimensional models. These models depend on just one parameter or a few parameters that measure the strength of tail dependence as a function of the distance between locations. We also propose two estimators for this parameter and prove consistency under domain of attraction conditions and asymptotic normality under appropriate extra conditions."
"10.1214/009053605000000912","2006","Consistent estimation of the basic neighborhood of {M}arkov random fields","1","For Markov random fields on Z(d) with finite state space, we address the statistical estimation of the basic neighborhood, the smallest region that determines the conditional distribution at a site on the condition that the values at all other sites are given. A modification of the Bayesian Information Criterion, replacing likelihood by pseudo-likelihood, is proved to provide strongly consistent estimation from observing a realization of the field on increasing finite regions: the estimated basic neighborhood equals the true one eventually almost surely, not assuming any prior bound on the size of the latter. Stationarity of the Markov field is not required, and phase transition does not affect the results."
"10.1214/009053605000000859","2006","Sequential change-point detection when unknown parameters are present in the pre-change distribution","1","In the sequential change-point detection literature, most research specifies a required frequency of false alarms at a given pre-change distribution f(theta) and tries to minimize the detection delay for every possible post-change distribution g(lambda). In this paper, motivated by a number of practical examples, we first consider the reverse question by specifying a required detection delay at a given post-change distribution and trying to minimize the frequency of false alarms for every possible pre-change distribution f(theta). We present asymptotically optimal procedures for one-parameter exponential families. Next, we develop a general theory for change-point problems when both the prechange distribution f(theta) and the post-change distribution g; involve unknown parameters. We also apply our approach to the special case of detecting shifts in the mean of independent normal observations."
"10.1214/009053606000000155","2006","Improved minimax predictive densities under {K}ullback-{L}eibler loss","4","Let X vertical bar mu similar to N-p(mu, v(x)I) and Y vertical bar mu - Np(mu, v(y)l) be independent p-dimensional multivariate normal vectors with common unknown mean mu. Based on only observing X = x, we consider the problem of obtaining a predictive density (p) over cap (y vertical bar x) for Y that is close to p(y vertical bar mu) as measured by expected Kullback-Leibler loss. A natural procedure for this problem is the (formal) Bayes predictive density (p) over cap (U)(y vertical bar x) under the uniform prior pi(U)(mu) equivalent to 1, which is best invariant and minimax. We show that any Bayes predictive density will be minimax if it is obtained by a prior yielding a marginal that is superharmonic or whose square root is superharmonic. This yields wide classes of minimax procedures that dominate (p) over cap (U)(y vertical bar x), including Bayes predictive densities under superharmonic priors. Fundamental similarities and differences with the parallel theory of estimating a multivariate normal mean under quadratic loss are described."
"10.1214/009053605000000868","2006","Extended statistical modeling under symmetry; the link toward quantum mechanics","0","We derive essential elements of quantum mechanics from a parametric structure extending that of traditional mathematical statistics. The basic setting is a set A of incompatible experiments, and a transformation group G on the cartesian product Pi of the parameter spaces of these experiments. The set of possible parameters is constrained to lie in a subspace of Pi, an orbit or a set of orbits of G. Each possible model is then connected to a parametric Hilbert space. The spaces of different experiments are linked unitarily, thus defining a common Hilbert space H. A state is equivalent to a question together with an answer: the choice of an experiment a c A plus a value for the corresponding parameter. Finally, probabilities are introduced through Born's formula, which is derived from a recent version of Gleason's theorem. This then leads to the usual formalism of elementary quantum mechanics in important special cases. The theory is illustrated by the example of a quantum particle with spin."
"10.1214/009053605000000903","2006","High-resolution asymptotics for the angular bispectrum of spherical random fields","1","In this paper we study the asymptotic behavior of the angular bispectrum of spherical random fields. Here, the asymptotic theory is developed in the framework of fixed-radius fields, which are observed with increasing resolution as the sample size grows. The results we present are then exploited in a set of procedures aimed at testing non-Gaussianity; for these statistics, we are able to show convergence to functionals of standard Brownian motion under the null hypothesis. Analytic results are also presented on the behavior of the tests in the presence of a broad class of non-Gaussian alternatives. The issue of testing for non-Gaussianity on spherical random fields has recently gained enormous empirical importance, especially in connection with the statistical analysis of cosmic microwave background radiation."
"10.1214/009053605000000705","2005","Multivariate {B}ayesian function estimation","2","Bayesian methods are developed for the multivariate nonparametric regression problem where the domain is taken to be a compact Riemannian manifold. In terms of the latter, the underlying geometry of the manifold induces certain symmetries on the multivariate nonparametric regression function. The Bayesian approach then allows one to incorporate hierarchical Bayesian methods directly into the spectral structure, thus providing a symmetry-adaptive multivariate Bayesian function estimator. One can also diffuse away some prior information in which the limiting case is a smoothing spline on the manifold. This, together with the result that the smoothing spline solution obtains the minimax rate of convergence in the multivariate nonparametric regression problem, provides good frequentist properties for the Bayes estimators. An application to astronomy is included."
"10.1214/009053605000000697","2005","Sobolev tests of goodness of fit of distributions on compact {R}iemannian manifolds","0","Classes of coordinate-invariant omnibus goodness-of-fit tests on compact Riemannian manifolds are proposed. The tests are based on Gine's Sobolev tests of uniformity. A condition for consistency is given. The tests are illustrated by an example on the rotation group SO(3)."
"10.1214/009053605000000147","2005","Nonquadratic estimators of a quadratic functional","3","Estimation of a quadratic functional over parameter spaces that are not quadratically convex is considered. It is shown, in contrast to the theory for quadratically convex parameter spaces, that optimal quadratic rules are often rate suboptimal. In such cases minimax rate optimal procedures are constructed based on local thresholding. These nonquadratic procedures are sometimes fully efficient even when optimal quadratic rules have slow rates of convergence. Moreover, it is shown that when estimating a quadratic functional nonquadratic procedures may exhibit different elbow phenomena than quadratic procedures."
"10.1214/009053605000000714","2005","Nonparametric methods for inference in the presence of instrumental variables","2","We suggest two nonparametric approaches, based on kernel methods and orthogonal series to estimating regression functions in the presence of instrumental variables. For the first time in this class of problems, we derive optimal convergence rates, and show that they are attained by particular estimators. In the presence of instrumental variables the relation that identifies the regression function also defines an ill-posed inverse problem, the ""difficulty"" of which depends on eigenvalues of a certain integral operator which is determined by the joint density of endogenous and instrumental variables. We delineate the role played by problem difficulty in determining both the optimal convergence rate and the appropriate choice of smoothing parameter."
"10.1214/009053605000000660","2005","Functional linear regression analysis for longitudinal data","15","We propose nonparametric methods for functional linear regression which are designed for sparse longitudinal data, where both the predictor and response are functions of a covariate such as time. Predictor and response processes have smooth random trajectories, and the data consist of a small number of noisy repeated measurements made at irregular times for a sample of subjects. In longitudinal studies, the number of repeated measurements per subject is often small and may be modeled as a discrete random number and, accordingly, only a finite and asymptotically nonincreasing number of measurements are available for each subject or experimental unit. We propose a functional regression approach for this situation, using functional principal component analysis, where we estimate the functional principal component scores through conditional expectations. This allows the prediction of an unobserved response trajectory from sparse measurements of a predictor trajectory. The resulting technique is flexible and allows for different patterns regarding the timing of the measurements obtained for predictor and response trajectories. Asymptotic properties for a sample of n subjects are investigated under mild conditions, as n -> infinity, and we obtain consistent estimation for the regression function. Besides convergence results for the components of functional linear regression, such as the regression parameter function, we construct asymptotic pointwise confidence bands for the predicted trajectories. A functional coefficient of determination as a measure of the variance explained by the functional regression model is introduced, extending the standard R-2 to the functional case. The proposed methods are illustrated with a simulation study, longitudinal primary biliary liver Cirrhosis data and an analysis of the longitudinal relationship between blood pressure and body mass index."
"10.1214/009053605000000723","2005","Universal optimality of {P}atterson's crossover designs","1","We show that the balanced crossover designs given by Patterson [Biometrika 39 (1952) 32-48] are (a) universally optimal (UO) for the joint estimation of direct and residual effects when the competing class is the class of connected binary designs and (b) UO for the estimation of direct (residual) effects when the competing class of designs is the class of connected designs (which includes the connected binary designs) in which no treatment is given to the same subject in consecutive periods. In both results, the formulation of UO is as given by Shah and Sinha [Unpublished manuscript (2002)].Further, we introduce a functional of practical interest, involving both direct and residual effects, and establish (c) optimality of Patterson's designs with respect to this functional when the class of competing designs is as in (b) above."
"10.1214/009053605000000679","2005","Majorization framework for balanced lattice designs","0","This paper aims to generalize and unity classical criteria for comparisons of balanced lattice designs, including fractional factorial designs, supersaturated designs and uniform designs. We present a general majorization framework for assessing designs, which includes a stringent criterion of majorization via pairwise coincidences and flexible surrogates via convex functions. Classical orthogonality, aberration and uniformity criteria are unified by choosing combinatorial and exponential kernels. A construction method is also sketched out."
"10.1214/009053605000000688","2005","Construction of optimal multi-level supersaturated designs","1","A supersaturated design is a design whose run size is not large enough for estimating all the main effects. The goodness of multi-level supersaturated designs can be judged by the generalized minimum aberration criterion proposed by Xu and Wu [Ann. Statist. 29 (2001) 1066-1077]. A new lower bound is derived and general construction methods are proposed for multi-level supersaturated designs. Inspired by the Addelman-Kempthorne construction of orthogonal arrays, several classes of optimal multi-level supersaturated designs are given in explicit form: Columns are labeled with linear or quadratic polynomials and rows are points over a finite field. Additive characters are used to study the properties of resulting designs. Some small optimal supersaturated designs of 3, 4 and 5 levels are listed with their properties."
"10.1214/009053605000000651","2005","On the two-phase framework for joint model and design-based inference","0","We establish a mathematical framework that formally validates the twophase ""super-population viewpoint"" proposed by Hartley and Sielken [Biometrics 31 (1975) 411-422] by defining a product probability space which includes both the design space and the model space. The methodology we develop combines finite population sampling theory and the classical theory of infinite population sampling to account for the underlying processes that produce the data under a unified approach. Our key results are the following: first, if the sample estimators converge in the design law and the model statistics converge in the model, then, under certain conditions, they are asymptotically independent, and they converge jointly in the product space; second, the sample estimating equation estimator is asymptotically normal around a super-population parameter."
"10.1214/009053605000000552","2005","Optimal designs for three-dimensional shape analysis with spherical harmonic descriptors","1","We determine optimal designs for some regression models which are frequently used for describing three-dimensional shapes. These models are based on a Fourier expansion of a function defined on the unit sphere in terms of spherical harmonic basis functions. In particular, it is demonstrated that the uniform distribution on the sphere is optimal with respect to all Phi(p) criteria proposed by Kiefer in 1974 and also optimal with respect to a criterion which maximizes a p mean of the r smallest eigenvalues of the variance-covariance matrix. This criterion is related to principal component analysis, which is the common tool for analyzing this type of image data. Moreover, discrete designs on the sphere are derived, which yield the same information matrix in the spherical harmonic regression model as the uniform distribution and are therefore directly implementable in practice. It is demonstrated that the new designs are substantially more efficient than the commonly used designs in three-dimensional shape analysis."
"10.1214/009053605000000570","2005","Correlated samples with fixed and nonnormal latent variables","0","A general structural equation model is fitted on a panel data set that consists of I correlated samples. The correlated samples could be data from correlated populations or correlated observations from occasions of panel data. We consider cases in which the full pseudo-normal likelihood cannot be used, for example, in highly unbalanced data where the participating individuals do not appear in consecutive years. The model is estimated by a partial likelihood that would be the full and correct likelihood for independent and normal samples. It is proved that the asymptotic standard errors (a.s.e.'s) for the most important parameters and an overall-fit measure are the same as the corresponding Ones derived under the standard assumptions of normality and independence for all the observations. These results are very important since they allow us to apply classical statistical methods for inference, which use only first- and second-order moments, to correlated and nonnormal data. Via a simulation study we show that the a.s.e.'s based on the first two moments have negligible bias and provide less variability than the a.s.e.'s computed by an alternative robust estimator that utilizes up to fourth moments. Our methodology and results are applied to real panel data, and it is shown that the correlated samples cannot be formulated and analyzed as independent samples. We also provide robust a.s.e.'s for the remaining parameters. Additionally, we show in the simulation that the efficiency loss for not considering the correlation over the samples is small and negligible in the cases with random and fixed variables."
"10.1214/009053605000000543","2005","Partially observed information and inference about non-{G}aussian mixed linear models","0","In mixed linear models with nonnormal data, the Gaussian Fisher information matrix is called a quasi-information matrix (QUIM). The QUIM plays an important role in evaluating the asymptotic covariance matrix of the estimators of the model parameters, including the variance components. Traditionally, there are two ways to estimate the information matrix: the estimated information matrix and the observed one. Because the analytic form of the QUIM involves parameters other than the variance components, for example, the third and fourth moments of the random effects, the estimated QUIM is not available. On the other hand, because of the dependence and normormality of the data, the observed QUIM is inconsistent. We propose an estimator of the QUIM that consists partially of an observed form and partially of an estimated one. We show that this estimator is consistent and computationally very easy to operate. The method is used to derive large sample tests of statistical hypotheses that involve the variance components in a non-Gaussian mixed linear model. Finite sample performance of the test is studied by simulations and compared with the delete-group jackknife method that applies to a special case of non-Gaussian mixed linear models."
"10.1214/009053605000000561","2005","Identification of multitype branching processes","1","We solve the problem of constructing an asymptotic global confidence region for the means and the covariance matrices of the reproduction distributions involved in a supercritical multitype branching process. Our approach is based on a central limit theorem associated with a quadratic law of large numbers performed by the maximum likelihood or the multidimensional Lotka-Nagaev estimator of the reproduction law means. The extension of this approach to the least squares estimator of the mean matrix is also briefly discussed."
"10.1214/009053605000000624","2005","On recursive estimation for time varying autoregressive processes","1","This paper focuses on recursive estimation of time varying autoregressive processes in a nonparametric setting. The stability of the model is revisited and uniform results are provided when the time-varying autoregressive parameters belong to appropriate smoothness classes. An adequate normalization for the correction term used in the recursive estimation procedure allows for very mild assumptions on the innovations distributions. The rate of convergence of the pointwise estimates is shown to be minimax in beta-Lipschitz classes for 0 < beta <= 1. For 1 < beta <= 2, this property no longer holds. This can be seen by using an asymptotic expansion of the estimation error. A bias reduction method is then proposed for recovering the minimax rate."
"10.1214/009053605000000606","2005","Distribution free goodness-of-fit tests for linear processes","3","This article proposes a class of goodness-of-fit tests for the autocorrelation function of a time series process, including those exhibiting long-range dependence. Test statistics for composite hypotheses are functionals of a (approximated) martingale transformation of the Bartlett T-p-process with estimated parameters, which converges in distribution to the standard Brownian motion under the null hypothesis. We discuss tests of different natures such as omnibus, directional and Portmanteau-type tests. A Monte Carlo study illustrates the performance of the different tests in practice."
"10.1214/009053605000000589","2005","Parameter estimates for fractional autoregressive spatial processes","0","A binomial-type operator on a stationary Gaussian process is introduced in order to model long memory in the spatial context. Consistent estimators of model parameters are demonstrated. In particular, it is shown that (d) over cap (N) - d = Op((Log N)3)/(N)), where d = (d(1), d(2)) denotes the long memory parameter."
"10.1214/009053605000000598","2005","Testing for a linear {MA} model against threshold {MA} models","1","This paper investigates the (conditional) quasi-likelihood ratio test for the threshold in MA models. Under the hypothesis of no threshold, it is shown that the test statistic converges weakly to a function of the centred Gaussian process. Under local alternatives, it is shown that this test has nontrivial asymptotic power. The results are based on a new weak convergence of a linear marked empirical process, which is independently of interest. This paper also gives an invertible expansion of the threshold MA models."
"10.1214/009053605000000615","2005","Sharp adaptive estimation of the drift function for ergodic diffusions","0","The global estimation problem of the drift function is considered for a large class of ergodic diffusion processes. The unknown drift S(.) is supposed to belong to a nonparametric class of smooth functions of order k >= 1, but the value of k is not known to the statistician. A fully data-driven procedure of estimating the drift function is proposed, using the estimated risk minimization method. The sharp adaptivity of this procedure is proven up to an optimal constant, when the quality of the estimation is measured by the integrated squared error weighted by the square of the invariant density."
"10.1214/009053605000000507","2005","Consistency of the jackknife-after-bootstrap variance estimator for the bootstrap quantiles of a {S}tudentized statistic","0","Efron [J Roy. Statist. Soc. Ser. B 54 (1992) 83-111] proposed a computationally efficient method, called the jackknife-after-bootstrap, for estimating the variance of a bootstrap estimator for independent data. For dependent data, a version of the jackk-iiife-after-bootstrap method has been recently proposed by Lahiri [Econometric Theory 18 (2002) 79-98.]. In this paper it is shown that the jackknife-after-bootstrap estimators of the variance of a bootstrap quantile are consistent for both dependent and independent data. Results from a simulation study are also presented."
"10.1214/009053605000000525","2005","Order selection for same-realization predictions in autoregressive processes","4","Assume that observations are generated from ail infinite-order autoregressive [AR(infinity)] process. Shibata [Ann. Statist. 8 (1980) 147-164] considered the problem of choosing a finite-order AR model, allowing the order to become infinite as the number of observations does in order to obtain a better approximation. He showed that, for the purpose of predicting the future of ail independent replicate, Akaike's information criterion (AIC) and its variants are asymptotically efficient. Although Shibata's concept of asymptotic efficiency has been widely accepted in the literature, it is not a natural property for time series analysis. This is because when new observations of a time series become available, they are not independent of the previous data. TO overcome this difficulty, in this paper we focus Oil order selection for forecasting the future of an observed time series, referred to as same-realization prediction. We present the first theoretical verification that AIC and its variants are still asymptotically efficient (in the sense defined ill Section 4) for same-realization predictions. To obtain this result, a technical condition, easily met in common practice, is introduced to simplify the complicated dependent structures among the selected orders, estimated parameters and future observations. In addition, a simulation Study is conducted to illustrate the practical implications of AIC. This study shows that AIC also yields a satisfactory same-realization prediction in finite samples. Oil the other hand, a limitation of AIC in same-realization settings is pointed Out. It is interesting to note that this limitation of AIC does not exist for corresponding independent cases."
"10.1214/009053605000000534","2005","High moment partial sum processes of residuals in {GARCH} models and their applications","1","In this paper we construct high moment partial sum processes based on residuals of a GARCH model when the mean is known to be 0. We consider partial sums of kill powers of residuals, CUSUM processes and self-normalized partial sum processes. The kth power partial sum process converges to a Brownian process Plus a correction term, where the correction term depends on the kill moment mu(k) of the innovation sequence. If mu(k) = 0, then the correction term is 0 and, thus, the kth power partial sum process converges weakly to the same Gaussian process as does the kill power partial sum of the i.i.d. innovations sequence. In particular, since mu(1) = 0, this holds for the first moment partial sum process, but fails for the second moment partial sum process. We also consider the CUSUM and the self-normalized processes, that is, standardized by the residual sample variance. These behave as if the residuals were asymptotically i.i.d. We also Study the joint distribution of the kth and (k + 1)st self-normalized partial sum processes. Applications to change-point problems and goodness-of-fit are considered, in particular, CUSUM statistics for testing GARCH model structure change and the Jarque-Bera omnibus statistic for testing normality of the unobservable innovation distribution of a GARCH model. The use of residuals for constructing a kernel density function estimation of the innovation distribution is discussed."
"10.1214/009053605000000516","2005","Fixed-domain asymptotics for a subclass of {M}at\'ern-type {G}aussian random fields","4","Stein [Statist. Sci. 4 (1989) 432-433] proposed the Matern-type Gaussian random fields as a very flexible class of models for computer experiments. This article considers a subclass of these models that are exactly once mean square differentiable. In particular, the likelihood function is determined in closed form, and under mild conditions the sieve maximum likelihood estimators for the parameters of the covariance function are shown to be weakly consistent with respect to fixed-domain asymptotics."
"10.1214/009053605000000633","2005","On adaptive estimation of linear functionals","1","Adaptive estimation of linear functionals over a collection of parameter spaces is considered. A between-class modulus of continuity, a geometric quantity, is shown to be instrumental in characterizing the degree of adaptability over two parameter spaces in the same way that the usual modulus Of Continuity captures the minimax difficulty of estimation over a single parameter space. A general construction of optimally adaptive estimators based on an ordered modulus of continuity is given. The results are complemented by several illustrative examples."
"10.1214/009053605000000499","2005","Cross-validation in nonparametric regression with outliers","3","A popular data-driven method for choosing the bandwidth in standard kernel regression is cross-validation. Even when there are outliers ill the data, robust kernel regression can be used to estimate the unknown regression curve [Robust and Nonlinear Time Series Analysis. Lecture Notes in Statist. (1984) 26 163-184]. However, Under these Circumstances Standard cross-validation is no longer a satisfactory bandwidth selector because it is unduly influenced by extreme prediction errors caused by the existence of these Outliers. A more robust method proposed here is a cross-validation method that discounts the extreme prediction errors. In large samples the robust method chooses consistent bandwidths, and the consistency of the method is practically independent of the form ill which extreme prediction errors are discounted. Additionally, evaluation of the method's finite sample behavior in a simulation demonstrates that the proposed method performs favorably. This method call also be applied to other problems, for example, model selection, that require cross-validation."
"10.1214/009053605000000444","2005","Penalized log-likelihood estimation for partly linear transformation models with current status data","4","We consider partly linear transformation models applied to current status data. The unknown quantities are the transformation function, a linear regression parameter and a nonparametric regression effect. It is shown that the penalized MLE for the regression parameter is asymptotically normal and efficient and converges at the parametric rate, although the penalized MLE for the transformation function and nonparametric regression effect are only n(1/3) consistent. Inference for the regression parameter based on a block jackknife is investigated. We also study computational issues and demonstrate the proposed methodology with a simulation study. The transformation models and partly linear regression terms, coupled with new estimation and inference techniques, provide flexible alternatives to the Cox model for current status data analysis."
"10.1214/009053605000000462","2005","Asymptotic normality of the {$L_k$}-error of the {G}renander estimator","3","We investigate the limit behavior of the L-k-distance between a decreasing density f and its nonparametric maximum likelihood estimator f, for k >= 1. Due to the inconsistency of (f) over cap (n) at zero, the case k = 2.5 turns out to be a kind of transition point. We extend asymptotic normality of the L-1-distance to the Lk-distance for I < k < 2.5, and obtain the analogous limiting result for a modification of the L-k-distance for k >= 2.5. Since the L-1-distance is the area between f and (f) over cap (n), which is also the area between the inverse g of f and the more tractable inverse U,, of f, the problem can be reduced immediately to deriving asymptotic normality of the L-1-distance between U-n and g. Although we lose this easy correspondence for k > 1, we show that the L-k-distance between f and (f) over cap (n) is asymptotically equivalent to the L-k-distance between U-n and g."
"10.1214/009053605000000435","2005","Estimation of the density of regression errors","2","Estimation of the density of regression errors is a fundamental issue in regression analysis and it is typically explored via a parametric approach. This article uses a nonparametric approach with the mean integrated squared error (MISE) criterion. It solves a long-standing problem, formulated two decades ago by Mark Pinsker, about estimation of a nonparametric error density in a nonparametric regression setting with the accuracy of an oracle that knows the underlying regression errors. The Solution implies that, under a mild assumption on the differentiability of the design density and regression function, the MISE of a data-driven error density estimator attains minimax rates and sharp constants known for the case of directly observed regression errors. The result holds for error densities with finite and infinite supports. Some extensions of this result for more general heteroscedastic models with possibly dependent errors and predictors are also obtained; in the latter case the marginal error density is estimated. In all considered cases a blockwise-shrinking Efromovich-Pinsker density estimate, based on plugged-in residuals, is used. The obtained results imply a theoretical justification of a customary practice in applied regression analysis to consider residuals as proxies for underlying regression errors. Numerical and real examples are presented and discussed, and the S-PLUS software is available."
"10.1214/009053605000000471","2005","Wavelet thresholding for nonnecessarily {G}aussian noise: functionality","1","For signals belonging to balls in smoothness classes and noise with enough moments, the asymptotic behavior of the minimax quadratic risk among soft-threshold estimates is investigated. In turn, these results, combined with a median filtering method, lead to asymptotics for denoising heavy tails via wavelet thresholding. Some further comparisons of wavelet thresholding and of kernel estimators are also briefly discussed."
"10.1214/009053605000000480","2005","Asymptotic results for maximum likelihood estimators in joint analysis of repeated measurements and survival time","0","Maximum likelihood estimation has been extensively used in the joint analysis of repeated measurements and survival time. However, there is a lack of theoretical justification of the asymptotic properties for the maximum likelihood estimators. This paper intends to fill this gap. Specifically, we prove the consistency of the maximum likelihood estimators and derive their asymptotic distributions. The maximum likelihood estimators are shown to be semi parametrically efficient."
"10.1214/009053605000000372","2005","Asymptotic behavior of the unconditional {NPMLE} of the length-biased survivor function from right censored prevalent cohort data","6","Right censored survival data collected On a cohort of prevalent cases with constant incidence are length-biased, and may be used to estimate the length-biased (i.e., prevalent-case) survival function. When the incidence rate is constant, so-called stationarity of the incidence, it is more efficient to use this structure for unconditional statistical inference than to carry out an analysis by conditioning on the observed truncation times. It is well known that, due to the informative censoring for prevalent cohort data, the Kaplan-Meier estimator is not the unconditional NPMLE of the length-biased survival function and the asymptotic properties of the NPMLE do not follow from any known result. We present here a detailed derivation of the asymptotic properties of the NPMLE of the length-biased Survival function from right censored prevalent cohort survival data With follow-up. In particular, we show that the NPMLE is uniformly strongly consistent, converges weakly to a Gaussian process, and is asymptotically efficient. One important spin-off from these results is that they yield the asymptotic properties of the NPMLE of the incident-case survival function [see Asgharian, M'Lan and Wolfson J. Amer Statist. Assoc. 97 (2002) 201-209], which is often of prime interest in a prevalent cohort Study. Our results generalize those given by Vardi and Zhang [Ann. Statist. 20 (1992) 1022-1039] under Multiplicative censoring, which we show arises as a degenerate case in a prevalent cohort setting."
"10.1214/009053605000000381","2005","Nonparametric estimation of mixing densities for discrete distributions","0","By a mixture density is meant a density of the form pi(mu) (.) = f pi(theta) (.) x mu(d theta), where (pi(theta))(theta Theta is an element of) is a family of probability densities and mu is a probability measure on Theta. We consider the problem of identifying the unknown part of this model, the mixing distribution A, from a finite sample of independent observations from pi(mu). Assuming that the mixing distribution has a density function, we wish to estimate this density within appropriate function classes. A general approach is proposed and its scope of application is investigated in the case of discrete distributions. Mixtures of power series distributions are more specifically studied. Standard methods for density estimation, Such as kernel estimators, are available in this context, and it has been shown that these methods are rate optimal or almost rate optimal in balls of various smoothness spaces. For instance, these results apply to mixtures of the Poisson distribution parameterized by its mean. Estimators based oil orthogonal polynomial sequences have also been proposed and shown to achieve similar rates. The general approach of this paper extends and simplifies such results. For instance, it allows LIS to prove asymptotic minimax efficiency over certain smoothness classes of the above-mentioned polynomial estimator in the Poisson case. We also study discrete location mixtures, or discrete deconvolution, and mixtures of discrete uniform distributions."
"10.1214/009053605000000417","2005","The topography of multivariate normal mixtures","0","Multivariate normal mixtures provide a flexible method of fitting high-dimensional data. It is shown that their topography, in the sense of their key features as a density, can be analyzed rigorously in lower dimensions by use of a ridgeline manifold that contains all critical points, as well as the ridges of the density. A plot of the elevations on the ridgeline shows the key features of the mixed density. In addition, by use of the ridgeline, We uncover a function that determines the number of modes of the mixed density when there are two components being mixed. A followup analysis then gives a Curvature function that can be used to prove a set of modality theorems."
"10.1214/009053605000000390","2005","Estimation of sums of random variables: examples and information bounds","1","This paper concerns the estimation of sums of functions of observable and unobservable variables. Lower bounds for the asymptotic variance and a convolution theorem are derived in general finite- and infinite-dimensional models. An explicit relationship is established between efficient influence functions for the estimation of sums of variables and the estimation of their means. Certain ""plug-in"" estimators are proved to be asymptotically efficient in finite-dimensional models, while ""u, v"" estimators of Robbins are proved to be efficient in infinite-dimensional mixture models. Examples include certain species, network and data confidentiality problems."
"10.1214/009053605000000426","2005","Recursive {M}onte {C}arlo filters: algorithms and theoretical analysis","4","Recursive Monte Carlo filters, also called particle filters, are a powerful tool to perform computations in general state space models. We discuss and compare the accept-reject version with the more common sampling importance resampling version of the algorithm. In particular, we show how auxiliary variable methods and stratification can be used in the accept-reject version, and we compare different resampling techniques. In a second part, we show laws of large numbers and a central limit theorem for these Monte Carlo filters by simple induction arguments that need only weak conditions. We also show that, under stronger conditions, the required sample size is independent of the length of the observed series."
"10.1214/009053605000000183","2005","Nonanticipating estimation applied to sequential analysis and changepoint detection","0","Suppose a process yields independent observations whose distributions belong to a family parameterized by theta E Theta. When the process is in control, the observations are i.i.d. with a known parameter value theta(0). When the process is out of control, the parameter changes. We apply an idea of Robbins and Siegmund [Proc. Sixth Berkeley Symp. Math. Statist. Probab. 4 (1972) 37-41] to construct a class of sequential tests and detection schemes whereby the unknown post-change parameters are estimated. This approach is especially useful in situations where the parametric space is intricate and mixture-type rules are operationally or conceptually difficult to formulate. We exemplify our approach by applying it to the problem of detecting a change in the shape parameter of a Gamma distribution, in both a univariate and a multivariate setting."
"10.1214/009053604000001282","2005","Approximating conditional distribution functions using dimension reduction","3","Motivated by applications to prediction and forecasting, we suggest methods for approximating the conditional distribution function of a random variable Y given a dependent random d-vector X. The idea is to estimate not the distribution of Y vertical bar X, but that of Y vertical bar theta(T)X' where the unit vector theta is selected so that the approximation is optimal under a least-squares criterion. We show that theta may be estimated root-n consistently, Furthermore, estimation of the conditional distribution function of Y, given theta(T)X, has the same first-order asymptotic properties that it would enjoy if theta were known. The proposed method is illustrated using both simulated and real-data examples, showing its effectiveness for both independent datasets and data from time series. Numerical work corroborates the theoretical result that theta can be estimated particularly accurately."
"10.1214/009053605000000129","2005","Uniform in bandwidth consistency of kernel-type function estimators","1","We introduce a general method to prove uniform in bandwidth consistency of kernel-type function estimators. Examples include the kernel density estimator, the Nadaraya-Watson regression estimator and the conditional empirical process. Our results may be useful to establish uniform consistency of data-driven bandwidth kernel-type function estimators."
"10.1214/009053605000000110","2005","Optimal smoothing in nonparametric mixed-effect models","2","Mixed-effect models are widely used for the analysis of correlated data such as longitudinal data and repeated measures. In this article, we study an approach to the nonparametric estimation of mixed-effect models. We consider models with parametric random effects and flexible fixed effects, and employ the penalized least squares method to estimate the models. The issue to be addressed is the selection of smoothing parameters through the generalized cross-validation method, which is shown to yield optimal smoothing for both real and latent random effects. Simulation studies are conducted to investigate the empirical performance of generalized cross-validation in the context. Real-data examples are presented to demonstrate the applications of the methodology."
"10.1214/009053605000000138","2005","Estimation of a function under shape restrictions. {A}pplications to reliability","1","This paper deals with a nonparametric shape respecting estimation method for U-shaped or unimodal functions. A general upper bound for the nonasymptotic L-1-risk of the estimator is given. The method is applied to the shape respecting estimation of several classical functions, among them typical intensity functions encountered in the reliability field. In each case, we derive from our upper bound the spatially adaptive property of our estimator with respect to the L-1-metric: it approximately behaves as the best variable binwidth histogram of the function under estimation."
"10.1214/009053604000001246","2005","Nonparametric regression penalizing deviations from additivity","0","Due to the curse of dimensionality, estimation in a multidimensional nonparametric regression model is in general not feasible. Hence, additional restrictions are introduced, and the additive model takes a prominent place. The restrictions imposed can lead to serious bias. Here, a new estimator is proposed which allows penalizing the nonadditive part of a regression function. This offers a smooth choice between the full and the additive model. As a byproduct, this penalty leads to a regularization in sparse regions. If the additive model does not hold, a small penalty introduces an additional bias compared to the full model which is compensated by the reduced bias due to using smaller bandwidths.For increasing penalties, this estimator converges to the additive smooth backfitting estimator of Mammen, Linton and Nielsen [Ann. Statist. 27 (1999) 1443-1490].The structure of the estimator is investigated and two algorithms are provided. A proposal for selection of tuning parameters is made and the respective properties are studied. Finally, a finite sample evaluation is performed for simulated and ozone data."
"10.1214/009053605000000101","2005","Bandwidth selection for smooth backfitting in additive models","7","The smooth backfitting introduced by Marnmen, Linton and Nielsen [Ann. Statist. 27 (1999) 1443-1490] is a promising technique to fit additive regression models and is known to achieve the oracle efficiency bound. In this paper, we propose and discuss three fully automated bandwidth selection methods for smooth backfitting in additive models. The first one is a penalized least squares approach which is based on higher-order stochastic expansions for the residual sums of squares of the smooth backfitting estimates. The other two are plug-in bandwidth selectors which rely on approximations of the average squared errors and whose utility is restricted to local linear fitting. The large sample properties of these bandwidth selection methods are given. Their finite sample properties are also compared through simulation experiments."
"10.1214/009053605000000093","2005","Large sample theory of intrinsic and extrinsic sample means on manifolds. {II}","10","This article develops nonparametric inference procedures for estimation and testing problems for means on manifolds. A central limit theorem for Frechet sample means is derived leading to an asymptotic distribution theory of intrinsic sample means on Riemannian manifolds. Central limit theorems are also obtained for extrinsic sample means w.r.t. an arbitrary embedding of a differentiable manifold in a Euclidean space. Bootstrap methods particularly suitable for these problems are presented. Applications are given to distributions on the sphere Sal (directional spaces), real projective space Rp(N-1) (axial spaces), complex projective space Cpk-2 (planar shape spaces) w.r.t. Veronese-Whitney embeddings and a threedimensional shape space Sigma(3)(4)."
"10.1214/009053604000001066","2005","Square root penalty: adaptation to the margin in classification and in edge estimation","5","We consider the problem of adaptation to the margin in binary classification. We suggest a penalized empirical risk minimization classifier that adaptively attains, up to a logarithmic factor, fast optimal rates of convergence for the excess risk, that is, rates that can be faster than n(-1/2), where n is the sample size. We show that our method also gives adaptive estimators for the problem of edge estimation."
"10.1214/009053605000000174","2005","Hierarchical testing designs for pattern recognition","1","We explore the theoretical foundations of a ""twenty questions"" approach to pattern recognition. The object of the analysis is the computational process itself rather than probability distributions (Bayesian inference) or decision boundaries (statistical learning). Our formulation is motivated by applications to scene interpretation in which there are a great many possible explanations for the data, one (""background"") is statistically dominant, and it is imperative to restrict intensive computation to genuinely ambiguous regions.The focus here is then on pattern filtering: Given a large set Y of possible patterns or explanations, narrow down the true one Y to a small (random) subset (Y) over cap subset of Y of ""detected"" patterns to be subjected to further, more intense, processing. To this end, we consider a family of hypothesis tests for Y is an element of A versus the nonspecific alternatives Y is an element of A(c). Each test has null type I error and the candidate sets A subset of Y are arranged in a hierarchy of nested partitions. These tests are then characterized by scope (vertical bar A vertical bar), power (or type II error) and algorithmic cost.We consider sequential testing strategies in which decisions are made iteratively, based on past outcomes, about which test to perform next and when to stop testing. The set (Y) over cap is then taken to be the set of patterns that have not been ruled out by the tests performed. The total cost of a strategy is the sum of the ""testing cost"" and the ""postprocessing cost"" (proportional to vertical bar(Y) over cap vertical bar) and the corresponding optimization problem is analyzed. As might be expected, under mild assumptions good designs for sequential testing strategies exhibit a steady progression from broad scope coupled with low power to high power coupled with dedication to specific explanations. In the assumptions ensuring this property a key role is played by the ratio cost/power. These ideas are illustrated in the context of detecting rectangles amidst clutter."
"10.1214/009053605000000084","2005","Generalizations of the familywise error rate","12","H-1,..., H-s. The usual approach to dealing with the multiplicity problem is to restrict attention to procedures that control the familywise error rate (FWER), the probability of even one false rejection. In many applications, particularly if s is large, one might be willing to tolerate more than one false rejection provided the number of such cases is controlled, thereby increasing the ability of the procedure to detect false null hypotheses. This suggests replacing control of the FWER by controlling the probability of k or more false rejections, which we call the k-FWER. We derive both single-step and stepdown procedures that control the k-FWER, without making any assumptions concerning the dependence structure of the p-values of the individual tests. In particular, we derive a stepdown procedure that is quite simple to apply, and prove that it cannot be improved without violation of control of the k-FWER. We also consider the false discovery proportion (FDP) defined by the number of false rejections divided by the total number of rejections (defined to be 0 if there are no rejections). The false discovery rate proposed by Benjamini and Hochberg [J. Roy. Statist. Soc. Ser. B 57 (1995) 289-300] controls E(FDP). Here, we construct methods such that, for any gamma and alpha, P{FDP > gamma <= alpha. Two stepdown methods are proposed. The first holds under mild conditions on the dependence structure of p-values, while the second is more conservative but holds without any dependence assumptions."
"10.1214/009053605000000039","2005","Testing for monotone increasing hazard rate","1","A test of the null hypothesis that a hazard rate is monotone nondecreasing, versus the alternative that it is not, is proposed. Both the test statistic and the means of calibrating it are new. Unlike previous approaches, neither is based on the assumption that the null distribution is exponential. Instead, empirical information is used to effectively identify and eliminate from further consideration parts of the line where the hazard rate is clearly increasing; and to confine subsequent attention only to those parts that remain. This produces a test with greater apparent power, without the excessive conservatism of exponential-based tests. Our approach to calibration borrows from ideas used in certain tests for unimodality of a density, in that a bandwidth is increased until a distribution with the desired properties is obtained. However, the test statistic does not involve any smoothing, and is, in fact, based directly on an assessment of convexity of the distribution function, using the conventional empirical distribution. The test is shown to have optimal power properties in difficult cases, where it is called upon to detect a small departure, in the form of a bump, from monotonicity. More general theoretical properties of the test and its numerical performance are explored."
"10.1214/009053605000000066","2005","On optimality of stepdown and stepup multiple test procedures","7","Consider the multiple testing problem of testing k null hypotheses, where the unknown family of distributions is assumed to satisfy a certain monotonicity assumption. Attention is restricted to procedures that control the familywise error rate in the strong sense and which satisfy a monotonicity condition. Under these assumptions, we prove certain maximin optimality results for some well-known stepdown and stepup procedures."
"10.1214/009053605000000020","2005","Nonparametric checks for single-index models","7","In this paper we study goodness-of-fit testing of single-index models. The large sample behavior of certain score-type test statistics is investigated. As a by-product, we obtain asymptotically distribution-free maximin tests for a large class of local alternatives. Furthermore, characteristic function based goodness-of-fit tests are proposed which are omnibus and able to detect peak alternatives. Simulation results indicate that the approximation through the limit distribution is acceptable already for moderate sample sizes. Applications to two real data sets are illustrated."
"10.1214/009053605000000048","2005","Optimal testing of equivalence hypotheses","0","In this paper we consider the construction of optimal tests of equivalence hypotheses. Specifically, assume X-1,..., X-n are i.i.d. with distribution P theta, with theta is an element of R-k. Let g(theta) be some real-valued parameter of interest. The null hypothesis asserts g(theta) is an element of (a, b) versus the alternative g(theta) is an element of (a, b). For example, such hypotheses occur in bioequivalence studies where one may wish to show two drugs, a brand name and a proposed generic version, have the same therapeutic effect. Little optimal theory is available for such testing problems, and it is the purpose of this paper to provide an asymptotic optimality theory. Thus, we provide asymptotic upper bounds for what is achievable, as well as asymptotically uniformly most powerful test constructions that attain the bounds. The asymptotic theory is based on Le Cam's notion of asymptotically normal experiments. In order to approximate a general problem by a limiting normal problem, a UMP equivalence test is obtained for testing the mean of a multivariate normal mean."
"10.1214/009053604000001138","2005","Breakdown and groups","3","The concept of breakdown point was introduced by Hampel [Ph.D. dissertation (1968), Univ. California, Berkeley; Ann. Math. Statist. 42 (1971) 1887-1896] and developed further by, among others, Huber [Robust Statistics (1981). Wiley, New York] and Donoho and Huber [In A Festschrift for Erich L. Lehmann (1983) 157-184. Wadsworth, Belmont, CA]. It has proved most successful in the context of location, scale and regression problems. Attempts to extend the concept to other situations have not met with general acceptance. In this paper we argue that this is connected to the fact that in the location, scale and regression problems the translation and affine groups give rise to a definition of equivariance for statistical functionals. Comparisons in terms of breakdown points seem only useful when restricted to equivariant functionals and even here the connection between breakdown and equivariance is a tenuous one."
"10.1214/009053605000000363","2005","Ignorability for categorical data","1","We study the problem of ignorability in likelihood-based inference from incomplete categorical data. Two versions of the coarsened at random assumption (car) are distinguished, their compatibility with the parameter distinctness assumption is investigated and several conditions for ignorability that do not require an extra parameter distinctness assumption are established.It is shown that car assumptions have quite different implications depending on whether the underlying complete-data model is saturated or parametric. In the latter case, car assumptions can become inconsistent with observed data."
"10.1214/009053605000000291","2005","On the {B}ahadur representation of sample quantiles for dependent sequences","2","We establish the Bahadur representation of sample quantiles for linear and some widely used nonlinear processes. Local fluctuations of empirical processes are discussed. Applications to the trimmed and Winsorized means are given. Our results extend previous ones by establishing sharper bounds under milder conditions and thus provide new insight into the theory of empirical processes for dependent random variables."
"10.1214/009053605000000309","2005","Exact local {W}hittle estimation of fractional integration","2","An exact form of the local Whittle likelihood is studied with the intent of developing a general-purpose estimation procedure for the memory parameter (d) that does not rely on tapering or differencing prefilters. The resulting exact local Whittle estimator is shown to be consistent and to have the same N(0, (1)/(4)) limit distribution for all values of d if the optimization covers an interval of width less than (9)/(2) and the initial value of the process is known."
"10.1214/009053605000000318","2005","Semiparametric estimation for stationary processes whose spectra have an unknown pole","0","We consider the estimation of the location of the pole and memory parameter, lambda(0) and alpha, respectively, of covariance stationary linear processes whose spectral density function f(lambda) satisfies f(lambda) similar to C vertical bar lambda - lambda(0)vertical bar(-alpha) in a neighborhood of lambda(0). We define a consistent estimator of lambda(0) and derive its limit distribution Z(lambda)0. As in related optimization problems, when the true parameter value can lie on the boundary of the parameter space, we show that Z(lambda 0) is distributed as a normal random variable when lambda(0) is an element of (0, pi), whereas for lambda(0) = 0 or pi, Z(lambda 0) is a mixture of discrete and continuous random variables with weights equal to 1/2. More specifically, when lambda(0) = 0, Z(lambda 0) is distributed as a normal random variable truncated at zero. Moreover, we describe and examine a two-step estimator of the memory parameter alpha, showing that neither its limit distribution nor its rate of convergence is affected by the estimation of lambda(0). Thus, we reinforce and extend previous results with respect to the estimation of alpha when lambda(0) is assumed to be known a priori. A small Monte Carlo study is included to illustrate the finite sample performance of our estimators."
"10.1214/009053605000000354","2005","Efficiency improvements in inference on stationary and nonstationary fractional time series","1","We consider a time series model involving a fractional stochastic component, whose integration order can lie in the stationary/invertible or nonstationary regions and be unknown, and an additive deterministic component consisting of a generalized polynomial. The model can thus incorporate competing descriptions of trending behavior. The stationary input to the stochastic component has parametric autocorrelation, but innovation with distribution of unknown form. The model is thus semiparametric, and we develop estimates of the parametric component which are asymptotically normal and achieve an M-estimation efficiency bound, equal to that found in work using an adaptive LAM/LAN approach. A major technical feature which we treat is the effect of truncating the autoregressive representation in order to form innovation proxies. This is relevant also when the innovation density is parameterized, and we provide a result for that case also. Our semiparametric estimates employ nonparametric series estimation, which avoids some complications and conditions in kernel approaches featured in much work on adaptive estimation of time series models; our work thus also contributes to methods and theory for nonfractional time series models, such as autoregressive moving averages. A Monte Carlo study of finite sample performance of the semiparametric estimates is included."
"10.1214/009053605000000336","2005","Bayesian {P}oisson process partition calculus with an application to {B}ayesian {L}\'evy moving averages","3","This article develops, and describes how to use, results concerning disintegrations of Poisson random measures. These results are fashioned as simple tools that can be tailor-made to address inferential questions arising in a wide range of Bayesian nonparametric and spatial statistical models. The Poisson disintegration method is based on the formal statement of two results concerning a Laplace functional change of measure and a Poisson Palm/Fubini calculus in terms of random partitions of the integers {1,..., n}. The techniques are analogous to, but much more general than, techniques for the Dirichlet process and weighted gamma process developed in [Ann. Statist. 12 (1984) 351-357] and [Ann. Inst. Statist. Math. 41 (1989) 227-245]. In order to illustrate the flexibility of the approach, large classes of random probability measures and random hazards or intensities which can be expressed as functionals of Poisson random measures are described. We describe a unified posterior analysis of classes of discrete random probability which identifies and exploits features common to all these models. The analysis circumvents many of the difficult issues involved in Bayesian nonparametric calculus, including a combinatorial component. This allows one to focus on the unique features of each process which are characterized via real valued functions h. The applicability of the technique is further illustrated by obtaining explicit posterior expressions for Levy-Cox moving average processes within the general setting of multiplicative intensity models. In addition, novel computational procedures, similar to efficient procedures developed for the Dirichlet process, are briefly discussed for these models."
"10.1214/009053605000000327","2005","A new class of generalized {B}ayes minimax ridge regression estimators","1","Let y = A beta + epsilon, where y is an N x 1 vector of observations, beta is a p x I vector of unknown regression coefficients, A is an N x p design matrix and E is a spherically symmetric error term with unknown scale parameter a. We consider estimation of under general quadratic loss functions, and, in particular, extend the work of Strawderman [J. Amer Statist. Assoc. 73 (1978) 623-627] and Casella [Ann. Statist. 8 (1980) 1036-1056, J. Amer. Statist. Assoc. 80 (1985) 753-758] by finding adaptive minimax estimators (which are, under the nonnality assumption, also generalized Bayes) of beta, which have greater numerical stability (i.e., smaller condition number) than the usual least squares estimator. In particular, we give a subclass of such estimators which, surprisingly, has a very simple form. We also show that under certain conditions the generalized Bayes minimax estimators in the normal case are also generalized Bayes and minimax in the general case of spherically symmetric errors."
"10.1214/009053605000000345","2005","Empirical {B}ayes selection of wavelet thresholds","22","This paper explores a class of empirical Bayes methods for level-dependent threshold selection in wavelet shrinkage. The prior considered for each wavelet coefficient is a mixture of an atom of probability at zero and a heavy-tailed density. The mixing weight, or sparsity parameter, for each level of the transform is chosen by marginal maximum likelihood. If estimation is carried out using the posterior median, this is a random thresholding procedure; the estimation can also be carried out using other thresholding rules with the same threshold. Details of the calculations needed for implementing the procedure are included. In practice, the estimates are quick to compute and there is software available. Simulations on the standard model functions show excellent performance, and applications to data drawn from various fields of application are used to explore the practical performance of the approach.By using a general result on the risk of the corresponding marginal maximum likelihood approach for a single sequence, overall bounds on the risk of the method are found subject to membership of the unknown function in one of a wide range of Besov classes, covering also the case of f of bounded variation. The rates obtained are optimal for any value of the parameter p in (0, infinity], simultaneously for a wide range of loss functions, each dominating the L-q norm of the sigma th derivative, with or >= 0 and 0 < q < 2.Attention is paid to the distinction between sampling the unknown function within white noise and sampling at discrete points, and between placing constraints on the function itself and on the discrete wavelet transform of its sequence of values at the observation points. Results for all relevant combinations of these scenarios are obtained. In some cases a key feature of the theory is a particular boundary-corrected wavelet basis, details of which are discussed.Overall, the approach described seems so far unique in combining the properties of fast computation, good theoretical properties and good performance in simulations and in practice. A key feature appears to be that the estimate of sparsity adapts to three different zones of estimation, first where the signal is not sparse enough for thresholding to be of benefit, second where an appropriately chosen threshold results in substantially improved estimation, and third where the signal is so sparse that the zero estimate gives the optimum accuracy rate."
"10.1214/009053605000000273","2005","Directions and projective shapes","2","This paper deals with projective shape analysis, which is a study of finite configurations of points modulo projective transformations. The topic has various applications in machine vision. We introduce a convenient projective shape space, as well as an appropriate coordinate system for this shape space. For generic configurations of k points in m dimensions, the resulting projective shape space is identified as a product of k - m - 2 copies of axial spaces RPm. This identification leads to the need for developing multivariate directional and multivariate axial analysis and we propose parametric models, as well as nonparametric methods, for these areas. In particular, we investigate the Frechet extrinsic mean for the multivariate axial case. Asymptotic distributions of the appropriate parametric and nonparametric tests are derived. We illustrate our methodology with examples from machine vision."
"10.1214/009053605000000264","2005","Statistical analysis on high-dimensional spheres and shape spaces","3","We consider the statistical analysis of data on high-dimensional spheres and shape spaces. The work is of particular relevance to applications where high-dimensional data are available-a commonly encountered situation in many disciplines. First the uniform measure on the infinite-dimensional sphere is reviewed, together with connections with Wiener measure. We then discuss densities of Gaussian measures with respect to Wiener measure. Some nonuniform distributions on infinite-dimensional spheres and shape spaces are introduced, and special cases which have important practical consequences are considered. We focus on the high-dimensional real and complex Bingham, uniform, von Mises-Fisher, Fisher-Bingham and the real and complex Watson distributions. Asymptotic distributions in the cases where dimension and sample size are large are discussed. Approximations for practical maximum likelihood based inference are considered, and in particular we discuss an application to brain shape modeling."
"10.1214/009053605000000200","2005","Variable selection using {MM} algorithms","25","Variable selection is fundamental to high-dimensional statistical modeling. Many variable selection techniques may be implemented by maximum penalized likelihood using various penalty functions. Optimizing the penalized likelihood function is often challenging because it may be nondifferentiable and/or nonconcave. This article proposes a new class of algorithms for finding a maximizer of the penalized likelihood for a broad class of penalty functions. These algorithms operate by perturbing the penalty function slightly to render it differentiable, then optimizing this differentiable function using a minorize-maximize (MM) algorithm. MM algorithms are useful extensions of the well-known class of EM algorithms, a fact that allows us to analyze the local and global convergence of the proposed algorithm using some of the techniques employed for EM algorithms. In particular, we prove that when our MM algorithms converge, they must converge to a desirable point; we also discuss conditions under which this convergence may be guaranteed. We exploit the Newton-Raphson-like aspect of these algorithms to propose a sandwich estimator for the standard errors of the estimators. Our method performs well in numerical tests."
"10.1214/009053605000000192","2005","Contour regression: a general approach to dimension reduction","18","We propose a novel approach to sufficient dimension reduction in regression, based on estimating contour directions of small variation in the response. These directions span the orthogonal complement of the minimal space relevant for the regression and can be extracted according to two measures of variation in the response, leading to simple and general contour regression (SCR and GCR) methodology. In comparison with existing sufficient dimension reduction techniques, this contour-based methodology guarantees exhaustive estimation of the central subspace under ellipticity of the predictor distribution and mild additional assumptions, while maintaining,root n-consistency and computational ease. Moreover, it proves robust to departures from ellipticity. We establish population properties for both SCR and GCR, and asymptotic properties for SCR. Simulations to compare performance with that of standard techniques such as ordinary least squares, sliced inverse regression, principal Hessian directions and sliced average variance estimation confirm the advantages anticipated by the theoretical analyses. We demonstrate the use of contour-based methods on a data set concerning soil evaporation."
"10.1214/009053605000000255","2005","Boosting with early stopping: convergence and consistency","6","Boosting is one of the most significant advances in machine learning for classification and regression. In its original and computationally flexible version, boosting seeks to minimize empirically a loss function in a greedy fashion. The resulting estimator takes an additive function form and is built iteratively by applying a base estimator (or learner) to updated samples depending on the previous iterations. An unusual regularization technique, early stopping, is employed based on CV or a test set.This paper studies numerical convergence, consistency and statistical rates of convergence of boosting with early stopping, when it is carried out over the linear span of a family of basis functions. For general loss functions, we prove the convergence of boosting's greedy optimization to the infinimum of the loss function over the linear span. Using the numerical convergence result, we find early-stopping strategies under which boosting is shown to be consistent based on i.i.d. samples, and we obtain bounds on the rates of convergence for boosting estimators. Simulation studies are also presented to illustrate the relevance of our theoretical results for providing insights to practical aspects of boosting.As a side product, these results also reveal the importance of restricting the greedy search step-sizes, as known in practice through the work of Friedman and others. Moreover, our results lead to rigorous proof that for a linearly separable problem, AdaBoost with epsilon -> 0 step-size becomes an L-1-margin maximizer when left to run to convergence."
"10.1214/009053605000000282","2005","Local {R}ademacher complexities","7","We propose new bounds on the error of learning algorithms in terms of a data-dependent notion of complexity. The estimates we establish give optimal rates and are based on a local and empirical version of Rademacher averages, in the sense that the Rademacher averages are computed from the data, on a subset of functions with small empirical error. We present some applications to classification and prediction with convex function classes, and with kernel classes in particular."
"10.1214/009053605000000228","2005","Complexities of convex combinations and bounding the generalization error in classification","3","We introduce and study several measures of complexity of functions from the convex hull of a given base class. These complexity measures take into account the sparsity of the weights of a convex combination as well as certain clustering properties of the base functions involved in it. We prove new upper confidence bounds on the generalization error of ensemble (voting) classification algorithms that utilize the new complexity measures along with the empirical distributions of classification margins, providing a better explanation of generalization performance of large margin classification methods."
"10.1214/009053604000001219","2005","On the {C}hernoff bound for efficiency of quantum hypothesis testing","1","The paper estimates the Chernoff rate for the efficiency of quantum IF hypothesis testing. For both joint and separate measurements, approximate bounds for the rate are given if both states are mixed, and exact expressions are derived if at least one of the states is pure. The efficiencies of tests with separate and joint measurements are compared. The results are illustrated by a test of quantum entanglement."
"10.1214/009053604000001228","2005","A general theory of minimum aberration and its applications","0","Minimum aberration is an increasingly popular criterion for comparing and assessing fractional factorial designs, and few would question its importance and usefulness nowadays. In the past decade or so, a great deal of work has been done on minimum aberration and its various extensions. This paper develops a general theory of minimum aberration based on a sound statistical principle. Our theory provides a unified framework for minimum aberration and further extends the existing work in the area. More importantly, the theory offers a systematic method that enables experimenters to derive their own aberration criteria. Our general theory also brings together two seemingly separate research areas: one on minimum aberration designs and the other on designs with requirement sets. To facilitate the design construction, we develop a complementary design theory for quite a general class of aberration criteria. As an immediate application, we present some construction results on a weak version of this class of criteria."
"10.1214/009053604000000887","2005","Optimal and efficient crossover designs for comparing test treatments with a control treatment","2","This paper deals exclusively with crossover designs for the purpose of comparing t test treatments with a control treatment when the number of periods is no larger than t + 1. Among other results it specifies sufficient conditions for a crossover design to be simultaneously A-optimal and MV-optimal in a very large and appealing class of crossover designs. It is expected that these optimal designs are highly efficient in the entire class of crossover designs. Some computationally useful tools are given and used to build assorted small optimal and efficient crossover designs. The model robustness of these newly discovered crossover designs is discussed."
"10.1214/009053604000000706","2005","Local central limit theorems, the high-order correlations of rejective sampling and logistic likelihood asymptotics","0","Let I-I,..., I-n be independent but not necessarily identically distributed Bernoulli random variables, and let X-n = Sigma(j=1)(n) I-j. For v in a bounded region, a local central limit theorem expansion of P(X-n = EXn + v) is developed to any given degree. By conditioning, this expansion provides information on the high-order correlation structure Of dependent, weighted sampling schemes of a population E (a special case of which is simple random sampling), where a set d C E is sampled with probability proportional to Pi A is an element of d(X)A, where x(A) are positive weights associated with individuals A is an element of E. These results are used to determine the asymptotic information, and demonstrate the consistency and asymptotic normality of the conditional and unconditional logistic likelihood estimator for unmatched case-control study designs in which sets of controls of the same size are sampled with equal probability."
"10.1214/009053604000001200","2005","Data-driven rate-optimal specification testing in regression models","0","We propose new data-driven smooth tests for a parametric regression function. The smoothing parameter is selected through a new criterion that favors a large smoothing parameter under the null hypothesis. The resulting test is adaptive rate-optimal and consistent against Pitman local alternatives approaching the parametric model at a rate arbitrarily close to I/root n. Asymptotic critical values come from the standard normal distribution and the bootstrap can be used in small samples. A general formalization allows one to consider a large class of linear smoothing methods, which can be tailored for detection of additive alternatives."
"10.1214/009053604000001165","2005","Extremal quantile regression","2","Quantile regression is an important tool for estimation of conditional quantiles of a response Y given a vector of covariates X. It can be used to measure the effect of covariates not only in the center of a distribution. but also in the upper and lower tails. This paper develops a theory of quantile regression in the tails. Specifically, it obtains the large sample properties of extremal (extreme order and intermediate order) quantile regression estimators for the linear quantile regression model with the tails restricted to the domain of minimum attraction and closed under tail equivalence across regressor values. This modeling setup combines restrictions of extreme value theory with leading homoscedastic and heteroscedastic linear specifications of regression analysis. In large samples, extreme order regression quantiles converge weakly to argmin functionals of stochastic integrals of Poisson processes that depend on regressors, while intermediate regression quantiles and their functionals converge to normal vectors with variance matrices dependent on the tail parameters and the regressor design."
"10.1214/009053604000001156","2005","Generalized functional linear models","18","We propose a generalized functional linear regression model for a regression situation where the response variable is a scalar and the predictor is a random function. A linear predictor is obtained by forming the scalar product of the predictor function with a smooth parameter function, and the expected value of the response is related to this linear predictor via a link function. If, in addition, a variance function is specified, this leads to a functional estimating equation which corresponds to maximizing a functional quasi-likelihood. This general approach includes the special cases of the functional linear model, as well as functional Poisson regression and functional binomial regression. The latter leads to procedures for classification and discrimination of stochastic processes and functional data. We also consider the situation where the link and variance functions are unknown and are estimated nonparametrically from the data, using a semiparametric quasi-likelihood procedure.An essential step in our proposal is dimension reduction by approximating the predictor processes with a truncated Karhunen-Loeve expansion. We develop asymptotic inference for the proposed class of generalized regression models. In the proposed asymptotic approach, the truncation parameter increases with sample size, and a martingale central limit theorem is applied to establish the resulting increasing dimension asymptotics. We establish asymptotic normality for a properly scaled distance between estimated and true functions that corresponds to a suitable L-2 metric and is defined through a generalized covariance operator. As a consequence, we obtain asymptotic tests and simultaneous confidence bands for the parameter function that determines the model.The proposed estimation, inference and classification procedures and variants with unknown link and variance functions are investigated in a simulation study. We find that the practical selection of the number of components works well with the AIC criterion, and this finding is supported by theoretical considerations. We include an application to the classification of medflies regarding their remaining longevity status, based on the observed initial egg-laying curve for each of 534 female medflies."
"10.1214/009053604000001147","2005","Spike and slab variable selection: frequentist and {B}ayesian strategies","5","Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty."
"10.1214/009053605000000011","2005","Confidence sets for nonparametric wavelet regression","10","We construct nonparametric confidence sets for regression functions using wavelets that are uniform over Besov balls. We consider both thresholding and modulation estimators for the wavelet coefficients. The confidence set is obtained by showing that a pivot process, constructed from the loss function, converges uniformly to a mean zero Gaussian process. Inverting this pivot yields a confidence set for the wavelet coefficients, and from this we obtain confidence sets on functionals of the regression curve."
"10.1214/009053604000001075","2005","Multiprocess parallel antithetic coupling for backward and forward {M}arkov chain {M}onte {C}arlo","1","Antithetic coupling is a general stratification strategy for reducing Monte Carlo variance without increasing the simulation size. The use of the antithetic principle in the Monte Carlo literature typically employs two strata via antithetic quantile coupling. We demonstrate here that further stratification, obtained by using k > 2 (e.g., k = 3-10) antithetically coupled variates, can offer substantial additional gain in Monte Carlo efficiency, in terms of both variance and bias. The reason for reduced bias is that antithetically coupled chains can provide a more dispersed search of the state space than multiple independent chains. The emerging area of perfect simulation provides a perfect setting for implementing the k-process parallel antithetic coupling for MCMC because, without antithetic coupling, this class of methods delivers genuine independent draws. Furthermore, antithetic backward coupling provides a very convenient theoretical tool for investigating antithetic forward coupling. However, the generation of k > 2 antithetic variates that are negatively associated, that is, they preserve negative correlation under monotone transformations, and extremely antithetic, that is, they are as negatively correlated as possible, is more complicated compared to the case with k = 2. In this paper, we establish a theoretical framework for investigating such issues. Among the generating methods that we compare, Latin hypercube sampling and its iterative extension appear to be general-purpose choices, making another direct link between Monte Carlo and quasi Monte Carlo."
"10.1214/009053604000001237","2005","Functionals of {D}irichlet processes, the {C}ifarelli-{R}egazzini identity and beta-gamma processes","0","Suppose that P theta(g) is a linear functional of a Dirichlet process with shape theta H, where theta > 0 is the total mass and H is a fixed probability measure. This paper describes how one can use the well-known Bayesian prior to posterior analysis of the Dirichlet process, and a posterior calculus for Gamma processes to ascertain properties of linear functionals of Dirichlet processes. In particular, in conjunction with a Gamma identity, we show easily that a generalized Cauchy-Stieltjes transform of a linear functional of a Dirichlet process is equivalent to the Laplace functional of a class of, what we define as, Beta-Gamma processes. This represents a generalization of an identity due to Cifarelli and Regazzini, which is also known as the Markov-Krein identity for mean functionals of Dirichlet processes. These results also provide new explanations and interpretations of results in the literature. The identities are analogues to quite useful identities for Beta and Gamma random variables. We give a result which can be used to ascertain specifications on H such that the Dirichlet functional is Beta distributed. This avoids the need for an inversion formula for these cases and points to the special nature of the Dirichlet process, and indeed the functional Beta-Gamma calculus developed in this paper."
"10.1214/009053605000000075","2005","Posterior propriety and admissibility of hyperpriors in normal hierarchical models","2","Hierarchical modeling is wonderful and here to stay, but hyperparameter priors are often chosen in a casual fashion. Unfortunately, as the number of hyperparameters grows, the effects of casual choices can multiply, leading to considerably inferior performance. As an extreme, but not uncommon, example use of the wrong hyperparameter priors can even lead to impropriety of the posterior.For exchangeable hierarchical multivariate normal models, we first determine when a standard class of hierarchical priors results in proper or improper posteriors. We next determine which elements of this class lead to admissible estimators of the mean under quadratic loss; such considerations provide one useful guideline for choice among hierarchical priors. Finally, computational issues with the resulting posterior distributions are addressed."
"10.1214/009053604000001273","2005","Invariant {B}ayesian estimation on manifolds","0","A frequent and well-founded criticism of the maximum a posteriori (MAP) and minimum mean squared error (MMSE) estimates of a continuous parameter gamma taking values in a differentiable manifold F is that they are not invariant to arbitrary ""reparameterizations"" of 1. This paper clarifies the issues surrounding this problem, by pointing out the difference between coordinate invariance, which is a sine qua non for a mathematically well-defined problem, and diffeomorphism invariance, which is a substantial issue, and then provides a solution. We first show that the presence of a metric structure on F can be used to define coordinate-invariant MAP and MMSE estimates, and we argue that this is the natural way to proceed. We then discuss the choice of a metric structure on F. By imposing an invariance criterion natural within a Bayesian framework, we show that this choice is essentially unique. It does not necessarily correspond to a choice of coordinates. In cases of complete prior ignorance, when Jeffreys' prior is used, the invariant MAP estimate reduces to the maximum likelihood estimate. The invariant MAP estimate coincides with the minimum message length (MML) estimate, but no discretization or approximation is used in its derivation."
"10.1214/009053604000001264","2005","Default priors for {G}aussian processes","2","Motivated by the statistical evaluation of complex computer models, we deal with the issue of objective prior specification for the parameters of Gaussian processes. In particular, we derive the Jeffreys-rule, independence Jeffreys and reference priors for this situation, and prove that the resulting posterior distributions are proper under a quite general set of conditions. A proper flat prior strategy, based on maximum likelihood estimates, is also considered, and all priors are then compared on the grounds of the frequentist properties of the ensuing Bayesian procedures. Computational issues are also addressed in the paper, and we illustrate the proposed solutions by means of an example taken from the field of complex computer model validation."
"10.1214/009053605000000057","2005","Analysis of binary spatial data by quasi-likelihood estimating equations","2","The goal of this paper is to describe the application of quasi-likelihood estimating equations for spatially correlated binary data. In this paper, a logistic function is used to model the marginal probability of binary responses in terms of parameters of interest. With mild assumptions on the correlations, the Leonov-Shiryaev formula combined with a comparison of characteristic functions can be used to establish asymptotic normality for linear combinations of the binary responses. The consistency and asymptotic normality for quasi-likelihood estimates can then be derived. By modeling spatial correlation with a variogram, we apply these asymptotic results to test independence of two spatially correlated binary outcomes and illustrate the concepts with a well-known example based on data from Lansing Woods. The comparison of generalized estimating equations and the proposed approach is also discussed."
"10.1214/009053604000001255","2005","Asymptotic results with generalized estimating equations for longitudinal data","0","We consider the marginal models of Liang and Zeger [Bioinetrika 73 (1986) 13-22] for the analysis of longitudinal data and we develop a theory of statistical inference for such models. We prove the existence, weak consistency and asymptotic normality of a sequence of estimators defined as roots of pseudo-likelihood equations."
"10.1214/009053604000001291","2005","Likelihood approach for marginal proportional hazards regression in the presence of dependent censoring","1","In many public health problems, an important goal is to identify the effect of some treatment/intervention on the risk of failure for the whole population. A marginal proportional hazards regression model is often used to analyze such an effect. When dependent censoring is explained by many auxiliary covariates, we utilize two working models to condense high-dimensional covariates to achieve dimension reduction. Then the estimator of the treatment effect is obtained by maximizing a pseudo-likelihood function over a sieve space. Such an estimator is shown to be consistent and asymptotically normal when either of the two working models is correct; additionally, when both working models are correct, its asymptotic variance is the same as the semiparametric efficiency bound."
"10.1214/009053604000001309","2005","Selecting likelihood weights by cross-validation","1","The (relevance) weighted likelihood was introduced to formally embrace a variety of statistical procedures that trade bias for precision. Unlike its classical counterpart, the weighted likelihood combines all relevant information while inheriting many of its desirable features including good asymptotic properties. However, in order to be effective, the weights involved in its construction need to be judiciously chosen. Choosing those weights is the subject of this article in which we demonstrate the use of cross-validation. We prove the resulting weighted likelihood estimator (WLE) to be weakly consistent and asymptotically normal. An application to disease mapping data is demonstrated."
"10.1214/009053604000000878","2005","Iterated smoothed bootstrap confidence intervals for population quantiles","0","This paper investigates the effects of smoothed bootstrap iterations on coverage probabilities of smoothed bootstrap and bootstrap-t confidence intervals for population quantiles, and establishes the optimal kernel bandwidths at various stages of the smoothing procedures. The conventional smoothed bootstrap and bootstrap-t methods have been known to yield onesided coverage errors of orders O(n(-1/2)) and o(n(-2/3)), respectively, for intervals based on the sample quantile of a random sample of size n. We sharpen the latter result to O(n(-5/6)) with proper choices of bandwidths at the bootstrapping and Studentization steps. We show further that calibration of the nominal coverage level by means of the iterated bootstrap succeeds in reducing the coverage error of the smoothed bootstrap percentile interval to the order O(n(-2/3)) and that of the smoothed bootstrap-t interval to O(n(-58/57)) provided that bandwidths are selected of appropriate orders. Simulation results confirm our asymptotic findings, suggesting that the iterated smoothed bootstrap-t method yields the most accurate coverage. On the other hand, the iterated smoothed bootstrap percentile method interval has the advantage of being shorter and more stable than the bootstrap-t intervals."
"10.1214/009053604000000904","2005","Generalized bootstrap for estimating equations","5","We introduce a generalized bootstrap technique for estimators obtained by solving estimating equations. Some special cases of this generalized bootstrap are the classical bootstrap of Efron. the delete-d jackknife and variations of the Bayesian bootstrap. The use of the proposed technique is discussed in some examples, Distributional consistency of the method is established and an asymptotic representation of the resampling variance estimator is obtained."
"10.1214/009053604000000922","2005","Depth weighted scatter estimators","1","General depth weighted scatter estimators are introduced and investigated. For general depth functions, we find out that these affine equivariant scatter estimators are Fisher consistent and unbiased for a wide range of multivariate distributions, and show that the sample scatter estimators are strong and root n-consistent and asymptotically normal, and the influence functions of the estimators exist and are bounded in general. We then concentrate on a specific case of the general depth weighted scatter estimators, the projection depth weighted scatter estimators, which include as a special case the well-known Stahel-Donoho scatter estimator whose limiting distribution has long been open until this paper. Large sample behavior, including consistency and asymptotic normality, and efficiency and finite sample behavior, including breakdown point and relative efficiency of the sample projection depth weighted scatter estimators, are thoroughly investigated. The influence function and the maximum bias of the projection depth weighted scatter estimators are derived and examined. Unlike typical high-breakdown competitors, the projection depth weighted scatter estimators can integrate high breakdown point and high efficiency while enjoying a bounded-influence function and a moderate maximum bias curve. Comparisons with leading estimators on asymptotic relative efficiency and gross error sensitivity reveal that the projection depth weighted scatter estimators behave very well overall and, consequently, represent very favorable choices of affine equivariant multivariate scatter estimators."
"10.1214/009053604000000940","2005","A robust method for cluster analysis","2","Let there be given a contaminated list of n R-d-valued observations coming from g different, normally distributed populations with a common covariance matrix. We compute the ML-estimator with respect to a certain statistical model with n - r outliers for the parameters of the g populations it detects outliers and simultaneously partitions their complement into g clusters. It turns out that the estimator unites both the minimum-covariance-determinant rejection method and the well-known pooled determinant criterion of cluster analysis. We also propose an efficient algorithm for approximating this estimator and study its breakdown points for mean values and pooled SSP matrix."
"10.1214/009053604000000913","2005","Efficient estimation of {B}anach parameters in semiparametric models","0","Consider a semiparametric model with a Euclidean parameter and an infinite-dimensional parameter, to be called a Banach parameter. Assume:(a) There exists an efficient estimator of the Euclidean parameter.(b) When the value of the Euclidean parameter is known, there exists an estimator of the Banach parameter, which depends on this value and is efficient within this restricted model.Substituting the efficient estimator of the Euclidean parameter for the value of this parameter in the estimator of the Banach parameter, one obtains an efficient estimator of the Banach parameter for the full semiparametric model with the Euclidean parameter unknown. This hereditary property of efficiency completes estimation in semiparametric models in which the Euclidean parameter has been estimated efficiently. Typically, estimation of both the Euclidean and the Banach parameter is necessary in order to describe the random phenomenon under study to a sufficient extent. Since efficient estimators are asymptotically linear, the above substitution method is a particular case of substituting asymptotically linear estimators of a Euclidean parameter into estimators that are asymptotically linear themselves and that depend on this Euclidean parameter. This more general substitution case is studied for its own sake as well, and a hereditary property for asymptotic linearity is proved."
"10.1214/009053604000000959","2005","Bandwidth choice for nonparametric classification","2","It is shown that, for kernel-based classification with univariate distributions and two populations, optimal bandwidth choice has a dichotomous character. If the two densities cross at just one point. where their curvature.,, have the same signs, then minimum Bayes risk is achieved using bandwidths which are an order of magnitude larger than those which minimize pointwise estimation error. On the other hand, if the curvature signs are different, or if there are multiple crossing points. then bandwidths of conventional size are generally appropriate. The range of different modes of behavior is narrower in multivariate settings. There, the optimal size of bandwidth is generally the same as that which is appropriate for pointwise density estimation. These properties motivate empirical rules for bandwidth choice."
"10.1214/009053604000000931","2005","Efficient estimation of a semiparametric partially linear varying coefficient model","4","In this paper we propose a general series method to estimate a semiparametric partially linear varying coefficient model. We establish the consistency and root n-normality property of the estimator of the finite-dimensional parameters of the model, We further show that, when the error is conditionally homoskedastic. this estimator is semiparametrically efficient in the sense that the inverse of the asymptotic variance of the estimator of the finite-dimensional parameter reaches the semiparametric efficiency bound of this model. A small-scale simulation is reported to examine the finite.sample performance of the proposed estimator, and an empirical application is presented to illustrate the usefulness of the proposed method in practice. We also discuss how to obtain an efficient estimation result when the error is conditional heteroskedastic."
"10.1214/009053604000000896","2005","Testing convex hypotheses on the mean of a {G}aussian vector. {A}pplication to testing qualitative hypotheses on a regression function","1","In this paper we propose a general methodology, based on multiple testing, for testing that the mean Of a Gaussian) vector in R(n) belongs to a convex set. We show that the test achieves its nominal level. and characterize a class of vectors over which the tests achieve a prescribed power, In the functional regression model this general methodology is applied to test some qualitative hypotheses oil the regression function. For example. we (est that the regression function is positive. increasing, convex, or more generally, satisfies a differential inequality. Uniform separation rates over classes of smooth functions are established and a comparison with other results in the literature is provided. A simulation study evaluates some of the procedures for testing monotonicity."
"10.1214/009053604000000832","2005","Nonparametric estimation over shrinking neighborhoods: superefficiency and adaptation","0","A theory of superefficiency and adaptation is developed under flexible performance measures which give a multiresolution view of risk and bridge the gap between pointwise and global estimation, This theory provides a useful benchmark for the evaluation of spatially adaptive estimators and shows that the possible degree or superefficiency for minimax rate optimal estimators critically depends on the size of the neighborhood over which the risk is measured.Wavelet procedures are given which adapt rate optimally for given shrinking neighborhoods including the extreme cases of mean squared error at a point and mean integrated squared error over the whole interval, These adaptive procedures are based on a new wavelet block thresholding scheme which combines both the commonly used horizontal blocking of wavelet coefficients (at the same resolution level) and vertical blocking of coefficients (across different resolution levels)."
"10.1214/009053604000001084","2005","Combining information from independent sources through confidence distributions","1","This paper develops new methodology, together with related theories, for combining information from independent studies through confidence distributions. A formal definition of a confidence distribution and its asymptotic counterpart (i.e., asymptotic confidence distribution) are given and illustrated in the context of combining information. Two general combination methods are developed: the first along the lines of combining p-values, with some notable differences in regard to optimality of Bahadur type efficiency;, the second by multiplying and normalizing confidence densities. The latter approach is inspired by the common approach of multiplying likelihood functions for combining parametric information. The paper also develops adaptive combining methods, with supporting asymptotic theory which should be of practical interest. The key point of the adaptive development is that the methods attempt to combine only the correct information, downweighting or excluding studies containing little or wrong information about the true parameter of interest. The combination methodologies are illustrated in simulated and real data examples with a variety of applications."
"10.1214/009053604000000986","2005","Characterization of {B}ayes procedures for multiple endpoint problems and inadmissibility of the step-up procedure","3","The problem of multiple endpoint testing for k endpoints is treated as a 2(k) finite action problem. The loss function chosen is a vector loss function consisting of two components. The two components lead to a vector risk. One component of the vector risk is the false rejection rate (FRR), that is, the expected number of false rejections. The other component is the false acceptance rate (FAR), that is, the expected number of acceptances for which the corresponding null hypothesis is false. This loss function is more stringent than the positive linear combination loss function of Lehmann [Ann. Math. Statist. 28 (1957) 1-25] and Cohen and Sackrowitz [Ann. Statist. (2005) 33 126-144] in the sense that the class of admissible rules is larger for this vector risk formulation than for the linear combination risk function. In other words, fewer procedures are inadmissible for the vector risk formulation. The statistical model assumed is that the vector of variables Z is multivariate normal with mean vector A and known intraclass covariance matrix E. The endpoint hypotheses are H-i : mu(i) = 0 vs K-i : mu(i) > 0, i = 1,..., k. A characterization of all symmetric Bayes procedures and their limits is obtained. The characterization leads to a complete class theorem. The complete class theorem is used to provide a useful necessary condition for admissibility of a procedure. The main result is that the step-up multiple endpoint procedure is shown to be inadmissible."
"10.1214/009053604000000968","2005","Decision theory results for one-sided multiple comparison procedures","1","A resurgence of interest in multiple hypothesis testing has Occurred in the last decade. Motivated by studies in genomics. microarrays, DNA sequencing, drug screening, clinical trials. bioassays, education and psychology, statisticians have been devoting considerable research energy in an effort to properly analyze multiple endpoint data. In response to new applications. new criteria and new methodology, many ad hoc procedures have emerged. The classical requirement has been to use procedures which control the strong familywise error rate (FWE) at some predetermined level a. That is, the probability of any false rejection of a true null hypothesis should be less than or equal to alpha. Finding desirable and powerful multiple test procedures is difficult under this requirement.One of the more recent ideas is concerned with controlling the false discovery rate (FDR), that is, the expected proportion of rejected hypotheses which are, in fact, true. Many multiple test procedures do control the FDR.A much earlier approach to multiple testing was formulated by Lehmann [Ann. Math. Statist. 23 (1952) 541-552 and 28 (1957) 1-25]. Lehmann's approach is decision theoretic and he treats the multiple endpoints problem as a 2(k) finite action problem when there are k endpoints. This approach is appealing since unlike the FWE and FDR criteria, the finite action approach pays attention to false acceptances as well as false rejections. In this paper we view the multiple endpoints problem as a 2(k) finite action problem. We study the popular procedures single-step, step-down and step-tip front the point of view of admissibility, Bayes and limit of Bayes properties. For our model, which is a prototypical one, and our loss function. we are able to demonstrate the following results under some fairly general conditions to he specified:(i) The single-step procedure is admissible,(ii) A sequence of prior distributions is given for which the step-down procedure is a limit of a sequence of Bayes procedures.(iii) For a vector risk function, where each component is the risk for an individual testing problem, various admissibility and inadmissibility results are obtained.In a companion paper [Cohen and Sackrowit/, Ann, Statist, 33 (2005) 145-158], we are able to give a characterization of Bayes, procedures and their limits. The characterization yields a complete class and the additional useful result that the step-up procedure is inadmissible. The inadmissibility of step-up is demonstrated there for a more stringent loss function. Additional decision theoretic type results are also obtained in this paper."
"10.1214/009053604000000977","2005","Minimax estimation with thresholding and its application to wavelet analysis","0","Many statistical practices involve choosing between a full model and reduced models where some coefficients are reduced to zero. Data were used to select a model with estimated coefficients. Is it possible to do so and still come up with an estimator always better than the traditional estimator based on the full model? The James-Stein estimator is such an estimator, having a property called minimaxity. However, the estimator considers only one reduced model, namely the origin. Hence it reduces no coefficient estimator to zero or every coefficient estimator to zero. In many applications including wavelet analysis, what should be more desirable is to reduce to zero only the estimators smaller than a threshold, called thresholding in this paper. Is it possible to construct this kind of estimators which are minimax?In this paper, we construct such minimax estimators which perform thresholding. We apply our recommended estimator to the wavelet analysis and show that it performs the best among the well-known estimators aiming simultaneously at estimation and model selection. Some of our estimators are also shown to be asymptotically optimal."
"10.1214/009053604000000995","2005","General empirical {B}ayes wavelet methods and exactly adaptive minimax estimation","7","In many statistical problems, stochastic signals can be represented as a sequence of noisy wavelet coefficients. In this paper. we develop general empirical Bayes methods for the estimation of true signal, Our estimators approximate certain oracle separable rules and achieve adaptation to ideal risks and exact minimax risks in broad collections of classes of signals, In particular, our estimators are Uniformly adaptive to the minimum risk of separable estimators and the exact minimax risks Simultaneously in Besov balls of all smoothness and shape indices, and they are uniformly superefficient in convergence rates in all compact sets it Besov spaces with a finite secondary shape parameter. Furthermore. in classes nested between Besov balls of the same smoothness index. our estimators dominate threshold and James-Stein estimators within an infinitesimal fraction of the minimax risks. More general block empirical Bayes estimators are developed. Both white noise with drift and nonparametric regression;Ire considered."
"10.1214/009053604000001048","2005","Analysis of variance---why it is more important than ever","0","Analysis of variance (ANOVA) is an extremely important method in exploratory and confirmatory data analysis. Unfortunately, in complex problems (e.g., split-plot designs), it is not always easy to set up an appropriate ANOVA. We propose a hierarchical analysis that automatically gives the correct ANOVA comparisons even in complex scenarios. The inferences for all means and variances are performed under a model with a separate batch of effects for each row of the ANOVA table.We connect to classical ANOVA by working with finite-sample variance components: fixed and random effects models are characterized by inferences about existing levels of a factor and new levels, respectively. We also introduce a new graphical display showing inferences about the standard deviations of each batch of effects.We illustrate with two examples from our applied data analysis, first illustrating the usefulness of our hierarchical computations and displays, and second showing how the ideas of ANOVA are helpful in understanding a previously fit hierarchical model."
"10.1214/009053604000000733","2004","Tusn\'ady's inequality revisited","0","Tusnady's inequality is the key ingredient in the KMT/Hungarian coupling of the empirical distribution function with a Brownian bridge. We present an elementary proof of a result that sharpens the Tusnady inequality, modulo constants. Our method uses the beta integral representation of Binomial tails, simple Taylor expansion and some novel bounds for the ratios of normal tail probabilities."
"10.1214/009053604000000689","2004","Saddlepoint approximation for moment generating functions of truncated random variables","0","We consider the problem of approximating the moment generating function (MGF) of a truncated random variable in terms of the MGF of the underlying (i.e., untruncated) random variable. The purpose of approximating the MGF is to enable the application of saddlepoint approximations to certain distributions determined by truncated random variables. Two important statistical applications are the following: the approximation of certain multivariate cumulative distribution functions; and the approximation of passage time distributions in ion channel models which incorporate time interval omission. We derive two types of representation for the MGF of a truncated random variable. One of these representations is obtained by exponential tilting. The second type of representation, which has two versions, is referred to as an exponential convolution representation. Each representation motivates a different approximation. It turns out that each of the three approximations is extremely accurate in those cases ""to which it is suited."" Moreover, there is a simple rule of thumb for deciding which approximation to use in a given case, and if this rule is followed, then our numerical and theoretical results indicate that the resulting approximation will be extremely accurate."
"10.1214/009053604000000742","2004","Saddlepoint approximation for {S}tudent's {$t$}-statistic with no moment conditions","0","A saddlepoint approximation of the Student's t-statistic was derived by Daniels and Young [Biometrika 78 (1991) 169-179] under the very stringent exponential moment condition that requires that the underlying density function go down at least as fast as a Normal density in the tails. This is a severe restriction on the approximation's applicability. In this paper we show that this strong exponential moment restriction can be completely dispensed with, that is, saddlepoint approximation of the Student's t-statistic remains valid without any moment condition. This confirms the folklore that the Student's t-statistic is robust against outliers. The saddlepoint approximation not only provides a very accurate approximation for the Student's t-statistic, but it also can be applied much more widely in statistical inference. As a result, saddlepoint approximations should always be used whenever possible. Some numerical work will be given to illustrate these points."
"10.1214/009053604000000724","2004","Rank-based optimal tests of the adequacy of an elliptic {VARMA} model","3","We are deriving optimal rank-based tests for the adequacy of a vector autoregressive-moving average (VARMA) model with elliptically contoured innovation density. These tests are based on the ranks of pseudo-Mahalanobis distances and on normed residuals computed from Tyler's [Ann. Statist. 15 (1987) 234-251] scatter matrix; they generalize the univariate signed rank procedures proposed by Hallin and Puri [J. Multivariate Anal. 39 (1991) 1-29]. Two types of optimality properties are considered, both in the local and asymptotic sense, a la Le Cam: (a) (fixed-score procedures) local asymptotic minimaxity at selected radial densities, and (b) (estimated-score procedures) local asymptotic minimaxity uniform over a class F of radial densities. Contrary to their classical counterparts, based on cross-covariance matrices, these tests remain valid under arbitrary elliptically symmetric innovation densities, including those with infinite variance and heavy-tails. We show that the AREs of our fixed-score procedures, with respect to traditional (Gaussian) methods, are the same as for the tests of randomness proposed in Hallin and Paindaveine [Bernoulli 8 (2002b) 787-815]. The multivariate serial extensions of the classical Chernoff-Savage and Hodges-Lehmann results obtained there thus also hold here; in particular, the van der Waerden versions of our tests are uniformly more powerful than those based on cross-covariances. As for our estimated-score procedures, they are fully adaptive, hence, uniformly optimal over the class of innovation densities satisfying the required technical assumptions."
"10.1214/009053604000000823","2004","Approximately unbiased tests of regions using multistep-multiscale bootstrap resampling","0","Approximately unbiased tests based on bootstrap probabilities are considered for the exponential family of distributions with unknown expectation parameter vector, where the null hypothesis is represented as an arbitrary-shaped region with smooth boundaries. This problem has been discussed previously in Efron and Tibshirani [Ann. Statist. 26 (1998) 1687-1718], and a corrected p-value with second-order asymptotic accuracy is calculated by the two-level bootstrap of Efron, Halloran and Holmes [Proc. Natl. Acad. Sci. U.S.A. 93 (1996) 13429-13434] based on the ABC bias correction of Efron [J. Amer Statist. Assoc. 82 (1987) 171-185]. Our argument is an extension of their asymptotic theory, where the geometry, such as the signed distance and the curvature of the boundary, plays an important role. We give another calculation of the corrected p-value without finding the ""nearest point"" on the boundary to the observation, which is required in the two-level bootstrap and is an implementational burden in complicated problems. The key idea is to alter the sample size of the replicated dataset from that of the observed dataset. The frequency of the replicates falling in the region is counted for several sample sizes, and then the p-value is calculated by looking at the change in the frequencies along the changing sample sizes. This is the multiscale bootstrap of Shimodaira [Systematic Biology 51 (2002) 492-508], which is third-order accurate for the multivariate normal model. Here we introduce a newly devised multistep-multiscale bootstrap, calculating a third-order accurate p-value for the exponential family of distributions. In fact, our p-value is asymptotically equivalent to those obtained by the double bootstrap of Hall [The Bootstrap and Edgeworth Expansion (1992) Springer, New York] and the modified signed likelihood ratio of Barndorff-Nielsen [Biometrika 73 (1986) 307-322] ignoring O(n(-3/2)) terms, yet the computation is less demanding and free from model specification. The algorithm is remarkably simple despite complexity of the theory behind it. The differences of the p-values are illustrated in simple examples, and the accuracies of the bootstrap methods are shown in a systematic way."
"10.1214/009053604000000805","2004","Bayesian-motivated tests of function fit and their asymptotic frequentist properties","0","We propose and analyze nonparametric tests of the null hypothesis that a function belongs to a specified parametric family. The tests are based on BIC approximations, pi(BIC), to the posterior probability of the null model, and may be carried out in either Bayesian or frequentist fashion. We obtain results on the asymptotic distribution Of pi(BIC) under both the null hypothesis and local alternatives. One version Of pi(BIC), call it pi*(BIC), uses a class of models that are orthogonal to each other and growing in number without bound as sample size, n, tends to infinity. We show that rootn(1 - pi*(BIC)) converges in distribution to a stable law under the null hypothesis. We also show that pi*(BIC) can detect local alternatives converging to the null at the rate rootlogn/n. A particularly interesting finding is that the power of the pi*(BIC)-based test is asymptotically equal to that of a test based on the maximum of alternative log-likelihoods. Simulation results and an example involving variable star data illustrate desirable features of the proposed tests."
"10.1214/009053604000000670","2004","Estimation of nonlinear models with {B}erkson measurement errors","1","This paper is concerned with general nonlinear regression models where the predictor variables are subject to Berkson-type measurement errors. The measurement errors are assumed to have a general parametric distribution, which is not necessarily normal. In addition, the distribution of the random error in the regression equation is nonparametric. A minimum distance estimator is proposed, which is based on the first two conditional moments of the response variable given the observed predictor variables. To overcome the possible computational difficulty of minimizing an objective function which involves multiple integrals, a simulation-based estimator is constructed. Consistency and asymptotic normality for both estimators are derived under fairly general regularity conditions."
"10.1214/009053604000000652","2004","Hybrid shrinkage estimators using penalty bases for the ordinal one-way layout","1","This paper constructs improved estimators of the means in the Gaussian saturated one-way layout with an ordinal factor. The least squares estimator for the mean vector in this saturated model is usually inadmissible. The hybrid shrinkage estimators of this paper exploit the possibility of slow variation in the dependence of the means on the ordered factor levels but do not assume it and respond well to faster variation if present. To motivate the development, candidate penalized least squares (PLS) estimators for the mean vector of a one-way layout are represented as shrinkage estimators relative to the penalty basis for the regression space. This canonical representation suggests further classes of candidate estimators for the unknown means: monotone shrinkage (MS) estimators or soft-thresholding (ST) estimators or, most generally, hybrid shrinkage (HS) estimators that combine the preceding two strategies. Adaptation selects the estimator within a candidate class that minimizes estimated risk. Under the Gaussian saturated one-way layout model, such adaptive estimators minimize risk asymptotically over the class of candidate estimators as the number of factor levels tends to infinity. Thereby, adaptive HS estimators asymptotically dominate adaptive MS and adaptive ST estimators as well as the least squares estimator. Local annihilators of polynomials, among them difference operators, generate penalty bases suitable for a range of numerical examples. In case studies, adaptive HS estimators recover high frequency details in the mean vector more reliably than PLS or MS estimators and low frequency details more reliably than ST estimators."
"10.1214/009053604000000661","2004","Determining the dimension of iterative {H}essian transformation","8","The central mean subspace (CMS) and iterative Hessian transformation (IHT) have been introduced recently for dimension reduction when the conditional mean is of interest. Suppose that X is a vector-valued predictor and Y is a scalar response. The basic problem is to find a lower-dimensional predictor n(T)X such that E(Y\X) = E(Y\n(T)X). The CMS defines the inferential object for this problem and IHT provides an estimating procedure. Compared with other methods, IHT requires fewer assumptions and has been shown to perform well when the additional assumptions required by those methods fail. In this paper we give an asymptotic analysis of IHT and provide stepwise asymptotic hypothesis tests to determine the dimension of the CMS, as estimated by IHT. Here, the original IHT method has been modified to be invariant under location and scale transformations. To provide empirical support for our asymptotic results, we will present a series of simulation studies. These agree well with the theory. The method is applied to analyze an ozone data set."
"10.1214/009053604000000850","2004","Local linear spatial regression","1","A local linear kernel estimator of the regression function x --> g(x) := E[Y-i\X-i = x], X is an element of R-d, of a stationary (d + 1)-dimensional spatial process {(Y-i, X-i), i is an element of Z(N)} observed over a rectangular domain of the form l(n) := {i = (i(1),..., i(N)) is an element of Z(N)\ 1 less than or equal to i(k) less than or equal to n(k), k = 1,..., N}, n = (n(1),..., n(N)) is an element of Z(N), is proposed and investigated. Under mild regularity assumptions, asymptotic normality of the estimators of g(x) and its derivatives is established. Appropriate choices of the bandwidths are proposed. The spatial process is assumed to satisfy some very general mixing conditions, generalizing classical time-series strong mixing concepts. The size of the rectangular domain In is allowed to tend to infinity at different rates depending on the direction in Z(N)."
"10.1214/009053604000000841","2004","From finite sample to asymptotics: a geometric bridge for selection criteria in spline regression","1","This paper studies, under the setting of spline regression, the connection between finite-sample properties of selection criteria and their asymptotic counterparts, focusing on bridging the gap between the two. We introduce a bias-variance decomposition of the prediction error, using which it is shown that in the asymptotics the bias term dominates the variability term, providing an explanation of the gap. A geometric exposition is provided for intuitive understanding. The theoretical and geometric results are illustrated through a numerical example."
"10.1214/009053604000000814","2004","Nonparametric estimation of an additive model with a link function","9","This paper describes an estimator of the additive components of a nonparametric additive model with a known link function. When the additive components are twice continuously differentiable, the estimator is asymptotically normally distributed with a rate of convergence in probability of n(-2/5). This is true regardless of the (finite) dimension of the explanatory variable. Thus, in contrast to the existing asymptotically normal estimator, the new estimator has no curse of dimensionality. Moreover, the estimator has an oracle property. The asymptotic distribution of each additive component is the same as it would be if the other components were known with certainty."
"10.1214/009053604000000698","2004","Central limit theorem for sequential {M}onte {C}arlo methods and its application to {B}ayesian inference","9","The term ""sequential Monte Carlo methods"" or, equivalently, ""particle filters,"" refers to a general class of iterative algorithms that performs Monte Carlo approximations of a given sequence of distributions of interest (pi(t)). We establish in this paper a central limit theorem for the Monte Carlo estimates produced by these computational methods. This result holds under minimal assumptions on the distributions pi(t), and applies in a general framework which encompasses most of the sequential Monte Carlo methods that have been considered in the literature, including the resample-move algorithm of Gilks and Berzuini [J. R. Stat. Soc. Ser B Stat. Methodol. 63 (2001) 127-146] and the residual resampling scheme. The corresponding asymptotic variances provide a convenient measurement of the precision of a given particle filter. We study, in particular, in some typical examples of Bayesian applications, whether and at which rate these asymptotic variances diverge in time, in order to assess the long term reliability of the considered algorithm."
"10.1214/009053604000000616","2004","A {B}ayesian {$\chi^2$} test for goodness-of-fit","1","This article describes an extension of classical chi(2) goodness-of-fit tests to Bayesian model assessment. The extension, which essentially involves evaluating Pearson's goodness-of-fit statistic at a parameter value drawn from its posterior distribution, has the important property that it is asymptotically distributed as a chi(2) random variable on K - 1 degrees of freedom, independently of the dimension of the underlying parameter vector. By examining the posterior distribution of this statistic, global goodness-of-fit diagnostics are obtained. Advantages of these diagnostics include ease of interpretation, computational convenience and favorable power properties. The proposed diagnostics can be used to assess the adequacy of a broad class of Bayesian models, essentially requiring only a finite-dimensional parameter vector and conditionally independent observations."
"10.1214/009053604000000625","2004","Normalized random measures driven by increasing additive processes","2","This paper introduces and studies a new class of nonparametric prior distributions. Random probability distribution functions are constructed via normalization of random measures driven by increasing additive processes. In particular, we present results for the distribution of means under both prior and posterior conditions and, via the use of strategic latent variables, undertake a full Bayesian analysis. Our class of priors includes the well-known and widely used mixture of a Dirichlet process."
"10.1214/009053604000000643","2004","Approximating a sequence of observations by a simple process","0","Given an arbitrary long but finite sequence of observations from a finite set, we construct a simple process that approximates the sequence, in the sense that with high probability the empirical frequency, as well as the empirical one-step transitions along a realization from the approximating process, are close to that of the given sequence.We generalize the result to the case where the one-step transitions are required to be in given polyhedra."
"10.1214/009053604000000580","2004","Asymptotic operating characteristics of an optimal change point detection in hidden {M}arkov models","1","Let xi(0),xi(1),...,xi(omega-1) be observations from the hidden Markov model with probability distribution P(theta)0, and let xi(omega), xi(omega+1),... be observations from the hidden Markov model with probability distribution P(theta)1. The parameters theta(0) and theta(1) are given, while the change point omega is unknown. The problem is to raise an alarm as soon as possible after the distribution changes from P(theta)0 to P(theta)1, but to avoid false alarms. Specifically, we seek a stopping rule N which allows us to observe the xi's sequentially, such that E(infinity)N is large, and subject to this constraint, Sup(k) E(k)(N - k|N greater than or equal to k) is as small as possible. Here E(k) denotes expectation under the change point k, and E(infinity) denotes expectation under the hypothesis of no change whatever.In this paper we investigate the performance of the Shiryayev-Roberts-Pollak (SRP) rule for change point detection in the dynamic system of hidden Markov models. By making use of Markov chain representation for the likelihood function, the structure of asymptotically minimax policy and of the Bayes rule, and sequential hypothesis testing theory for Markov random walks, we show that the SRP procedure is asymptotically minimax in the sense of Pollak [Ann. Statist. 13 (1985) 206-227]. Next, we present a second-order asymptotic approximation for the expected stopping time of such a stopping scheme when omega = 1. Motivated by the sequential analysis in hidden Markov models, a nonlinear renewal theory for Markov random walks is also given."
"10.1214/009053604000000021","2004","Asymptotic properties of the maximum likelihood estimator in autoregressive models with {M}arkov regime","4","An autoregressive process with Markov regime is an autoregressive process for which the regression function at each time point is given by a nonobservable Markov chain. In this paper we consider the asymptotic properties of the maximum likelihood estimator in a possibly nonstationary process of this kind for which the hidden state space is compact but not necessarily finite. Consistency and asymptotic normality are shown to follow from uniform exponential forgetting of the initial distribution for the hidden Markov chain conditional on the observations."
"10.1214/009053604000000797","2004","Nonparametric estimation of scalar diffusions based on low frequency data","2","We study the problem of estimating the coefficients of a diffusion (X-t, t greater than or equal to 0); the estimation is based on discrete data X-nDelta, n = 0, 1,..., N. The sampling frequency Delta(-1) is constant, and asymptotics are taken as the number N of observations tends to infinity. We prove that the problem of estimating both the diffusion coefficient (the volatility) and the drift in a nonparametric setting is ill-posed: the minimax rates of convergence for Sobolev constraints and squared-error loss coincide with that of a, respectively, first- and second-order linear inverse problem. To ensure ergodicity and limit technical difficulties we restrict ourselves to scalar diffusions living on a compact interval with reflecting boundary conditions.Our approach is based on the spectral analysis of the associated Markov semigroup. A rate-optimal estimation of the coefficients is obtained via the nonparametric estimation of an eigenvalue-eigenfunction pair of the transition operator of the discrete time Markov chain (X-nDelta, n = 0, 1,..., N) in a suitable Sobolev norm, together with an estimation of its invariant density."
"10.1214/009053604000000427","2004","Estimators of diffusions with randomly spaced discrete observations: a general theory","4","We provide a general method to analyze the asymptotic properties of a variety of estimators of continuous time diffusion processes when the data are not only discretely sampled in time but the time separating successive observations may possibly be random. We introduce a new operator, the generalized infinitesimal generator, to obtain Taylor expansions of the asymptotic moments of the estimators. As a special case, our results apply to the situation where the data are discretely sampled at a fixed nonrandom time interval. We include as specific examples estimators based on maximum-likelihood and discrete approximations such as the Euler scheme."
"10.1214/009053604000000599","2004","Geometric isomorphism and minimum aberration for factorial designs with quantitative factors","3","Factorial designs have broad applications in agricultural, engineering and scientific studies. In constructing and studying properties of factorial designs, traditional design theory treats all factors as nominal. However, this is not appropriate for experiments that involve quantitative factors. For designs with quantitative factors, level permutation of one or more factors in a design matrix could result in different geometric structures, and, thus, different design properties. In this paper indicator functions are introduced to represent factorial designs. A polynomial form of indicator functions is used to characterize the geometric structure of those designs. Geometric isomorphism is defined for classifying designs with quantitative factors. Based on indicator functions, a new aberration criteria is proposed and some minimum aberration designs are presented."
"10.1214/009053604000000382","2004","Optimal designs for a class of nonlinear regression models","2","For a broad class of nonlinear regression models we investigate the local E- and c-optimal design problem. It is demonstrated that in many cases the optimal designs with respect to these optimality criteria are supported at the Chebyshev points, which are the local extrerna of the equi-oscillating best approximation of the function f(0) equivalent to 0 by a normalized linear combination of the regression functions in the corresponding linearized model. The class of models includes rational, logistic and exponential models and for the rational regression models the E- and c-optimal design problem is solved explicitly in many cases."
"10.1214/009053604000000715","2004","Bump hunting with non-{G}aussian kernels","2","It is well known that the number of modes of a kernel density estimator is monotone nonincreasing in the bandwidth if the kernel is a Gaussian density. There is numerical evidence of nonmonotonicity in the case of some non-Gaussian kernels, but little additional information is available. The present paper provides theoretical and numerical descriptions of the extent to which the number of modes is a nonmonotone function of bandwidth in the case of general compactly supported densities. Our results address popular kernels used in practice, for example, the Epanechnikov, biweight and triweight kernels, and show that in such cases nonmonotonicity is present with strictly positive probability for all sample sizes n greater than or equal to 3. In the Epanechnikov and biweight cases the probability of nonmonotonicity equals 1 for all n greater than or equal to 2. Nevertheless, in spite of the prevalence of lack of monotonicity revealed by these results, it is shown that the notion of a critical bandwidth (the smallest bandwidth above which the number of modes is guaranteed to be monotone) is still well defined. Moreover, just as in the Gaussian case, the critical bandwidth is of the same size as the bandwidth that minimises mean squared error of the density estimator. These theoretical results, and new numerical evidence, show that the main effects of nonmonotonicity occur for relatively small bandwidths, and have negligible impact on many aspects of bump hunting."
"10.1214/009053604000000607","2004","Attributing a probability to the shape of a probability density","0","We discuss properties of two methods for ascribing probabilities to the shape of a probability distribution. One is based on the idea of counting the number of modes of a bootstrap version of a standard kernel density estimator. We argue that the simplest form of that method suffers from the same difficulties that inhibit level accuracy of Silverman's bandwidth-based test for modality: the conditional distribution of the bootstrap form of a density estimator is not a good approximation to the actual distribution of the estimator. This difficulty is less pronounced if the density estimator is oversmoothed, but the problem of selecting the extent of oversmoothing is inherently difficult. It is shown that the optimal bandwidth, in the sense of producing optimally high sensitivity, depends on the widths of putative bumps in the unknown density and is exactly as difficult to determine as those bumps are to detect. We also develop a second approach to ascribing a probability to shape, using Muller and Sawitzki's notion of excess mass. In contrast to the context just discussed, it is shown that the bootstrap distribution of empirical excess mass is a relatively good approximation to its true distribution. This leads to empirical approximations to the likelihoods of different levels of ""modal sharpness,"" or ""delineation,"" of modes of a density. The technique is illustrated numerically."
"10.1214/009053604000000012","2004","Equivalence theory for density estimation, {P}oisson processes and {G}aussian white noise with drift","0","This paper establishes the global asymptotic equivalence between a Poisson process with variable intensity and white noise with drift under sharp smoothness conditions on the unknown function. This equivalence is also extended to density estimation models by Poissonization. The asymptotic equivalences are established by constructing explicit equivalence mappings. The impact of such asymptotic equivalence results is that an investigation in one of these nonparametric models automatically yields asymptotically analogous results in the other models."
"10.1214/009053604000000788","2004","On the posterior distribution of the number of components in a finite mixture","1","The posterior distribution of the number of components k in a finite mixture satisfies a set of inequality constraints. The result holds irrespective of the parametric form of the mixture components and under assumptions on the prior distribution weaker than those routinely made in the literature on Bayesian analysis of finite mixtures. The inequality constraints can be used to perform an ""internal"" consistency check of MCMC estimates of the posterior distribution of k and to provide improved estimates which are required to satisfy the constraints. Bounds on the posterior probability of k components are derived using the constraints. Implications on prior distribution specification and on the adequacy of the posterior distribution of k as a tool for selecting an adequate number of components in the mixture are also explored."
"10.1214/009053604000000409","2004","New approaches to {B}ayesian consistency","10","We use martingales to study Bayesian consistency. We derive sufficient conditions for both Hettinger and Kullback-Leibler consistency, which do not rely on the use of a sieve. Alternative sufficient conditions for Hellinger consistency are also found and demonstrated on examples."
"10.1214/009053604000000779","2004","On optimal spatial subsample size for variance estimation","2","We consider the problem of determining the optimal block (or subsample) size for a spatial subsampling method for spatial processes observed on regular grids. We derive expansions for the mean square error of the subsampling variance estimator, which yields an expression for the theoretically optimal block size. The optimal block size is shown to depend in an intricate way on the geometry of the spatial sampling region as well as characteristics of the underlying random field. Final expressions for the optimal block size make use of some nontrivial estimates of lattice point counts in shifts of convex sets. Optimal block sizes are computed for sampling regions of a number of commonly encountered shapes. Numerical studies are performed to compare subsampling methods as well as procedures for estimating the theoretically best block size."
"10.1214/009053604000000418","2004","On the testability of the car assumption","1","In recent years a popular nonparametric model for coarsened data is an assumption on the coarsening mechanism called coarsening at random (CAR). It has been conjectured in several papers that this assumption cannot be tested by the data, that is, the assumption does not restrict the possible distributions of the data. In this paper we will show that this conjecture is not always true; an example will be current status data. We will also give conditions when the conjecture is true, and in doing so, we will introduce a generalized version of the CAR assumption. As an illustration, we retrieve the well-known result that the CAR assumption cannot be tested in the case of right-censored data."
"10.1214/009053604000000751","2004","Wavelet-based estimation with multiple sampling rates","0","We suggest an adaptive sampling rule for obtaining information from noisy signals using wavelet methods. The technique involves increasing the sampling rate when relatively high-frequency terms are incorporated into the wavelet estimator, and decreasing it when, again using thresholded terms as an empirical guide, signal complexity is judged to have decreased. Through sampling in this way the algorithm is able to accurately recover relatively complex signals without increasing the long-run average expense of sampling. It achieves this level of performance by exploiting the opportunities for near-real time sampling that are available if one uses a relatively high primary resolution level when constructing the basic wavelet estimator. In the practical problems that motivate the work, where signal to noise ratio is particularly high and the long-run average sampling rate may be several hundred thousand operations per second, high primary resolution levels are quite feasible."
"10.1214/009053604000000760","2004","The {H}ough transform estimator","0","This article pursues a statistical study of the Hough transform, the celebrated computer vision algorithm used to detect the presence of lines in a noisy image. We first study asymptotic properties of the Hough transform estimator, whose objective is to find the line that ""best"" fits a set of planar points. In particular, we establish strong consistency and rates of convergence, and characterize the limiting distribution of the Hough transform estimator. While the convergence rates are seen to be slower than those found in some standard regression methods, the Hough transform estimator is shown to be more robust as measured by its breakdown point. We next study the Hough transform in the context of the problem of detecting multiple lines. This is addressed via the framework of excess mass functionals and modality testing. Throughout, several numerical examples help illustrate various properties of the estimator. Relations between the Hough transform and more mainstream statistical paradigms and methods are discussed as well."
"10.1214/009053604000000210","2004","Sieve empirical likelihood ratio tests for nonparametric functions","2","Generalized likelihood ratio statistics have been proposed in Fan, Zhang and Zhang [Ann. Statist. 29 (2001) 153-193] as a generally applicable method for testing norparametic hypotheses about nonparametric functions. The likelihood ratio statistics are constructed based on the assumption that the distributions of stochastic errors are in a certain parametric family. We extend their work to the case where the error distribution is completely unspecified via newly proposed sieve empirical likelihood ratio (SELR) tests. The approach is also applied to test conditional estimating equations on the distributions of stochastic errors. It is shown that the proposed SELR statistics follow asymptotically resealed chi(2)-distributions, with the scale constants and the degrees of freedom being independent of the nuisance parameters. This demonstrates that the Wilks phenomenon observed in Fan, Zhang and Zhang [Ann. Statist. 29 (2001) 153-193] continues to hold under more relaxed models and a larger class of techniques. The asymptotic power of the proposed test is also derived, which achieves the optimal rate for nonparametric hypothesis testing. The proposed approach has two advantages over the generalized likelihood ratio method: it requires one only to specify some conditional estimating equations rather than the entire distribution of the stochastic error, and the procedure adapts automatically to the unknown error distribution including heteroscedasticity. A simulation study is conducted to evaluate our proposed procedure empirically."
"10.1214/009053604000000634","2004","Robust nonparametric inference for the median","0","We consider the problem of constructing robust nonparametric confidence intervals and tests of hypothesis for the median when the data distribution is unknown and the data may contain a small fraction of contamination. We propose a modification of the sign test (and its associated confidence interval) which attains the nominal significance level (probability coverage) for any distribution in the contamination neighborhood of a continuous distribution. We also define some measures of robustness and efficiency under contamination for confidence intervals and tests. These measures are computed for the proposed procedures."
"10.1214/009053604000000049","2004","An adaptation theory for nonparametric confidence intervals","0","A nonparametric adaptation theory is developed for the construction of confidence intervals for linear functionals. A between class modulus of continuity captures the expected length of adaptive confidence intervals. Sharp lower bounds are given for the expected length and an ordered modulus of continuity is used to construct adaptive confidence procedures which are within a constant factor of the lower bounds. In addition, minimax theory over nonconvex parameter spaces is developed."
"10.1214/009053604000000391","2004","Periodic boxcar deconvolution and {D}iophantine approximation","4","We consider the nonparametric estimation of a periodic function that is observed in additive Gaussian white noise after convolution with a ""boxcar,"" the indicator function of an interval. This is an idealized model for the problem of recovery of noisy signals and images observed with ""motion blur."" If the length of the boxcar is rational, then certain frequencies are irretreviably lost in the periodic model. We consider the rate of convergence of estimators when the length of the boxcar is irrational, using classical results on approximation of irrationals by continued fractions. A basic question of interest is whether the minimax rate of convergence is slower than for nonperiodic problems with 1/f-like convolution filters. The answer turns out to depend on the type and smoothness of functions being estimated in a manner not seen with ""homogeneous"" filters."
"10.1214/009053604000000436","2004","Maximum {F}isher information in mixed state quantum systems","0","We deal with the maximization of classical Fisher information in a quantum system depending on an unknown parameter. This problem has been raised by physicists, who defined [Helstrom (1967) Phys. Lett. A 25 101-102] a quantum counterpart of classical Fisher information, which has been found to constitute an upper bound for classical information itself [Braunstein and Caves (1994) Phys. Rev. Lett. 72 3439-3443]. It has then become of relevant interest among statisticians, who investigated the relations between classical and quantum information and derived a condition for equality in the particular case of two-dimensional pure state systems [Barndorff-Nielsen and Gill (2000) J. Phys. A 33 4481-4490].In this paper we show that this condition holds even in the more general setting of two-dimensional mixed state systems. We also derive the expression of the maximum Fisher information achievable and its relation with that attainable in pure states."
"10.1214/009053604000000445","2004","Simultaneous prediction of independent {P}oisson observables","2","Simultaneous predictive distributions for independent Poisson observables are investigated. A class of improper prior distributions for Poisson means is introduced. The Bayesian predictive distributions based on priors from the introduced class are shown to be admissible under the Kullback-Leibler loss. A Bayesian predictive distribution based on a prior in this class dominates the Bayesian predictive distribution based on the Jeffreys prior."
"10.1214/009053604000000454","2004","Statistical properties of the method of regularization with periodic {G}aussian reproducing kernel","0","The method of regularization with the Gaussian reproducing kernel is popular in the machine learning literature and successful in many practical applications. In this paper we consider the periodic version of the Gaussian kernel regularization. We show in the white noise model setting, that in function spaces of very smooth functions, such as the infinite-order Sobolev space and the space of analytic functions, the method under consideration is asymptotically minimax; in finite-order Sobolev spaces, the method is rate optimal, and the efficiency in terms of constant when compared with the minimax estimator is reasonably high. The smoothing parameters in the periodic Gaussian regularization can be chosen adaptively without loss of asymptotic efficiency. The results derived in this paper give a partial explanation of the success of the Gaussian reproducing kernel in practice. Simulations are carried out to study the finite sample properties of the periodic Gaussian regularization."
"10.1214/009053604000000058","2004","Generalization bounds for averaged classifiers","0","We study a simple learning algorithm for binary classification. Instead of predicting with the best hypothesis in the hypothesis class, that is, the hypothesis that minimizes the training error, our algorithm predicts with a weighted average of all hypotheses, weighted exponentially with respect to their training error. We show that the prediction of this algorithm is much more stable than the prediction of an algorithm that predicts with the best hypothesis. By allowing the algorithm to abstain from predicting on some examples, we show that the predictions it makes when it does not abstain are very reliable. Finally, we show that the probability that the algorithm abstains is comparable to the generalization error of the best hypothesis in the class."
"10.1214/009053604000000463","2004","Complexity regularization via localized random penalties","7","In this article, model selection via penalized empirical loss minimization in nonparametric classification problems is studied. Data-dependent penalties are constructed, which are based on estimates of the complexity of a small subclass of each model class, containing only those functions with small empirical loss. The penalties are novel since those considered in the literature are typically based on the entire model class. Oracle inequalities using these penalties are established, and the advantage of the new penalties over those based on the complexity of the whole model class is demonstrated."
"10.1214/009053604000000472","2004","Construction of {$E(s^2)$}-optimal supersaturated designs","2","Booth and Cox proposed the E(s(2)) criterion for constructing two-level supersaturated designs. Nguyen [Technometrics 38 (1996) 69-73] and Tang and Wu [Canad. J. Statist 25 (1997) 191-201] independently derived a lower bound for E(s(2)). This lower bound can be achieved only when m is a multiple of N - 1, where m is the number of factors and N is the run size. We present a method that uses difference families to construct designs that satisfy this lower bound. We also derive better lower bounds for the case where the Nguyen-Tang-Wu bound is not achievable. Our bounds cover more cases than a bound recently obtained by Butler, Mead, Eskridge and Gilmour [J. R. Stat. Soc. Ser. B Stat. Methodol. 63 (2001) 621-632]. New E(s(2))-optimal designs are obtained by using a computer to search for designs that achieve the improved bounds."
"10.1214/009053604000000481","2004","Optimality of neighbor-balanced designs for total effects","1","The purpose of this paper is to study optimality of circular neighbor-balanced block designs when neighbor effects are present in the model. In the literature many optimality results are established for direct effects and neighbor effects separately, but few for total effects, that is, the sum of direct effect of treatment and relevant neighbor effects. We show that circular neighbor-balanced designs are universally optimal for total effects among designs with no self neighbor. Then we give efficiency factors of these designs, and show some situations where a design with self neighbors is preferable to a neighbor-balanced design."
"10.1214/009053604000000030","2004","Needles and straw in haystacks: empirical {B}ayes estimates of possibly sparse sequences","18","An empirical Bayes approach to the estimation of possibly sparse sequences observed in Gaussian white noise is set out and investigated. The prior considered is a mixture of an atom of probability at zero and a heavy-tailed density gamma, with the mixing weight chosen by marginal maximum likelihood, in the hope of adapting between sparse and dense sequences. If estimation is then carried Out using the posterior median, this is a random thresholding procedure. Other thresholding rules employing the same threshold can also be used. Probability bounds on the threshold chosen by the marginal maximum likelihood approach lead to overall risk bounds over classes of signal sequences of length n, allowing for sparsity of various kinds and degrees. The signal classes considered are ""nearly black"" sequences where only a proportion eta is allowed to be nonzero, and sequences with normalized l(p) norm bounded by eta, for eta > 0 and 0 < p less than or equal to 2. Estimation error is measured by mean qth power loss, for 0 < q less than or equal to 2. For all the classes considered, and for all q in (0, 2), the method achieves the optimal estimation rate as n --> infinity and eta --> 0 at various rates, and in this sense adapts automatically to the sparseness or otherwise of the underlying signal. In addition the risk is uniformly bounded over all signals. If the posterior mean is used as the estimator, the results still hold for q > 1. Simulations show excellent performance. For appropriately chosen functions gamma, the method is computationally tractable and software is available. The extension to a modified thresholding method relevant to the estimation of very sparse sequences is also considered."
"10.1214/009053604000000490","2004","Convergence rates for posterior distributions and adaptive estimation","3","The goal of this paper is to provide theorems on convergence rates of posterior distributions that can be applied to obtain good convergence rates in the context of density estimation as well as regression. We show how to choose priors so that the posterior distributions converge at the optimal rate without prior knowledge of the degree of smoothness of the density function or the regression function to be estimated."
"10.1214/009053604000000508","2004","Estimating marginal survival function by adjusting for dependent censoring using many covariates","1","One goal in survival analysis of right-censored data is to estimate the marginal survival function in the presence of dependent censoring. When many auxiliary covariates are sufficient to explain the dependent censoring, estimation based on either a semiparametric model or a nonparametric model of the conditional survival function can be problematic due to the high dimensionality of the auxiliary information. In this paper, we use two working models to condense these high-dimensional covariates in dimension reduction; then an estimate of the marginal survival function can be derived nonparametrically in a low-dimensional space. We show that such an estimator has the following double robust property: when either working model is correct, the estimator is consistent and asymptotically Gaussian; when both working models are correct, the asymptotic variance attains the efficiency bound."
"10.1214/009053604000000517","2004","Statistical estimation in the proportional hazards model with risk set sampling","0","Thomas' partial likelihood estimator of regression parameters is widely used in the analysis of nested case-control data with Cox's model. This paper proposes a new estimator of the regression parameters, which is consistent and asymptotically normal. Its asymptotic variance is smaller than that of Thomas' estimator away from the null. Unlike some other existing estimators, the proposed estimator does not rely on any more data than strictly necessary for Thomas' estimator and is easily computable from a closed form estimating equation with a unique solution. The variance estimation is obtained as minus the inverse of the derivative of the estimating function and therefore the inference is easily available. A numerical example is provided in support of the theory."
"10.1214/009053604000000526","2004","A {B}ernstein-von {M}ises theorem in the nonparametric right-censoring model","2","In the recent Bayesian nonparametric literature, many examples have been reported in which Bayesian estimators and posterior distributions do not achieve the optimal convergence rate, indicating that the Bernstein-von Mises theorem does not hold. In this article, we give a positive result in this direction by showing that the Bernstein-von Mises theorem holds in survival models for a large class of prior processes neutral to the right. We also show that, for an arbitrarily given convergence rate n(-alpha) with 0 < alpha < 1/2, a prior process neutral to the right can be chosen so that its posterior distribution achieves the convergence rate n(-alpha)."
"10.1214/009053604000000535","2004","Robust inference for univariate proportional hazards frailty regression models","12","We consider a class of semiparametric regression models which are one-parameter extensions of the Cox [J. Roy. Statist. Soc. Ser B 34 (1972) 187-220] model for right-censored univariate failure times. These models assume that the hazard given the covariates and a random frailty unique to each individual has the proportional hazards form multiplied by the frailty. The frailty is assumed to have mean 1 within a known one-parameter family of distributions. Inference is based on a nonparametric likelihood. The behavior of the likelihood maximizer is studied under general conditions where the fitted model may be misspecified. The joint estimator of the regression and frailty parameters as well as the baseline hazard is shown to be uniformly consistent for the pseudo-value maximizing the asymptotic limit of the likelihood. Appropriately standardized, the estimator converges weakly to a Gaussian process. When the model is correctly specified, the procedure is semiparametric efficient, achieving the semiparametric information bound for all parameter components. It is also proved that the bootstrap gives valid inferences for all parameters, even under misspecification. We demonstrate analytically the importance of the robust inference in several examples. In a randomized clinical trial, a valid test of the treatment effect is possible when other prognostic factors and the frailty distribution are both misspecified. Under certain conditions on the covariates, the ratios of the regression parameters are, still identifiable. The practical utility of the procedure is illustrated on a non-Hodgkin's lymphoma dataset."
"10.1214/009053604000000544","2004","Uniform asymptotics for robust location estimates when the scale is unknown","0","Most asymptotic results for robust estimates rely on regularity conditions that are difficult to verily in practice. Moreover, these results apply to fixed distribution functions. In the robustness context the distribution of the data remains largely unspecified and hence results that hold uniformly over a set of possible distribution functions are of theoretical and practical interest. Also, it is desirable to be able to determine the size of the set of distribution functions where the uniform properties hold. In this paper we study the problem of obtaining verifiable regularity conditions that suffice to yield uniform consistency and uniform asymptotic normality for location robust estimates when the scale of the errors is unknown. We study M-location estimates calculated with an S-scale and we obtain uniform asymptotic results over contamination neighborhoods. Moreover, we show how to calculate the maximum size of the contamination neighborhoods where these uniform results hold. There is a trade-off between the size of these neighborhoods and the breakdown point of the scale estimate."
"10.1214/009053604000000553","2004","Game theory, maximum entropy, minimum discrepancy and robust {B}ayesian decision theory","3","We describe and develop a close relationship between two problems that have customarily been regarded as distinct: that of maximizing entropy, and that of minimizing worst-case expected loss. Using a formulation grounded in the equilibrium theory of zero-sum games between Decision Maker and Nature, these two problems are shown to be dual to each other, the solution to each providing that to the other. Although Topsoe described this connection for the Shannon entropy over 20 years ago, it does not appear to be widely known even in that important special case.We here generalize this theory to apply to arbitrary decision problems and loss functions. We indicate how an appropriate generalized definition of entropy can be associated with such a problem, and we show that, subject to certain regularity conditions, the above-mentioned duality continues to apply in this extended context. This simultaneously provides a possible rationale for maximizing entropy and a tool for finding robust Bayes acts. We also describe the essential identity between the problem of maximizing entropy and that of minimizing a related discrepancy or divergence between distributions. This leads to an extension, to arbitrary discrepancies, of a well-known minimax theorem for the case of Kullback-Leibler divergence (the ""redundancy-capacity theorem"" of information theory).For the important case of families of distributions having certain mean values specified, we develop simple sufficient conditions and methods for identifying the desired solutions. We use this theory to introduce a new concept of ""generalized exponential family"" linked to the specific decision problem under consideration, and we demonstrate that this shares many of the properties of standard exponential families.Finally, we show that the existence of an equilibrium in our game can be rephrased in terms of a ""Pythagorean property"" of the related divergence, thus generalizing previously announced results for Kullback-Leibler and Bregman divergences."
"10.1214/009053604000000562","2004","Asymptotic global robustness in {B}ayesian decision theory","0","In Bayesian decision theory, it is known that robustness with respect to the loss and the prior can be improved by adding new observations. In this article we study the rate of robustness improvement with respect to the number of observations n. Three usual measures of posterior global robustness are considered: the (range of the) Bayes actions set derived from a class of loss functions, the maximum regret of using a particular loss when the subjective loss belongs to a given class and the range of the posterior expected loss when the loss function ranges over a class. We show that the rate of convergence of the first measure of robustness is rootn, while it is n for the other measures under reasonable assumptions on the class of loss functions. We begin with the study of two particular cases to illustrate our results."
"10.1214/009053604000000571","2004","Breakdown points for maximum likelihood estimators of location-scale mixtures","2","ML-estimation based on mixtures of Normal distributions is a widely used tool for cluster analysis. However, a single outlier can make the parameter estimation of at least one of the mixture components break down. Among others, the estimation of mixtures of t-distributions by McLachlan and Peel [Finite Mixture Models (2000) Wiley, New York] and the addition of a further mixture component accounting for ""noise"" by Fraley and Raftery [The Computer J. 41 (1998) 578-588] were suggested as more robust alternatives. In this paper, the definition of an adequate robustness measure for cluster analysis is discussed and bounds for the breakdown points of the mentioned methods are given. It turns out that the two alternatives, while adding stability in the presence of outliers of moderate size, do not possess a substantially better breakdown behavior than estimation based on Normal mixtures. If the number of clusters s is treated as fixed, r additional points suffice for all three methods to let the parameters of r clusters explode. Only in the case of r = s is this not possible for t-mixtures. The ability to estimate the number of mixture components, for example, by use of the Bayesian information criterion of Schwarz [Ann. Statist. 6 (1978) 461-464], and to isolate gross outliers as clusters of one point, is crucial for all improved breakdown behavior of all three techniques. Furthermore, a mixture of Normals with an improper uniform distribution is proposed to achieve more robustness in the case of a fixed number of components."
"10.1214/009053604000000373","2004","Monomial ideals and the {S}carf complex for coherent systems in reliability theory","0","A certain type of integer grid, called here an echelon grid, is an object found both in coherent systems whose components have a finite or countable number of levels and in algebraic geometry. If alpha = (alpha(1),...,alpha(d)) is an integer vector representing the state of a system, then the corresponding algebraic object is a monomial x(1)(alpha1) x(d)(alphad) in the indeterminates x(1),...x(d). The idea is to relate a coherent system to nionornial ideals, so that the so-called Scarf complex of the monomial ideal yields ail inclusion-exclusion identity for the probability of failure, which uses many fewer terms than the classical identity. Moreover in the ""general position"" case we obtain via the Scarf complex the tube bounds given by Naiman and Wynn [J. Inequal. Pure Appl. Math. (2001) 2 1-16]. Examples are given for the binary case but the full utility is for general multistate coherent systems and a comprehensive example is given."
"10.1214/009053604000000355","2004","The empirical process on {G}aussian spherical harmonics","1","We establish weak convergence of the empirical process on the spherical harmonics of a Gaussian random field in the presence of an unknown angular power spectrum. This result suggests various Gaussianity tests with an asymptotic justification. The issue of testing for Gaussianity on isotropic spherical random fields has recently received strong empirical attention in the cosmological literature, in connection with the statistical analysis of cosmic microwave background radiation."
"10.1214/009053604000000346","2004","Estimation of fractal dimension for a class of non-{G}aussian stationary processes and fields","0","We present the asymptotic distribution theory for a class of increment-based estimators of the fractal dimension of a random field of the form g {X (t)}, where g: R --> R is an unknown smooth function and X (t) is a real-valued stationary Gaussian field on R-d d = 1 or 2, whose covariance function obeys a power law at the origin. The relevant theoretical framework here is ""fixed domain"" (or ""infill"") asymptotics. Surprisingly, the limit theory in this non-Gaussian case is somewhat richer than in the Gaussian case (the latter is recovered when g is affine), in part because estimators of the type considered may have an asymptotic variance which is random in the limit. Broadly, when g is smooth and nonaffine, three types of limit distributions can arise, types (i), (ii) and (iii), say. Each type can be represented as a random integral. More specifically, type (i) can be represented as the integral of a certain random function with respect to Lebesgue measure; type (ii) can be represented as the integral of a second random function with respect to an independent Gaussian random measure; and type (iii) can be represented as a Wiener-It (o) over cap integral of order 2. Which type occurs depends on a combination of the following factors: the roughness of X(t), whether d = 1 or d = 2 and the order of the increment which is used. Another notable feature of our results is that, even though the estimators we consider are based on a variogram, no moment conditions are required on the observed field g{X(t)} for the limit theory to hold. The results of a numerical study are also presented."
"10.1214/009053604000000337","2004","Bounds on coverage probabilities of the empirical likelihood ratio confidence regions","3","This paper studies the least upper bounds on coverage probabilities of the empirical likelihood ratio confidence regions based on estimating equations. The implications of the bounds on empirical likelihood inference are also discussed."
"10.1214/009053604000000328","2004","Empirical-likelihood-based confidence interval for the mean with a heavy-tailed distribution","1","Empirical-likelihood-based confidence intervals for a mean were introduced by Owen [Biometrika 75 (1988) 237-249], where at least a finite second moment is required. This excludes some important distributions, for example, those in the domain of attraction of a stable law with index between 1 and 2. In this article we use a method similar to Qin and Wong [Scand. J. Statist. 23 (1996) 209-219] to derive an empirical-likeillood-based confidence interval for the mean when the underlying distribution has heavy tails. Our method can easily be extended to obtain a confidence interval for any order of moment of a heavy-tailed distribution."
"10.1214/009053604000000319","2004","Semiparametric density estimation by local {$L_2$}-fitting","2","This article examines density estimation by combining a parametric approach with a nonparametric factor. The plug-in parametric estimator is seen as a crude estimator of the true density and is adjusted by a nonparametric factor. The nonparametric factor is derived by a criterion called local L-2-fitting. A class of estimators that have multiplicative adjustment is provided, including estimators proposed by several authors as special cases, and the asymptotic theories are developed. Theoretical comparison reveals that the estimators in this class are better than, or at least competitive with, the traditional kernel estimator in a broad class of densities. The asymptotically best estimator in this class can be obtained from the elegant feature of the bias function."
"10.1214/009053604000000300","2004","Density estimation for biased data","1","The concept of biased data is well known and its practical applications range front social sciences and biology to economics and quality control. These observations arise when a sampling procedure chooses an observation with probability that depends on the value of the observation. This is an interesting sampling procedure because it favors some observations and neglects others. It is known that biasing does not change rates of nonparametric density estimation, but no results are available about sharp constants. This article presents asymptotic results on sharp minimax density estimation. In particular, a coefficient of difficulty is introduced that shows the relationship between sample sizes of direct and biased samples that imply the same accuracy of estimation. The notion of the restricted local minimax, where a low-frequency part of the estimated density is known, is introduced; it sheds new light on the phenomenon of nonparametric superefficiency. Results of a numerical study are presented."
"10.1214/009053604000000364","2004","Densities, spectral densities and modality","2","This paper considers the problem of specifying a simple approximating density function for a given data set (x(1),.... x(n)). Simplicity is measured by the number of modes but several different definitions of approximation are introduced. The taut string method is used to control the numbers of modes and to produce candidate approximating densities. Refinements are introduced that improve the local adaptivity of the procedures and the method is extended to spectral densities."
"10.1214/009053604000000292","2004","Testing predictor contributions in sufficient dimension reduction","12","We develop tests of the hypothesis of no effect for selected predictors in regression, without assuming a model for the conditional distribution of the response given the predictors. Predictor effects need not be limited to the mean function and smoothing is not required. The general approach is based on sufficient dimension reduction, the idea being to replace the predictor vector with a lower-dimensional version without loss of information on the regression. Methodology using sliced inverse regression is developed in detail."
"10.1214/009053604000000283","2004","A stochastic process approach to false discovery control","43","This paper extends the theory of false discovery rates (FDR) pioneered by Benjamini and Hochberg [J. Roy. Statist. Soc. Set. B 57 (1995) 289-300]. We develop a framework in which the False Discovery Proportion (FDP)-the number of false rejections divided by the number of rejections-is treated as a stochastic process. After obtaining the limiting distribution of the process, we demonstrate the validity of a class of procedures I-or controlling the False Discovery Rate (the expected FDP). We construct a confidence envelope for the whole FDP process. From these envelopes we derive confidence thresholds, for controlling the quantiles of the distribution of the FDP as well as controlling the number of false discoveries. We also investigate methods for estimating the p-value distribution."
"10.1214/009053604000000274","2004","Martingale transforms goodness-of-fit tests in regression models","2","This paper discusses two goodness-of-fit testing problems. The first problem pertains to fitting an error distribution to an assumed nonlinear parametric regression model, while the second pertains to fitting a parametric regression model when the error distribution is unknown. For the first problem the paper contains tests based on a certain martingale type transform of residual empirical processes. The advantage of this transform is that the corresponding tests are asymptotically distribution free. For the second problem the proposed asymptotically distribution free tests are based on innovation martingale transforms. A Monte Carlo study shows that the simulated level of the proposed tests is close to the asymptotic level for moderate sample sizes."
"10.1214/009053604000000265","2004","Higher criticism for detecting sparse heterogeneous mixtures","23","Higher criticism, or second-level significance testing, is a multiple-comparisons concept mentioned in passing by Tukey. It concerns a situation where there are many independent tests of significance and one is interested in rejecting the joint null hypothesis. Tukey suggested comparing the fraction of observed significances at a given alpha-level to the expected fraction under the joint null. In fact, he suggested standardizing the difference of the two quantities and forming a z-score; the resulting z-score tests the significance of the body of significance tests.We consider a generalization, where we maximize this z-score over a range of significance levels 0 < &alpha; &LE; &alpha;(0). We are able to show that the resulting higher critic-ism statistic is effective at resolving a very subtle testing problem: testing whether n normal means are all zero versus the alternative that a small fraction is nonzero.The subtlety of this ""sparse normal means"" testing problem can be seen from work of Ingster and Jin, who studied such problems in great detail. In their Studies, they identified an interesting range of cases where the small fraction of nonzero means is so small that the alternative hypothesis exhibits little noticeable effect on the distribution of the p-values either for the bulk of the tests or for the few most highly significant tests. In this range, when the amplitude of nonzero means is calibrated with the fraction of nonzero means, the likelihood ratio test for a precisely specified alternative would still succeed in separating the two hypotheses.We show that the higher criticism is successful throughout the same region of amplitude sparsity where the likelihood ratio test would succeed. Since it does not require a specification of the alternative, this shows that higher criticism is in a sense optimally adaptive to unknown sparsity and size of the nonnull effects. While our theoretical work is largely asymptotic, we provide Simulations in finite samples and suggest some possible applications. We also show that higher critcism works well over a range of non-Gaussian cases."
"10.1214/009053604000000256","2004","Nonconcave penalized likelihood with a diverging number of parameters","48","A class of variable selection procedures for parametric models via nonconcave penalized likelihood was proposed by Fan and Li to simultaneously estimate parameters and select important variables. They demonstrated that this class of procedures has an oracle property when the number of parameters is finite. However, in most model selection problems the number of parameters should be large and grow with the sample size. In this paper some asymptotic properties of the nonconcave penalized likelihood are established for situations in which the number of parameters tends to infinity as the sample size increases. Under regularity conditions we have established an oracle property and the asymptotic normality of the penalized likelihood estimators. Furthermore, the consistency of the sandwich formula of the covariance matrix is demonstrated. Nonconcave penalized likelihood ratio statistics are discussed, and their asymptotic distributions under the null hypothesis are obtained by imposing some mild conditions on the penalty functions. The asymptotic results are augmented by a simulation Study, and the newly developed methodology is illustrated by an analysis of a court case on the sexual discrimination of salary."
"10.1214/009053604000000247","2004","Consistent covariate selection and post model selection inference in semiparametric regression","7","This paper presents a model selection technique of estimation in semiparametric regression models of the type Y-i = beta'X-i + f (T-i) + W-i, i = 1,..., n. The parametric and nonparametric components are estimated simultaneously by this procedure. Estimation is based on a collection of finite-dimensional models, using a penalized least squares criterion for selection. We show that by tailoring the penalty terms developed for nonparametric regression to semiparametric models, we can consistently estimate the subset of nonzero coefficients of the linear part. Moreover, the selected estimator of the linear component is asymptotically normal."
"10.1214/009053604000000238","2004","Optimal predictive model selection","7","Often the goal of model selection is to choose a model for future prediction, and it is natural to measure the accuracy of a future prediction by squared error loss. Under the Bayesian approach, it is commonly perceived that the optimal predictive model is the model with highest posterior probability, but this is not necessarily the case. In this paper we show that, for selection among normal linear models, the optimal predictive model is often the median probability model, which is defined as the model consisting of those variables which have overall posterior probability greater than or equal to 1/2 of being in a model. The median probability model often differs from the highest probability model."
"10.1214/009053604000000229","2004","Training samples in objective {B}ayesian model selection","7","Often the goal of model selection is to choose a model for future prediction, and it is natural to measure the accuracy of a future prediction by squared error loss. Under the Bayesian approach, it is commonly perceived that the optimal predictive model is the model with highest posterior probability, but this is not necessarily the case. In this paper we show that, for selection among normal linear models, the optimal predictive model is often the median probability model, which is defined as the model consisting of those variables which have overall posterior probability greater than or equal to 1/2 of being in a model. The median probability model often differs from the highest probability model."
"10.1214/009053604000000201","2004","Mean squared error of empirical predictor","0","The term ""empirical predictor"" refers to a two-stage predictor of a linear combination of fixed and random effects. In the first stage, a predictor is obtained but it involves unknown parameters; thus, in the second stage, the unknown parameters are replaced by their estimators. In this paper, we consider mean squared errors (MSE) of empirical predictors under a general setup, where ML or REML estimators are used for the second stage. We obtain second-order approximation to the MSE as well as an estimator of the MSE correct to the same order. The general results are applied to mixed linear models to obtain a second-order approximation to the MSE of the empirical best linear unbiased predictor (EBLUP) of a linear mixed effect and an estimator of the MSE of EBLUP whose bias is correct to second order. The general mixed linear model includes the mixed ANOVA model and the longitudinal model as special cases."
"10.1214/009053604000000184","2004","Sufficient burn-in for {G}ibbs samplers for a hierarchical random effects model","0","We consider Gibbs and block Gibbs samplers for a Bayesian hierarchical version of the one-way random effects model. Drift and minorization conditions are established for the underlying Markov chains. The drift and minorization are used in conjunction with results from J. S. Rosenthal [J. Amer. Statist. Assoc. 90 (1995) 558-566] and G. O. Roberts and R. L. Tweedie [Stochastic Process. Appl. 80 (1999) 211-229] to construct analytical upper bounds on the distance to stationarity. These lead to upper bounds on the amount of burn-in that is required to get the chain within a prespecified (total variation) distance of the stationary distribution. The results are illustrated with a numerical example."
"10.1214/009053604000000175","2004","Finite sample properties of multiple imputation estimators","1","Finite sample properties of multiple imputation estimators under the linear regression model are studied. The exact bias of the multiple imputation variance estimator is presented. A method of reducing the bias is presented and simulation is used to make comparisons. We also show that the suggested method can be used for a general class of linear estimators."
"10.1214/009053604000000166","2004","Missing at random, likelihood ignorability and model completeness","0","This paper provides further insight into the key concept of missing at random (MAR) in incomplete data analysis. Following the usual selection modelling approach we envisage two models with separable parameters: a model for the response of interest and a model for the missing data mechanism (MDM). If the response model is given by a complete density family, then frequentist inference from the likelihood function ignoring the MDM is valid if and only if the MDM is MAR. This necessary and sufficient condition also holds more generally for models for coarse data, such as censoring. Examples are given to show the necessity of the completeness of the underlying model for this equivalence to hold."
"10.1214/009053604000000157","2004","Information bounds for {C}ox regression models with missing data","0","We derive information bounds for the regression parameters in Cox models when data are missing at random. These calculations are of interest for understanding the behavior of efficient estimation in case-cohort designs, a type of two-phase design often used in cohort studies. The derivations make use of key lemmas appearing in Robins, Rotnitzky and Zhao [J. Amer Statist. Assoc. 89 (1994) 846-866] and Robins, Hsieh and Newey [J. Roy. Statist. Soc. Ser. B 57 (1995) 409-424], but in a form suited for our purposes here. We begin by summarizing the results of Robins, Rotnitzky and Zhao in a form that leads directly to the projection method which will be of use for our model of interest. We then proceed to derive new information bounds for the regression parameters of the Cox model with data Missing At Random (MAR). In the final section we exemplify our calculations with several models of interest in cohort studies, including an i.i.d. version of the classical case-cohort design of Prentice [Biometrika 73 (1986) 1-11] and Self and Prentice [Ann. Statist. 16 (1988) 64-81]."
"10.1214/009053604000000148","2004","Selecting optimal multistep predictors for autoregressive processes of unknown order","0","We consider the problem of choosing the optimal (in the sense of mean-squared prediction error) multistep predictor for an autoregressive (AR) process of finite but unknown order. If a working AR model (which is possibly misspecified) is adopted for multistep predictions, then two competing types of multistep predictors (i.e., plug-in and direct predictors) can be obtained from this model. We provide some interesting examples to show that when both plug-in and direct predictors are considered, the optimal multistep prediction results cannot be guaranteed by correctly identifying the underlying model's order. This finding challenges the traditional model (order) selection criteria, which usually aim to choose the order of the true model. A new prediction selection criterion, which attempts to seek the best combination of the prediction order and the prediction method, is proposed to rectify this difficulty. When the underlying model is stationary, the validity of the proposed criterion is justified theoretically. To obtain this result, asymptotic properties of accumulated squares of multistep prediction errors are investigated. In addition to overcoming the above difficulty, some other advantages of the proposed criterion are also mentioned."
"10.1214/009053604000000139","2004","Local {W}hittle estimation in nonstationary and unit root cases","0","Asymptotic properties of the local Whittle estimator in the nonstationary case (d > 1/2) are explored. For 1/2 < d less than or equal to 1, the estimator is shown to be consistent, and its limit distribution and the rate of convergence depend on the value of d. For d = 1, the limit distribution is mixed normal. For d > 1 and when the process has a polynomial trend of order alpha > 1/2, the estimator is shown to be inconsistent and to converge in probability to unity."
"10.1214/009053604000000120","2004","The efficiency of the estimators of the parameters in {GARCH} processes","0","We propose a class of estimators for the parameters of a GARCH(p, q) sequence. We show that our estimators are consistent and asymptotically normal under mild conditions. The quasi-maximum likelihood and the likelihood estimators are discussed in detail. We show that the maximum likelihood estimator is optimal. If the tail of the distribution of the innovations is polynomial, even a quasi-maximum likelihood estimator based on exponential density performs better than the standard normal density-based quasi-likelihood estimator of Lee and Hansen and Lumsdaine."
"10.1214/009053604000000111","2004","Estimating invariant laws of linear processes by {$U$}-statistics","0","Suppose we observe an invertible linear process with independent mean-zero innovations and with coefficients depending on a finite-dimensional parameter, and we want to estimate the expectation of some function under the stationary distribution of the process. The usual estimator would be the empirical estimator. It can be improved using the fact that the innovations are centered. We construct an even better estimator using the representation of the observations as infinite-order moving averages of the innovations. Then the expectation of the function under the stationary distribution can be written as the expectation under the distribution of an infinite series in terms of the innovations, and it can be estimated by a U-statistic of increasing order (also called an ""infinite-order U-statistic"") in terms of the estimated innovations. The estimator can be further improved using the fact that the innovations are centered. This improved estimator is optimal if the coefficients of the linear process are estimated optimally. The variance reduction of our estimator over the empirical estimator can be considerable."
"10.1214/009053604000000102","2004","Statistical inference for time-inhomogeneous volatility models","0","This paper offers a new approach for estimating and forecasting the volatility of financial time series. No assumption is made about the parametric form of the processes. On the contrary, we only suppose that the volatility can be approximated by a constant over some interval. In such a framework, the main problem consists of filtering this interval of time homogeneity; then the estimate of the volatility can be simply obtained by local averaging. We construct a locally adaptive volatility, estimate (LAVE) which can perform this task and investigate it both from the theoretical point of view and through Monte Carlo simulations. Finally, the LAVE procedure is applied to a data set of nine exchange rates and a comparison with a standard GARCH model is also provided. Both models appear to be capable of explaining many of the features of the data; nevertheless, the new approach seems to be superior to the GARCH method as far as the out-of-sample results are concerned."
"10.1214/009053604000000094","2004","Minimax estimation of linear functionals over nonconvex parameter spaces","0","The minimax theory for estimating linear functionals is extended to the case of a finite union of convex parameter spaces. Upper and lower bounds for the minimax risk can still be described in terms of a modulus of continuity. However in contrast to the theory for convex parameter spaces rate optimal procedures are often required to be nonlinear. A construction of such nonlinear procedures is given. The results developed in this paper have important applications to the theory of adaptation."
"10.1214/009053604000000085","2004","Confidence balls in {G}aussian regression","10","Starting from the observation of an R(n)-Gaussian vector of mean f and covariance matrix sigma(2)I(n) (I(n) is the identity matrix), we propose a method for building a Euclidean confidence ball around f, with prescribed probability of coverage. For each n, we describe its nonasymptotic property and show its optimality with respect to some criteria."
"10.1214/009053604000000076","2004","Multiscale likelihood analysis and complexity penalized estimation","0","We describe here a framework for a certain class of multiscale likelihood factorizations wherein, in analogy to a wavelet decomposition of an L-2 function, a given likelihood function has an alternative representation as a product of conditional densities reflecting information in both the data and the parameter vector localized in position and scale. The framework is developed as a set of sufficient conditions for the existence of such factorizations, formulated in analogy to those underlying a standard multiresolution analysis for wavelets, and hence can be viewed as a multiresolution analysis for likelihoods. We then consider the use of these factorizations in the task of nonparametric, complexity penalized likelihood estimation. We study the risk properties of certain thresholding and partitioning estimators, and demonstrate their adaptivity and near-optimality, in a minimax sense over a broad range of function spaces, based on squared Hellinger distance as a loss function. In particular, our results provide an illustration of how properties of classical wavelet-based estimators can be obtained in a single, unified framework that includes models for continuous, count and categorical data types."
"10.1214/009053604000000067","2004","Least angle regression","0","The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a C-p estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates."
"10.1214/aos/1079120141","2004","M{M} algorithms for generalized {B}radley-{T}erry models","0","The Bradley-Terry model for paired comparisons is a simple and much-studied means to describe the probabilities of the possible outcomes when individuals are judged against one another in pairs. Among the many studies of the model in the past 75 years, numerous authors have generalized it in several directions, sometimes providing iterative algorithms for obtaining maximum likelihood estimates for the generalizations. Building on a theory of algorithms known by the initials MM, for minorization-maximization, this paper presents a powerful technique for producing iterative maximum likelihood estimation algorithms for a wide class of generalizations of the Bradley-Terry model. While algorithms for problems of this type have tended to be custom-built in the literature, the techniques in this paper enable their mass production. Simple conditions are stated that guarantee that each algorithm described will produce a sequence that converges to the unique maximum likelihood estimator. Several of the algorithms and convergence results herein are new."
"10.1214/aos/1079120140","2004","Multinomial-{P}oisson homogeneous models for contingency tables","0","A unified approach to maximum likelihood inference for a broad, new class of contingency table models is presented. The model class comprises multinomial-Poisson homogeneous (MPH) models, which can be characterized by an independent sampling plan and a system of homogeneous constraints, h(m) - 0, where m is the vector of expected table counts. Maximum likelihood (ML) fitting and large-sample inference for MPH models are described. The MPH models are partitioned into well-defined equivalence classes and explicit comparisons of the large-sample behaviors of ML estimators of equivalent models are given. The equivalence theory not only unifies a large collection of previously known results, it also leads to useful generalizations and many new results. The practical, computational implication is that ML fit results for any particular MPH model can be obtained directly from the ML fit results for any conveniently chosen equivalent model. Issues of hypothesis testability and parameter estimability are also addressed. To illustrate, an example based on statistics journal citation patterns is given for which the data can be used to test the hypothesis that a certain model holds, but they cannot be used to estimate any of that model's parameters."
"10.1214/aos/1079120139","2004","A generalized {EWMA} control chart and its comparison with the optimal {EWMA}, {CUSUM} and {GLR} schemes","0","It is known that both the optimal exponentially weighted moving average (EWMA) and cumulative sum (CUSUM) control charts are based on a given reference value delta, which, for the CUSUM chart, is the magnitude of a shift in the mean to be detected quickly. In this paper a generalized EWMA control chart (GEWMA) which does not depend on delta is proposed for detecting the mean shift. We compare theoretically the GEWMA control chart with the optimal EWMA, CUSUM and the generalized likelihood ratio (GLR) control charts. The results of the comparison in which the in-control average run length approaches infinity show that the GEWMA control chart is better than the optimal EWMA control chart in detecting a mean shift of any size and is also better than the CUSUM control chart in detecting the mean shift which is not in the interval (0.7842delta, 1.3798delta). Moreover, the GLR control chart has the best performance in detecting mean shift among the four control charts except when detecting a particular mean shift delta, when the in-control average run length approaches infinity."
"10.1214/aos/1079120138","2004","Optimality of the {CUSUM} procedure in continuous time","0","The optimality of CUSUM under a Lorden-type criterion setting is considered. We demonstrate the optimality of the CUSUM test for lto processes, in a sense similar to Lorden's, but with a criterion that replaces expected delays by the corresponding Kullback-Leibler divergence."
"10.1214/aos/1079120137","2004","Asymptotic properties of doubly adaptive biased coin designs for multitreatment clinical trials","0","A general doubly adaptive biased coin design is proposed for the allocation of subjects to K treatments in a clinical trial. This design follows the same spirit as Efron's biased coin design and applies to the cases where the desired allocation proportions are unknown, but estimated sequentially. Strong consistency, a law of the iterated logarithm and asymptotic normality of this design are obtained under some widely satisfied conditions. For two treatments, a new family of designs is proposed and shown to be less variable than both the randomized play-the-winner rule and the adaptive randomized design. Also the proposed design tends toward a randomization scheme (with a fixed target proportion) as the size of the experiment increases."
"10.1214/aos/1079120136","2004","Consistent estimation of distributions with type {II} bias with applications in competing risks problems","0","A random variable X is symmetric about 0 if X and -X have the same distribution. There is a large literature on the estimation of a distribution function (DF) under the symmetry restriction and tests for checking this symmetry assumption. Often the alternative describes some notion of skewness or one-sided bias. Various notions can be described by an ordering of the distributions of X and -X. One such important ordering is that P(0 < X less than or equal to x) - P(-x less than or equal to X < 0) is increasing in x > 0. The distribution of X is said to have a Type 11 positive bias in this case. If X has a density f, then this corresponds to the density ordering f (-x) less than or equal to f (x) for x > 0. It is known that the nonparametric maximum likelihood estimator (NPMLE) of the DF under this restriction is inconsistent. We provide a projection-type estimator that is similar to a consistent estimator of two DFs under uniform stochastic ordering, where the NPMLE also fails to be consistent. The weak convergence of the estimator has been derived which can be used for testing the null hypothesis of symmetry against this one-sided alternative. It also turns out that the same procedure can be used to estimate two cumulative incidence functions in a competing risks problem under the restriction that the cause specific hazard rates are ordered. We also provide some real life examples."
"10.1214/aos/1079120135","2004","On the forward and backward algorithms of projection pursuit","0","This article provides a historic review of the forward and backward projection pursuit algorithms, previously thought to be equivalent, and points out an important difference between the two. In doing so, a small error in the original exploratory projection pursuit article by Friedman [J Amer. Statist. Assoc. 82 (1987) 249-266] is corrected. The implication of the difference is briefly discussed in the context of an application in which projection pursuit density estimation is used as a building block for nonparametric discriminant analysis."
"10.1214/aos/1079120134","2004","Estimation of proportional covariances in the presence of certain linear restrictions","0","Proportionality of covariance matrices of n independent p-dimensional normal distributions with the same type of linear restrictions of the inverse covariances is considered. Conditions for existence and uniqueness of the maximum likelihood estimator are obtained through the development of general results for scale-invariant natural exponential families."
"10.1214/aos/1079120133","2004","Influence function and maximum bias of projection depth based estimators","0","Location estimators induced from depth functions increasingly have been pursued and studied in the literature. Among them are those induced from projection depth functions. These projection depth based estimators have favorable properties among their competitors. In particular, they possess the best possible finite sample breakdown point robustness. However, robustness of estimators cannot be revealed by the finite sample breakdown point alone. The influence function, gross error sensitivity, maximum bias and contamination sensitivity are also important aspects of robustness. In this article, we study these other robustness aspects of two types of projection depth based estimators: projection medians and projection depth weighted means. The latter includes the Stahel-Donoho estimator as a special case. Exact maximum bias, the influence function, and contamination and gross error sensitivity are derived and studied for both types of estimators. Sharp upper bounds for the maximum bias and the influence functions are established. Comparisons based on these robustness criteria reveal that the projection depth based estimators enjoy desirable local as well as global robustness and are very competitive among their competitors."
"10.1214/aos/1079120132","2004","On the {S}tahel-{D}onoho estimator and depth-weighted means of multivariate data","0","The depth of multivariate data can be used to construct weighted means as robust estimators of location. The use of projection depth leads to the Stahel-Donoho estimator as a special case. In contrast to maximal depth estimators, the depth-weighted means are shown to be asymptotically normal under appropriate conditions met by depth functions commonly used in the current literature. We also confirm through a finite-sample study that the Stahel-Donoho estimator achieves a desirable balance between robustness and efficiency at Gaussian models."
"10.1214/aos/1079120131","2004","Optimal aggregation of classifiers in statistical learning","0","Classification can be considered as nonparametric estimation of sets, where the risk is defined by means of a specific distance between sets associated with misclassification error. It is shown that the rates of convergence of classifiers depend on two parameters: the complexity of the class of candidate sets and the margin parameter. The dependence is explicitly given, indicating that optimal fast rates approaching O (n(-1)) can be attained, where n is the sample size, and that the proposed classifiers have the property of robustness to the margin. The main result of the paper concerns optimal aggregation of classifiers: we suggest a classifier that automatically adapts both to the complexity and to the margin, and attains the optimal fast rates, up to a logarithmic factor."
"10.1214/aos/1079120130","2004","Statistical behavior and consistency of classification methods based on convex risk minimization","0","We study how closely the optimal Bayes error rate can be approximately reached using a classification algorithm that computes a classifier by minimizing a convex upper bound of the classification error function. The measurement of closeness is characterized by the loss function used in the estimation. We show that such a classification scheme can be generally regarded as a (nonmaximum-likelihood) conditional in-class probability estimate, and we use this analysis to compare various convex loss functions that have appeared in the literature. Furthermore, the theoretical insight allows us to design good loss functions with desirable properties. Another aspect of our analysis is to demonstrate the consistency of certain classification methods using convex risk minimization. This study sheds light on the good performance of some recently proposed linear classification methods including boosting and support vector machines. It also shows their limitations and suggests possible improvements."
"10.1214/aos/1079120129","2004","On the {B}ayes-risk consistency of regularized boosting methods","0","The probability of error of classification methods based on convex combinations of simple base classifiers by ""boosting"" algorithms is investigated. The main result of the paper is that certain regularized boosting algorithms provide Bayes-risk consistent classifiers under the sole assumption that the Bayes classifier may be approximated by a convex combination of the base classifiers. Nonasymptotic distribution-free bounds are also developed which offer interesting new insight into how boosting works and help explain its success in practical classification problems."
"10.1214/aos/1079120128","2004","Process consistency for {A}da{B}oost","0","Recent experiments and theoretical studies show that AdaBoost can overfit in the limit of large time. If running the algorithm forever is suboptimal, a natural question is how low can the prediction error be during the process of AdaBoost? We show under general regularity conditions that during the process of AdaBoost a consistent prediction is generated, which has the prediction error approximating the optimal Bayes error as the sample size increases. This result suggests that, while running the algorithm forever can be suboptimal, it is reasonable to expect that some regularization method via truncation of the process may lead to a near-optimal performance for sufficiently large sample size."
"10.1214/aos/1079120126","2004","Population theory for boosting ensembles","0","Tree ensembles are looked at in distribution space, that is, the limit case of ""infinite"" sample size. It is shown that the simplest kind of trees is complete in D-dimensional L-2(P) space if the number of terminal nodes T is greater than D. For such trees we show that the AdaBoost algorithm gives an ensemble converging to the Bayes risk."
"10.1214/aos/1074290337","2003","Testing conditional moment restrictions","0","Let (x,z) be a pair of observable random vectors. We construct a new ""smoothed"" empirical likelihood-based test for the hypothesis E{g(z, theta) \x} = 0 w.p. 1, where g is a vector of known functions and theta an unknown finite-dimensional parameter. We show that the test statistic is asymptotically normal under the null hypothesis and derive its asymptotic distribution under a sequence of local alternatives. Furthermore, the test is shown to possess an optimality property in large samples. Simulation evidence suggests that it also behaves well in small samples."
"10.1214/aos/1074290336","2003","Efficient and adaptive nonparametric test for the two-sample problem","0","The notion of efficient test for a Euclidean parameter in a semiparametric model was introduced by Stein [Proc. Third Berkeley Symp. Math. Statist. Probab. 1 (1956) 187-195]. Such tests are locally most powerful for a wide class of infinite-dimensional nuisance parameters. The first formal application of this notion to a suitably parametrized two-sample problem was provided by Hajek [Ann. Math. Statist. 33 (1962) 1124-1147]. However, this and subsequent solutions appear to be not well-suited for practical applications. This article aims to show that an adaptive two-sample test introduced recently by Janic-Wroblewska and Ledwina [Scand. J Statist. 27 (2000) 281-297] is locally most powerful under a more realistic setting."
"10.1214/aos/1074290335","2003","The positive false discovery rate: a {B}ayesian interpretation and the {$q$}-value","21","Multiple hypothesis testing is concerned with controlling the rate of false positives when testing several hypotheses simultaneously. One multiple hypothesis testing error measure is the false discovery rate (FDR), which is loosely defined to be the expected proportion of false positives among all significant hypotheses. The FDR is especially appropriate for exploratory analyses in which one is interested in finding several significant results among many tests. In this work, we introduce a modified version of the FDR called the ""positive false discovery rate"" (pFDR). We discuss the advantages and disadvantages of the pFDR and investigate its statistical properties. When assuming the test statistics follow a mixture distribution, we show that the pFDR can be written as a Bayesian posterior probability and can be connected to classification theory. These properties remain asymptotically true under fairly general conditions, even under certain forms of dependence. Also, a new quantity called the ""q-value"" is introduced and investigated, which is a natural ""Bayesian posterior p-value,"" or rather the pFDR analogue of the p-value."
"10.1214/aos/1074290334","2003","Weak consistency of extreme value estimators in {$C[0,1]$}","0","We prove that when the distribution of a stochastic process in C [0, 1] is in the domain of attraction of a max-stable process, then natural estimators for the extreme-value index (which is now a continuous function) and for the mean measure of the limiting Poisson process are consistent in the appropriate topologies. The ultimate goal, estimating probabilities of small (failure) sets, will be considered later."
"10.1214/aos/1074290333","2003","Kernel-type estimators for the extreme value index","0","A large part of the theory of extreme value index estimation is developed for positive extreme value indices. The best-known estimator of a positive extreme value index is probably the Hill estimator. This estimator belongs to the category of moment estimators, but can also be interpreted as a quasi-maximum likelihood estimator. It has been generalized to a kernel-type estimator, but this kernel-type estimator can, similarly to the Hill estimator, only be used for the estimation of positive extreme value indices. In the present paper, we introduce kernel-type estimators which can be used for estimating the extreme value index over the whole (positive and negative) range. We present a number of results on their distributional behavior and compare their performance with the performance of other estimators, such as moment-type estimators for the whole range and the quasi-maximum likelihood estimator, based on the generalized Pareto distribution. We also discuss an automatic bandwidth selection method and introduce a kernel estimator for a second-order parameter, controlling the speed of convergence."
"10.1214/aos/1074290332","2003","Autoregressive-aided periodogram bootstrap for time series","0","A bootstrap methodology for the periodogram of a stationary process is proposed which is based on a combination of a time domain parametric and a frequency domain nonparametric bootstrap. The parametric fit is used to generate periodogram ordinates that imitate the essential features of the data and the weak dependence structure of the periodogram while a nonparametric (kernel-based) correction is applied in order to catch features not represented by the parametric fit. The asymptotic theory developed shows validity of the proposed bootstrap procedure for a large class of periodogram statistics. For important classes of stochastic processes, validity of the new procedure is also established for periodograin statistics not captured by existing frequency domain bootstrap methods based on independent periodogram replicates."
"10.1214/aos/1074290331","2003","Bayesian bootstrap for proportional hazards models","0","We propose two Bayesian bootstrap extensions, the binomial and Poisson forms, for proportional hazards models. The binomial form Bayesian bootstrap is the limit of the posterior distribution with a beta process prior as the amount of the prior information vanishes, and thus can be considered as a default nonparametric Bayesian analysis. It is also the same as Lo's Bayesian bootstrap for censored data when covariates are absent. The Poisson form Bayesian bootstrap is equivalent to the Bayesian analysis with Cox's protile likelihood. When the baseline distribution is discrete, thus when the data set has many ties, simulation study suggests that the binomial form Bayesian bootstrap performs better than standard frequentist procedures in the frequentist sense. An advantage of the proposed Bayesian bootstrap procedures over the standard Bayesian analysis is conceptual and computational simplicity. Finally, it is shown that both Bayesian bootstrap posteriors are asymptotically equivalent to the sampling distribution of the maximum likelihood estimator."
"10.1214/aos/1074290330","2003","Asymptotic results in jackknifing nonsmooth functions of the sample mean vector","0","The asymptotic behavior of jackknife estimators and jackknife variance estimators is investigated for nonsmooth functions of the sample mean vector. An application of jackknifing a suitable estimator of the intrinsic diversity profile is also presented."
"10.1214/aos/1074290329","2003","Bootstrap confidence bands for regression curves and their derivatives","0","Confidence bands for regression curves and their first p derivatives are obtained via local pth order polynomial estimation. The method allows for multiparameter local likelihood estimation as well as other unbiased estimating equations. As an alternative to the confidence bands obtained by asymptotic distribution theory, we also study smoothed bootstrap confidence bands. Simulations illustrate the finite sample properties of the methodology."
"10.1214/aos/1074290328","2003","Maximal meaningful events and applications to image analysis","0","We discuss the mathematical properties of a recently introduced method for computing geometric structures in a digital image without any a priori information. This method is based on a basic principle of perception which we call the Helmholtz principle. According to this principle, an observed geometric structure is perceptually ""meaningful"" if the expectation of its number of occurrences (in other words, its number of false alarms, NF) is very small in a random image. It is ""maximal meaningful"" if its NF is minimal among the meaningful structures of the same kind which it contains or is contained in. This definition meets the gestalt theory requirement that parts of a whole are not perceived. We explain by large-deviation estimates why this definition leads to an a priori knowledge-free method, compatible with phenomenology. We state a principle according to which maximal structures do not meet. We prove this principle in the large-deviations framework in the case of alignments in a digital image. We show why these results make maximal meaningful structures computable and display several applications."
"10.1214/aos/1074290327","2003","Estimating deformations of stationary processes","0","This paper studies classes of nonstationary processes, such as warped processes and frequency-modulated processes, that result from the deformation of stationary processes. Estimating deformations can often provide important information about an underlying physical phenomenon. A computational harmonic analysis viewpoint shows that the deformed autocovariance satisfies a transport equation at small scales, with a velocity proportional to a deformation gradient. We derive an estimator of the deformation from a single realization of the deformed process, with a proof of consistency under appropriate assumptions."
"10.1214/aos/1074290326","2003","Rotation space random fields with an application to f{MRI} data","0","Siegmund and Worsley considered the problem of testing for a signal with unknown location and scale in a Gaussian random field defined on RN. The test statistic was the maximum of a Gaussian random field in an (N + 1)-dimensional ""scale space,"" N dimensions for location and one dimension for the scale of a smoothing kernel. Siegmund and Worsley used two methods, one involving the expected Euler characteristic of the excursion set and the other involving the volume of tubes, to derive an approximate null distribution. The purpose of this paper is to extend the scale space result to the rotation space random field when N = 2, where the maximum is taken over all rotations of the filter as well as scales. We apply this result to the problem of searching for activation in brain images obtained by functional magnetic resonance imaging (fMRI)."
"10.1214/aos/1074290325","2003","Asymptotics and the theory of inference","5","Asymptotic analysis has always been very useful for deriving distributions in statistics in cases where the exact distribution is unavailable. More importantly, asymptotic analysis can also provide insight into the inference process itself, suggesting what information is available and how this information may be extracted. The development of likelihood inference over the past twenty-some years provides an illustration of the interplay between techniques of approximation and statistical theory."
"10.1214/aos/1065705122","2003","Optimal designs for estimating individual coefficients in {F}ourier regression models","0","In the common trigonometric regression model, we investigate the optimal design problem for the estimation of the individual coefficients, where the explanatory variable varies in the interval [-alpha, alpha], 0 < alpha less than or equal to pi. It is demonstrated that the structure of the optimal design depends sensitively on the size of the design space. For many important cases, optimal designs can be found explicitly, where the complexity of the solution depends on the value of the parameter alpha and the order of the term, for which the corresponding coefficient has to be estimated. The main tool of our approach is the reduction of the problem for the trigonometric regression model to a design problem for a polynomial regression. In particular, we determine the optimal designs for estimating the parameters corresponding to the cosine terms explicitly, if the design space is sufficiently small, and prove that under this condition all optimal designs for estimating the parameters corresponding to the sine terms are supported at the same points."
"10.1214/aos/1065705121","2003","Dimension reduction for the conditional mean in regressions with categorical predictors","7","Consider the regression of a response Y on a vector of quantitative predictors X and a categorical predictor W. In this article we describe a first method for reducing the dimension of X without loss of information on the conditional mean E(Y\X, W) and without requiring a prespecified parametric model. The method, which allows for, but does not require, parametric versions of the subpopulation mean functions E(Y\X, W = w), includes a procedure for inference about the dimension of X after reduction. This work integrates previous studies on dimension reduction for the conditional mean E(Y\X) in the absence of categorical predictors and dimension reduction for the full conditional distribution of Y\(X, W). The methodology we describe may be particularly useful for constructing low-dimensional summary plots to aid in model-building at the outset of an analysis. Our proposals provide an often parsimonious alternative to the standard technique of modeling with interaction terms to adapt a mean function for different subpopulations determined by the levels of W. Examples illustrating this and other aspects of the development are presented."
"10.1214/aos/1065705120","2003","Local asymptotics for polynomial spline regression","7","In this paper we develop a general theory of local asymptotics for least squares estimates over polynomial spline spaces in a regression problem. The polynomial spline spaces we consider include univariate splines, tensor product splines, and bivariate or multivariate splines on triangulations. We establish asymptotic normality of the estimate and study the magnitude of the bias due to spline approximation. The asymptotic normality holds uniformly over the points where the regression function is to be estimated and uniformly over a broad class of design densities, error distributions and regression functions. The bias is controlled by the minimum L-infinity norm of the error when the target regression function is approximated by a function in the polynomial spline space that is used to define the estimate. The control of bias relies on the stability in L-infinity norm of L-2 projections onto polynomial spline spaces. Asymptotic normality of least squares estimates over polynomial or trigonometric polynomial spaces is also treated by the general theory. In addition, a preliminary analysis of additive models is provided."
"10.1214/aos/1065705119","2003","Ridgelets: estimating with ridge functions","0","Feedforward neural networks, projection pursuit regression, and more generally, estimation via ridge functions have been proposed as an approach to bypass the curse of dimensionality and are now becoming widely applied to approximation or prediction in applied sciences. To address problems inherent to these methods-ranging from the construction of neural networks to their efficiency and capability-Canes [Appl. Comput. Harmon. Anal. 6 (1999) 197-218] developed a new system that allows the representation of arbitrary functions as superpositions of specific ridge functions, the ridgelets.In a nonparametric regression setting, this article suggests expanding noisy data into a ridgelet series and applying a scalar nonlinearity to the coefficients (damping); this is unlike existing approaches based on stepwise additions of elements. The procedure is simple, constructive, stable and spatially adaptive-and fast algorithms have been developed to implement it.The ridgelet estimator is nearly optimal for estimating functions with certain kinds of spatial inhomogeneities. In addition, ridgelets help to identify new classes of estimands-corresponding to a new notion of smoothness-that are well suited for ridge functions estimation. While the results are stated in a decision theoretic framework, numerical experiments are also presented to illustrate the practical performance of the methodology."
"10.1214/aos/1065705118","2003","Singular {W}ishart and multivariate beta distributions","0","In this article, we consider the case when the number of observations n is less than the dimension p of the random vectors which are assumed to be independent and identically distributed as normal with nonsingular covariance matrix. The central and noncentral distributions of the singular Wishart matrix S = XX', where X is the p x n matrix of observations are derived with respect to Lebesgue measure. Properties of this distribution are given. When the covariance matrix is singular, pseudo singular Wishart distribution is also derived. The result is extended to any distribution of the type f (XX') for the central case. Singular multivariate beta distributions with respect to Lebesgue measure are also given."
"10.1214/aos/1065705117","2003","Testing homogeneity of multivariate normal mean vectors under an order restriction when the covariance matrices are common but unknown","0","Suppose that an order restriction is imposed among several p-variate normal mean vectors. We are interested in testing the homogeneity of these mean vectors under this restriction. This problem is a multivariate extension of Bartholomew's [Biometrika 46 (1959) 36-48]. When the covariance matrices are known, this problem has been studied by Sasabuchi, Inutsuka and Kulatunga [Hiroshima Math. J 22 (1992) 551-560], Sasabuchi, Kulatunga and Saito [Amer. J Math. Management Sci. 18 (1998) 131-158] and some others. In the present paper, we consider the case when the covariance matrices are common but unknown. We propose a test statistic, study its upper tail probability under the null hypothesis and estimate its critical points."
"10.1214/aos/1065705116","2003","Enriched conjugate and reference priors for the {W}ishart family on symmetric cones","0","A general Wishart family on a symmetric cone is a natural exponential family (NEF) having a homogeneous quadratic variance function. Using results in the abstract theory of Euclidean Jordan algebras, the structure of conditional reducibility is shown to hold for such a family, and we identify the associated parameterization phi and analyze its properties. The enriched standard conjugate family for phi and the mean parameter mu are defined and discussed. This family is considerably more flexible than the standard conjugate one. The reference priors for phi and mu are obtained and shown to belong to the enriched standard conjugate family; in particular, this allows us to verify that reference posteriors are always proper. The above results extend those available for NEFs having a simple quadratic variance function. Specifications of the theory to the cone of real symmetric and positive-definite matrices are discussed in detail and allow us to perform Bayesian inference on the covariance matrix Sigma of a multivariate normal model under the enriched standard conjugate family. In particular, commonly employed Bayes estimates, such as the posterior expectation of Sigma and Sigma(-1), are provided in closed form."
"10.1214/aos/1065705115","2003","Projection-based depth functions and associated medians","6","A class of projection-based depth functions is introduced and studied. These projection-based depth functions possess desirable properties of statistical depth functions and their sample versions possess strong and order rootn uniform consistency. Depth regions and contours induced from projection-based depth functions are investigated. Structural properties of depth regions and contours and general continuity and convergence results of sample depth regions are obtained.Affine equivariant multivariate medians induced from projection-based depth functions are probed. The limiting distributions as well as the strong and order rootn consistency of the sample projection medians are established. The finite sample performance of projection medians is compared with that of a leading depth-induced median, the Tukey halfspace median (induced from the Tukey halfspace depth function). It turns out that, with appropriate choices of univariate location and scale estimators, the projection medians have a very high finite sample breakdown point and relative efficiency, much higher than those of the halfspace median.Based on the results obtained, it is found that projection depth functions and projection medians behave very well overall compared with their competitors and consequently are good alternatives to statistical depth functions and affine equivariant multivariate location estimators, respectively."
"10.1214/aos/1065705114","2003","A scatter matrix estimate based on the zonotope","0","We introduce a new scatter matrix functional which is a multivariate affine equivariant extension of the mean deviation E(\x - Med(x)\). The estimate is constructed using the data vectors (centered with the multivariate Oja median) and their angular distances. The angular distance is based on Randles interdirections. The new estimate is called the zonoid covariance matrix (the ZCM), as it is the regular covariance matrix of the centers of the facets of the zonotope based on the data set. There is a kind of symmetry between the zonoid covariance matrix and the affine equivariant sign covariance matrix; interchanging the roles of data vectors and hyperplanes yields the sign covariance matrix as the zonoid covariance matrix. (It turns out that the symmetry relies on the zonoid of the distribution and its projection body which is also a zonoid.) The influence function and limiting distribution of the new scatter estimate, the ZCM, are derived to consider the robustness and efficiency properties of the estimate. Finite-sample efficiencies are studied in a small simulation study. The influence function of the ZCM is unbounded (linear in the radius of the contamination vector) but less influential in the tails than that of the regular covariance matrix (quadratic in the radius). The estimate is highly efficient in the multivariate normal case and performs better than the regular covariance matrix for heavy-tailed distributions."
"10.1214/aos/1065705113","2003","Financial options and statistical prediction intervals","0","The paper shows how to convert statistical prediction sets into worst case hedging strategies for derivative securities. The prediction sets can, in particular, be ones for volatilities and correlations of the underlying securities, and for interest rates. This permits a transfer of statistical conclusions into prices for options and similar financial instruments. A prime feature of our results is that one can construct the trading strategy as if the prediction set had a 100% probability. If, in fact, the set has probability 1 - alpha, the hedging strategy will work with at least the same probability. Different types of prediction regions are considered. The starting value A(0) for the trading strategy corresponding to the 1 - alpha prediction region is a form of long term value at risk. At the same time, A(0) is coherent."
"10.1214/aos/1065705112","2003","A concrete statistical realization of {K}leinberg's stochastic discrimination for pattern recognition. {I}. {T}wo-class classification","0","The method of stochastic discrimination (SD) introduced by Kleinberg is a new method in statistical pattern recognition. It works by producing many weak classifiers and then combining them to form a strong classifier. However, the strict mathematical assumptions in Kleinberg [The Annals of Statistics 24 (1996) 2319-2349] are rarely met in practice. This paper provides an applicable way to realize the SD algorithm. We recast SD in a probability-space framework and present a concrete statistical realization of SD for two-class pattern recognition. We weaken Kleinberg's theoretically strict assumptions of uniformity and indiscernibility by introducing near uniformity and weak indiscernibility. Such weaker notions are easily encountered in practical applications. We present a systematic resampling method to produce weak classifiers and then establish corresponding classification rules of SD. We analyze the performance of SD theoretically and explain why SD is overtraining-resistant and why SD has a high convergence rate. Testing results on real and simulated data sets are also given."
"10.1214/aos/1059655916","2003","Edgeworth expansion for {$U$}-statistics under minimal conditions","0","Berry-Esseen bounds for U-statistics under the optimal moment conditions were derived by Koroljuk and Borovskich and Friedrich. Under the same optimal moment assumptions with an additional nonlattice condition, we establish a one-term Edgeworth expansion with remainder o(n(-1/2)) for U-statistics."
"10.1214/aos/1059655915","2003","Edgeworth expansions for semiparametric {W}hittle estimation of long memory","0","The semiparametric local Whittle or Gaussian estimate of the long memory parameter is known to have especially nice limiting distributional properties, being asymptotically normal with a limiting variance that is completely known. However in moderate samples the normal approximation may not be very good, so we consider a refined, Edgeworth, approximation, for both a tapered estimate and the original untapered one. For the tapered estimate, our higher-order correction involves two terms, one of order m(-1/2) (where m is the bandwidth number in the estimation), the other a bias term, which increases in m; depending on the relative magnitude of the terms, one or the other may dominate, or they may balance. For the untapered estimate we obtain an expansion in which, for m increasing fast enough, the correction consists only of a bias term. We discuss applications of our expansions to improved statistical inference and bandwidth choice. We assume Gaussianity, but in other respects our assumptions seem mild."
"10.1214/aos/1059655914","2003","On the asymptotic distribution of scrambled net quadrature","1","Recently, in a series of articles, Owen proposed the use of scrambled (t, m, s) nets and (t, s) sequences in high-dimensional numerical integration. These scrambled nets and sequences achieve the superior accuracy of equidistribution methods while allowing for the simpler error estimation techniques of Monte Carlo methods. The main aim of this article is to use Stein's method to study the asymptotic distribution of the scrambled (0, m, s) net integral estimate. In particular, it is shown that, for suitably smooth integrands on the s-dimensional unit hypercube, the estimate has an asymptotic normal distribution."
"10.1214/aos/1059655913","2003","Accelerated randomized stochastic optimization","1","We propose a general class of randomized gradient estimates to be employed in a recursive search for the minimum of an unknown multivariate regression function. Here only two observations per iteration step are used. Special cases include random direction stochastic approximation (Kushner and Clark), simultaneous perturbation stochastic approximation (Spall) and a special kernel based stochastic approximation method (Polyak and Tsybakov). If the unknown regression is p-smooth (p greater than or equal to 2) at the point of minimum, these methods achieve the optimal rate of convergence O(n(-(p-1)/(2p))). For both the classical stochastic approximation scheme (Kiefer and Wolfowitz) and the averaging scheme (Ruppert and Polyak) the related asymptotic distributions are computed."
"10.1214/aos/1059655912","2003","Convergence of the {M}onte {C}arlo expectation maximization for curved exponential families","0","The Monte Carlo expectation maximization (MCEM) algorithm is a versatile tool for inference in incomplete data models, especially when used in combination with Markov chain Monte Carlo simulation methods. In this contribution, the almost-sure convergence of the MCEM algorithm is established. It is shown, using uniform versions of ergodic theorems for Markov chains, that MCEM converges under weak conditions on the simulation kernel. Practical illustrations are presented, using a hybrid random walk Metropolis Hastings sampler and an independence sampler. The rate of convergence is studied, showing the impact of the simulation schedule on the fluctuation of the parameter estimate at the convergence. A novel averaging procedure is then proposed to reduce the simulation variance and increase the rate of convergence."
"10.1214/aos/1059655911","2003","Regression {$M$}-estimators with non-i.i.d.\ doubly censored data","0","Considering the linear regression model with fixed design, the usual M-estimator with a complete sample of the response variables is expressed as a functional of a generalized weighted bivariate empirical process, and its asymptotic normality is directly derived through the Hadamard differentiability property of this functional and the weak convergence of this generalized weighted empirical process. The result reveals the direct relationship between the M-estimator and the distribution function of the error variables in the linear model, which leads to the construction of the M-estimator when the response variables are subject to double censoring. For this proposed regression M-estimator with non-i.i.d. doubly censored data, strong consistency and asymptotic normality are established."
"10.1214/aos/1059655910","2003","On {$M$}-estimators and normal quantiles","0","This paper explores a class of robust estimators of normal quantiles filling the gap between maximum likelihood estimators and empirical quantiles. Our estimators are linear combinations of M-estimators. Their asymptotic variances can be arbitrarily close to variances of the maximum likelihood estimators. Compared with empirical quantiles, the new estimators offer considerable reduction of variance at near normal probability distributions."
"10.1214/aos/1059655909","2003","Saddlepoint approximations and tests based on multivariate {$M$}-estimates","3","We consider multidimensional M-functional parameters defined by expectations of score functions associated with multivariate M-estimators and tests for hypotheses concerning multidimensional smooth functions of these parameters. We propose a test statistic suggested by the exponent in the saddlepoint approximation to the density of the function of the M-estimates. This statistic is analogous to the log likelihood ratio in the parametric case. We show that this statistic is approximately distributed as a chi-squared variate and obtain a Lugannani-Rice style adjustment giving a relative error of order n(-1). We propose an empirical exponential likelihood statistic and consider a test based on this statistic. Finally we present numerical results for three examples including one in robust regression."
"10.1214/aos/1059655908","2003","A note on nonparametric estimation of linear functionals","0","Precise asymptotic descriptions of the minimax affine risks and biasvariance tradeoffs for estimating linear functionals are given for a broad class of moduli. The results are complemented by illustrative examples including one where it is possible to construct an estimator which is fully adaptive over a range of parameter spaces."
"10.1214/aos/1059655907","2003","Large sample theory for semiparametric regression models with two-phase, outcome dependent sampling","0","Outcome-dependent, two-phase sampling designs can dramatically reduce the costs of observational studies by judicious selection of the most informative subjects for purposes of detailed covariate measurement. Here we derive asymptotic information bounds and the form of the efficient score and influence functions for the semiparametric regression models studied by Lawless, Kalbfleisch and Wild (1999) under two-phase sampling designs. We show that the maximum likelihood estimators for both the parametric and nonparametric parts of the model are asymptotically normal and efficient. The efficient influence function for the parametric part agrees with the more general information bound calculations of Robins, Hsieh and Newey (1995). By verifying the conditions of Murphy and van der Vaart (2000) for a least favorable parametric submodel, we provide asymptotic justification for statistical inference based on profile likelihood."
"10.1214/aos/1059655906","2003","Asymptotic estimation theory of multipoint linkage analysis under perfect marker information","2","We consider estimation of a disease susceptibility locus tau at a chromosome. With perfect marker data available, the estimator (tau) over cap (N) of r based on N pedigrees has a rate of convergence N-1 under mild regularity conditions. The limiting distribution is the arg max of a certain compound Poisson process. Our approach is conditional on observed phenotypes, and therefore treats parametric and nonparametric linkage, as well as quantitative trait loci methods within a unified framework. A constant appearing in the asymptotics, the so-called asymptotic slope-to-noise ratio, is introduced as a performance measure for a given genetic model, score function and weighting scheme. This enables us to define asymptotically optimal score functions and weighting schemes. Interestingly, traditional N-1/2 theory breaks down, in that, for instance, the ML-estimator is not asymptotically optimal. Further, the asymptotic estimation theory automatically takes uncertainty of r into account, which is otherwise handled by means of multiple testing and Bonferroni-type corrections.Other potential applications of our approach that we discuss are general sampling criteria for planning of linkage studies, appropriate grid size of marker maps, robustness w.r.t. choice of map function (dropping assumption of no interference) and quantification of information loss due to heterogeneity (with linked or unlinked trait loci).We also discuss relations to pointwise performance criteria and pay special attention to weak genetic models, so-called local specificity models."
"10.1214/aos/1059655905","2003","Decompounding: an estimation problem for {P}oisson random sums","0","Given a sample from a compound Poisson distribution, we consider estimation of the corresponding rate parameter and base distribution. This has applications in insurance mathematics and queueing theory. We propose a plug-in type estimator that is based on a suitable inversion of the compounding operation. Asymptotic results for this estimator are obtained via a local analysis of the decompounding functional."
"10.1214/aos/1059655904","2003","Nonparametric estimators which can be ``plugged-in''","0","We consider nonparametric estimation of an object such as a probability density or a regression function. Can such an estimator achieve the ratewise minimax rate of convergence on suitable function spaces, while, at the same time, when ""plugged-in,"" estimate efficiently (at a rate of n(-1/2) with the best constant) many functionals of the object? For example, can we have a density estimator whose definite integrals are efficient estimators of the cumulative distribution function? We show that this is impossible for very large sets, for example, expectations of all functions bounded by M < infinity. However, we also show that it is possible for sets as large as indicators of all quadrants, that is, distribution functions. We give appropriate constructions of such estimates."
"10.1214/aos/1051027885","2003","Efficient detection of random coefficients in autoregressive models","0","The problem of detecting randomness in the coefficients of an AR(p) model, that is, the problem of testing ordinary AR(p) dependence against the alternative of a random coefficient autoregressive [RCAR(p)] model is considered. A nonstandard LAN property is established for RCAR(p) models in the vicinity of AR(p) ones. Two main problems arise in this context. The first problem is related to the statistical model itself: Gaussian assumptions are highly unrealistic in a nonlinear context, and innovation densities should be treated as nuisance parameters. The resulting semiparametric model however appears to be severely nonadaptive. In contrast with the linear ARMA case, pseudo-Gaussian likelihood methods here are invalid under non-Gaussian densities; even the innovation variance cannot be estimated without a strict loss of efficiency. This problem is solved using a general result by Hallin and Werker, which provides semiparametrically efficient central sequences without going through explicit tangent space calculations. The second problem is related to the fact that the testing problem under study is intrinsically one-sided, while the case of multiparameter one-sided alternatives is not covered by classical asymptotic theory under LAN. A concept of locally asymptotically most stringent somewhere efficient test is proposed in order to cope with this one-sided nature of the problem."
"10.1214/aos/1051027884","2003","On adaptive estimation in nonstationary {ARMA} models with {GARCH} errors","0","This paper considers adaptive estimation in nonstationary autoregressive moving average models with the noise sequence satisfying a generalized autoregressive conditional heteroscedastic process. The locally asymptotic quadratic form of the log-likelihood ratio for the model is obtained. It is shown that the limit experiment is neither LAN nor LAMN, but is instead LABF. For the model with symmetric density of the rescaled error, a new efficiency criterion is established for a class of defined M-v-estimators. It is shown that such efficient estimators can be constructed when the density is known. Using the kernel estimator for the score function, adaptive estimators are constructed when the density of the rescaled error is symmetric, and it is shown that the adaptive procedure for the parameters in the conditional mean part uses the full sample without splitting. These estimators are demonstrated to be asymptotically efficient in the class of M-v-estimators. The paper includes the results that the stationary ARMA-GARCH model is LAN, and that the parameters in the model with symmetric density of the rescaled error are adaptively estimable after a reparameterization of the GARCH process. This paper also establishes the locally asymptotic quadratic form of the log-likelihood ratio for nonlinear time series models with ARCH-type errors."
"10.1214/aos/1051027883","2003","A necessary and sufficient condition for asymptotic independence of discrete {F}ourier transforms under short- and long-range dependence","4","Let {X-t} be a stationary time series and let d(T) (lambda) denote the discrete Fourier transform (DFT) of {X-0,...,XT-1} with a data taper. The main results of this paper provide a characterization of asymptotic independence of the DFTs in terms of the distance between their arguments under both short- and long-range dependence of the process (Xt). Further, asymptotic joint distributions of the DFTs d(T) (lambda(1T)) and d(T) (lambda(2T)) are also established for the cases T(lambda(1T) - lambda(2T)) = O(1) as T --> infinity (asymptotically close ordinates) and \T(lambda(1T) - lambda(2T))\ --> infinity as T --> infinity (asymptotically distant ordinates). Some implications of the main results on the estimation of the index of dependence are also discussed."
"10.1214/aos/1051027882","2003","Valid asymptotic expansions for the maximum likelihood estimator of the parameter of a stationary, {G}aussian, strongly dependent process","0","We establish the validity of an Edgeworth expansion to the distribution of the maximum likelihood estimator of the parameter of a stationary, Gaussian, strongly dependent process. The result covers ARFIMA-type models, including fractional Gaussian noise. The method of proof consists of three main ingredients: (i) verification of a suitably modified version of Durbin's general conditions for the validity of the Edgeworth expansion to the joint density of the log-likelihood derivatives; (ii) appeal to a simple result of Skovgaard to obtain from this an Edgeworth expansion for the joint distribution of the log-likelihood derivatives; (iii) appeal to and extension of arguments of Bhattacharya and Ghosh to accomplish the passage from the result on the log-likelihood derivatives to the result for the maximum likelihood estimators. We develop and make extensive use of a uniform version of a theorem of Dahlhaus on products of Toeplitz matrices; the extension of Dahlhaus' result is of interest in its own right. A small numerical study of the efficacy of the Edgeworth expansion is presented for the case of fractional Gaussian noise."
"10.1214/aos/1051027881","2003","Distributional results for means of normalized random measures with independent increments","0","We consider the problem of determining the distribution of means of random probability measures which are obtained by normalizing increasing additive processes. A solution is found by resorting to a well-known inversion formula for characteristic functions due to Gurland. Moreover, expressions of the posterior distributions of those means, in the presence of exchangeable observations, are given. Finally, a section is devoted to the illustration of two examples of statistical relevance."
"10.1214/aos/1051027880","2003","Adaptive {B}ayesian inference on the mean of an infinite-dimensional normal distribution","0","We consider the problem of estimating the mean of an infinite-dimensional normal distribution from the Bayesian perspective. Under the assumption that the unknown true mean satisfies a ""smoothness condition,"" we first derive the convergence rate of the posterior distribution for a prior that is the infinite product of certain normal distributions and compare with the minimax rate of convergence for point estimators. Although the posterior distribution can achieve the optimal rate of convergence, the required prior depends on a ""smoothness parameter"" q. When this parameter q is unknown, besides the estimation of the mean, we encounter the problem of selecting a model. In a Bayesian approach, this uncertainty in the model selection can be handled simply by further putting a prior on the index of the model. We show that if q takes values only in a discrete set, the resulting hierarchical prior leads to the same convergence rate of the posterior as if we had a single model. A slightly weaker result is presented when q is unrestricted. An adaptive point estimator based on the posterior distribution is also constructed."
"10.1214/aos/1051027879","2003","Current status and right-censored data structures when observing a marker at the censoring time","0","We study nonparametric estimation with two types of data structures. In the first data structure n i.i.d. copies of (C, N(C)) are observed, where N is a finite state counting process jumping at time-variables of interest and C a random monitoring time. In the second data structure n i.i.d. copies of (C boolean AND T, I(T less than or equal to C), N (C boolean AND T)) are observed, where N is a counting process with a final jump at time T (e.g., death). This data structure includes observing right-censored data on T and a marker variable at the censoring time.In these data structures, easy to compute estimators, namely (weighted)pool-adjacent-violator estimators for the marginal distributions of the unobservable time variables, and the Kaplan-Meier estimator for the time T till the final observable event, are available. These estimators ignore seemingly important information in the data. In this paper we prove that, at many continuous data generating distributions the ad hoc estimators yield asymptotically efficient estimators of rootn-estimable parameters."
"10.1214/aos/1051027878","2003","Bayesian analysis of proportional hazard models","0","This paper is concerned with Bayesian analysis of the proportional hazard model with left truncated and right censored data. We use a process neutral to the right as the prior of the baseline survival function and a finite-dimensional prior is placed on the regression coefficient, We then obtain the exact form of the joint posterior distribution of the regression coefficient and the baseline cumulative hazard function. As a by-product, we prove the propriety of the posterior distribution with the constant prior on the regression coefficient."
"10.1214/aos/1051027877","2003","Estimating multiplicative and additive hazard functions by kernel methods","0","We propose new procedures for estimating the component functions in both additive and multiplicative nonparametric marker-dependent hazard models. We work with a full counting process framework that allows for left truncation and right censoring and time-varying covariates. Our procedures are based on kernel hazard estimation as developed by Nielsen and Linton and on the idea of marginal integration. We provide a central limit theorem for the marginal integration estimator. We then define estimators based on finite-step backfitting in both additive and multiplicative cases and prove that these estimators are asymptotically normal and have smaller variance than the marginal integration method."
"10.1214/aos/1051027876","2003","Estimation in a {C}ox regression model with a change-point according to a threshold in a covariate","7","We consider a nonregular Cox model for independent and identically distributed right censored survival times, with a change-point according to the unknown threshold of a covariate. The maximum partial likelihood estimators of the parameters and the estimator of the baseline cumulative hazard are studied. We prove that the estimator of the change-point is n-consistent and the estimator of the regression parameters are n(1/2)-consistent, and we establish the asymptotic distributions of the estimators. The estimators of the regression parameters and of the baseline cumulative hazard are adaptive in the sense that they do not depend on the knowledge of the change-point."
"10.1214/aos/1051027875","2003","Inference in components of variance models with low replication","0","in components of variance models the data are viewed as arising through a sum of two random variables, representing between- and within-group variation, respectively. The former is generally interpreted as a group effect, and the latter as error. It is assumed that these variables are stochastically independent and that the distributions of the group effect and the error do not vary from one instance to another. If each group effect can be replicated a large number of times, then standard methods can be used to estimate the distributions of both the group effect and the error. This cannot be achieved without replication, however. How feasible is distribution estimation if it is not possible to replicate prolifically? Can the distributions of random effects and errors be estimated consistently from a small number of replications of each of a large number of noisy group effects, for example, in a nonparametric setting? Often extensive replication is practically infeasible, in particular, if inherently small numbers of individuals exhibit any given group effect. Yet it is quite unclear how to conduct inference in this case. We show that inference is possible, even if the number of replications is as small as 2. Two methods are proposed, both based on Fourier inversion. One, which is substantially more computer intensive than the other, exhibits better performance in numerical experiments."
"10.1214/aos/1051027873","2003","Stochastic approximation","2","Stochastic approximation, introduced by Robbins and Monro in 1951, has become an important and vibrant subject in optimization, control and signal processing. This paper reviews Robbins' contributions to stochastic approximation and gives an overview of several related developments."
"10.1214/aos/1051027872","2003","Compound decision theory and empirical {B}ayes methods","2",""
"10.1214/aos/1051027871","2003","Robbins, empirical {B}ayes and microarrays","6","Empirical Bayes was Herbert Robbins' most influential contribution to statistical theory. It is also an idea of great practical potential. That potential is realized in the analysis of microarrays, a new biogenetic technology for the simultaneous measurement of thousands of gene expression levels."
"10.1214/aos/1051027870","2003","Herbert {R}obbins and sequential analysis","0","This paper reviews Herbert Robbins' research in sequential analysis (excluding stochastic approximation) from 1952 until roughly 1980. Its relation to the research of his contemporaries and its impact on subsequent research are described."
"10.1214/aos/1056562472","2003","Hidden projection properties of some nonregular fractional factorial designs and their applications","0","In factor screening, often only a few factors among a large pool of potential factors are active. Under such assumption of effect sparsity, in choosing a design for factor screening, it is important to consider projections of the design onto small subsets of factors. Cheng showed that as long as the run size of a two-level orthogonal array of strength two is not a multiple of 8, its projection onto any four factors allows the estimation of all the main effects and two-factor interactions when the higher-order interactions are negligible. This result applies, for example, to all Plackett-Burman designs whose run sizes are not multiples of 8. It is shown here that the same hidden projection property also holds for Paley designs of sizes greater than 8, even when their run sizes are multiples of 8. A key result is that such designs do not have defining words of length three or four. Applications of this result to the construction of E(s(2))-optimal supersaturated designs are also discussed. In particular, certain designs constructed by using Wu's method are shown to be E(s(2))-optimal. The article concludes with some three-level designs with good projection properties."
"10.1214/aos/1056562471","2003","Structure function for aliasing patterns in {$2^{l-n}$} design with multiple groups of factors","1","A general approach to studying fractional factorial designs with multiple groups of factors is proposed. A structure function is generated by the defining contrasts among different groups of factors and the remaining columns. The structure function satisfies a first-order partial differential equation. By solving this equation, general results about the structures and properties of the designs are obtained. As an important application, practical rules for the selection of ""optimal"" single arrays for robust parameter design experiments are derived."
"10.1214/aos/1056562470","2003","Indicator function and its application in two-level factorial designs","4","A two-level factorial design can be uniquely represented by a polynomial indicator function. Therefore, properties of factorial designs can be studied through their indicator functions. This paper shows that the indicator function is an effective tool in studying two-level factorial designs. The indicator function is used to generalize the aberration criterion of a regular two-level fractional factorial design to all two-level factorial designs. An important identity of generalized aberration is proved. The connection between a uniformity measure and aberration is also extended to all two-level factorial designs."
"10.1214/aos/1056562469","2003","Universal optimality of balanced uniform crossover designs","0","Kunert [Ann. Statist. 12 (1984) 1006-1017] proved that, in the class of repeated measurement designs based on t treatments, p = t periods and n = lambdat experimental units, a balanced uniform design is universally optimal for direct treatment effects if t greater than or equal to 3 and lambda = 1, or if t greater than or equal to 6 and lambda = 2. This result is generalized to t greater than or equal to 3 as long as lambda less than or equal to (t - 1)/2."
"10.1214/aos/1056562468","2003","S{PRT} and {CUSUM} in hidden {M}arkov models","2","In this paper, we study the problems of sequential probability ratio tests for parameterized hidden Markov models. We investigate in some detail the performance of the tests and derive corrected Brownian approximations for error probabilities and expected sample sizes. Asymptotic optimality of the sequential probability ratio test for testing simple hypotheses based on hidden Markov chain data is established. Next, we consider the cumulative sum (CUSUM) procedure for change point detection in this model. Based on the renewal property of the stopping rule, CUSUM can be regarded as a repeated one-sided sequential probability ratio test. Asymptotic optimality of the CUSUM procedure is proved in the sense of Lorden (1971). Motivated by the sequential analysis in hidden Markov models, Wald's likelihood ratio identity and Wald's equation for products of Markov random matrices are also given. We apply these results to several types of hidden Markov models: i.i.d. hidden Markov models, switch Gaussian regression and switch Gaussian autoregression, which are commonly used in digital communications, speech recognition, bioinformatics and economics."
"10.1214/aos/1056562467","2003","Sequential methods for design-adaptive estimation of discontinuities in regression curves and surfaces","0","In fault-line estimation in spatial problems it is sometimes possible to choose design points sequentially, by working one's way gradually through the ""response plane,"" rather than distributing design points across the plane prior to conducting statistical analysis. For example, when estimating a change line in the concentration of resources on or under the sea bed, individual measurements can be particularly expensive to make. In such cases, sequential, design-adaptive methods are attractive. Appropriate methodology is largely lacking, however, and the potential advantages of taking a sequential approach are unclear. In the present paper we address both these problems. We suggest a methodology based on ""sequential refinement with reassessment"" that relies upon assessing the correctness of each sequential result, and reappraising previous results if significance tests show that there is reason for concern. We focus part of our attention on univariate problems, and we show how methods for the spatial case can be constructed from univariate ones."
"10.1214/aos/1056562466","2003","Nonparametric comparison of regression curves: an empirical process approach","0","We propose a new test for the comparison of two regression curves that is based on a difference of two marked empirical processes based on residuals. The large sample behavior of the corresponding statistic is studied to provide a full nonparametric comparison of regression curves. In contrast to most procedures suggested in the literature, the new procedure is applicable in the case of different design points and heteroscedasticity. Moreover, it is demonstrated that the proposed test detects continuous alternatives converging to the null at a rate N-1/2 and that, in contrast to all other available procedures based on marked empirical processes, the new test allows the optimal choice of bandwidths for curve estimation (e.g., N-1/5 in the case of twice differentiable regression functions). As a by-product we explain the problems of a related test proposed by Kulasekera [J. Amer Statist. Assoc. 90 (1995) 1085-1093] and Kulasekera and Wang [J. Amer. Statist. Assoc. 92 (1997) 500-511] with respect to accuracy in the approximation of the level. These difficulties mainly originate from the comparison with the quantiles of an inappropriate limit distribution.A simulation study is conducted to investigate the finite sample properties of a wild bootstrap version of the new test and to compare it with the so far available procedures. Finally, heteroscedastic data is analyzed in order to demonstrate the benefits of the new test compared to the so far available procedures which require homoscedasticity."
"10.1214/aos/1056562465","2003","Moderate deviations of minimum contrast estimators under contamination","0","Since statistical models are simplifications of reality, it is important in estimation theory to study the behavior of estimators also under distributions (slightly) different from the proposed model. In testing theory, when dealing with test statistics where nuisance parameters are estimated, knowledge of the behavior of the estimators of the nuisance parameters is needed under alternatives to evaluate the power. In this paper the moderate deviation behavior of minimum contrast estimators is investigated not only under the supposed model, but also under distributions close to the model. A particular example is the (multivariate) maximum likelihood estimator determined within the proposed model. The set-up is quite general, including also, for instance, discrete distributions.The rate of convergence under alternatives is determined both when comparing the minimum contrast estimator with a ""natural"" parameter in the parameter space and when comparing it with the proposed ""true"" value in the parameter space. It turns out that under the model the asymptotic optimality of the maximum likelihood estimator in the local sense continues to hold in the moderate deviation area."
"10.1214/aos/1056562464","2003","Likelihood ratio of unidentifiable models and multilayer neural networks","0","This paper discusses the behavior of the maximum likelihood estimator (MLE), in the case that the true parameter cannot be identified uniquely. Among many statistical models with unidentitiability, neural network models are the main concern of this paper. It is known in some models with unidentifiability that the asymptotics of the likelihood ratio of the MLE has an unusually larger order. Using the framework of locally conic models put forth by Dacunha-Castelle and Gassiat as a generalization of Hartigan's idea, a useful sufficient condition of such larger orders is derived. This result is applied to neural network models, and a larger order is proved if the true function is given by a smaller model. Also, under the condition that the model has at least two redundant hidden units, a log n lower bound for the likelihood ratio is derived."
"10.1214/aos/1056562463","2003","Asymptotics for likelihood ratio tests under loss of identifiability","0","This paper describes the large sample properties of the likelihood ratio test statistic (LRTS) when the parameters characterizing the true null distribution are not unique. It is well known that the classical asymptotic theory for the likelihood ratio test does not apply to such problems and the LRTS may not have the typical chi-squared type limiting distribution. This paper establishes a general quadratic approximation of the log-likelihood ratio function in a Hellinger neighborhood of the true density which is valid with or without loss of identifiability of the true distribution. Under suitable conditions, the asymptotic null distribution of the LRTS under loss of identifiability can be obtained by maximizing the quadratic form. These results extend the work of Chernoff and Le Cam. In particular, applications to testing the number of mixture components in finite mixture models are discussed."
"10.1214/aos/1056562462","2003","How do bootstrap and permutation tests work?","0","Resampling methods are frequently used in practice to adjust critical values of nonparametric tests. In the present paper a comprehensive and unified approach for the conditional and unconditional analysis of linear resampling statistics is presented. Under fairly mild assumptions we prove tightness and an asymptotic series representation for their weak accumulation points. From this series it becomes clear which part of the resampling statistic is responsible for asymptotic normality. The results leads to a discussion of the asymptotic correctness of resampling methods as well as their applications in testing hypotheses. They are conditionally correct iff a central limit theorem holds for the original test statistic. We prove unconditional correctness iff the central limit theorem holds or when symmetric random variables are resampled by a scheme of asymptotically random signs. Special cases are the m(n) out of k(n) bootstrap, the weighted bootstrap, the wild bootstrap and all kinds of permutation statistics. The program is carried out for convergent partial sums of rowwise independent infinitesimal triangular arrays in detail. These results are used to compare power functions of conditional resampling tests and their unconditional counterparts. The proof uses the method of random scores for permutation type statistics."
"10.1214/aos/1056562461","2003","Slice sampling","10","Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can sample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal ""slice"" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such ""slice sampling"" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by ""overrelaxation,"" and for multivariate slice sampling by ""reflection"" from the edges of the slice."
"10.1214/aos/1046294467","2003","Asymptotics for generalized estimating equations with large cluster sizes","0","Generalized estimating equations are used in regression analysis of longitudinal data, where observations on each subject are correlated. Statistical analysis using such methods is based on the asymptotic properties of regression parameter estimators. This paper presents asymptotic results when either the number of independent subjects or the cluster sizes (the number of observations on each subject) or both go to infinity. A set of (information matrix based) general conditions is developed, which leads to the weak and strong consistency as well as the asymptotic normality of the estimators. Most of the results are parallel to the elegant work of Fahrmeir and Kaufmann on maximum likelihood estimators related to the generalized linear models. The conditions for weak consistency and asymptotic normality are verified for several examples of general interest."
"10.1214/aos/1046294466","2003","The complex {W}ishart distribution and the symmetric group","0","Let V be the space of (r, r) Hermitian matrices and let Omega be the cone of the positive definite ones. We say that the random variable S, taking its values in (&UOmega;) over bar, has the complex Wishart distribution gamma(p,sigma) if E(exp trace(thetaS)) = (det(I-r - sigmatheta))(-p), where sigma and sigma(-1) - theta are in Omega, and where p = 1, 2,..., r - 1 or p > r - 1. In this paper, we compute all moments of S and S-1. The techniques involve in particular the use of the irreducible characters of the symmetric group."
"10.1214/aos/1046294465","2003","Multivariate saddlepoint tail probability approximations","0","This paper presents a saddlepoint approximation to the cumulative distribution function of a random vector. The proposed approximation has accuracy comparable to that of existing expansions valid in two dimensions, and may be applied to random vectors of arbitrary length, subject only to the requirement that the distribution approximated either have a density or be confined to a lattice, and have a cumulant generating function. The result is derived by directly inverting the multivariate moment generating function. The result is applied to sufficient statistics from a regression model with exponential errors, and compared to an existing method in two dimensions. The result is also applied to multivariate inference from a data set arising from a case-control study of endometrial cancer."
"10.1214/aos/1046294464","2003","Model selection in nonparametric regression","7","Model selection using a penalized data-splitting device is studied in the context of nonparametric regression. Finite sample bounds under mild conditions are obtained. The resulting estimates are adaptive for large classes of functions."
"10.1214/aos/1046294463","2003","Adaptive tests of linear hypotheses by model selection","0","We propose a new test, based on model selection methods, for testing that the expectation of a Gaussian vector with n independent components belongs to a linear subspace of R(n) against a nonparametric alternative. The testing procedure is available when the variance of the observations is unknown and does not depend on any prior information on the alternative. The properties of the test are nonasymptotic and we prove that the test is rate optimal [up to a possible log(n) factor] over various classes of alternatives simultaneously. We also provide a simulation study in order to evaluate the procedure when the purpose is to test goodness-of-fit in a regression model."
"10.1214/aos/1046294462","2003","Nonparametric estimation of component distributions in a multivariate mixture","0","Suppose k-variate data are drawn from a mixture of two distributions, each having independent components. It is desired to estimate the univariate marginal distributions in each of the products, as well as the mixing proportion. This is the setting of two-class, fully parametrized latent models that has been proposed for estimating the distributions of medical test results when disease status is unavailable. The problem is one of inference in a mixture of distributions without training data, and until now it has been tackled only in a fully parametric setting. We investigate the possibility of using nonparametric methods. Of course, when k = 1 the problem is not identifiable from a nonparametric viewpoint. We show that the problem is ""almost"" identifiable when k = 2; there, the set of all possible representations can be expressed, in terms of any one of those representations, as a two-parameter family. Furthermore, it is proved that when k greater than or equal to 3 the problem is nonparametrically identifiable under particularly mild regularity conditions. In this case we introduce root-n consistent nonparametric estimators of the 2k univariate marginal distributions and the mixing proportion. Finite-sample and asymptotic properties of the estimators are described."
"10.1214/aos/1046294461","2003","Nonparametric estimation of convex models via mixtures","3","We present a general approach to estimating probability measures constrained to lie in a convex set. We represent constrained measures as mixtures of simple, known extreme measures, and so the problem of estimating a constrained measure becomes one of estimating an unconstrained mixing measure. Convex constraints arise in many modeling situations, such as estimation of the mean and estimation under stochastic ordering constraints. We describe mixture representation techniques for these and other situations, and discuss applications to maximum likelihood and Bayesian estimation."
"10.1214/aos/1046294460","2003","Wavelet threshold estimation for additive regression models","0","Additive regression models have turned out to be useful statistical tools in the analysis of high-dimensional data. The attraction of such models is that the additive component can be estimated with the same optimal convergence rate as a one-dimensional nonparametric regression. However, this optimal property holds only when all the additive components have the same degree of ""homogeneous"" smoothness. In this paper, we propose a two-step wavelet thresholding estimation process in which the estimator is adaptive to different degrees of smoothness in different components and also adaptive to the ""inhomogeneous"" smoothness described by the Besov space. The estimator of an additive component constructed by the proposed procedure is shown to attain the one-dimensional optimal convergence rate even when the components have different degrees of ""inhomogeneous"" smoothness."
"10.1214/aos/1046294459","2003","Wavelet thresholding for non-necessarily {G}aussian noise: idealism","0",""
"10.1214/aos/1046294458","2003","Thresholding estimators for linear inverse problems and deconvolutions","0","Thresholding algorithms in an orthonormal basis are studied to estimate noisy discrete signals degraded by a linear operator whose inverse is not bounded. For signals in a set Theta, sufficient conditions are established on the basis to obtain a maximum risk with minimax rates of convergence. Deconvolutions with kernels having a Fourier transform which vanishes at high frequencies are examples of unstable inverse problems, where a thresholding in a wavelet basis is a suboptimal estimator. A new ""mirror wavelet"" basis is constructed to obtain a deconvolution risk which is proved to be asymptotically equivalent to the minimax risk over bounded variation signals. This thresholding estimator is used to restore blurred satellite images."
"10.1214/aos/1046294457","2003","Image denoising: pointwise adaptive approach","0","A new method of pointwise adaptation has been proposed and studied in Spokoiny [(1998) Ann. Statist. 26 1356-1378] in the context of estimation of piecewise smooth univariate functions. The present paper extends that method to estimation of bivariate grey-scale images composed of large homogeneous regions with smooth edges and observed with noise on a gridded design. The proposed estimator (f) over cap (x) at a point x is simply the average of observations over a window (U) over cap (x) selected in a data-driven way. The theoretical properties of the procedure are studied for the case of piecewise constant images. We present a nonasymptotic bound for the accuracy of estimation at a specific grid point x as a function of the number of pixels n, of the distance from the point of estimation to the closest boundary and of smoothness properties and orientation of this boundary. It is also shown that the proposed method provides a near-optimal rate of estimation near edges and inside homogeneous regions. We briefly discuss algorithmic aspects and the complexity of the procedure. The numerical examples demonstrate a reasonable performance of the method and they are in agreement with the theoretical issues. An example from satellite (SAR) imaging illustrates the applicability of the method."
"10.1214/aos/1046294456","2003","Large sample theory of intrinsic and extrinsic sample means on manifolds. {I}","0","Sufficient conditions are given for the uniqueness of intrinsic and extrinsic means as measures of location of probability measures Q on Riemannian manifolds. It is shown that, when uniquely defined, these are estimated consistently by the corresponding indices of the empirical (Q) over cap (n). Asymptotic distributions of extrinsic sample means are derived. Explicit computations of these indices of (Q) over cap (n) and their asymptotic dispersions are carried out for distributions on the sphere S-d (directional spaces), real projective space RPN-1 (axial spaces) and CPk-2 (planar shape spaces)."
"10.1093/biomet/ass021","2012","Quadratic inference function approach to merging longitudinal studies: validation and joint estimation","0","Merging data from multiple studies has been widely adopted in biomedical research. In this paper, we consider two major issues related to merging longitudinal datasets. We first develop a rigorous hypothesis testing procedure to assess the validity of data merging, and then propose a flexible joint estimation procedure that enables us to analyse merged data and to account for different within-subject correlations and follow-up schedules in different studies. We establish large sample properties for the proposed procedures. We compare our method with meta analysis and generalized estimating equations and show that our test provides robust control of Type I error against both misspecification of working correlation structures and heterogeneous dispersion parameters. Our joint estimating procedure leads to an improvement in estimation efficiency on all regression coefficients after data merging is validated."
"10.1093/biomet/ass035","2012","Designs of variable resolution","0","Prior information or background knowledge may suggest that interactions arise only within certain factors. When such knowledge is available, we propose using a new class of designs: designs of variable resolution. Several constructions are presented. Statistical justifications for using such designs from minimum G(2) aberration and design efficiency perspectives are provided."
"10.1093/biomet/ass030","2012","The fitting of complex parametric models","0","Consider parametric models that are too complicated to allow calculation of a likelihood but from which observations can be simulated. We examine parameter estimators that are linear functions of a possibly large set of candidate features. A combination of simulations based on a fractional design and sets of discriminant analyses is then used to find an optimal estimator of the vector parameter and its covariance matrix. The procedure is an alternative to the approximate Bayesian computation scheme."
"10.1093/biomet/ass025","2012","Positive definite estimators of large covariance matrices","0","Using convex optimization, we construct a sparse estimator of the covariance matrix that is positive definite and performs well in high-dimensional settings. A lasso-type penalty is used to encourage sparsity and a logarithmic barrier function is used to enforce positive definiteness. Consistency and convergence rate bounds are established as both the number of variables and sample size diverge. An efficient computational algorithm is developed and the merits of the approach are illustrated with simulations and a speech signal classification example."
"10.1093/biomet/ass027","2012","On the robustness of the adaptive lasso to model misspecification","0","Penalization methods have been shown to yield both consistent variable selection and oracle parameter estimation under correct model specification. In this article, we study such methods under model misspecification, where the assumed form of the regression function is incorrect, including generalized linear models for uncensored outcomes and the proportional hazards model for censored responses. Estimation with the adaptive least absolute shrinkage and selection operator, lasso, penalty is proven to achieve sparse estimation of regression coefficients under misspecification. The resulting estimators are selection consistent, asymptotically normal and oracle, where the selection is based on the limiting values of the parameter estimators obtained using the misspecified model without penalization. We further derive conditions under which the penalized estimators from the misspecified model may yield selection consistency under the true model. The robustness is explored numerically via simulation and an application to the Wisconsin Epidemiological Study of Diabetic Retinopathy."
"10.1093/biomet/ass014","2012","Penalized empirical likelihood and growing dimensional general estimating equations","0","When a parametric likelihood function is not specified for a model, estimating equations may provide an instrument for statistical inference. Qin and Lawless (1994) illustrated that empirical likelihood makes optimal use of these equations in inferences for fixed low-dimensional unknown parameters. In this paper, we study empirical likelihood for general estimating equations with growing high dimensionality and propose a penalized empirical likelihood approach for parameter estimation and variable selection. We quantify the asymptotic properties of empirical likelihood and its penalized version, and show that penalized empirical likelihood has the oracle property. The performance of the proposed method is illustrated via simulated applications and a data analysis."
"10.1093/biomet/ass024","2012","Inner envelopes: efficient estimation in multivariate linear regression","0","In this article we propose a new model, called the inner envelope model, which leads to efficient estimation in the context of multivariate normal linear regression. The asymptotic distribution and the consistency of its maximum likelihood estimators are established. Theoretical results, simulation studies and examples all show that the efficiency gains can be substantial relative to standard methods and to the maximum likelihood estimators from the envelope model introduced recently by Cook et al. (2010). Compared to the envelope model, the inner envelope model is based on a different construction and it can produce substantial efficiency gains in situations where the envelope model offers no gains. In effect, inner envelopes open a new frontier to the way in which reducing subspaces can be used to improve efficiency in multivariate problems."
"10.1093/biomet/ass028","2012","Objective {B}ayes, conditional inference and the signed root likelihood ratio statistic","0","Bayesian properties of the signed root likelihood ratio statistic are analysed. Conditions for first-order probability matching are derived by the examination of the Bayesian posterior and frequentist means of this statistic. Second-order matching conditions are shown to arise from matching of the Bayesian posterior and frequentist variances of a mean-adjusted version of the signed root statistic. Conditions for conditional probability matching in ancillary statistic models are derived and discussed."
"10.1093/biomet/ass023","2012","On the stick-breaking representation of normalized inverse {G}aussian priors","0","Random probability measures are the main tool for Bayesian nonparametric inference, with their laws acting as prior distributions. Many well-known priors used in practice admit different, though equivalent, representations. In terms of computational convenience, stick-breaking representations stand out. In this paper we focus on the normalized inverse Gaussian process and provide a completely explicit stick-breaking representation for it. This result is of interest both from a theoretical viewpoint and for statistical practice."
"10.1093/biomet/ass031","2012","Modelling covariance structure in bivariate marginal models for longitudinal data","0","It can be more challenging to efficiently model the covariance matrices for multivariate longitudinal data than for the univariate case, due to the correlations arising between multiple responses. The positive-definiteness constraint and the high dimensionality are further obstacles in covariance modelling. In this paper, we develop a data-based method by which the parameters in the covariance matrices are replaced by unconstrained and interpretable parameters with reduced dimensions. The maximum likelihood estimators for the mean and covariance parameters are shown to be consistent and asymptotically normally distributed. Simulations and real data analysis show that the new approach performs very well even when modelling bivariate nonstationary dependence structures."
"10.1093/biomet/ass026","2012","An efficient method of estimation for longitudinal surveys with monotone missing data","0","Panel attrition is frequently encountered in panel sample surveys. When it is related to the observed study variable, the classical approach of nonresponse adjustment using a covariate-dependent dropout mechanism can be biased. We consider an efficient method of estimation with monotone panel attrition when the response probability depends on the previous values of study variable as well as other covariates. Because of the monotone structure of the missing pattern, the response mechanism is missing at random. The proposed estimator is asymptotically optimal in the sense that it minimizes the asymptotic variance of a class of estimators that can be written as a linear combination of the unbiased estimators of the panel estimates for each wave, and incorporates all available information using generalized least squares. Variance estimation is discussed and results from a simulation study are presented."
"10.1093/biomet/ass018","2012","Predictive accuracy of covariates for event times","0","We propose a graphical measure, the generalized negative predictive function, to quantify the predictive accuracy of covariates for survival time or recurrent event times. This new measure characterizes the event-free probabilities over time conditional on a thresholded linear combination of covariates and has direct clinical utility. We show that this function is maximized at the set of covariates truly related to event times and thus can be used to compare the predictive accuracy of different sets of covariates. We construct nonparametric estimators for this function under right censoring and prove that the proposed estimators, upon proper normalization, converge weakly to zero-mean Gaussian processes. To bypass the estimation of complex density functions involved in the asymptotic variances, we adopt the bootstrap approach and establish its validity. Simulation studies demonstrate that the proposed methods perform well in practical situations. Two clinical studies are presented."
"10.1093/biomet/ass017","2012","Nonparametric incidence estimation from prevalent cohort survival data","0","Incidence is an important epidemiological concept most suitably studied using an incident cohort study. However, data are often collected from the more feasible prevalent cohort study, whereby diseased individuals are recruited through a cross-sectional survey and followed in time. In the absence of temporal trends in survival, we derive an efficient nonparametric estimator of the cumulative incidence based on such data and study its asymptotic properties. Arbitrary calendar time variations in disease incidence are allowed. Age-specific incidence and adjustments for both stratified sampling and temporal variations in survival are also discussed. Simulation results are presented and data from the Canadian Study of Health and Aging are analysed to infer the incidence of dementia in the Canadian elderly population."
"10.1093/biomet/ass020","2012","On the estimation of an average rigid body motion","0","This paper investigates the definition and the estimation of the Frechet mean of a random rigid body motion in R-p. The sample space SE(p) contains objects M=(R,t) where R is a pxp rotation matrix and t is a px1 translation vector. This work is motivated by applications in biomechanics where the posture of a joint at a given time is expressed as MSE(3), the rigid body displacement needed to map a system of axes on one segment of the joint to a similar system on the other segment. This posture can also be reported as M-1=(R-T,-R(T)t) by interchanging the role of the two segments. Several definitions of a Frechet mean for a random motion are proposed using weighted least squares distances. A special emphasis is given to a Frechet mean that is equivariant with respect to the inverse transform; this means that if P is the Frechet mean for M then P-1 is the Frechet mean for M-1, where M is a random SE(p) object. The sampling properties of moment estimators of the Frechet means are studied in a large concentration setting, where the scatter of the random Ms around their mean value P is small, and as the sample size goes to 8. Some simple exponential family models for SE(p) data that generalize Downs' (1972) Fisher-von Mises matrix distribution for rotation matrices are introduced and the least squares mean values for these distributions are calculated. Asymptotic comparisons between the estimators presented in this work are carried out for a particular model when p=2. A numerical example involving the motion of the ankle is presented to illustrate the methodology."
"10.1093/biomet/ass019","2012","On multilinear principal component analysis of order-two tensors","0","Principal component analysis is commonly used for dimension reduction in analysing high-dimensional data. Multilinear principal component analysis aims to serve a similar function for analysing tensor structure data, and has empirically been shown effective in reducing dimensionality. In this paper, we investigate its statistical properties and demonstrate its advantages. Conventional principal component analysis, which vectorizes the tensor data, may lead to inefficient and unstable prediction due to the often extremely large dimensionality involved. Multilinear principal component analysis, in trying to preserve the data structure, searches for low-dimensional projections and, thereby, decreases dimensionality more efficiently. The asymptotic theory of order-two multilinear principal component analysis, including asymptotic efficiency and distributions of principal components, associated projections, and the explained variance, is developed. A test of dimensionality is also proposed. Finally, multilinear principal component analysis is shown to improve conventional principal component analysis in analysing the Olivetti faces dataset, which is achieved by extracting a more modularly oriented basis set in reconstructing the test faces."
"10.1093/biomet/ass022","2012","Analysis of principal nested spheres","0","A general framework for a novel non-geodesic decomposition of high-dimensional spheres or high-dimensional shape spaces for planar landmarks is discussed. The decomposition, principal nested spheres, leads to a sequence of submanifolds with decreasing intrinsic dimensions, which can be interpreted as an analogue of principal component analysis. In a number of real datasets, an apparent one-dimensional mode of variation curving through more than one geodesic component is captured in the one-dimensional component of principal nested spheres. While analysis of principal nested spheres provides an intuitive and flexible decomposition of the high-dimensional sphere, an interesting special case of the analysis results in finding principal geodesics, similar to those from previous approaches to manifold principal component analysis. An adaptation of our method to Kendall's shape space is discussed, and a computational algorithm for fitting principal nested spheres is proposed. The result provides a coordinate system to visualize the data structure and an intuitive summary of principal modes of variation, as exemplified by several datasets."
"10.1093/biomet/ass015","2012","Inferring stochastic dynamics from functional data","0","In most current data modelling for time-dynamic systems, one works with a prespecified differential equation and attempts to estimate its parameters. In contrast, we demonstrate that in the case of functional data, the equation itself can be inferred. Assuming only that the dynamics are described by a first-order nonlinear differential equation with a random component, we obtain data-adaptive dynamic equations from the observed data via a simple smoothing-based procedure. We prove consistency and introduce diagnostics to ascertain the fraction of variance that is explained by the deterministic part of the equation. This approach is shown to yield useful insights into the time-dynamic nature of human growth."
"10.1093/biomet/ass034","2012","Nonparametric estimation of diffusions: a differential equations approach","0","We consider estimation of scalar functions that determine the dynamics of diffusion processes. It has been recently shown that nonparametric maximum likelihood estimation is ill-posed in this context. We adopt a probabilistic approach to regularize the problem by the adoption of a prior distribution for the unknown functional. A Gaussian prior measure is chosen in the function space by specifying its precision operator as an appropriate differential operator. We establish that a Bayesian-Gaussian conjugate analysis for the drift of one-dimensional nonlinear diffusions is feasible using high-frequency data, by expressing the loglikelihood as a quadratic function of the drift, with sufficient statistics given by the local time process and the end points of the observed path. Computationally efficient posterior inference is carried out using a finite element method. We embed this technology in partially observed situations and adopt a data augmentation approach whereby we iteratively generate missing data paths and draws from the unknown functional. Our methodology is applied to estimate the drift of models used in molecular dynamics and financial econometrics using high- and low-frequency observations. We discuss extensions to other partially observed schemes and connections to other types of nonparametric inference."
"10.1093/biomet/ass012","2012","Inference for additive interaction under exposure misclassification","0","Results are given concerning inferences that can be drawn about interaction when binary exposures are subject to certain forms of independent nondifferential misclassification. Tests for interaction, using the misclassified exposures, are valid provided the probability of misclassification satisfies certain bounds. Results are given for additive statistical interactions, for causal interactions corresponding to synergism in the sufficient cause framework and for so-called compositional epistasis. Both two-way and three-way interactions are considered. The results require only that the probability of misclassification be no larger than 1/2 or 1/4, depending on the test. For additive statistical interaction, a method to correct estimates and confidence intervals for misclassification is described. The consequences for power of interaction tests under exposure misclassification are explored through simulations."
"10.1093/biomet/ass002","2012","A generalized {D}unnett test for multi-arm multi-stage clinical studies with treatment selection","0","We generalize the Dunnett test to derive efficacy and futility boundaries for a flexible multi-arm multi-stage clinical trial for a normally distributed endpoint with known variance. We show that the boundaries control the familywise error rate in the strong sense. The method is applicable for any number of treatment arms, number of stages and number of patients per treatment per stage. It can be used for a wide variety of boundary types or rules derived from alpha-spending functions. Additionally, we show how sample size can be computed under a least favourable configuration power requirement and derive formulae for expected sample sizes."
"10.1093/biomet/ass001","2012","Information dynamics and optimal sampling in capture-recapture","0","The build up of information in a continued capture-recapture experiment of simple random sampling of an open population is studied by predicting the conditional approximate Fisher information for abundance in data from one survey given the previous data. By neglecting the stochasticity in survival, a simple approximate likelihood is obtained. Optimal temporal allocation of a given total effort is found by numerical optimization for various objective functions based on the approximate Fisher information. For aerial photographic surveys of bowhead whales, the performance of estimates of abundance and of demographic parameters is compared between constant yearly survey effort and nominally optimal sampling by simulating a realistic model over 50 years."
"10.1093/biomet/asr082","2012","Structuring shrinkage: some correlated priors for regression","0","This paper develops a rich class of sparsity priors for regression effects that encourage shrinkage of both regression effects and contrasts between effects to zero whilst leaving sizeable real effects largely unshrunk. The construction of these priors uses some properties of normal-gamma distributions to include design features in the prior specification, but has general relevance to any continuous sparsity prior. Specific prior distributions are developed for serial dependence between regression effects and correlation within groups of regression effects."
"10.1093/biomet/asr073","2012","A new residual for ordinal outcomes","0","We propose a new residual for regression models of ordinal outcomes, defined as E{sign(y,Y)}, where y is the observed outcome and Y is a random variable from the fitted distribution. This new residual is a single value per subject irrespective of the number of categories of the ordinal outcome, contains directional information between the observed value and the fitted distribution, and does not require the assignment of arbitrary numbers to categories. We study its properties, describe its connections with other residuals, ranks and ridits, and demonstrate its use in model diagnostics."
"10.1093/biomet/ass010","2012","Empirical bootstrap bias correction and estimation of prediction mean square error in small area estimation","0","We develop a method for bias correction, which models the error of the target estimator as a function of the corresponding estimator obtained from bootstrap samples, and the original estimators and bootstrap estimators of the parameters governing the model fitted to the sample data. This is achieved by considering a number of plausible parameter values, generating a pseudo original sample for each parameter and bootstrap samples for each such sample, and then searching for an appropriate functional relationship. Under certain conditions, the procedure also permits estimation of the mean square error of the bias corrected estimator. The method is applied for estimating the prediction mean square error in small area estimation of proportions under a generalized mixed model. Empirical comparisons with jackknife and bootstrap methods are presented."
"10.1093/biomet/ass013","2012","Improved double-robust estimation in missing data and causal inference models","0","Recently proposed double-robust estimators for a population mean from incomplete data and for a finite number of counterfactual means can have much higher efficiency than the usual double-robust estimators under misspecification of the outcome model. In this paper, we derive a new class of double-robust estimators for the parameters of regression models with incomplete cross-sectional or longitudinal data, and of marginal structural mean models for cross-sectional data with similar efficiency properties. Unlike the recent proposals, our estimators solve outcome regression estimating equations. In a simulation study, the new estimator shows improvements in variance relative to the standard double-robust estimator that are in agreement with those suggested by asymptotic theory."
"10.1093/biomet/ass007","2012","Multiple imputation in quantile regression","0","We propose a multiple imputation estimator for parameter estimation in a quantile regression model when some covariates are missing at random. The estimation procedure fully utilizes the entire dataset to achieve increased efficiency, and the resulting coefficient estimators are root-n consistent and asymptotically normal. To protect against possible model misspecification, we further propose a shrinkage estimator, which automatically adjusts for possible bias. The finite sample performance of our estimator is investigated in a simulation study. Finally, we apply our methodology to part of the Eating at American's Table Study data, investigating the association between two measures of dietary intake."
"10.1093/biomet/ass005","2012","Corrected-loss estimation for quantile regression with covariate measurement errors","0","We study estimation in quantile regression when covariates are measured with errors. Existing methods require stringent assumptions, such as spherically symmetric joint distribution of the regression and measurement error variables, or linearity of all quantile functions, which restrict model flexibility and complicate computation. In this paper, we develop a new estimation approach based on corrected scores to account for a class of covariate measurement errors in quantile regression. The proposed method is simple to implement. Its validity requires only linearity of the particular quantile function of interest, and it requires no parametric assumptions on the regression error distributions. Finite-sample results demonstrate that the proposed estimators are more efficient than the existing methods in various models considered."
"10.1093/biomet/ass004","2012","Nonparametric inference for assessing treatment efficacy in randomized clinical trials with a time-to-event outcome and all-or-none compliance","0","To evaluate the biological efficacy of a treatment in a randomized clinical trial, one needs to compare patients in the treatment arm who actually received treatment with the subgroup of patients in the control arm who would have received treatment had they been randomized into the treatment arm. In practice, subgroup membership in the control arm is usually unobservable. This paper develops a nonparametric inference procedure to compare subgroup probabilities with right-censored time-to-event data and unobservable subgroup membership in the control arm. We also present a procedure to estimate the onset and duration of treatment effect. The performance of our method is evaluated by simulation. An illustration is given using a randomized clinical trial for melanoma."
"10.1093/biomet/asr081","2012","Efficient estimation for the {C}ox model with varying coefficients","0","A proportional hazards model with varying coefficients allows one to examine the extent to which covariates interact nonlinearly with an exposure variable. A global partial likelihood method, in contrast with the local partial likelihood method of Fan et al. (2006), is proposed for estimation of varying coefficient functions. The proposed estimators are proved to be consistent and asymptotically normal. Semiparametric efficiency of the estimators is demonstrated in terms of their linear functionals. Evidence in support of the superiority of the method is presented in numerical studies and real examples."
"10.1093/biomet/ass008","2012","Likelihood approaches for the invariant density ratio model with biased-sampling data","0","The full likelihood approach in statistical analysis is regarded as the most efficient means for estimation and inference. For complex length-biased failure time data, computational algorithms and theoretical properties are not readily available, especially when a likelihood function involves infinite-dimensional parameters. Relying on the invariance property of length-biased failure time data under the semiparametric density ratio model, we present two likelihood approaches for the estimation and assessment of the difference between two survival distributions. The most efficient maximum likelihood estimators are obtained by the em algorithm and profile likelihood. We also provide a simple numerical method for estimation and inference based on conditional likelihood, which can be generalized to k-arm settings. Unlike conventional survival data, the mean of the population failure times can be consistently estimated given right-censored length-biased data under mild regularity conditions. To check the semiparametric density ratio model assumption, we use a test statistic based on the area between two survival distributions. Simulation studies confirm that the full likelihood estimators are more efficient than the conditional likelihood estimators. We analyse an epidemiological study to illustrate the proposed methods."
"10.1093/biomet/ass009","2012","Analysing bivariate survival data with interval sampling and application to cancer epidemiology","0","In biomedical studies, ordered bivariate survival data are frequently encountered when bivariate failure events are used as outcomes to identify the progression of a disease. In cancer studies, interest could be focused on bivariate failure times, for example, time from birth to cancer onset and time from cancer onset to death. This paper considers a sampling scheme, termed interval sampling, in which the first failure event is identified within a calendar time interval, the time of the initiating event can be retrospectively confirmed and the occurrence of the second failure event is observed subject to right censoring. In a cancer data application, the initiating, first and second events could correspond to birth, cancer onset and death. The fact that the data are collected conditional on the first failure event occurring within a time interval induces bias. Interval sampling is widely used for collection of disease registry data by governments and medical institutions, though the interval sampling bias is frequently overlooked by researchers. This paper develops statistical methods for analysing such data. Semiparametric methods are proposed under semi-stationarity and stationarity. Numerical studies demonstrate that the proposed estimation approaches perform well with moderate sample sizes. We apply the proposed methods to ovarian cancer registry data."
"10.1093/biomet/ass006","2012","Pointwise nonparametric maximum likelihood estimator of stochastically ordered survivor functions","0","In this paper, we consider estimation of survivor functions from groups of observations with right-censored data when the groups are subject to a stochastic ordering constraint. Many methods and algorithms have been proposed to estimate distribution functions under such restrictions, but none have completely satisfactory properties when the observations are censored. We propose a pointwise constrained nonparametric maximum likelihood estimator, which is defined at each time t by the estimates of the survivor functions subject to constraints applied at time t only. We also propose an efficient method to obtain the estimator. The estimator of each constrained survivor function is shown to be nonincreasing in t, and its consistency and asymptotic distribution are established. A simulation study suggests better small and large sample properties than for alternative estimators. An example using prostate cancer data illustrates the method."
"10.1093/biomet/asr084","2012","Global optimality of nonconvex penalized estimators","0","Nonconvex penalties such as the smoothly clipped absolute deviation or minimax concave penalties have desirable properties such as the oracle property, even when the dimension of the predictive variables is large. However, checking whether a given local minimizer has such properties is not easy since there can be many local minimizers. In this paper, we give sufficient conditions under which a local minimizer is unique, and show that the oracle estimator becomes the unique local minimizer with probability tending to one."
"10.1093/biomet/ass003","2012","Componentwise classification and clustering of functional data","0","The infinite dimension of functional data can challenge conventional methods for classification and clustering. A variety of techniques have been introduced to address this problem, particularly in the case of prediction, but the structural models that they involve can be too inaccurate, or too abstract, or too difficult to interpret, for practitioners. In this paper, we develop approaches to adaptively choose components, enabling classification and clustering to be reduced to finite-dimensional problems. We explore and discuss properties of these methodologies. Our techniques involve methods for estimating classifier error rate and cluster tightness, and for choosing both the number of components, and their locations, to optimize these quantities. A major attraction of this approach is that it allows identification of parts of the function domain that convey important information for classification and clustering. It also permits us to determine regions that are relevant to one of these analyses but not the other."
"10.1093/biomet/asr085","2012","Doubly misspecified models","0","Estimation bias arising from local model uncertainty and incomplete data has been studied by Copas & Eguchi (2005) under the assumption of a correctly specified marginal model. We extend the approach to allow additional local uncertainty in the assumed marginal model, arguing that this is almost unavoidable for nonlinear problems. We present a general bias analysis and sensitivity procedure for such doubly misspecified models and illustrate the breadth of application through three examples: logistic regression with a missing confounder, measurement error for binary responses and survival analysis with frailty. We show that a double-the-variance rule is not conservative under double misspecification. The ideas are brought together in a meta-analysis of studies of rehabilitation rates for juvenile offenders."
"10.1093/biomet/asr053","2012","Stochastic blockmodels with a growing number of classes","0","We present asymptotic and finite-sample results on the use of stochastic blockmodels for the analysis of network data. We show that the fraction of misclassified network nodes converges in probability to zero under maximum likelihood fitting when the number of classes is allowed to grow as the root of the network size and the average network degree grows at least poly-logarithmically in this size. We also establish finite-sample confidence bounds on maximum-likelihood blockmodel parameter estimates from data comprising independent Bernoulli random variates; these results hold uniformly over class assignment. We provide simulations verifying the conditions sufficient for our results, and conclude by fitting a logit parameterization of a stochastic blockmodel with covariates to a network data example comprising self-reported school friendships, resulting in block estimates that reveal residual structure."
"10.1093/biomet/asr080","2012","Dependence modelling for spatial extremes","0","Current dependence models for spatial extremes are based upon max-stable processes. Within this class, there are few inferentially viable models available, and we propose one further model. More problematic are the restrictive assumptions that must be made when using max-stable processes to model dependence for spatial extremes: it must be assumed that the dependence structure of the observed extremes is compatible with a limiting model that holds for all events more extreme than those that have already occurred. This problem has long been acknowledged in the context of finite-dimensional multivariate extremes, in particular when data display dependence at observable levels, but are independent in the limit. We propose a flexible class of models that is suitable for such data in a spatial context. In addition, we consider the situation where the extremal dependence structure may vary with distance. We apply our models to spatially referenced significant wave height data from the North Sea, finding evidence that their extremal structure is not compatible with a limiting dependence model."
"10.1093/biomet/asr064","2012","Optimality of group testing in the presence of misclassification","0","Several optimality properties of Dorfman's (1943) group testing procedure are derived for estimation of the prevalence of a rare disease whose status is classified with error. Exact ranges of disease prevalence are obtained for which group testing provides more efficient estimation when group size increases."
"10.1093/biomet/asr061","2012","On robust estimation via pseudo-additive information","0","We consider a robust parameter estimator minimizing an empirical approximation to the q-entropy and show its relationship to minimization of power divergences through a simple parameter transformation. The estimator balances robustness and efficiency through a tuning constant q and avoids kernel density smoothing. We derive an upper bound to the estimator mean squared error under a contaminated reference model and use it as a min-max criterion for selecting q."
"10.1093/biomet/asr083","2012","Estimating overdispersion when fitting a generalized linear model to sparse data","0","We consider the problem of fitting a generalized linear model to overdispersed data, focussing on a quasilikelihood approach in which the variance is assumed to be proportional to that specified by the model, and the constant of proportionality, phi, is used to obtain appropriate standard errors and model comparisons. It is common practice to base an estimate of phi on Pearson's lack-of-fit statistic, with or without Farrington's modification. We propose a new estimator that has a smaller variance, subject to a condition on the third moment of the response variable. We conjecture that this condition is likely to be achieved for the important special cases of count and binomial data. We illustrate the benefits of the new estimator using simulations for both count and binomial data."
"10.1093/biomet/asr075","2012","Proportional likelihood ratio models for mean regression","1","The proportional likelihood ratio model introduced in Luo & Tsai (2012) is adapted to explicitly model the means of observations. This is useful for the estimation of and inference on treatment effects, particularly in designed experiments and allows the data analyst greater control over model specification and parameter interpretation."
"10.1093/biomet/asr060","2012","A proportional likelihood ratio model","1","We propose a semiparametric proportional likelihood ratio model which is particularly suitable for modelling a nonlinear monotonic relationship between the outcome variable and a covariate. This model extends the generalized linear model by leaving the distribution unspecified, and has a strong connection with semiparametric models such as the selection bias model (Gilbert et al., 1999), the density ratio model (Qin, 1998; Fokianos & Kaimi, 2006), the single-index model (Ichimura, 1993) and the exponential tilt regression model (Rathouz & Gao, 2009). A maximum likelihood estimator is obtained for the new model and its asymptotic properties are derived. An example and simulation study illustrate the use of the model."
"10.1093/biomet/asr072","2012","A maximum pseudo-profile likelihood estimator for the {C}ox model under length-biased sampling","0","This paper considers semiparametric estimation of the Cox proportional hazards model for right-censored and length-biased data arising from prevalent sampling. To exploit the special structure of length-biased sampling, we propose a maximum pseudo-profile likelihood estimator, which can handle time-dependent covariates and is consistent under covariate-dependent censoring. Simulation studies show that the proposed estimator is more efficient than its competitors. A data analysis illustrates the methods and theory."
"10.1093/biomet/asr065","2012","Mean residual life models with time-dependent coefficients under right censoring","0","The mean residual life provides the remaining life expectancy of a subject who has survived to a certain time-point. When covariates are present, regression models are needed to study the association between the mean residual life function and potential regression covariates. In this paper, we propose a flexible class of semiparametric mean residual life models where some effects may be time-varying and some may be constant over time. In the presence of right censoring, we use the inverse probability of censoring weighting approach and develop inference procedures for estimating the model parameters. In addition, we provide graphical and numerical methods for model checking and tests for examining whether or not the covariate effects vary with time. Asymptotic and finite sample properties of the proposed estimators are established and the approach is applied to real life datasets collected from clinical trials."
"10.1093/biomet/asr062","2012","Estimating treatment effects with treatment switching via semicompeting risks models: an application to a colorectal cancer study","0","Treatment switching is a frequent occurrence in clinical trials, where, during the course of the trial, patients who fail on the control treatment may change to the experimental treatment. Analysing the data without accounting for switching yields highly biased and inefficient estimates of the treatment effect. In this paper, we propose a novel class of semiparametric semicompeting risks transition survival models to accommodate treatment switches. Theoretical properties of the proposed model are examined and an efficient expectation-maximization algorithm is derived for obtaining the maximum likelihood estimates. Simulation studies are conducted to demonstrate the superiority of the model compared with the intent-to-treat analysis and other methods proposed in the literature. The proposed method is applied to data from a colorectal cancer clinical trial."
"10.1093/biomet/asr076","2012","A functional generalized method of moments approach for longitudinal studies with missing responses and covariate measurement error","0","Covariate measurement error and missing responses are typical features in longitudinal data analysis. There has been extensive research on either covariate measurement error or missing responses, but relatively little work has been done to address both simultaneously. In this paper, we propose a simple method for the marginal analysis of longitudinal data with time-varying covariates, some of which are measured with error, while the response is subject to missingness. Our method has a number of appealing properties: assumptions on the model are minimal, with none needed about the distribution of the mismeasured covariate; implementation is straightforward and its applicability is broad. We provide both theoretical justification and numerical results."
"10.1093/biomet/asr068","2012","A moving average {C}holesky factor model in covariance modelling for longitudinal data","0","We propose new regression models for parameterizing covariance structures in longitudinal data analysis. Using a novel Cholesky factor, the entries in this decomposition have a moving average and log-innovation interpretation and are modelled as linear functions of covariates. We propose efficient maximum likelihood estimates for joint mean-covariance analysis based on this decomposition and derive the asymptotic distributions of the coefficient estimates. Furthermore, we study a local search algorithm, computationally more efficient than traditional all subset selection, based on bic for model selection, and show its model selection consistency. Thus, a conjecture of Pan & MacKenzie (2003) is verified. We demonstrate the finite-sample performance of the method via analysis of data on CD4 trajectories and through simulations."
"10.1093/biomet/asr067","2012","Bayesian analysis of multistate event history data: beta-{D}irichlet process prior","0","Bayesian analysis of a finite state Markov process, which is popularly used to model multistate event history data, is considered. A new prior process, called a beta-Dirichlet process, is introduced for the cumulative intensity functions and is proved to be conjugate. In addition, the beta-Dirichlet prior is applied to a Bayesian semiparametric regression model. To illustrate the application of the proposed model, we analyse a dataset of credit histories."
"10.1093/biomet/asr059","2012","Directed acyclic graphs with edge-specific bounds","0","We give a definition of a bounded edge within the causal directed acyclic graph framework. A bounded edge generalizes the notion of a signed edge and is defined in terms of bounds on a ratio of survivor probabilities. We derive rules concerning the propagation of bounds. Bounds on causal effects in the presence of unmeasured confounding are also derived using bounds related to specific edges on a graph. We illustrate the theory developed by an example concerning estimating the effect of antihistamine treatment on asthma in the presence of unmeasured confounding."
"10.1093/biomet/asr077","2012","Optimal allocation to maximize the power of two-sample tests for binary response","0","We study allocations that maximize the power of tests of equality of two treatments having binary outcomes. When a normal approximation applies, the asymptotic power is maximized by minimizing the variance, leading to a Neyman allocation that assigns observations in proportion to the standard deviations. This allocation, which in general requires knowledge of the parameters of the problem, is recommended in a large body of literature. Under contiguous alternatives the normal approximation indeed applies, and in this case the Neyman allocation reduces to a balanced design. However, when studying the power under a noncontiguous alternative, a large deviations approximation is needed, and the Neyman allocation is no longer asymptotically optimal. In the latter case, the optimal allocation depends on the parameters, but is rather close to a balanced design. Thus, a balanced design is a viable option for both contiguous and noncontiguous alternatives. Finite sample studies show that a balanced design is indeed generally quite close to being optimal for power maximization. This is good news as implementation of a balanced design does not require knowledge of the parameters."
"10.1093/biomet/asr063","2012","Combining data from two independent surveys: a model-assisted approach","0","Combining information from two or more independent surveys is a problem frequently encountered in survey sampling. We consider the case of two independent surveys, where a large sample from survey 1 collects only auxiliary information and a much smaller sample from survey 2 provides information on both the variables of interest and the auxiliary variables. We propose a model-assisted projection method of estimation based on a working model, but the reference distribution is design-based. We generate synthetic or proxy values of a variable of interest by first fitting the working model, relating the variable of interest to the auxiliary variables, to the data from survey 2 and then predicting the variable of interest associated with the auxiliary variables observed in survey 1. The projection estimator of a total is simply obtained from the survey 1 weights and associated synthetic values. We identify the conditions for the projection estimator to be asymptotically unbiased. Domain estimation using the projection method is also considered. Replication variance estimators are obtained by augmenting the synthetic data file for survey 1 with additional synthetic columns associated with the columns of replicate weights. Results from a simulation study are presented."
"10.1093/biomet/asr071","2012","Optimal fractions of two-level factorials under a baseline parameterization","0","Two-level fractional factorial designs are considered under a baseline parameterization. The criterion of minimum aberration is formulated in this context and optimal designs under this criterion are investigated. The underlying theory and the concept of isomorphism turn out to be significantly different from their counterparts under orthogonal parameterization, and this is reflected in the optimal designs obtained."
"10.1093/biomet/asr079","2012","Conservative hypothesis tests and confidence intervals using importance sampling","0","Importance sampling is a common technique for Monte Carlo approximation, including that of p-values. Here it is shown that a simple correction of the usual importance sampling p-values provides valid p-values, meaning that a hypothesis test created by rejecting the null hypothesis when the p-value is at most alpha will also have a Type I error rate of at most alpha. This correction uses the importance weight of the original observation, which gives valuable diagnostic information under the null hypothesis. Using the corrected p-values can be crucial for multiple testing and also in problems where evaluating the accuracy of importance sampling approximations is difficult. Inverting the corrected p-values provides a useful way to create Monte Carlo confidence intervals that maintain the nominal significance level and use only a single Monte Carlo sample."
"10.1093/biomet/asr078","2012","Modelling the distribution of the cluster maxima of exceedances of subasymptotic thresholds","0","A standard approach to model the extreme values of a stationary process is the peaks over threshold method, which consists of imposing a high threshold, identifying clusters of exceedances of this threshold and fitting the maximum value from each cluster using the generalized Pareto distribution. This approach is strongly justified by underlying asymptotic theory. We propose an alternative model for the distribution of the cluster maxima that accounts for the subasymptotic theory of extremes of a stationary process. This new distribution is a product of two terms, one for the marginal distribution of exceedances and the other for the dependence structure of the exceedance values within a cluster. We illustrate the improvement in fit, measured by the root mean square error of the estimated quantiles, offered by the new distribution over the peaks over thresholds analysis using simulated and hydrological data, and we suggest a diagnostic tool to help identify when the proposed model is likely to lead to an improved fit."
"10.1093/biomet/asr066","2012","A direct approach to sparse discriminant analysis in ultra-high dimensions","0","Sparse discriminant methods based on independence rules, such as the nearest shrunken centroids classifier (Tibshirani et al., 2002) and features annealed independence rules (Fan & Fan, 2008), have been proposed as computationally attractive tools for feature selection and classification with high-dimensional data. A fundamental drawback of these rules is that they ignore correlations among features and thus could produce misleading feature selection and inferior classification. We propose a new procedure for sparse discriminant analysis, motivated by the least squares formulation of linear discriminant analysis. To demonstrate our proposal, we study the numerical and theoretical properties of discriminant analysis constructed via lasso penalized least squares. Our theory shows that the method proposed can consistently identify the subset of discriminative features contributing to the Bayes rule and at the same time consistently estimate the Bayes classification direction, even when the dimension can grow faster than any polynomial order of the sample size. The theory allows for general dependence among features. Simulated and real data examples show that lassoed discriminant analysis compares favourably with other popular sparse discriminant proposals."
"10.1093/biomet/asr074","2012","Factor profiled sure independence screening","0","We propose a method of factor profiled sure independence screening for ultrahigh-dimensional variable selection. The objective of this method is to identify nonzero components consistently from a sparse coefficient vector. The new method assumes that the correlation structure of the high-dimensional data can be well represented by a set of low-dimensional latent factors, which can be estimated consistently by eigenvalue-eigenvector decomposition. The estimated latent factors should then be profiled out from both the response and the predictors. Such an operation, referred to as factor profiling, produces uncorrelated predictors. Therefore, sure independence screening can be applied subsequently and the resulting screening result is consistent for model selection, a major advantage that standard sure independence screening does not share. We refer to the new method as factor profiled sure independence screening. Numerical studies confirm its outstanding performance."
"10.1093/biomet/asr046","2012","Studies in the history of probability and statistics, {L}: {K}arl {P}earson and the rule of three","0","Karl Pearson's role in the transformation that took the 19th century statistics of Laplace and Gauss into the modern era of 20th century multivariate analysis is examined from a new point of view. By viewing Pearson's work in the context of a motto he adopted from Charles Darwin, a philosophical theme is identified in Pearson's statistical work, and his three major achievements are briefly described."
"10.1093/biomet/asr044","2011","Generalized linear time series regression","0","We consider a cross-section model that contains an individual component, a deterministic time trend and an unobserved latent common time series component. We show the following oracle property: the parameters of the latent time series and the parameters of the deterministic time trend can be estimated with the same asymptotic accuracy as if the parameters of the individual component were known. We consider this model in two settings: least squares fits of linear specifications of the individual component and the parameters of the deterministic time trend and, more generally, quasilikelihood estimation in a generalized linear time series model."
"10.1093/biomet/asr050","2011","Empirical likelihood and quantile regression in longitudinal data analysis","0","We propose a novel quantile regression approach for longitudinal data analysis which naturally incorporates auxiliary information from the conditional mean model to account for within-subject correlations. The efficiency gain is quantified theoretically and demonstrated empirically via simulation studies and the analysis of a real dataset."
"10.1093/biomet/asr052","2011","Wild bootstrap for quantile regression","0","The existing theory of the wild bootstrap has focused on linear estimators. In this note, we broaden its validity by providing a class of weight distributions that is asymptotically valid for quantile regression estimators. As most weight distributions in the literature lead to biased variance estimates for nonlinear estimators of linear regression, we propose a modification of the wild bootstrap that admits a broader class of weight distributions for quantile regression. A simulation study on median regression is carried out to compare various bootstrap methods. With a simple finite-sample correction, the wild bootstrap is shown to account for general forms of heteroscedasticity in a regression model with fixed design points."
"10.1093/biomet/asr040","2011","Likelihood analysis of the binary instrumental variable model","0","Instrumental variables are widely used for the identification of the causal effect of one random variable on another under unobserved confounding. The distribution of the observable variables for a discrete instrumental variable model satisfies certain inequalities but no conditional independence relations. Such models are usually tested by checking whether the relative frequency estimators of the parameters satisfy the constraints. This ignores sampling uncertainty in the data. Using the observable constraints for the instrumental variable model, a likelihood analysis is conducted. A significance test for its validity is developed, and a bootstrap algorithm for computing confidence intervals for the causal effect is proposed. Applications are given to illustrate the advantage of the suggested approach."
"10.1093/biomet/asr057","2011","False discovery rate for scanning statistics","0","The false discovery rate is a criterion for controlling Type I error in simultaneous testing of multiple hypotheses. For scanning statistics, due to local dependence, clusters of neighbouring hypotheses are likely to be rejected together. In such situations, it is more intuitive and informative to group neighbouring rejections together and count them as a single discovery, with the false discovery rate defined as the proportion of clusters that are falsely declared among all declared clusters. Assuming that the number of false discoveries, under this broader definition of a discovery, is approximately Poisson and independent of the number of true discoveries, we examine approaches for estimating and controlling the false discovery rate, and provide examples from biological applications."
"10.1093/biomet/asr042","2011","Blocking, efficiency and weighted optimality","0","Optimal blocking is explored for experiments, such as those incorporating one or more controls, where not all treatment comparisons are of equal interest. Weighted optimality functions are employed in gaining both analytic and enumerative results; a catalogue of smaller optimal designs is provided. It is shown how design selection based on functions of variances, and on functions of efficiency factors, are both subsumed by the weighted approach."
"10.1093/biomet/asr058","2011","Inverse probability weighting for clustered nonresponse","0","Correlated nonresponse within clusters arises in certain survey settings. It is often represented by a random effects model and assumed to be cluster-specific nonignorable, in the sense that survey and nonresponse outcomes are conditionally independent given cluster-level random effects. Two basic forms of inverse probability weights are considered: response propensity weights based on a marginal model, and weights based on predicted random effects. It is shown that both approaches can lead to biased estimation under cluster-specific nonignorable nonresponse, when the cluster sample sizes are small. We propose a new form of weighted estimator based upon conditional logistic regression, which can avoid this bias. An associated estimator of variance and an extension to observational studies with clustered treatment assignment are also described. Properties of the alternative estimators are illustrated in a small simulation study."
"10.1093/biomet/asr037","2011","Elliptical graphical modelling","0","We propose elliptical graphical models based on conditional uncorrelatedness as a robust generalization of Gaussian graphical models. Letting the population distribution be elliptical instead of normal allows the fitting of data with arbitrarily heavy tails. We study the class of proportionally affine equivariant scatter estimators and show how they can be used to perform elliptical graphical modelling. This leads to a new class of partial correlation estimators and analogues of the classical deviance test. General expressions for the asymptotic variance of partial correlation estimators, unconstrained and under decomposable models, are given, and the asymptotic chi square approximation for the pseudo-deviance test statistic is proved. The feasibility of our approach is demonstrated by a simulation study, using, among others, Tyler's scatter estimator, which is distribution-free within the elliptical model."
"10.1093/biomet/asr033","2011","Quantifying the failure of bootstrap likelihood ratio tests","0","When testing geometrically irregular parametric hypotheses, the bootstrap is an intuitively appealing method to circumvent difficult distribution theory. It has been shown, however, that the usual bootstrap is inconsistent in estimating the asymptotic distributions involved in such problems. This paper is concerned with the asymptotic size of likelihood ratio tests when critical values are computed using the inconsistent bootstrap. We clarify how the asymptotic size of such a test can be obtained from the size of the corresponding bootstrap test in the relevant limiting normal experiment. For boundary problems, that is, hypotheses given by convex cones, we show the bootstrap test to always be anticonservative, and we compute the size numerically for different two-dimensional examples. The examples illustrate that the size can be below or above the nominal level, and reveal that the relationship between the size of the test and the geometry of the considered hypotheses is surprisingly subtle."
"10.1093/biomet/asr048","2011","Estimation of latent factors for high-dimensional time series","2","This paper deals with the dimension reduction of high-dimensional time series based on a lower-dimensional factor process. In particular, we allow the dimension of time series N to be as large as, or even larger than, the length of observed time series T. The estimation of the factor loading matrix and the factor process itself is carried out via an eigenanalysis of a NxN non-negative definite matrix. We show that when all the factors are strong in the sense that the norm of each column in the factor loading matrix is of the order N(1/2), the estimator of the factor loading matrix is weakly consistent in L(2)-norm with the convergence rate independent of N. Thus the curse is cancelled out by the blessing of dimensionality. We also establish the asymptotic properties of the estimators when factors are not strong. The proposed method together with the asymptotic properties are illustrated in a simulation study. An application to an implied volatility data set, with a trading strategy derived from the fitted factor model, is also reported."
"10.1093/biomet/asr051","2011","Threshold estimation based on a {$p$}-value framework in dose-response and regression settings","0","We use p-values to identify the threshold level at which a regression function leaves its baseline value, a problem motivated by applications in toxicological and pharmacological dose-response studies and environmental statistics. We study the problem in two sampling settings: one where multiple responses can be obtained at a number of different covariate levels, and the other the standard regression setting involving limited number of response values at each covariate. Our procedure involves testing the hypothesis that the regression function is at its baseline at each covariate value and then computing the potentially approximate p-value of the test. An estimate of the threshold is obtained by fitting a piecewise constant function with a single jump discontinuity, known as a stump, to these observed p-values, as they behave in markedly different ways on the two sides of the threshold. The estimate is shown to be consistent and its finite sample properties are studied through simulations. Our approach is computationally simple and extends to the estimation of the baseline value of the regression function, heteroscedastic errors and to time series. It is illustrated on some real data applications."
"10.1093/biomet/asr036","2011","A goodness-of-fit test of logistic regression models for case-control data with measurement error","0","We study goodness-of-fit tests for logistic regression models for case-control data when some covariates are measured with error. We first study the applicability of traditional test methods for this problem, simply ignoring measurement error, and show that in some scenarios they are effective despite the inconsistency of the parameter estimators. We then develop a test procedure based on work of Zhang (2001) that can simultaneously test the validity of logistic regression and correct the bias in parameter estimators for case-control data with nondifferential classical additive normal measurement error. Instead of using the information matrix considered by Zhang (2001), our test statistic uses preselected functions to reduce dimensionality. Simulation studies and an application illustrate its usefulness."
"10.1093/biomet/asr041","2011","Covariate selection for the nonparametric estimation of an average treatment effect","0","Observational studies in which the effect of a nonrandomized treatment on an outcome of interest is estimated are common in domains such as labour economics and epidemiology. Such studies often rely on an assumption of unconfounded treatment when controlling for a given set of observed pre-treatment covariates. The choice of covariates to control in order to guarantee unconfoundedness should primarily be based on subject matter theories, although the latter typically give only partial guidance. It is tempting to include many covariates in the controlling set to try to make the assumption of an unconfounded treatment realistic. Including unnecessary covariates is suboptimal when the effect of a binary treatment is estimated nonparametrically. For instance, when using a n(1/2)-consistent estimator, a loss of efficiency may result from using covariates that are irrelevant for the unconfoundedness assumption. Moreover, bias may dominate the variance when many covariates are used. Embracing the Neyman-Rubin model typically used in conjunction with nonparametric estimators of treatment effects, we characterize subsets from the original reservoir of covariates that are minimal in the sense that the treatment ceases to be unconfounded given any proper subset of these minimal sets. These subsets of covariates are shown to be identified under mild assumptions. These results lead us to propose data-driven algorithms for the selection of minimal sets of covariates."
"10.1093/biomet/asr055","2011","Optimizing randomized trial designs to distinguish which subpopulations benefit from treatment","0","It is a challenge to evaluate experimental treatments where it is suspected that the treatment effect may only be strong for certain subpopulations, such as those having a high initial severity of disease, or those having a particular gene variant. Standard randomized controlled trials can have low power in such situations. They also are not optimized to distinguish which subpopulations benefit from a treatment. With the goal of overcoming these limitations, we consider randomized trial designs in which the criteria for patient enrollment may be changed, in a preplanned manner, based on interim analyses. Since such designs allow data-dependent changes to the population enrolled, care must be taken to ensure strong control of the familywise Type I error rate. Our main contribution is a general method for constructing randomized trial designs that allow changes to the population enrolled based on interim data using a prespecified decision rule, for which the asymptotic, familywise Type I error rate is strongly controlled at a specified level alpha. As a demonstration of our method, we prove new, sharp results for a simple, two-stage enrichment design. We then compare this design to fixed designs, focusing on each design's ability to determine the overall and subpopulation-specific treatment effects."
"10.1093/biomet/asr049","2011","The {A}alen additive gamma frailty hazards model","0","In this paper, we consider clustered right-censored time-to-event data. Such data can be analysed either using a marginal model if one is interested in population effects or using so-called frailty models if one is interested in covariate effects on the individual level and in estimation of correlation. The Cox frailty model has been studied extensively in the last decade or so and estimation techniques and large sample results are now available. It is, however, difficult to deal with time-changing covariate effects when using the Cox model. An appealing alternative model is the Aalen additive hazards model, in which it is easy to work with time dynamics. In this paper, we describe an innovative approach to estimation in the Aalen additive gamma frailty hazards model. We give the large sample properties of the estimators and investigate their small sample properties by Monte Carlo simulation. A real example is provided for illustration."
"10.1093/biomet/asr045","2011","Forward adaptive banding for estimating large covariance matrices","0","We propose a simple forward adaptive banding method for estimating large covariance matrices using the modified Cholesky decomposition. This approach requires the fitting of a prespecified set of models due to the adaptive banding structure and can be efficiently implemented. Aside from its computational attractiveness, we propose a novel Bayes information criterion that gives consistent model selection for estimating high dimensional covariance matrices. The method compares favourably to its competitors in simulation study."
"10.1093/biomet/asr054","2011","Sparse estimation of a covariance matrix","1","We suggest a method for estimating a covariance matrix on the basis of a sample of vectors drawn from a multivariate normal distribution. In particular, we penalize the likelihood with a lasso penalty on the entries of the covariance matrix. This penalty plays two important roles: it reduces the effective number of parameters, which is important even when the dimension of the vectors is smaller than the sample size since the number of parameters grows quadratically in the number of variables, and it produces an estimate which is sparse. In contrast to sparse inverse covariance estimation, our method's close relative, the sparsity attained here is in the covariance matrix itself rather than in the inverse matrix. Zeros in the covariance matrix correspond to marginal independencies; thus, our method performs model selection while providing a positive definite estimate of the covariance. The proposed penalized maximum likelihood problem is not convex, so we use a majorize-minimize approach in which we iteratively solve convex approximations to the original nonconvex problem. We discuss tuning parameter selection and demonstrate on a flow-cytometry dataset how our method produces an interpretable graphical display of the relationship between variables. We perform simulations that suggest that simple elementwise thresholding of the empirical covariance matrix is competitive with our method for identifying the sparsity structure. Additionally, we show how our method can be used to solve a previously studied special case in which a desired sparsity pattern is prespecified."
"10.1093/biomet/asr043","2011","Square-root lasso: pivotal recovery of sparse signals via conic programming","0","We propose a pivotal method for estimating high-dimensional sparse linear regression models, where the overall number of regressors p is large, possibly much larger than n, but only s regressors are significant. The method is a modification of the lasso, called the square-root lasso. The method is pivotal in that it neither relies on the knowledge of the standard deviation Sigma nor does it need to pre-estimate Sigma. Moreover, the method does not rely on normality or sub-Gaussianity of noise. It achieves near-oracle performance, attaining the convergence rate Sigma{(s/n) log p}(1/2) in the prediction norm, and thus matching the performance of the lasso with known Sigma. These performance results are valid for both Gaussian and non-Gaussian errors, under some mild moment restrictions. We formulate the square-root lasso as a solution to a convex conic programming problem, which allows us to implement the estimator using efficient algorithmic methods, such as interior-point and first-order methods."
"10.1093/biomet/asr056","2011","Nonparametric estimation of the variogram and its spectrum","0","In the study of intrinsically stationary spatial processes, a new nonparametric variogram estimator is proposed through its spectral representation. The methodology is based on estimation of the variogram's spectrum by solving a regularized inverse problem through quadratic programming. The estimated variogram is guaranteed to be conditionally negative-definite. Simulation shows that our estimator is flexible and generally has smaller mean integrated squared error than the parametric estimator under model misspecification. Our methodology is applied to a spatial dataset of decadal temperature changes."
"10.1093/biomet/asr047","2011","Non-{G}aussian spatiotemporal modelling through scale mixing","0","We construct non-Gaussian processes that vary continuously in space and time with nonseparable covariance functions. Starting from a general and flexible way of constructing valid nonseparable covariance functions through mixing over separable covariance functions, the resulting models are generalized by allowing for outliers as well as regions with larger variances. We induce this through scale mixing with separate positive-valued processes. Smooth mixing processes are applied to the underlying correlated processes in space and in time, thus leading to regions in space and time of increased spread. An uncorrelated mixing process on the nugget effect accommodates outliers. Posterior and predictive Bayesian inference with these models is implemented through a Markov chain Monte Carlo sampler. An application to temperature data in the Basque country illustrates the potential of this model in the identification of outliers and regions with inflated variance, and shows that this improves the predictive performance."
"10.1093/biomet/asr026","2011","Multinomial logit bias reduction via the {P}oisson log-linear model","0","For the parameters of a multinomial logistic regression, it is shown how to obtain the bias-reducing penalized maximum likelihood estimator by using the equivalent Poisson log-linear model. The calculation needed is not simply an application of the Jeffreys prior penalty to the Poisson model. The development allows a simple and computationally efficient implementation of the reduced-bias estimator, using standard software for generalized linear models."
"10.1093/biomet/asr027","2011","On protected estimation of an odds ratio model with missing binary exposure and confounders","0","We describe an estimator of the parameter indexing a model for the conditional odds ratio between a binary exposure and a binary outcome given a high-dimensional vector of confounders, when the exposure and a subset of the confounders are missing, not necessarily simultaneously, in a subsample. We argue that a recently proposed estimator restricted to complete-cases confers more protection to model misspecification than existing ones in the sense that the set of data laws under which it is consistent strictly contains each set of data laws under which each of the previous estimators are consistent."
"10.1093/biomet/asr014","2011","Construction of {$\phi_p$}-optimal exact designs with minimum experimental run size for a linear log contrast model in mixture experiments","0","We propose a new method with minimum experimental run size using the properties of Hadamard matrices through which some phi(p)-optimal exact designs including A-, D- and E-optimal designs are constructed for a linear log contrast model in mixture experiments."
"10.1093/biomet/asr022","2011","Robust designs through partially clear two-factor interactions","1","Orthogonal arrays with clear two-factor interactions provide a class of designs that are robust to nonnegligible effects. If certain prior knowledge is available, then robust designs allow additional factors to be studied. This is done through partially clear two-factor interactions. We study the existence and construction of such robust designs and present an upper bound on the maximum number of clear two-factor interactions."
"10.1093/biomet/asr028","2011","Nested orthogonal array-based {L}atin hypercube designs","0","We propose two methods for constructing a new type of design, called a nested orthogonal array-based Latin hypercube design, intended for multi-fidelity computer experiments. Such designs are two nested space-filling designs in which the large design achieves stratification in both bivariate and univariate margins and the small design achieves stratification in univariate margins. These designs have better space-filling properties than nested Latin hypercube designs in which the large design possesses uniformity in univariate margins only. The first method expands an ordinary Latin hypercube design to a larger design that achieves uniformity in any one- or two-dimensional projection. The second method uses an orthogonal array with strength two to simultaneously construct a pair of nested orthogonal array-based Latin hypercube designs. Examples are given to illustrate the proposed methods. Sampling properties of the proposed designs are derived."
"10.1093/biomet/asr024","2011","Sudoku-based space-filling designs","0","Sudoku is played by millions of people across the globe. It has simple rules and is very addictive. The game board is a nine-by-nine grid of numbers from one to nine. Several entries within the grid are provided and the remaining entries must be filled in subject to no row, column, or three-by-three subsquare containing duplicate numbers. By exploiting these three types of uniformity, we propose an approach to constructing a new type of design, called a Sudoku-based space-filling design. Such a design can be divided into groups of subdesigns so that the complete design and each subdesign achieve maximum uniformity in univariate and bivariate margins. Examples are given illustrating the proposed construction method. Applications of such designs include computer experiments with qualitative and quantitative factors, linking parameters in engineering and crossvalidation."
"10.1093/biomet/asr031","2011","Generalized varying coefficient models with unknown link function","0","We propose a new estimation method for generalized varying coefficient models where the link function is specified up to some smoothness conditions. Consistency and asymptotic normality of the estimated varying coefficient functions are established. Simulation results and a real data application demonstrate the usefulness of the new method."
"10.1093/biomet/asr023","2011","Conditional {A}kaike information under generalized linear and proportional hazards mixed models","0","We study model selection for clustered data, when the focus is on cluster specific inference. Such data are often modelled using random effects, and conditional Akaike information was proposed in Vaida & Blanchard (2005) and used to derive an information criterion under linear mixed models. Here we extend the approach to generalized linear and proportional hazards mixed models. Outside the normal linear mixed models, exact calculations are not available and we resort to asymptotic approximations. In the presence of nuisance parameters, a profile conditional Akaike information is proposed. Bootstrap methods are considered for their potential advantage in finite samples. Simulations show that the performance of the bootstrap and the analytic criteria are comparable, with bootstrap demonstrating some advantages for larger cluster sizes. The proposed criteria are applied to two cancer datasets to select models when the cluster-specific inference is of interest."
"10.1093/biomet/asr007","2011","Efficient restricted estimators for conditional mean models with missing data","0","Consider a conditional mean model with missing data on the response or explanatory variables due to two-phase sampling or nonresponse. Robins et al. (1994) introduced a class of augmented inverse-probability-weighted estimators, depending on a vector of functions of explanatory variables and a vector of functions of coarsened data. Tsiatis (2006) studied two classes of restricted estimators, class 1 with both vectors restricted to finite-dimensional linear subspaces and class 2 with the first vector of functions restricted to a finite-dimensional linear subspace. We introduce a third class of restricted estimators, class 3, with the second vector of functions restricted to a finite-dimensional subspace. We derive a new estimator, which is asymptotically optimal in class 1, by the methods of nonparametric and empirical likelihood. We propose a hybrid strategy to obtain estimators that are asymptotically optimal in class 1 and locally optimal in class 2 or class 3. The advantages of the hybrid, likelihood estimator based on classes 1 and 3 are shown in a simulation study and a real-data example."
"10.1093/biomet/asr035","2011","Marginal methods for correlated binary data with misclassified responses","0","Misclassification is a longstanding concern in medical research. Although there has been much research concerning error-prone covariates, relatively little work has been directed to problems with response variables subject to error. In this paper we focus on misclassification in clustered or longitudinal outcomes. We propose marginal analysis methods to handle binary responses which are subject to misclassification. The proposed methods have several appealing features, including simultaneous inference for both marginal mean and association parameters, and they can handle misclassified responses for a number of practical scenarios, such as the case with a validation subsample or replicates. Furthermore, the proposed methods are robust to model misspecification in a sense that no full distributional assumptions are required. Numerical studies demonstrate satisfactory performance of the proposed methods under a variety of settings."
"10.1093/biomet/asr034","2011","A construction principle for multivariate extreme value distributions","0","We present a construction principle for the spectral density of a multivariate extreme value distribution. It generalizes the pairwise beta model introduced in the literature recently and may be used to obtain new parametric models from lower dimensional spectral densities. We illustrate the flexibility of this new class of models and apply it to a wind speed dataset."
"10.1093/biomet/asr029","2011","Aggregation-cokriging for highly multivariate spatial data","1","Best linear unbiased prediction of spatially correlated multivariate random processes, often called cokriging in geostatistics, requires the solution of a large linear system based on the covariance and cross-covariance matrix of the observations. For many problems of practical interest, it is impossible to solve the linear system with direct methods. We propose an efficient linear unbiased predictor based on a linear aggregation of the covariables. The primary variable together with this single meta-covariable is used to perform cokriging. We discuss the optimality of the approach under different covariance structures, and use it to create reanalysis type high-resolution historical temperature fields."
"10.1093/biomet/asr017","2011","Testing parametric assumptions of trends of a nonstationary time series","0","The paper considers testing whether the mean trend of a nonstationary time series is of certain parametric forms. A central limit theorem for the integrated squared error is derived, and a hypothesis-testing procedure is proposed. The method is illustrated in a simulation study, and is applied to assess the mean pattern of lifetime-maximum wind speeds of global tropical cyclones from 1981 to 2006. We also revisit the trend pattern in the central England temperature series."
"10.1093/biomet/asr032","2011","Functional mixed effects spectral analysis","0","In many experiments, time series data can be collected from multiple units and multiple time series segments can be collected from the same unit. This article introduces a mixed effects Cramer spectral representation which can be used to model the effects of design covariates on the second-order power spectrum while accounting for potential correlations among the time series segments collected from the same unit. The transfer function is composed of a deterministic component to account for the population-average effects and a random component to account for the unit-specific deviations. The resulting log-spectrum has a functional mixed effects representation where both the fixed effects and random effects are functions in the frequency domain. It is shown that, when the replicate-specific spectra are smooth, the log-periodograms converge to a functional mixed effects model. A data-driven iterative estimation procedure is offered for the periodic smoothing spline estimation of the fixed effects, penalized estimation of the functional covariance of the random effects, and unit-specific random effects prediction via the best linear unbiased predictor."
"10.1093/biomet/asr030","2011","Semiparametric inference in mixture models with predictive recursion marginal likelihood","0","Predictive recursion is an accurate and computationally efficient algorithm for nonparametric estimation of mixing densities in mixture models. In semiparametric mixture models, however, the algorithm fails to account for any uncertainty in the additional unknown structural parameter. As an alternative to existing profile likelihood methods, we treat predictive recursion as a filter approximation by fitting a fully Bayes model, whereby an approximate marginal likelihood of the structural parameter emerges and can be used for inference. We call this the predictive recursion marginal likelihood. Convergence properties of predictive recursion under model misspecification also lead to an attractive construction of this new procedure. We show pointwise convergence of a normalized version of this marginal likelihood function. Simulations compare the performance of this new approach with that of existing profile likelihood methods and with Dirichlet process mixtures in density estimation. Mixed-effects models and an empirical Bayes multiple testing application in time series analysis are also considered."
"10.1093/biomet/asq082","2011","A class of mixtures of dependent tail-free processes","0","We propose a class of dependent processes in which density shape is regressed on one or more predictors through conditional tail-free probabilities by using transformed Gaussian processes. A particular linear version of the process is developed in detail. The resulting process is flexible and easy to fit using standard algorithms for generalized linear models. The method is applied to growth curve analysis, evolving univariate random effects distributions in generalized linear mixed models, and median survival modelling with censored data and covariate-dependent errors."
"10.1093/biomet/asr025","2011","Bayesian isotonic density regression","0","Density regression models allow the conditional distribution of the response given predictors to change flexibly over the predictor space. Such models are much more flexible than nonparametric mean regression models with nonparametric residual distributions, and are well supported in many applications. A rich variety of Bayesian methods have been proposed for density regression, but it is not clear whether such priors have full support so that any true data-generating model can be accurately approximated. This article develops a new class of density regression models that incorporate stochastic-ordering constraints which are natural when a response tends to increase or decrease monotonely with a predictor. Theory is developed showing large support. Methods are developed for hypothesis testing, with posterior computation relying on a simple Gibbs sampler. Frequentist properties are illustrated in a simulation study, and an epidemiology application is considered."
"10.1093/biomet/asr021","2011","The covariate-adaptive biased coin design for balancing clinical trials in the presence of prognostic factors","0","The present paper deals with sequential designs intended to balance the allocations of two competing treatments in the presence of prognostic factors. After giving a theoretical framework on the optimality of balanced designs that can arise when covariates are taken into account, we propose a new family of covariate-adaptive randomized designs that represents higher order approximation to balance treatments, both globally and also across covariates. We derive the theoretical properties of the suggested designs in terms of loss of precision and predictability. The performance of this proposal is illustrated through a simulation study and compared with those of other procedures suggested in the literature."
"10.1093/biomet/asr019","2011","Sample size formulae for two-stage randomized trials with survival outcomes","0","Two-stage randomized trials are growing in importance in developing adaptive treatment strategies, i.e. treatment policies or dynamic treatment regimes. Usually, the first stage involves randomization to one of the several initial treatments. The second stage of treatment begins when an early nonresponse criterion or response criterion is met. In the second-stage, nonresponding subjects are re-randomized among second-stage treatments. Sample size calculations for planning these two-stage randomized trials with failure time outcomes are challenging because the variances of common test statistics depend in a complex manner on the joint distribution of time to the early nonresponse criterion or response criterion and the primary failure time outcome. We produce simple, albeit conservative, sample size formulae by using upper bounds on the variances. The resulting formulae only require the working assumptions needed to size a standard single-stage randomized trial and, in common settings, are only mildly conservative. These sample size formulae are based on either a weighted Kaplan-Meier estimator of survival probabilities at a fixed time-point or a weighted version of the log-rank test."
"10.1093/biomet/asr002","2011","An {A}kaike-type information criterion for model selection under inequality constraints","0","The Akaike information criterion for model selection presupposes that the parameter space is not subject to order restrictions or inequality constraints. Anraku (1999) proposed a modified version of this criterion, called the order-restricted information criterion, for model selection in the one-way analysis of variance model when the population means are monotonic. We propose a generalization of this to the case when the population means may be restricted by a mixture of linear equality and inequality constraints. If the model has no inequality constraints, then the generalized order-restricted information criterion coincides with the Akaike information criterion. Thus, the former extends the applicability of the latter to model selection in multi-way analysis of variance models when some models may have inequality constraints while others may not. Simulation shows that the information criterion proposed in this paper performs well in selecting the correct model."
"10.1093/biomet/asr006","2011","The dimple in {G}neiting's spatial-temporal covariance model","0","Gneiting (2002) proposed a nonseparable covariance model for spatial-temporal data. In the present paper we show that in certain circumstances his model possesses a counterintuitive dimple. In some cases, the magnitude of the dimple can be nontrivial."
"10.1093/biomet/asr020","2011","On the likelihood function of {G}aussian max-stable processes","1","We derive a closed form expression for the likelihood function of a Gaussian max-stable process indexed by R-d at p < d+1 sites, d >= 1. We demonstrate the gain in efficiency in the maximum composite likelihood estimators of the covariance matrix from p=2 to p=3 sites in R-2 by means of a Monte Carlo simulation study."
"10.1093/biomet/asr004","2011","Empirical likelihood for small area estimation","0","Current methodologies in small area estimation are mostly either parametric or heavily dependent on the assumed linearity of the estimators of the small area means. We discuss an alternative empirical likelihood-based Bayesian approach, which neither requires a parametric likelihood nor assumes linearity of the estimators, and can handle both discrete and continuous data in a unified manner. Empirical likelihoods for both area- and unit-level models are introduced. We discuss the suitability of the proposed likelihoods in Bayesian inference and illustrate their performances on a real dataset and a simulation study."
"10.1093/biomet/asr011","2011","On balanced random imputation in surveys","0","Random imputation methods are often used in practice because they tend to preserve the distribution of the variable being imputed, which is an important property when the goal is to estimate population quantiles. However, this type of imputation method introduces additional variability, the imputation variance, due to the random selection of residuals. In this paper, we propose a class of random balanced imputation methods under which the imputation variance is eliminated while the distribution of the variable being imputed is preserved. The rationale behind balanced imputation is to select residuals at random so that appropriate constraints are satisfied. We describe an algorithm for selecting the random residuals that can be viewed as an adaptation of the cube algorithm proposed in the context of balanced sampling (Deville & Tille, 2004). Results of a simulation study support our findings."
"10.1093/biomet/asr001","2011","Optimal design for additive partially nonlinear models","0","We develop optimal design theory for additive partially nonlinear regression models, showing that Bayesian and standardized maximin D-optimal designs can be found as the products of the corresponding optimal designs in one dimension. A sufficient condition under which analogous results hold for D-s-optimality is derived to accommodate situations in which only a subset of the model parameters is of interest. To facilitate prediction of the response at unobserved locations, we prove similar results for Q-optimality in the class of all product designs. The usefulness of this approach is demonstrated through an application from the automotive industry, where optimal designs for least squares regression splines are determined and compared with designs commonly used in practice."
"10.1093/biomet/asr008","2011","Maximum likelihood estimation of a generalized threshold stochastic regression model","0","There is hardly any literature on modelling nonlinear dynamic relations involving nonnormal time series data. This is a serious lacuna because nonnormal data are far more abundant than normal ones, for example, time series of counts and positive time series. While there are various forms of nonlinearities, the class of piecewise-linear models is particularly appealing for its relative ease of tractability and interpretation. We propose to study the generalized threshold model which specifies that the conditional probability distribution of the response variable belongs to an exponential family, and the conditional mean response is linked to some piecewise-linear stochastic regression function. We introduce a likelihood-based estimation scheme, and the consistency and limiting distribution of the maximum likelihood estimator are derived. We illustrate the proposed approach with an analysis of a hare abundance time series, which gives new insights on how phase-dependent predator-prey-climate interactions shaped the ten-year hare population cycle. A simulation study is conducted to examine the finite-sample performance of the proposed estimation method."
"10.1093/biomet/asr012","2011","Distribution estimators and confidence intervals for stereological volumes","0","Assessing the precision of volume estimates from systematic samples is a question of great practical importance, but statistically a challenging task due to the strong spatial dependence of the data and typically small sample sizes. The approach taken in this paper is more ambitious than earlier methodologies, the goal of which was estimation of the variance of a volume estimator v, rather than estimation of the distribution of v. We shall show that bootstrap methods yield consistent estimators of the distribution of v, and also suggest a variety of confidence intervals for the true volume. Our new methodology covers cases where serial sections are exactly periodic, as well as instances where the physical slicing procedure introduces errors in the placement of the sampling points. Measurement errors within sections are also taken into account. The performance of the method is illustrated by a simulation study with synthetic data, and also applied to real datasets."
"10.1093/biomet/asq079","2011","Maximum smoothed likelihood for multivariate mixtures","0","We introduce an algorithm for estimating the parameters in a finite mixture of completely unspecified multivariate components in at least three dimensions under the assumption of conditionally independent coordinate dimensions. We prove that this algorithm, based on a majorization-minimization idea, possesses a desirable descent property just as any em algorithm does. We discuss the similarities between our algorithm and a related one, the so-called nonlinearly smoothed em algorithm for the non-mixture setting. We also demonstrate via simulation studies that the new algorithm gives very similar results to another algorithm that has been shown empirically to be effective but that does not satisfy any descent property. We provide code for implementing the new algorithm in a publicly available R package."
"10.1093/biomet/asr015","2011","The union closure method for testing a fixed sequence of families of hypotheses","0","Statistical analyses often involve testing multiple hypotheses that are naturally grouped into a fixed sequence of families. An effective approach to control the familywise error rate is to prioritize the importance of prespecification in the testing order. A gatekeeping testing procedure examines the first family with no multiple adjustment and then examines the subsequent family depending on the decision made with respect to the previous one. In this paper, we describe the union closure method that can be used to design gatekeeping procedures. A bipolar disorder trial with three primary and two secondary outcomes is presented as an example. Power comparisons based on the bipolar disorder trial show that the proposed gatekeeping procedures under the union closure framework are more powerful than competing methods."
"10.1093/biomet/asr016","2011","Testing against a high-dimensional alternative in the generalized linear model: asymptotic type {I} error control","0","Testing a low-dimensional null hypothesis against a high-dimensional alternative in a generalized linear model may lead to a test statistic that is a quadratic form in the residuals under the null model. Using asymptotic arguments, we show that the distribution of such a test statistic can be approximated by a ratio of quadratic forms in normal variables, for which algorithms are readily available. For generalized linear models, the asymptotic distribution shows good control of type I error for moderate to small samples, even when the number of covariates in the model far exceeds the sample size."
"10.1093/biomet/asr010","2011","Sure independence screening and compressed random sensing","0","Compressed sensing is a very powerful and popular tool for sparse recovery of high dimensional signals. Random sensing matrices are often employed in compressed sensing. In this paper we introduce a new method named aggressive betting using sure independence screening for sparse noiseless signal recovery. The proposal exploits the randomness structure of random sensing matrices to greatly boost computation speed. When using sub-Gaussian sensing matrices, which include the Gaussian and Bernoulli sensing matrices as special cases, our proposal has the exact recovery property with overwhelming probability. We also consider sparse recovery with noise and explicitly reveal the impact of noise-to-signal ratio on the probability of sure screening."
"10.1093/biomet/asq080","2011","Efficient semiparametric regression for longitudinal data with nonparametric covariance estimation","0","For longitudinal data, when the within-subject covariance is misspecified, the semiparametric regression estimator may be inefficient. We propose a method that combines the efficient semiparametric estimator with nonparametric covariance estimation, and is robust against misspecification of covariance models. We show that kernel covariance estimation provides uniformly consistent estimators for the within-subject covariance matrices, and the semiparametric profile estimator with substituted nonparametric covariance is still semiparametrically efficient. The finite sample performance of the proposed estimator is illustrated by simulation. In an application to CD4 count data from an AIDS clinical trial, we extend the proposed method to a functional analysis of the covariance model."
"10.1093/biomet/asr005","2011","Time-dependent cross ratio estimation for bivariate failure times","0","In the analysis of bivariate correlated failure time data, it is important to measure the strength of association among the correlated failure times. One commonly used measure is the cross ratio. Motivated by Cox's partial likelihood idea, we propose a novel parametric cross ratio estimator that is a flexible continuous function of both components of the bivariate survival times. We show that the proposed estimator is consistent and asymptotically normal. Its finite sample performance is examined using simulation studies, and it is applied to the Australian twin data."
"10.1093/biomet/asq083","2011","Nonparametric inference for competing risks current status data with continuous, discrete or grouped observation times","0","New methods and theory have recently been developed to nonparametrically estimate cumulative incidence functions for competing risks survival data subject to current status censoring. In particular, the limiting distribution of the nonparametric maximum likelihood estimator and a simplified naive estimator have been established under certain smoothness conditions. In this paper, we establish the large-sample behaviour of these estimators in two additional models, namely when the observation time distribution has discrete support and when the observation times are grouped. These asymptotic results are applied to the construction of confidence intervals in the three different models. The methods are illustrated on two datasets regarding the cumulative incidence of different types of menopause from a cross-sectional sample of women in the United States and subtype-specific HIV infection from a sero-prevalence study in injecting drug users in Thailand."
"10.1093/biomet/asr009","2011","Bayesian influence analysis: a geometric approach","0","In this paper we develop a general framework of Bayesian influence analysis for assessing various perturbation schemes to the data, the prior and the sampling distribution for a class of statistical models. We introduce a perturbation model to characterize these various perturbation schemes. We develop a geometric framework, called the Bayesian perturbation manifold, and use its associated geometric quantities including the metric tensor and geodesic to characterize the intrinsic structure of the perturbation model. We develop intrinsic influence measures and local influence measures based on the Bayesian perturbation manifold to quantify the effect of various perturbations to statistical models. Theoretical and numerical examples are examined to highlight the broad spectrum of applications of this local influence method in a formal Bayesian analysis."
"10.1093/biomet/asr013","2011","Sparse {B}ayesian infinite factor models","0","We focus on sparse modelling of high-dimensional covariance matrices using Bayesian latent factor models. We propose a multiplicative gamma process shrinkage prior on the factor loadings which allows introduction of infinitely many factors, with the loadings increasingly shrunk towards zero as the column index increases. We use our prior on a parameter-expanded loading matrix to avoid the order dependence typical in factor analysis models and develop an efficient Gibbs sampler that scales well as data dimensionality increases. The gain in efficiency is achieved by the joint conjugacy property of the proposed prior, which allows block updating of the loadings matrix. We propose an adaptive Gibbs sampler for automatically truncating the infinite loading matrix through selection of the number of important factors. Theoretical results are provided on the support of the prior and truncation approximation bounds. A fast algorithm is proposed to produce approximate Bayes estimates. Latent factor regression methods are developed for prediction and variable selection in applications with high-dimensional correlated predictors. Operating characteristics are assessed through simulation studies, and the approach is applied to predict survival times from gene expression data."
"10.1093/biomet/asr003","2011","Sample size and power analysis for sparse signal recovery in genome-wide association studies","1","Genome-wide association studies have successfully identified hundreds of novel genetic variants associated with many complex human diseases. However, there is a lack of rigorous work on evaluating the statistical power for identifying these variants. In this paper, we consider sparse signal identification in genome-wide association studies and present two analytical frameworks for detailed analysis of the statistical power for detecting and identifying the disease-associated variants. We present an explicit sample size formula for achieving a given false non-discovery rate while controlling the false discovery rate based on an optimal procedure. Sparse genetic variant recovery is also considered and a boundary condition is established in terms of sparsity and signal strength for almost exact recovery of both disease-associated variants and nondisease-associated variants. A data-adaptive procedure is proposed to achieve this bound. The analytical results are illustrated with a genome-wide association study of neuroblastoma."
"10.1093/biomet/asr018","2011","False discovery rates and copy number variation","1","Copy number changes, the gains and losses of chromosome segments, are a common type of genetic variation among healthy individuals as well as an important feature in tumour genomes. Microarray technology enables us to simultaneously measure, with moderate accuracy, copy number variation at more than a million chromosome locations and for hundreds of subjects. This leads to massive data sets and complicated inference problems concerning which locations are more likely to vary. In this paper we consider a relatively simple false discovery rate approach to copy number analysis. More careful parametric change-point methods can then be focused on promising regions of the genome."
"10.1093/biomet/asq074","2011","Testing a linear time series model against its threshold extension","0","This paper derives the asymptotic null distribution of a quasilikelihood ratio test statistic for an autoregressive moving average model against its threshold extension. The null hypothesis is that of no threshold, and the error term could be dependent. The asymptotic distribution is rather complicated, and all existing methods for approximating a distribution in the related literature fail to work. Hence, a novel bootstrap approximation based on stochastic permutation is proposed in this paper. Besides being robust to the assumptions on the error term, our method enjoys more flexibility and needs less computation when compared with methods currently used in the literature. Monte Carlo experiments give further support to the new approach, and an illustration is reported."
"10.1093/biomet/asq068","2011","Recapture models under equality constraints for the conditional capture probabilities","0","We introduce a general class of capture-recapture models in which capture probabilities depend on capture history. We discuss constrained versions of the saturated model based on equality constraints. Inference can be performed through a simple estimating equation. The approach is illustrated on a dataset concerning Great Copper butterflies in Willamette Valley of Oregon."
"10.1093/biomet/asq071","2011","A novel reversible jump algorithm for generalized linear models","0","We propose a novel methodology to construct proposal densities in reversible jump algorithms that obtain samples from parameter subspaces of competing generalized linear models with differing dimensions. The derived proposal densities are not restricted to moves between nested models and are applicable even to models that share no common parameters. We illustrate our methodology on competing logistic regression and log-linear graphical models, demonstrating how our suggested proposal densities, together with the resulting freedom to propose moves between any models, improve the performance of the reversible jump algorithm."
"10.1093/biomet/asq081","2011","Data-driven selection of the spline dimension in penalized spline regression","0","A number of criteria exist to select the penalty in penalized spline regression, but the selection of the number of spline basis functions has received much less attention in the literature. We propose a likelihood-based criterion to select the number of basis functions in penalized spline regression. The criterion is easy to apply and we describe its theoretical and practical properties."
"10.1093/biomet/asq078","2011","Assessing the validity of weighted generalized estimating equations","0","The inverse probability weighted generalized estimating equations approach (Robins et al. 1994; Robins et al. 1995), effectively removes bias and provides valid statistical inference for regression parameter estimation in marginal models when longitudinal data contain missing values. The validity of the weighted generalized estimating equations regarding consistent estimation depends on whether the underlying missing data process is properly modelled. However, there is little work available to examine whether or not this condition holds. In this paper we propose a test constructed from two sets of estimating equations: one set is known to be unbiased, but the other set is not known. We utilize the quadratic inference function (Qu et al. 2000) method to assess their compatibility, which is equivalent to testing for the validity of the weighted generalized estimating equations approach. We conduct simulation studies to assess the performance of the proposed method. The test procedure is illustrated through a real data example."
"10.1093/biomet/asq075","2011","The effect of correlation in false discovery rate estimation","1","The objective of this paper is to quantify the effect of correlation in false discovery rate analysis. Specifically, we derive approximations for the mean, variance, distribution and quantiles of the standard false discovery rate estimator for arbitrarily correlated data. This is achieved using a negative binomial model for the number of false discoveries, where the parameters are found empirically from the data. We show that correlation may increase the bias and variance of the estimator substantially with respect to the independent case, and that in some cases, such as an exchangeable correlation structure, the estimator fails to be consistent as the number of tests becomes large."
"10.1093/biomet/asq064","2011","Variance estimation for generalized {C}avalieri estimators","0","The precision of stereological estimators based on systematic sampling is of great practical importance. This paper presents methods of data-based variance estimation for generalized Cavalieri estimators where errors in sampling positions may occur. Variance estimators are derived under perturbed systematic sampling, systematic sampling with cumulative errors and systematic sampling with random dropouts."
"10.1093/biomet/asq069","2011","Nonparametric estimation for length-biased and right-censored data","0","This paper considers survival data arising from length-biased sampling, where the survival times are left truncated by uniformly distributed random truncation times. We propose a nonparametric estimator that incorporates the information about the length-biased sampling scheme. The new estimator retains the simplicity of the truncation product-limit estimator with a closed-form expression, and has a small efficiency loss compared with the nonparametric maximum likelihood estimator, which requires an iterative algorithm. Moreover, the asymptotic variance of the proposed estimator has a closed form, and a variance estimator is easily obtained by plug-in methods. Numerical simulation studies with practical sample sizes are conducted to compare the performance of the proposed method with its competitors. A data analysis of the Canadian Study of Health and Aging is conducted to illustrate the methods and theory."
"10.1093/biomet/asq059","2011","A unified framework for studying parameter identifiability and estimation in biased sampling designs","0","Based on the odds ratio representation of a joint density, we propose a unified framework to study parameter identifiability in biased sampling designs. It is shown that most of these designs encountered in practice can be reformulated within the proposed framework and, as a result, the question of parameter identifiability can be largely clarified. Estimation of the identifiable parameters is considered and traditional results on the equivalence of the prospective and retrospective likelihoods are extended. Information contained in data on certain identifiable parameters is often very limited. Such parameters can be poorly estimated by the likelihood approach with practically attainable sample sizes, which can substantially affect the estimates of parameters of primary interest. A partially penalized likelihood approach is proposed to address this. Simulation results suggest that the proposed approach has good performance."
"10.1093/biomet/asq066","2011","Estimation of covariate effects in generalized linear mixed models with informative cluster sizes","0","In standard regression analyses of clustered data, one typically assumes that the expected value of the response is independent of cluster size. However, this is often false. For example, in studies of surgical interventions, investigators have frequently found surgery volume and outcomes to be related to the skill level of the surgeons. This paper examines the effect of ignoring response-dependent, informative, cluster sizes on standard analytical methods such as mixed-effects models and conditional likelihood methods using analytic calculations, simulation studies and an example from a study of periodontal disease. We consider the case in which cluster sizes and responses share random effects which we assume to be independent of the covariates. Our focus is on maximum likelihood methods that ignore informative cluster sizes, and we show that they exhibit little bias in estimating covariate effects that are uncorrelated with the random effects associated with cluster sizes. However, estimation of covariate effects that are associated with the random effects can be biased. In particular, for models with random intercepts only, ignoring informative cluster sizes can yield biased estimators of the intercept but little bias in estimation of all covariate effects."
"10.1093/biomet/asq063","2011","Partial envelopes for efficient estimation in multivariate linear regression","1","We introduce the partial envelope model, which leads to a parsimonious method for multivariate linear regression when some of the predictors are of special interest. It has the potential to achieve massive efficiency gains compared with the standard model in the estimation of the coefficients for the selected predictors. The partial envelope model is a variation on the envelope model proposed by Cook et al. (2010) but, as it focuses on part of the predictors, it has looser restrictions and can further improve the efficiency. We develop maximum likelihood estimation for the partial envelope model and discuss applications of the bootstrap. An example is provided to illustrate some of its operating characteristics."
"10.1093/biomet/asq073","2011","Parametric fractional imputation for missing data analysis","0","Parametric fractional imputation is proposed as a general tool for missing data analysis. Using fractional weights, the observed likelihood can be approximated by the weighted mean of the imputed data likelihood. Computational efficiency can be achieved using the idea of importance sampling and calibration weighting. The proposed imputation method provides efficient parameter estimates for the model parameters specified in the imputation model and also provides reasonable estimates for parameters that are not part of the imputation model. Variance estimation is discussed and results from a limited simulation study are presented."
"10.1093/biomet/asq070","2011","Horvitz-{T}hompson estimators for functional data: asymptotic confidence bands and optimal allocation for stratified sampling","0","When dealing with very large datasets of functional data, survey sampling approaches are useful in order to obtain estimators of simple functional quantities, without being obliged to store all the data. We propose a Horvitz-Thompson estimator of the mean trajectory. In the context of a superpopulation framework, we prove, under mild regularity conditions, that we obtain uniformly consistent estimators of the mean function and of its variance function. With additional assumptions on the sampling design we state a functional central limit theorem and obtain asymptotic confidence bands. Stratified sampling is studied in detail, and we also obtain a functional version of the usual optimal allocation rule, considering a mean variance criterion. These techniques are illustrated by a test population of N=18 902 electricity meters for which we have individual electricity consumption measures every 30 minutes over one week. We show that stratification can substantially improve both the accuracy of the estimators and reduce the width of the global confidence bands compared with simple random sampling without replacement."
"10.1093/biomet/asq077","2011","On asymptotic normality and variance estimation for nondifferentiable survey estimators","0","Survey estimators of population quantities such as distribution functions and quantiles contain nondifferentiable functions of estimated quantities. The theoretical properties of such estimators are substantially more complicated to derive than those of differentiable estimators. In this article, we provide a unified framework for obtaining the asymptotic design-based properties of two common types of nondifferentiable estimators. Estimators of the first type have an explicit expression, while those of the second are defined only as the solution to estimating equations. We propose both analytical and replication-based design-consistent variance estimators for both cases, based on kernel regression. The practical behaviour of the variance estimators is demonstrated in a simulation experiment."
"10.1093/biomet/asq076","2011","A self-normalized confidence interval for the mean of a class of nonstationary processes","0","We construct an asymptotic confidence interval for the mean of a class of nonstationary processes with constant mean and time-varying variances. Due to the large number of unknown parameters, traditional approaches based on consistent estimation of the limiting variance of sample mean through moving block or non-overlapping block methods are not applicable. Under a block-wise asymptotically equal cumulative variance assumption, we propose a self-normalized confidence interval that is robust against the nonstationarity and dependence structure of the data. We also apply the same idea to construct an asymptotic confidence interval for the mean difference of nonstationary processes with piecewise constant means. The proposed methods are illustrated through simulations and an application to global temperature series."
"10.1093/biomet/asq062","2011","Particle approximations of the score and observed information matrix in state space models with application to parameter estimation","0","Particle methods are popular computational tools for Bayesian inference in nonlinear non-Gaussian state space models. For this class of models, we present two particle algorithms to compute the score vector and observed information matrix recursively. The first algorithm is implemented with computational complexity O(N) and the second with complexity O(N(2)), where N is the number of particles. Although cheaper, the performance of the O(N) method degrades quickly, as it relies on the approximation of a sequence of probability distributions whose dimension increases linearly with time. In particular, even under strong mixing assumptions, the variance of the estimates computed with the O(N) method increases at least quadratically in time. The more expensive O(N(2)) method relies on a nonstandard particle implementation and does not suffer from this rapid degradation. It is shown how both methods can be used to perform batch and recursive parameter estimation."
"10.1093/biomet/asq065","2011","Bootstrap inference for mean reflection shape and size-and-shape with three-dimensional landmark data","0","Working within the framework of a multi-dimensional scaling approach to shape analysis, we develop bootstrap methods for inference about mean reflection shape and size-and-shape based on labelled landmark data. The approach is developed in general dimensions though we focus on the three-dimensional case. We consider two pivotal statistics which we use to construct bootstrap confidence regions for the mean reflection shape or size-and-shape, and present simulation results which show that these statistics perform well in a variety of examples. We also suggest regularized versions of the test statistics that are suitable for more challenging cases where sample size is not sufficiently large in relation to the number of landmarks and present numerical results confirming that regularization indeed leads to better performance. An algorithm for producing a graphical representation of the confidence region for the mean reflection shape is presented and applied in an example involving molecular dynamics simulation data."
"10.1093/biomet/asq067","2011","Bayesian geostatistical modelling with informative sampling locations","0","We consider geostatistical models that allow the locations at which data are collected to be informative about the outcomes. A Bayesian approach is proposed, which models the locations using a log Gaussian Cox process, while modelling the outcomes conditionally on the locations as Gaussian with a Gaussian process spatial random effect and adjustment for the location intensity process. We prove posterior propriety under an improper prior on the parameter controlling the degree of informative sampling, demonstrating that the data are informative. In addition, we show that the density of the locations and mean function of the outcome process can be estimated consistently under mild assumptions. The methods show significant evidence of informative sampling when applied to ozone data over Eastern U.S.A."
"10.1093/biomet/asq072","2011","The multivariate beta process and an extension of the {P}olya tree model","0","We introduce a novel stochastic process that we term the multivariate beta process. The process is defined for modelling-dependent random probabilities and has beta marginal distributions. We use this process to define a probability model for a family of unknown distributions indexed by covariates. The marginal model for each distribution is a Polya tree prior. An important feature of the proposed prior is the easy centring of the nonparametric model around any parametric regression model. We use the model to implement nonparametric inference for survival distributions. The nonparametric model that we introduce can be adopted to extend the support of prior distributions for parametric regression models."
"10.1093/biomet/asq060","2011","Joint estimation of multiple graphical models","0","Gaussian graphical models explore dependence relationships between random variables, through the estimation of the corresponding inverse covariance matrices. In this paper we develop an estimator for such models appropriate for data from several graphical models that share the same variables and some of the dependence structure. In this setting, estimating a single graphical model would mask the underlying heterogeneity, while estimating separate models for each category does not take advantage of the common structure. We propose a method that jointly estimates the graphical models corresponding to the different categories present in the data, aiming to preserve the common structure, while allowing for differences between the categories. This is achieved through a hierarchical penalty that targets the removal of common zeros in the inverse covariance matrices across categories. We establish the asymptotic consistency and sparsity of the proposed estimator in the high-dimensional case, and illustrate its performance on a number of simulated networks. An application to learning semantic connections between terms from webpages collected from computer science departments is included."
"10.1093/biomet/asq011","2010","Copula inference under censoring","0","This paper discusses copula model selection procedures and goodness-of-fit tests under censoring. The proposed methodology is based on a comparison of nonparametric and model-based estimators of the probability integral transformation, K. New weighted estimators for K are introduced. The resulting tests are compared to an existing approach by simulation and illustrated with an example involving bleeding changes in a woman's reproductive history."
"10.1093/biomet/asq017","2010","The horseshoe estimator for sparse signals","4","This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator's advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator's tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior."
"10.1093/biomet/asq010","2010","Efficient scalable schemes for monitoring a large number of data streams","0","The sequential changepoint detection problem is studied in the context of global online monitoring of a large number of independent data streams. We are interested in detecting an occurring event as soon as possible, but we do not know when the event will occur, nor do we know which subset of data streams will be affected by the event. A family of scalable schemes is proposed based on the sum of the local cumulative sum, CUSUM, statistics from each individual data stream, and is shown to asymptotically minimize the detection delays for each and every possible combination of affected data streams, subject to the global false alarm constraint. The usefulness and limitations of our asymptotic optimality results are illustrated by numerical simulations and heuristic arguments. The Appendices contain a probabilistic result on the first epoch to simultaneous record values for multiple independent random walks."
"10.1093/biomet/asq012","2010","Calibrating parametric subject-specific risk estimation","1","For modern evidence-based medicine, decisions on disease prevention or management strategies are often guided by a risk index system. For each individual, the system uses his/her baseline information to estimate the risk of experiencing a future disease-related clinical event. Such a risk scoring scheme is usually derived from an overly simplified parametric model. To validate a model-based procedure, one may perform a standard global evaluation via, for instance, a receiver operating characteristic analysis. In this article, we propose a method to calibrate the risk index system at a subject level. Specifically, we developed point and interval estimation procedures for t-year mortality rates conditional on the estimated parametric risk score. The proposals are illustrated with a dataset from a large clinical trial with post-myocardial infarction patients."
"10.1093/biomet/asq004","2010","Risk-adjusted monitoring of time to event","0","Recently there has been interest in risk-adjusted cumulative sum charts, CUSUMs, to monitor the performance of e. g. hospitals, taking into account the heterogeneity of patients. Even though many outcomes involve time, only conventional regression models are commonly used. In this article we investigate how time to event models may be used for monitoring purposes. We consider monitoring using CUSUMs based on the partial likelihood ratio between an out-of-control state and an in-control state. We consider both proportional and nonproportional alternatives, as well as a head start. Against proportional alternatives, we present an analytic method of computing the expected number of observed events before stopping or the probability of stopping before a given observed number of events. In a stationary set-up, the former is roughly proportional to the average run length in calendar time. Adding a head start changes the threshold only slightly if the expected number of events until hitting is used as a criterion. However, it changes the threshold substantially if a false alarm probability is used. In simulation studies, charts based on survival analysis perform better than simpler monitoring schemes. We present one example from retail finance and one medical application."
"10.1093/biomet/asq009","2010","Efficient estimation in multi-phase case-control studies","0","In this paper we discuss the analysis of multi-phase, or multi-stage, case-control studies and present an efficient semiparametric maximum-likelihood approach that unifies and extends earlier work, including the seminal case-control paper by Prentice & Pyke (1979), work by Breslow & Cain (1988), Scott & Wild (1991), Breslow & Holubkov (1997) and others. The theoretical derivations apply to arbitrary binary regression models but we present results for logistic regression and show that the approach can be implemented by including additional intercept terms in the logistic model and then making some simple corrections to the score and information equations used in a Newton-Raphson or Fisher-scoring maximization of the prospective loglikelihood."
"10.1093/biomet/asq014","2010","A theory for testing hypotheses under covariate-adaptive randomization","1","The covariate-adaptive randomization method was proposed for clinical trials long ago but little theoretical work has been done for statistical inference associated with it. Practitioners often apply test procedures available for simple randomization, which is controversial since procedures valid under simple randomization may not be valid under other randomization schemes. In this paper, we provide some theoretical results for testing hypotheses after covariate-adaptive randomization. We show that one way to obtain a valid test procedure is to use a correct model between outcomes and covariates, including those used in randomization. We also show that the simple two sample t-test, without using any covariate, is conservative under covariate-adaptive biased coin randomization in terms of its Type I error, and that a valid bootstrap t-test can be constructed. The powers of several tests are examined theoretically and empirically. Our study provides guidance for applications and sheds light on further research in this area."
"10.1093/biomet/asq019","2010","Evidence factors in observational studies","2","Some experiments involve more than one random assignment of treatments to units. An analogous situation arises in certain observational studies, although randomization is not used, so each assignment may be biased. If each assignment is suspect, it is natural to ask whether there are separate pieces of information, dependent upon different assumptions, and perhaps whether conclusions about treatment effects are not critically dependent upon one or another suspect assumption. The design of an observational study contains evidence factors if it permits several statistically independent tests of the same null hypothesis about treatment effects, where these tests rely on different assumptions about treatment assignments at several levels of assignment. Two designs and two empirical examples are considered, one example of each design. In the dose-control design, there are matched pairs of a treated subject and an untreated control, and doses of treatment vary between pairs for treated subjects; this yields two evidence factors. In the varied intensity design, there are matched sets with two treated subjects and one or more untreated controls, where the two treated subjects within the same matched set receive different doses of treatment, and in a technically different way, the design yields two evidence factors."
"10.1093/biomet/asq006","2010","On the relative efficiency of using summary statistics versus individual-level data in meta-analysis","0","Meta-analysis is widely used to synthesize the results of multiple studies. Although meta-analysis is traditionally carried out by combining the summary statistics of relevant studies, advances in technologies and communications have made it increasingly feasible to access the original data on individual participants. In the present paper, we investigate the relative efficiency of analyzing original data versus combining summary statistics. We show that, for all commonly used parametric and semiparametric models, there is no asymptotic efficiency gain by analyzing original data if the parameter of main interest has a common value across studies, the nuisance parameters have distinct values among studies, and the summary statistics are based on maximum likelihood. We also assess the relative efficiency of the two methods when the parameter of main interest has different values among studies or when there are common nuisance parameters across studies. We conduct simulation studies to confirm the theoretical results and provide empirical comparisons from a genetic association study."
"10.1093/biomet/asq018","2010","Sufficient dimension reduction through discretization-expectation estimation","1","In the context of sufficient dimension reduction, the goal is to parsimoniously recover the central subspace of a regression model. Many inverse regression methods use slicing estimation to recover the central subspace. The efficacy of slicing estimation depends heavily upon the number of slices. However, the selection of the number of slices is an open and long-standing problem. In this paper, we propose a discretization-expectation estimation method, which avoids selecting the number of slices, while preserving the integrity of the central subspace. This generic method assures root-n consistency and asymptotic normality of slicing estimators for many inverse regression methods, and can be applied to regressions with multivariate responses. A BIC-type criterion for the dimension of the central subspace is proposed. Comprehensive simulations and an illustrative application show that our method compares favourably with existing estimators."
"10.1093/biomet/asp084","2010","The maximal data piling direction for discrimination","1","We study a discriminant direction vector that generally exists only in high-dimension, low sample size settings. Projections of data onto this direction vector take on only two distinct values, one for each class. There exist infinitely many such directions in the subspace generated by the data; but the maximal data piling vector has the longest distance between the projections. This paper investigates mathematical properties and classification performance of this discrimination method."
"10.1093/biomet/asp075","2010","The distribution-based {$p$}-value for the outlier sum in differential gene expression analysis","0","Outlier sums were proposed by Tibshirani & Hastie (2007) and Wu (2007) for detecting outlier genes where only a small subset of disease samples shows unusually high gene expression, but they did not develop their distributional properties and formal statistical inference. In this study, a new outlier sum for detection of outlier genes is proposed, its asymptotic distribution theory is developed, and the p-value based on this outlier sum is formulated. Its analytic form is derived on the basis of the large-sample theory. We compare the proposed method with existing outlier sum methods by power comparisons. Our method is applied to DNA microarray data from samples of primary breast tumors examined by Huang et al. (2003). The results show that the proposed method is more efficient in detecting outlier genes."
"10.1093/biomet/asp066","2010","Nonparametric {B}ayesian inference for the spectral density function of a random field","0","A powerful technique for inference concerning spatial dependence in a random field is to use spectral methods based on frequency domain analysis. Here we develop a nonparametric Bayesian approach to statistical inference for the spectral density of a random field. We construct a multi-dimensional Bernstein polynomial prior for the spectral density and devise a Markov chain Monte Carlo algorithm to simulate from the posterior of the spectral density. The posterior sampling enables us to obtain a smoothed estimate of the spectral density as well as credible bands at desired levels. Simulation shows that our proposed method is more robust than a parametric approach. For illustration, we analyse a soil data example."
"10.1093/biomet/asp071","2010","Weighted least squares approximate restricted likelihood estimation for vector autoregressive processes","0","We derive a weighted least squares approximate restricted likelihood estimator for a k-dimensional pth-order autoregressive model with intercept. Exact likelihood optimization of this model is generally infeasible due to the parameter space, which is complicated and high-dimensional, involving pk(2) parameters. The weighted least squares estimator has significantly reduced bias and mean squared error than the ordinary least squares estimator for both stationary and nonstationary processes. Furthermore, at the unit root, the limiting distribution of the weighted least squares approximate restricted likelihood estimator is shown to be the zero-intercept Dickey-Fuller distribution, unlike the ordinary least squares with intercept estimator that has a different distribution with significantly higher bias."
"10.1093/biomet/asp079","2010","Global and local spectral-based tests for periodicities","0","We investigate tests for periodicity based on a spectral analysis of a time series, differentiating between global and local spectral-based tests. Global tests use information across the entire frequency band,whereas local tests are based on a window around the test frequency.We show that many spectral-based tests can be expressed in terms of a regression-based F test, which allows for approximate size and power calculations. Since global tests are usually derived assuming white noise errors, we extend to the correlated noise case. We demonstrate via a Monte Carlo study that although the global test may have better size and power, local tests are easier to use, and are comparable or better in terms of the power to detect periodicities, especially for spectra with a large dynamic range. We apply this methodology to a nonbehavioural test of hearing."
"10.1093/biomet/asp074","2010","Pseudo-score confidence intervals for parameters in discrete statistical models","0","We propose pseudo-score confidence intervals for parameters in models for discrete data. The confidence interval is obtained by inverting a test that uses a Pearson chi-squared statistic to compare fitted values for the working model with fitted values of the model when a parameter of interest takes various fixed values. For multinomial models, the pseudo-score method simplifies to the score method when the model is saturated and otherwise it is asymptotically equivalent to score and likelihood ratio test-based inferences. For cases in which ordinary score methods are impractical, such as when the likelihood function is not an explicit function of model parameters, the pseudo-score method is feasible. We illustrate the method for four such examples. Generalizations of the method are also presented for future research, including inference for complex sampling designs using a quasilikelihood Pearson statistic that compares fitted values for two models relative to the variance of the observations under the simpler model."
"10.1093/biomet/asp083","2010","A note on the sensitivity to assumptions of a generalized linear mixed model","0","A simple case of Poisson regression is used to study the potential gain in efficiency from using a mixed model representation. Possible systematic errors arising from misspecification of the random terms in the model are examined. It is shown in particular that for a special but realistic problem, appreciable bias may arise from misspecification of a random component."
"10.1093/biomet/asp062","2010","On doubly robust estimation in a semiparametric odds ratio model","1","We consider the doubly robust estimation of the parameters in a semiparametric conditional odds ratio model. Our estimators are consistent and asymptotically normal in a union model that assumes either of two variation independent baseline functions is correctly modelled but not necessarily both. Furthermore, when either outcome has finite support, our estimators are semiparametric efficient in the union model at the intersection submodel where both nuisance functions models are correct. For general outcomes, we obtain doubly robust estimators that are nearly efficient at the intersection submodel. Our methods are easy to implement as they do not require the use of the alternating conditional expectations algorithm of Chen (2007)."
"10.1093/biomet/asq001","2010","Mean loglikelihood and higher-order approximations","1","Higher-order approximations to p-values can be obtained from the loglikelihood function and a reparameterization that can be viewed as a canonical parameter in an exponential family approximation to the model. This approach clarifies the connection between Skovgaard (1996) and Fraser et al. (1999a), and shows that the Skovgaard approximation can be obtained directly using the mean loglikelihood function."
"10.1093/biomet/asp072","2010","Estimation of the retransformed conditional mean in health care cost studies","1","We propose a new approach for analyzing skewed and heteroscedastic health care cost data through regression of the conditional quantiles of the transformed cost. Using the appealing equivariance property of quantiles to monotone transformations, we propose a distribution-free estimator of the conditional mean cost on the original scale. The proposed method is extended to a two-part heteroscedastic model to account for zero costs commonly seen in health care cost studies. Simulation studies indicate that the proposed estimator has competitive and more robust performance than existing estimators in various heteroscedastic models."
"10.1093/biomet/asp082","2010","A semiparametric random effects model for multivariate competing risks data","0","We propose a semiparametric random effects model for multivariate competing risks data when the failures of a particular type are of interest. Under this model, the marginal cumulative incidence functions follow a generalized semiparametric additive model. The associations between the cause-specific failure times can be studied through dependence parameters of copula functions that are allowed to depend on cluster-level covariates. A cross-odds ratio-type measure is proposed to describe the associations between cause-specific failure times, and its relationship to the dependence parameters is explored. We develop a two-stage estimation procedure where the marginal models are estimated in the first stage and the dependence parameters are estimated in the second stage. The large sample properties of the proposed estimators are derived. The proposed procedures are applied to Danish twin data to model the cumulative incidence for the age of natural menopause and to investigate the association in the onset of natural menopause between monozygotic and dizygotic twins."
"10.1093/biomet/asp065","2010","Stochastic approximation with virtual observations for dose-finding on discrete levels","0","Phase I clinical studies are experiments in which a new drug is administered to humans to determine the maximum dose that causes toxicity with a target probability. Phase I dose-finding is often formulated as a quantile estimation problem. For studies with a biological endpoint, it is common to define toxicity by dichotomizing the continuous biomarker expression. In this article, we propose a novel variant of the Robbins-Monro stochastic approximation that utilizes the continuous measurements for quantile estimation. The Robbins-Monro method has seldom seen clinical applications, because it does not perform well for quantile estimation with binary data and it works with a continuum of doses that are generally not available in practice. To address these issues, we formulate the dose-finding problem as root-finding for the mean of a continuous variable, for which the stochastic approximation procedure is efficient. To accommodate the use of discrete doses, we introduce the idea of virtual observation that is defined on a continuous dosage range. Our proposed method inherits the convergence properties of the stochastic approximation algorithm and its computational simplicity. Simulations based on real trial data show that our proposed method improves accuracy compared with the continual re-assessment method and produces results robust to model misspecification."
"10.1093/biomet/asp077","2010","On the use of stochastic ordering to test for trend with clustered binary data","0","We introduce the use of stochastic ordering for defining treatment-related trend in clustered exchangeable binary data for both when cluster sizes are fixed and when they vary randomly. In the latter case, there is a well-documented tendency for such data to be sparse, a problem we address by making an assumption of interpretability or, equivalently, marginal compatibility. Our procedures are based on a representation of the joint distribution of binary exchangeable random variables by a saturated model, and may hence be considered nonparametric. The definition of trend by stochastic ordering is proposed to ensure a flexibility that allows for various forms of monotone increases in response to the cluster as a whole to be included in the evaluation of the trend. We obtain maximum likelihood estimates of probability functions under stochastic ordering using mixture-likelihood-based algorithms. Since the data are sparse, we avoid the use of asymptotic results and obtain p-values of the likelihood ratio procedures by permutation resampling. We demonstrate how the proposed framework can be used in risk assessment, and provide comparisons with existing procedures."
"10.1093/biomet/asp068","2010","Marginal analyses of longitudinal data with an informative pattern of observations","0","We consider solutions to generalized estimating equations with singular working correlation matrices, of which the estimator of Diggle et al. (2007) is a special case. We give explicit conditions for consistent estimation when the pattern of observations may be informative. In such cases, simulations reveal reduced bias and reduced mean squared error compared with existing alternatives. A study of peritoneal dialysis is used to illustrate the methodology."
"10.1093/biomet/asp069","2010","Functional quadratic regression","0","We extend the common linear functional regression model to the case where the dependency of a scalar response on a functional predictor is of polynomial rather than linear nature. Focusing on the quadratic case, we demonstrate the usefulness of the polynomial functional regression model, which encompasses linear functional regression as a special case. Our approach works under mild conditions for the case of densely spaced observations and also can be extended to the important practical situation where the functional predictors are derived from sparse and irregular measurements, as is the case in many longitudinal studies. A key observation is the equivalence of the functional polynomial model with a regression model that is a polynomial of the same order in the functional principal component scores of the predictor processes. Theoretical analysis as well as practical implementations are based on this equivalence and on basis representations of predictor processes. We also obtain an explicit representation of the regression surface that defines quadratic functional regression and provide functional asymptotic results for an increasing number of model components as the number of subjects in the study increases. The improvements that can be gained by adopting quadratic as compared to linear functional regression are illustrated with a case study that includes absorption spectra as functional predictors."
"10.1093/biomet/asp078","2010","Cross-covariance functions for multivariate random fields based on latent dimensions","5","The problem of constructing valid parametric cross-covariance functions is challenging. We propose a simple methodology, based on latent dimensions and existing covariance models for univariate random fields, to develop flexible, interpretable and computationally feasible classes of cross-covariance functions in closed form. We focus on spatio-temporal cross-covariance functions that can be nonseparable, asymmetric and can have different covariance structures, for instance different smoothness parameters, in each component. We discuss estimation of these models and perform a small simulation study to demonstrate our approach. We illustrate our methodology on a trivariate spatio-temporal pollution dataset from California and demonstrate that our cross-covariance performs better than other competing models."
"10.1093/biomet/asq037","2010","Marginal log-linear parameterization of conditional independence models","0","Models defined by a set of conditional independence restrictions play an important role in statistical theory and applications, especially, but not only, in graphical modelling. In this paper we identify a subclass of these consisting of hierarchical marginal log-linear models, as defined by Bergsma & Rudas (2002a). Such models are smooth, which implies the applicability of standard asymptotic theory and simplifies interpretation. Furthermore, we give a marginal log-linear parameterization and a minimal specification of the models in the subclass, which implies the applicability of standard methods to compute maximum likelihood estimates and simplifies the calculation of the degrees of freedom of chi-squared statistics to test goodness-of-fit. The utility of the results is illustrated by applying them to block-recursive Markov models associated with chain graphs."
"10.1093/biomet/asq041","2010","Parameter redundancy with covariates","0","We show how to determine the parameter redundancy status of a model with covariates from that of the same model without covariates, thereby simplifying the calculation considerably. A matrix decomposition is necessary to ensure that the symbolic computation computer programmes return correct results. The paper is illustrated by mark-recovery and latent-class models, with associated Maple code."
"10.1093/biomet/asq049","2010","A note on overadjustment in inverse probability weighted estimation","0","Standardized means, commonly used in observational studies in epidemiology to adjust for potential confounders, are equal to inverse probability weighted means with inverse weights equal to the empirical propensity scores. More refined standardization corresponds with empirical propensity scores computed under more flexible models. Unnecessary standardization induces efficiency loss. However, according to the theory of inverse probability weighted estimation, propensity scores estimated under more flexible models induce improvement in the precision of inverse probability weighted means. This apparent contradiction is clarified by explicitly stating the assumptions under which the improvement in precision is attained."
"10.1093/biomet/asq054","2010","On the equivalence of prospective and retrospective likelihood methods in case-control studies","0","We present new approaches to analyzing case-control studies using prospective likelihood methods. In the classical framework, we extend the equality of the profile likelihoods to the Barndorff-Nielsen modified profile likelihoods for prospective and retrospective models. This enables simple and accurate approximate conditional inference for stratified case-control studies of moderate stratum size. In the Bayesian framework, we provide sufficient conditions on priors for the prospective model parameters to yield a prospective marginal posterior density equal to its retrospective counterpart. Our results extend the prospective-retrospective equivalence in the Bayesian paradigm with a more general class of priors than has previously been investigated."
"10.1093/biomet/asq024","2010","Some insights into continuum regression and its asymptotic properties","0","Continuum regression encompasses ordinary least squares regression, partial least squares regression and principal component regression under the same umbrella using a nonnegative parameter gamma. However, there seems to be no literature discussing the asymptotic properties for arbitrary continuum regression parameter gamma. This article establishes a relation between continuum regression and sufficient dimension reduction and studies the asymptotic properties of continuum regression for arbitrary gamma under inverse regression models. Theoretical and simulation results show that the continuum seems unnecessary when the conditional distribution of the predictors given the response follows the multivariate normal distribution."
"10.1093/biomet/asq047","2010","On the {V}oronoi estimator for the intensity of an inhomogeneous planar {P}oisson process","0","The Voronoi estimator may be defined for any location as the inverse of the area of the corresponding Voronoi cell. We investigate the statistical properties of this estimator for the intensity of an inhomogeneous Poisson process, and demonstrate it is approximately unbiased with a gamma sampling distribution. We also introduce the centroidal Voronoi estimator, a simple extension based on spatial regularization of the point pattern. Simulations show the Voronoi estimator has remarkably low bias, while the centroidal Voronoi estimator has slightly more bias but is much less variable. The performance is compared to kernel estimators using two simulated datasets and a dataset consisting of earthquakes within the continental United States."
"10.1093/biomet/asq032","2010","Varying coefficient transformation models with censored data","0","A maximum likelihood method with spline smoothing is proposed for linear transformation models with varying coefficients. The estimation and inference procedures are computationally easy. Under some regularity conditions, the estimators are proved to be consistent and asymptotically normal. A simulation study using the Stanford transplant data is presented to show that the proposed method performs well with a finite sample and is easy to use in practice."
"10.1093/biomet/asq051","2010","Probability-based {L}atin hypercube designs for slid-rectangular regions","1","Existing space-filling designs are based on the assumption that the experimental region is rectangular, while in practice this assumption can be violated. Motivated by a data centre thermal management study, a class of probability-based Latin hypercube designs is proposed to accommodate a specific type of irregular region. A heuristic algorithm is proposed to search efficiently for optimal designs. Unbiased estimators are proposed, their variances are given and their performances are compared empirically. The proposed method is applied to obtain an optimal sensor placement plan to monitor and study the thermal distribution in a data centre."
"10.1093/biomet/asq046","2010","Enhancing the sample average approximation method with {$U$} designs","0","Many computational problems in statistics can be cast as stochastic programs that are optimization problems whose objective functions are multi-dimensional integrals. The sample average approximation method is widely used for solving such a problem, which first constructs a sampling-based approximation to the objective function and then finds the solution to the approximated problem. Independent and identically distributed sampling is a prevailing choice for constructing such approximations. Recently it was found that the use of Latin hypercube designs can improve sample average approximations. In computer experiments, U designs are known to possess better space-filling properties than Latin hypercube designs. Inspired by this fact, we propose to use U designs to further enhance the accuracy of the sample average approximation method. Theoretical results are derived to show that sample average approximations with U designs can significantly outperform those with Latin hypercube designs. Numerical examples are provided to corroborate the developed theoretical results."
"10.1093/biomet/asq055","2010","Compound optimal allocation for individual and collective ethics in binary clinical trials","0","In recent years, several authors have investigated response-adaptive allocation rules for comparative clinical trials, in order to favour, at each stage of the trial, the treatment that appears to be best. In this paper, we define admissible allocations, namely treatment assignments that cannot be simultaneously improved upon with respect to both a specific design criterion, reflecting the inferential properties of the experiment, and the proportion of patients assigned to the best treatment or treatments; we survey existing designs from this viewpoint. We also suggest combining information and ethical considerations by taking a suitable weighted mean of two corresponding standardized criteria, with weights that depend on the actual treatment effects. This compound criterion leads to a locally optimal allocation that can be targeted by some response-adaptive randomization rule. The paper mainly deals with the case of two treatments, but the suggested methodology is shown to extend to more than two."
"10.1093/biomet/asq053","2010","Estimation of controlled direct effects on a dichotomous outcome using logistic structural direct effect models","0","We consider the problem of assessing whether an exposure affects a dichotomous outcome other than by modifying a given mediator. The standard approach, logistic regression adjusting for both exposure and the mediator, is known to be biased in the presence of confounders for the mediator-outcome relationship. Because additional regression adjustment for such confounders is only justified when they are not affected by the exposure, inverse probability weighting has been advocated, but is not ideally tailored to mediators that are continuous or have strong measured predictors. We overcome this limitation by developing inference for a novel class of causal models that are closely related to Robins' logistic structural direct effect models, but do not inherit their difficulties of estimation. We study identification and efficient estimation under the assumption that all confounders for the exposure-outcome and mediator-outcome relationships have been measured, and find adequate performance in simulation studies. We discuss extensions to case-control studies and relevant implications for the generic problem of adjustment for time-varying confounding."
"10.1093/biomet/asq057","2010","Penalized high-dimensional empirical likelihood","1","We propose penalized empirical likelihood for parameter estimation and variable selection for problems with diverging numbers of parameters. Our results are demonstrated for estimating the mean vector in multivariate analysis and regression coefficients in linear models. By using an appropriate penalty function, we showthat penalized empirical likelihood has the oracle property. That is, with probability tending to 1, penalized empirical likelihood identifies the true model and estimates the nonzero coefficients as efficiently as if the sparsity of the true model was known in advance. The advantage of penalized empirical likelihood as a nonparametric likelihood approach is illustrated by testing hypotheses and constructing confidence regions. Numerical simulations confirm our theoretical findings."
"10.1093/biomet/asq061","2010","Consistent selection of the number of clusters via crossvalidation","0","In cluster analysis, one of the major challenges is to estimate the number of clusters. Most existing approaches attempt to minimize some distance-based dissimilarity measure within clusters. This article proposes a novel selection criterion that is applicable to all kinds of clustering algorithms, including distance based or non-distance based algorithms. The key idea is to select the number of clusters that minimizes the algorithm's instability, which measures the robustness of any given clustering algorithm against the randomness in sampling.Anovel estimation scheme for clustering instability is developed based on crossvalidation. The proposed selection criterion's effectiveness is demonstrated on a variety of numerical experiments, and its asymptotic selection consistency is established when the dataset is properly split."
"10.1093/biomet/asq045","2010","Bootstrap confidence intervals and hypothesis tests for extrema of parameters","0","The bootstrap provides effective and accurate methodology for a wide variety of statistical problems which might not otherwise enjoy practicable solutions. However, there still exist important problems where standard bootstrap estimators are not consistent, and where alternative approaches, for example the m-out-of-n bootstrap and asymptotic methods, also face significant challenges. One of these is the problem of constructing confidence intervals or hypothesis tests for extrema of parameters, for example for the maximum of p parameters where each has to be estimated from data. In the present paper we suggest approaches to solving this problem. We use the bootstrap to construct an accurate estimator of the joint distribution of centred parameter estimators, and we base the procedure, either a confidence interval or a hypothesis test, on that distribution estimator. Our methodology is designed so that it errs on the side of conservatism, modulo the small inaccuracy of the bootstrap step."
"10.1093/biomet/asq043","2010","A weighted estimating equation approach for inhomogeneous spatial point processes","0","We introduce a new estimation method for parametric intensity function models of inhomogeneous spatial point processes based on weighted estimating equations. The weights can incorporate information on both inhomogeneity and dependence of the process. Simulations show that significant efficiency gains can be achieved for non-Poisson processes, compared to the Poisson maximum likelihood estimator. An application to tropical forest data illustrates the use of the proposed method."
"10.1093/biomet/asq044","2010","Nonparametric {B}ayesian density estimation on manifolds with applications to planar shapes","0","Statistical analysis on landmark-based shape spaces has diverse applications in morphometrics, medical diagnostics, machine vision and other areas. These shape spaces are non-Euclidean quotient manifolds. To conduct nonparametric inferences, one may define notions of centre and spread on this manifold and work with their estimates. However, it is useful to consider full likelihood-based methods, which allow nonparametric estimation of the probability density. This article proposes a broad class of mixture models constructed using suitable kernels on a general compact metric space and then on the planar shape space in particular. Following a Bayesian approach with a nonparametric prior on the mixing distribution, conditions are obtained under which the Kullback-Leibler property holds, implying large support and weak posterior consistency. Gibbs sampling methods are developed for posterior computation, and the methods are applied to problems in density estimation and classification with shape-based predictors. Simulation studies show improved estimation performance relative to existing approaches."
"10.1093/biomet/asq050","2010","Censored quantile regression with partially functional effects","0","Quantile regression offers a flexible approach to analyzing survival data, allowing each covariate effect to vary with quantiles. In practice, constancy is often found to be adequate for some covariates. In this paper, we study censored quantile regression tailored to the partially functional effect setting with a mixture of varying and constant effects. Such a model can offer a simpler view regarding covariate-survival association and, moreover, can enable improvement in estimation efficiency. We propose profile estimating equations and present an iterative algorithm that can be readily and stably implemented. Asymptotic properties of the resultant estimators are established. A simple resampling-based inference procedure is developed and justified. Extensive simulation studies demonstrate efficiency gains of the proposed method over a naive two-stage procedure. The proposed method is illustrated via an application to a recent renal dialysis study."
"10.1093/biomet/asq048","2010","Noncrossing quantile regression curve estimation","0","Since quantile regression curves are estimated individually, the quantile curves can cross, leading to an invalid distribution for the response. A simple constrained version of quantile regression is proposed to avoid the crossing problem for both linear and nonparametric quantile curves. A simulation study and a reanalysis of tropical cyclone intensity data shows the usefulness of the procedure. Asymptotic properties of the estimator are equivalent to the typical approach under standard conditions, and the proposed estimator reduces to the classical one if there is no crossing. The performance of the constrained estimator has shown significant improvement by adding smoothing and stability across the quantile levels."
"10.1093/biomet/asq058","2010","Most-predictive design points for functional data predictors","1","We suggest a way of reducing the very high dimension of a functional predictor, X, to a low number of dimensions chosen so as to give the best predictive performance. Specifically, if X is observed on a fine grid of design points t(1),..., t(r), we propose a method for choosing a small subset of these, say t(i1),..., t(ik), to optimize the prediction of a response variable, Y. The values t(ij) are referred to as the most predictive design points, or covariates, for a given value of k, and are computed using information contained in a set of independent observations (X-i, Y-i) of (X, Y). The algorithm is based on local linear regression, and calculations can be accelerated using linear regression to preselect the design points. Boosting can be employed to further improve the predictive performance. We illustrate the usefulness of our ideas through simulations and examples drawn from chemometrics, and we develop theoretical arguments showing that the methodology can be applied successfully in a range of settings."
"10.1093/biomet/asq056","2010","Additive modelling of functional gradients","0","We consider the problem of estimating functional derivatives and gradients in the framework of a regression setting where one observes functional predictors and scalar responses. Derivatives are then defined as functional directional derivatives that indicate how changes in the predictor function in a specified functional direction are associated with corresponding changes in the scalar response. For a model-free approach, navigating the curse of dimensionality requires the imposition of suitable structural constraints. Accordingly, we develop functional derivative estimation within an additive regression framework. Here, the additive components of functional derivatives correspond to derivatives of nonparametric one-dimensional regression functions with the functional principal components of predictor processes as arguments. This approach requires nothing more than estimating derivatives of one-dimensional nonparametric regressions, and thus is computationally very straightforward to implement, while it also provides substantial flexibility, fast computation and consistent estimation. We illustrate the consistent estimation and interpretation of the resulting functional derivatives and functional gradient fields in a study of the dependence of lifetime fertility of flies on early life reproductive trajectories."
"10.1093/biomet/asq042","2010","On the behaviour of marginal and conditional {AIC} in linear mixed models","0","In linear mixed models, model selection frequently includes the selection of random effects. Two versions of the Akaike information criterion, aic, have been used, based either on the marginal or on the conditional distribution. We show that the marginal aic is not an asymptotically unbiased estimator of the Akaike information, and favours smaller models without random effects. For the conditional aic, we show that ignoring estimation uncertainty in the random effects covariance matrix, as is common practice, induces a bias that can lead to the selection of any random effect not predicted to be exactly zero. We derive an analytic representation of a corrected version of the conditional aic, which avoids the high computational cost and imprecision of available numerical approximations. An implementation in an R package (R Development Core Team, 2010) is provided. All theoretical results are illustrated in simulation studies, and their impact in practice is investigated in an analysis of childhood malnutrition in Zambia."
"10.1093/biomet/asq034","2010","Strictly stationary solutions of autoregressive moving average equations","0","Necessary and sufficient conditions for the existence of a strictly stationary solution of the equations defining an autoregressive moving average process driven by an independent and identically distributed noise sequence are determined. No moment assumptions on the driving noise sequence are made."
"10.1093/biomet/asq028","2010","Empirical likelihood methods for two-dimensional shape analysis","0","We consider empirical likelihood for the mean similarity shape of objects in two dimensions described by labelled landmarks. The restriction to two dimensions permits the representation of preshapes as complex unit vectors. We focus on the use of empirical likelihood techniques for the construction of confidence regions for the mean shape and for testing the hypothesis of a common mean shape across several populations. Theoretical properties and computational details are discussed and the results of a simulation study are presented. Our results show that bootstrap calibrated empirical likelihood performs well in practice in the planar shape setting."
"10.1093/biomet/asq021","2010","Properties of nested sampling","0","Nested sampling is a simulation method for approximating marginal likelihoods. We establish that nested sampling has an approximation error that vanishes at the standard Monte Carlo rate and that this error is asymptotically Gaussian. It is shown that the asymptotic variance of the nested sampling approximation typically grows linearly with the dimension of the parameter. We discuss the applicability and efficiency of nested sampling in realistic problems, and compare it with two current methods for computing marginal likelihood. Finally, we propose an extension that avoids resorting to Markov chain Monte Carlo simulation to obtain the simulated points."
"10.1093/biomet/asq026","2010","Estimating species richness by a {P}oisson-compound gamma model","0","We propose a Poisson-compound gamma approach for species richness estimation. Based on the denseness and nesting properties of the gamma mixture, we fix the shape parameter of each gamma component at a unified value, and estimate the mixture using nonparametric maximum likelihood. A least-squares crossvalidation procedure is proposed for the choice of the common shape parameter. The performance of the resulting estimator of N is assessed using numerical studies and genomic data."
"10.1093/biomet/asq023","2010","Attributable fraction functions for censored event times","0","Attributable fractions are commonly used to measure the impact of risk factors on disease incidence in the population. These static measures can be extended to functions of time when the time to disease occurrence or event time is of interest. The present paper deals with nonparametric and semiparametric estimation of attributable fraction functions for cohort studies with potentially censored event time data. The semiparametric models include the familiar proportional hazards model and a broad class of transformation models. The proposed estimators are shown to be consistent, asymptotically normal and asymptotically efficient. Extensive simulation studies demonstrate that the proposed methods perform well in practical situations. A cardiovascular health study is provided. Connections to causal inference are discussed."
"10.1093/biomet/asq039","2010","A semiparametric additive rate model for recurrent events with an informative terminal event","0","We propose a semiparametric additive rate model for modelling recurrent events in the presence of a terminal event. The dependence between recurrent events and terminal event is nonparametric. A general transformation model is used to model the terminal event. We construct an estimating equation for parameter estimation and derive the asymptotic distributions of the proposed estimators. Simulation studies demonstrate that the proposed inference procedure performs well in realistic settings. Application to a medical study is presented."
"10.1093/biomet/asq036","2010","Analysis of cohort studies with multivariate and partially observed disease classification data","0","Complex diseases like cancers can often be classified into subtypes using various pathological and molecular traits of the disease. In this article, we develop methods for analysis of disease incidence in cohort studies incorporating data on multiple disease traits using a two-stage semiparametric Cox proportional hazards regression model that allows one to examine the heterogeneity in the effect of the covariates by the levels of the different disease traits. For inference in the presence of missing disease traits, we propose a generalization of an estimating equation approach for handling missing cause of failure in competing-risk data. We prove asymptotic unbiasedness of the estimating equation method under a general missing-at-random assumption and propose a novel influence-function-based sandwich variance estimator. The methods are illustrated using simulation studies and a real data application involving the Cancer Prevention Study II nutrition cohort."
"10.1093/biomet/asq035","2010","Bounded, efficient and doubly robust estimation with inverse weighting","1","Consider estimating the mean of an outcome in the presence of missing data or estimating population average treatment effects in causal inference. A doubly robust estimator remains consistent if an outcome regression model or a propensity score model is correctly specified. We build on a previous nonparametric likelihood approach and propose new doubly robust estimators, which have desirable properties in efficiency if the propensity score model is correctly specified, and in boundedness even if the inverse probability weights are highly variable. We compare the new and existing estimators in a simulation study and find that the robustified likelihood estimators yield overall the smallest mean squared errors."
"10.1093/biomet/asq030","2010","Sufficient cause interactions for categorical and ordinal exposures with three levels","1","Definitions are given for weak and strong sufficient cause interactions in settings in which the outcome is binary and in which there are two exposures of interest that are categorical or ordinal. Weak sufficient cause interactions concern cases in which a mechanism will operate under certain values of the two exposures but not when one or the other of the exposures takes some other value. Strong sufficient cause interactions concern cases in which a mechanism will operate under certain values of the two exposures but not when one or the other of the exposures takes any other value. Empirical conditions are derived for such interactions when exposures have two or three levels and are related to regression coefficients in linear and log-linear models. When the exposures are binary, the notions of a weak and a strong sufficient cause interaction coincide, but not when the exposures are categorical or ordinal. The results are applied to examples concerning gene-gene and gene-environment interactions."
"10.1093/biomet/asq025","2010","Detecting simultaneous changepoints in multiple sequences","2","We discuss the detection of local signals that occur at the same location in multiple one-dimensional noisy sequences, with particular attention to relatively weak signals that may occur in only a fraction of the sequences. We propose simple scan and segmentation algorithms based on the sum of the chi-squared statistics for each individual sample, which is equivalent to the generalized likelihood ratio for a model where the errors in each sample are independent. The simple geometry of the statistic allows us to derive accurate analytic approximations to the significance level of such scans. The formulation of the model is motivated by the biological problem of detecting recurrent DNA copy number variants in multiple samples. We show using replicates and parent-child comparisons that pooling data across samples results in more accurate detection of copy number variants. We also apply the multisample segmentation algorithm to the analysis of a cohort of tumour samples containing complex nested and overlapping copy number aberrations, for which our method gives a sparse and intuitive cross-sample summary."
"10.1093/biomet/asq040","2010","Accurate and robust tests for indirect inference","0","In this paper we propose accurate parameter and over-identification tests for indirect inference. Under the null hypothesis the new tests are asymptotically chi(2)-distributed with a relative error of order n(-1). They exhibit better finite sample accuracy than classical tests for indirect inference, which have the same asymptotic distribution but an absolute error of order n(-1/2). Robust versions of the tests are also provided. We illustrate their accuracy in nonlinear regression, Poisson regression with overdispersion and diffusion models."
"10.1093/biomet/asq031","2010","On the asymptotic behaviour of the pseudolikelihood ratio test statistic with boundary problems","0","This paper considers the asymptotic distribution of the likelihood ratio statistic T for testing a subset of parameter of interest theta, theta = (gamma,eta), H(0) : gamma = gamma(0), based on the pseudolikelihood L(theta, phi), where phi is a consistent estimator of phi the nuisance parameter. We show that the asymptotic distribution of T under H(0) is a weighted sum of independent chi- squared variables. Some sufficient conditions are provided for the limiting distribution to be a chi- squared variable. When the true value of the parameter of interest, theta(0), or the true value of the nuisance parameter, phi(0), lies on the boundary of parameter space, the problem is shown to be asymptotically equivalent to the problem of testing the restricted mean of a multivariate normal distribution based on one observation from a multivariate normal distribution with misspecified covariance matrix, or from a mixture of multivariate normal distributions. A variety of examples are provided for which the limiting distributions of T may be mixtures of chi- squared variables. We conducted simulation studies to examine the performance of the likelihood ratio test statistics in variance component models and teratological experiments."
"10.1093/biomet/asq029","2010","A class of grouped {B}runk estimators and penalized spline estimators for monotone regression","0","We study a class of monotone univariate regression estimators. We use B-splines to approximate an underlying regression function and estimate spline coefficients based on grouped data. We investigate asymptotic properties of two monotone estimators: a grouped Brunk estimator and a penalized monotone estimator. These estimators are consistent at the boundary and their mean square errors achieve optimal convergence rates under suitable assumptions of the true regression function. Asymptotic distributions are developed and are shown to be independent of spline degrees and the number of knots. Simulation results and car data illustrate performance of the proposed estimators."
"10.1093/biomet/asq027","2010","Shape curves and geodesic modelling","2","A family of shape curves is introduced that is useful for modelling the changes in shape in a series of geometrical objects. The relationship between the preshape sphere and the shape space is used to define a general family of curves based on horizontal geodesics on the preshape sphere. Methods for fitting geodesics and more general curves in the non-Euclidean shape space of point sets are discussed, based on minimizing sums of squares of Procrustes distances. Likelihood-based inference is considered. We illustrate the ideas by carrying out statistical analysis of two-dimensional landmarks on rats' skulls at various times in their development and three-dimensional landmarks on lumbar vertebrae from three primate species."
"10.1093/biomet/asq033","2010","Penalized {B}regman divergence for large-dimensional regression and classification","1","Regularization methods are characterized by loss functions measuring data fits and penalty terms constraining model parameters. The commonly used quadratic loss is not suitable for classification with binary responses, whereas the loglikelihood function is not readily applicable to models where the exact distribution of observations is unknown or not fully specified. We introduce the penalized Bregman divergence by replacing the negative loglikelihood in the conventional penalized likelihood with Bregman divergence, which encompasses many commonly used loss functions in the regression analysis, classification procedures and machine learning literature. We investigate new statistical properties of the resulting class of estimators with the number p(n) of parameters either diverging with the sample size n or even nearly comparable with n, and develop statistical inference tools. It is shown that the resulting penalized estimator, combined with appropriate penalties, achieves the same oracle property as the penalized likelihood estimator, but asymptotically does not rely on the complete specification of the underlying distribution. Furthermore, the choice of loss function in the penalized classifiers has an asymptotically relatively negligible impact on classification performance. We illustrate the proposed method for quasilikelihood regression and binary classification with simulation evaluation and real-data application."
"10.1093/biomet/asq022","2010","A new approach to {C}holesky-based covariance regularization in high dimensions","3","In this paper we propose a new regression interpretation of the Cholesky factor of the covariance matrix, as opposed to the well-known regression interpretation of the Cholesky factor of the inverse covariance, which leads to a new class of regularized covariance estimators suitable for high-dimensional problems. Regularizing the Cholesky factor of the covariance via this regression interpretation always results in a positive definite estimator. In particular, one can obtain a positive definite banded estimator of the covariance matrix at the same computational cost as the popular banded estimator of Bickel & Levina (2008b), which is not guaranteed to be positive definite. We also establish theoretical connections between banding Cholesky factors of the covariance matrix and its inverse and constrained maximum likelihood estimation under the banding constraint, and compare the numerical performance of several methods in simulations and on a sonar data example."
"10.1093/biomet/asq038","2010","Penalized likelihood methods for estimation of sparse high-dimensional directed acyclic graphs","0","Directed acyclic graphs are commonly used to represent causal relationships among random variables in graphical models. Applications of these models arise in the study of physical and biological systems where directed edges between nodes represent the influence of components of the system on each other. Estimation of directed graphs from observational data is computationally NP-hard. In addition, directed graphs with the same structure may be indistinguishable based on observations alone. When the nodes exhibit a natural ordering, the problem of estimating directed graphs reduces to the problem of estimating the structure of the network. In this paper, we propose an efficient penalized likelihood method for estimation of the adjacency matrix of directed acyclic graphs, when variables inherit a natural ordering. We study variable selection consistency of lasso and adaptive lasso penalties in high-dimensional sparse settings, and propose an error-based choice for selecting the tuning parameter. We show that although the lasso is only variable selection consistent under stringent conditions, the adaptive lasso can consistently estimate the true graph under the usual regularity assumptions."
"10.1093/biomet/asq020","2010","Optimal designs for the emax, log-linear and exponential models","0","We derive locally D- and ED(p)-optimal designs for the exponential, log-linear and three-parameter emax models. For each model the locally D- and ED(p)-optimal designs are supported at the same set of points, while the corresponding weights are different. This indicates that for a given model, D-optimal designs are efficient for estimating the smallest dose that achieves 100p% of the maximum effect in the observed dose range. Conversely, ED(p)-optimal designs also yield good D-efficiencies. We illustrate the results using several examples and demonstrate that locally D- and ED(p)-optimal designs for the emax, log-linear and exponential models are relatively robust with respect to misspecification of the model parameters."
"10.1093/biomet/asq002","2010","Objective {B}ayes and conditional inference in exponential families","1","Objective Bayes methodology is considered for conditional frequentist inference about a canonical parameter in a multi-parameter exponential family. A condition is derived under which posterior Bayes quantiles match the conditional frequentist coverage to a higher-order approximation in terms of the sample size. This condition is on the model, not on the prior, and it ensures that any first-order probability matching prior in the unconditional sense automatically yields higher-order conditional probability matching. Objective Bayes methods are compared to parametric bootstrap and analytic methods for higher-order conditional frequentist inference."
"10.1093/biomet/asq015","2010","Likelihood ratio statistics based on an integrated likelihood","0","An integrated likelihood depends only on the parameter of interest and the data, so it can be used as a standard likelihood function for likelihood-based inference. In this paper, the higher-order asymptotic properties of the signed integrated likelihood ratio statistic for a scalar parameter of interest are considered. These results are used to construct a modified integrated likelihood ratio statistic and to suggest a class of prior densities to use in forming the integrated likelihood. The properties of the integrated likelihood ratio statistic are compared to those of the standard likelihood ratio statistic. Several examples show that the integrated likelihood ratio statistic can be a useful alternative to the standard likelihood ratio statistic."
"10.1093/biomet/asq013","2010","A sequential smoothing algorithm with linear computational cost","0","In this paper we propose a new particle smoother that has a computational complexity of O(N), where N is the number of particles. This compares favourably with the O(N(2)) computational cost of most smoothers. The new method also overcomes some degeneracy problems in existing algorithms. Through simulation studies we show that substantial gains in efficiency are obtained for practical amounts of computational cost. It is shown both through these simulation studies, and by the analysis of an athletics dataset, that our new method also substantially outperforms the simple filter-smoother, the only other smoother with computational cost that is O(N)."
"10.1093/biomet/asq007","2010","Estimating linear dependence between nonstationary time series using the locally stationary wavelet model","1","Large volumes of neuroscience data comprise multiple, nonstationary electrophysiological or neuroimaging time series recorded from different brain regions. Accurately estimating the dependence between such neural time series is critical, since changes in the dependence structure are presumed to reflect functional interactions between neuronal populations. We propose a new dependence measure, derived from a bivariate locally stationary wavelet time series model. Since wavelets are localized in both time and scale, this approach leads to a natural, local and multi-scale estimate of nonstationary dependence. Our methodology is illustrated by application to a simulated example, and to electrophysiological data relating to interactions between the rat hippocampus and prefrontal cortex during working memory and decision making."
"10.1093/biomet/asq003","2010","Interval estimation for drop-the-losers designs","1","In the first stage of a two-stage, drop-the-losers design, a candidate for the best treatment is selected. At the second stage, additional observations are collected to decide whether the candidate is actually better than the control. The design also allows the investigator to stop the trial for ethical reasons at the end of the first stage if there is already strong evidence of futility or superiority. Two types of tests have recently been developed, one based on the combined means and the other based on the combined p-values, but corresponding interval estimators are unavailable except in special cases. The problem is that, in most cases, the interval estimators depend on the mean configuration of all treatments in the first stage, which is unknown. In this paper, we prove a basic stochastic ordering lemma that enables us to bridge the gap between hypothesis testing and interval estimation. The proposed confidence intervals achieve the nominal confidence level in certain special cases. Simulations show that decisions based on our intervals are usually more powerful than those based on existing methods."
"10.1093/biomet/asq005","2010","Semiparametric dimension reduction estimation for mean response with missing data","0","Model misspecification can be a concern for high-dimensional data. Nonparametric regression obviates model specification but is impeded by the curse of dimensionality. This paper focuses on the estimation of the marginal mean response when there is missingness in the response and multiple covariates are available. We propose estimating the mean response through nonparametric functional estimation, where the dimension is reduced by a parametric working index. The proposed semiparametric estimator is robust to model misspecification: it is consistent for any working index if the missing mechanism of the response is known or correctly specified up to unknown parameters; even with misspecification in the missing mechanism, it is consistent so long as the working index can recover E(Y vertical bar X), the conditional mean response given the covariates. In addition, when the missing mechanism is correctly specified, the semiparametric estimator attains the optimal efficiency if E(Y vertical bar X) is recoverable through the working index. Robustness and efficiency of the proposed estimator is further investigated by simulations. We apply the proposed method to a clinical trial for HIV."
"10.1093/biomet/asq016","2010","Dimension reduction for non-elliptically distributed predictors: second-order methods","1","Many classical dimension reduction methods, especially those based on inverse conditional moments, require the predictors to have elliptical distributions, or at least to satisfy a linearity condition. Such conditions, however, are too strong for some applications. Li and Dong (2009) introduced the notion of the central solution space and used it to modify first-order methods, such as sliced inverse regression, so that they no longer rely on these conditions. In this paper we generalize this idea to second-order methods, such as sliced average variance estimation and directional regression. In doing so we demonstrate that the central solution space is a versatile framework: we can use it to modify essentially all inverse conditional moment-based methods to relax the distributional assumption on the predictors. Simulation studies and an application show a substantial improvement of the modified methods over their classical counterparts."
"10.1093/biomet/asq008","2010","Variable selection in high-dimensional linear models: partially faithful distributions and the {PC}-simple algorithm","0","We consider variable selection in high-dimensional linear models where the number of covariates greatly exceeds the sample size. We introduce the new concept of partial faithfulness and use it to infer associations between the covariates and the response. Under partial faithfulness, we develop a simplified version of the PC algorithm (Spirtes et al., 2000), which is computationally feasible even with thousands of covariates and provides consistent variable selection under conditions on the random design matrix that are of a different nature than coherence conditions for penalty-based approaches like the lasso. Simulations and application to real data show that our method is competitive compared to penalty-based approaches. We provide an efficient implementation of the algorithm in the R-package pcalg."
"10.1093/biomet/asp070","2010","Forecasting for quantile self-exciting threshold autoregressive time series models","0","Self-exciting threshold autoregressive time series models have been used extensively, and the conditional mean obtained from these models can be used to predict the future value of a random variable. In this paper we consider quantile forecasts of a time series based on the quantile self-exciting threshold autoregressive time series models proposed by Cai and Stander (2008) and present a new forecasting method for them. Simulation studies and application to real time series show that the method works very well."
"10.1093/biomet/asp080","2010","On {B}ayesian testimation and its application to wavelet thresholding","0","We consider the problem of estimating the unknown response function in the Gaussian white noise model. We first utilize the recently developed Bayesian maximum a posteriori testimation procedure of Abramovich et al. (2007) for recovering an unknown high-dimensional Gaussian mean vector. The existing results for its upper error bounds over various sparse l(p)-balls are extended to more general cases. We show that, for a properly chosen prior on the number of nonzero entries of the mean vector, the corresponding adaptive estimator is asymptotically minimax in a wide range of sparse and dense l(p)-balls. The proposed procedure is then applied in a wavelet context to derive adaptive global and level-wise wavelet estimators of the unknown response function in the Gaussian white noise model. These estimators are then proven to be, respectively, asymptotically near-minimax and minimax in a wide range of Besov balls. These results are also extended to the estimation of derivatives of the response function. Simulated examples are conducted to illustrate the performance of the proposed level-wise wavelet estimator in finite sample situations, and to compare it with several existing counterparts."
"10.1093/biomet/asp076","2010","Sharp bounds on causal effects in case-control and cohort studies","0","Evaluating the causal effect of an exposure on a response from case-control and cohort studies is a major concern in epidemiological and medical research. Since causal effects are in general nonidentifiable from such studies, this paper derives bounds on two causal measures: the causal risk difference and the causal risk ratio. We use the potential response approach and a linear programming method to derive sharp bounds on the causal risk difference, and a novel fractional programming method to derive bounds on the causal risk ratio. In addition, in the presence of missing data, we consider three different missingness mechanisms and propose sharp bounds under these situations. The results provide new guidance on causal inference in case-control and cohort studies."
"10.1093/biomet/asp073","2010","Generalized empirical likelihood methods for analyzing longitudinal data","0","Efficient estimation of parameters is a major objective in analyzing longitudinal data. We propose two generalized empirical likelihood-based methods that take into consideration within-subject correlations. A nonparametric version of the Wilks theorem for the limiting distributions of the empirical likelihood ratios is derived. It is shown that one of the proposed methods is locally efficient among a class of within-subject variance-covariance matrices. A simulation study is conducted to investigate the finite sample properties of the proposed methods and compares them with the block empirical likelihood method by You et al. (2006) and the normal approximation with a correctly estimated variance-covariance. The results suggest that the proposed methods are generally more efficient than existing methods that ignore the correlation structure, and are better in coverage compared to the normal approximation with correctly specified within-subject correlation. An application illustrating our methods and supporting the simulation study results is presented."
"10.1093/biomet/asp081","2010","Incorporating prior probabilities into high-dimensional classifiers","0","In standard parametric classifiers, or classifiers based on nonparametric methods but where there is an opportunity for estimating population densities, the prior probabilities of the respective populations play a key role. However, those probabilities are largely ignored in the construction of high-dimensional classifiers, partly because there are no likelihoods to be constructed or Bayes risks to be estimated. Nevertheless, including information about prior probabilities can reduce the overall error rate, particularly in cases where doing so is most important, i.e. when the classification problem is particularly challenging and error rates are not close to zero. In this paper we suggest a general approach to reducing error rate in this way, by using a method derived from Breiman's bagging idea. The potential improvements in performance are identified in theoretical and numerical work, the latter involving both applications to real data and simulations. The method is simple and explicit to apply, and does not involve choice of any tuning parameters."
"10.1093/biomet/asp067","2010","Systematic sampling with errors in sample locations","2","Systematic sampling of points in continuous space is widely used in microscopy and spatial surveys. Classical theory provides asymptotic expressions for the variance of estimators based on systematic sampling as the grid spacing decreases. However, the classical theory assumes that the sample grid is exactly periodic; real physical sampling procedures may introduce errors in the placement of the sample points. This paper studies the effect of errors in sample positioning on the variance of estimators in the case of one-dimensional systematic sampling. First we sketch a general approach to variance analysis using point process methods. We then analyze three different models for the error process, calculate exact expressions for the variances, and derive asymptotic variances. Errors in the placement of sample points can lead to substantial inflation of the variance, dampening of zitterbewegung, that is fluctuation effects, and a slower order of convergence. This suggests that the current practice in some areas of microscopy may be based on over-optimistic predictions of estimator accuracy."
"10.1093/biomet/asp054","2009","A note on a conjectured sharpness principle for probabilistic forecasting with calibration","0","This note proves a weak sharpness principle as conjectured by Gneiting et al. (2007) in connection with probabilistic forecasting subject to calibration constraints."
"10.1093/biomet/asp048","2009","A note on adaptive {B}onferroni and {H}olm procedures under dependence","0","Hochberg & Benjamini (1990) first presented adaptive procedures for controlling familywise error rate. However, until now, it has not been proved that these procedures control the familywise error rate. We introduce a simplified version of Hochberg & Benjamini's adaptive Bonferroni and Holm procedures. Assuming a conditional dependence model, we prove that the former procedure controls the familywise error rate in finite samples while the latter controls it approximately."
"10.1093/biomet/asp060","2009","A note on automatic variable selection using smooth-threshold estimating equations","0","This paper develops smooth-threshold estimating equations that can automatically eliminate irrelevant parameters by setting them as zero. The resulting estimator enjoys the oracle property in the sense of Fan & Li (2001), even in estimators for which the covariance assumption of Wang & Leng (2007) is violated, such as the Buckley-James estimator. Furthermore, the estimator can be obtained without solving a convex optimization problem. A bic-type criterion for tuning parameter selection is also proposed. It is shown that the criterion achieves consistent model selection. A numerical study confirms the performance of the method."
"10.1093/biomet/asp043","2009","A note on the variance of doubly-robust {G}-estimators","0","A recursive variance calculation is derived for doubly-robust G-estimators for dynamic treatment regimes in a multi-interval setting. Treatment decision parameters are not assumed to be shared across treatment intervals; this independence of parameters permits sequential estimation of the G-estimators' variance when G-estimation is performed in a sequential fashion. The recursive variance calculation is both natural and computationally feasible. This development can easily be adapted to other complex estimating procedures that require nuisance parameter estimation."
"10.1093/biomet/asp040","2009","Semiparametric methods for evaluating risk prediction markers in case-control studies","2","The performance of a well-calibrated risk model for a binary disease outcome can be characterized by the population distribution of risk and displayed with the predictiveness curve. Better performance is characterized by a wider distribution of risk, since this corresponds to better risk stratification in the sense that more subjects are identified at low and high risk for the disease outcome. Although methods have been developed to estimate predictiveness curves from cohort studies, most studies to evaluate novel risk prediction markers employ case-control designs. Here we develop semiparametric methods that accommodate case-control data. The semiparametric methods are flexible, and naturally generalize methods previously developed for cohort data. Applications to prostate cancer risk prediction markers illustrate the methods."
"10.1093/biomet/asp052","2009","Adaptive approximate {B}ayesian computation","5","Sequential techniques can enhance the efficiency of the approximate Bayesian computation algorithm, as in Sisson et al.'s (2007) partial rejection control version. While this method is based upon the theoretical works of Del Moral et al. (2006), the application to approximate Bayesian computation results in a bias in the approximation to the posterior. An alternative version based on genuine importance sampling arguments bypasses this difficulty, in connection with the population Monte Carlo method of Cappe et al. (2004), and it includes an automatic scaling of the forward kernel. When applied to a population genetics example, it compares favourably with two other versions of the approximate algorithm."
"10.1093/biomet/asp056","2009","Maximum likelihood estimation using composite likelihoods for closed exponential families","0","In certain multivariate problems the full probability density has an awkward normalizing constant, but the conditional and/or marginal distributions may be much more tractable. In this paper we investigate the use of composite likelihoods instead of the full likelihood. For closed exponential families, both are shown to be maximized by the same parameter values for any number of observations. Examples include log-linear models and multivariate normal models. In other cases the parameter estimate obtained by maximizing a composite likelihood can be viewed as an approximation to the full maximum likelihood estimate. An application is given to an example in directional data based on a bivariate von Mises distribution."
"10.1093/biomet/asp045","2009","Nested {L}atin hypercube designs","3","We propose an approach to constructing nested Latin hypercube designs. Such designs are useful for conducting multiple computer experiments with different levels of accuracy. A nested Latin hypercube design with two layers is defined to be a special Latin hypercube design that contains a smaller Latin hypercube design as a subset. Our method is easy to implement and can accommodate any number of factors. We also extend this method to construct nested Latin hypercube designs with more than two layers. Illustrative examples are given. Some statistical properties of the constructed designs are derived."
"10.1093/biomet/asp044","2009","Sliced space-filling designs","2","We propose an approach to constructing a new type of design, a sliced space-filling design, intended for computer experiments with qualitative and quantitative factors. The approach starts with constructing a Latin hypercube design based on a special orthogonal array for the quantitative factors and then partitions the design into groups corresponding to different level combinations of the qualitative factors. The points in each group have good space-filling properties. Some illustrative examples are given."
"10.1093/biomet/asp041","2009","A unified approach to linearization variance estimation from survey data after imputation for item nonresponse","3","Variance estimation after imputation is an important practical problem in survey sampling. When deterministic imputation or stochastic imputation is used, we show that the variance of the imputed estimator can be consistently estimated by a unifying linearize and reverse approach. We provide some applications of the approach to regression imputation, fractional categorical imputation, multiple imputation and composite imputation. Results from a simulation study, under a factorial structure for the sampling, response and imputation mechanisms, show that the proposed linearization variance estimator performs well in terms of relative bias, assuming a missing at random response mechanism."
"10.1093/biomet/asp063","2009","Tests and confidence intervals for secondary endpoints in sequential clinical trials","0","In a sequential clinical trial whose stopping rule depends on the primary endpoint, inference on secondary endpoints is an important long-standing problem. Ignoring the possibility of early stopping based on the primary endpoint may result in substantial bias. To address this problem, a commonly used approach is to develop bias correction by estimating the bias in the case of bivariate normal outcomes and appealing to joint asymptotic normality of the statistics associated with the primary and secondary endpoints. We propose herein a new approach that uses resampling and a novel ordering scheme in the sample space of sequential statistics observed up to a stopping time. This approach is shown to provide accurate inference in complex clinical trials, including time-sequential trials with survival endpoints and covariates."
"10.1093/biomet/asp059","2009","Marginal hazards model for case-cohort studies with multiple disease outcomes","0","Case-cohort study designs are widely used to reduce the cost of large cohort studies while achieving the same goals, especially when the disease rate is low. A key advantage of the case-cohort study design is its capacity to use the same subcohort for several diseases or for several subtypes of disease. In order to compare the effect of a risk factor on different types of diseases, times to different events need to be modelled simultaneously. Valid statistical methods that take the correlations among the outcomes from the same subject into account need to be developed. To this end, we consider marginal proportional hazards regression models for case-cohort studies with multiple disease outcomes. We also consider generalized case-cohort designs that do not require sampling all the cases, which is more realistic for multiple disease outcomes. We propose an estimating equation approach for parameter estimation with two different types of weights. Consistency and asymptotic normality of the proposed estimators are established. Large sample approximation works well in small samples in simulation studies. The proposed methods are applied to the Busselton Health Study."
"10.1093/biomet/asp064","2009","Nonparametric estimation for right-censored length-biased data: a pseudo-partial likelihood approach","1","To estimate the lifetime distribution of right-censored length-biased data, we propose a pseudo-partial likelihood approach that allows us to derive two nonparametric estimators. With its closed-form estimators and explicit limiting variances, this approach retains the simplicity of conditional analysis, and has only a small efficiency loss compared with the unconditional analysis. Under some regularity conditions, we show that the two estimators are uniformly consistent and converge weakly to Gaussian processes. A simulation study demonstrates that the proposed estimators have satisfactory finite-sample performance. Application to an Alzheimer's disease study is reported."
"10.1093/biomet/asp046","2009","Nonparametric estimation of the probability of illness in the illness-death model under cross-sectional sampling","0","Cross-sectional sampling is an attractive design that saves resources but results in biased data. For proper inference, one should first discover the bias function and then weigh observations appropriately. We consider cross-sectioning of the illness-death model with the aim of estimating the probability of visiting the illness state before death. We develop simple consistent and asymptotically normal estimators under various assumptions on the model and data collection and, in particular, compare designs with and without a follow-up. These designs are common in surveillance of hospital acquired infections, but estimators currently in use do not properly correct the bias."
"10.1093/biomet/asp050","2009","Generalized fiducial inference for wavelet regression","0","We apply Fisher's fiducial idea to wavelet regression, first developing a general methodology for handling model selection problems within the fiducial framework. We propose fiducial-based methods for wavelet curve estimation and the construction of pointwise confidence intervals. We show that these confidence intervals have asymptotically correct coverage. Simulations demonstrate that they possess promising empirical properties."
"10.1093/biomet/asp031","2009","A {S}tudent {$t$}-mixture autoregressive model with applications to heavy-tailed financial data","0","We introduce the class of Student t-mixture autoregressive models, which is promising for financial time series modelling. The model is able to capture serial correlations, time-varying means and volatilities, and the shape of the conditional distributions can be time varied from short-tailed to long-tailed, or from unimodal to multimodal. The use of t-distributed errors in each component of the model allows conditional leptokurtic distributions that account for the commonly observed excess unconditional kurtosis in financial data. Methods of parameter estimation and model selection are given. Finally, the proposed modelling procedure is illustrated through a real example."
"10.1093/biomet/asp033","2009","Improving efficiency and robustness of the doubly robust estimator for a population mean with incomplete data","4","Considerable recent interest has focused on doubly robust estimators for a population mean response in the presence of incomplete data, which involve models for both the propensity score and the regression of outcome on covariates. The usual doubly robust estimator may yield severely biased inferences if neither of these models is correctly specified and can exhibit nonnegligible bias if the estimated propensity score is close to zero for some observations. We propose alternative doubly robust estimators that achieve comparable or improved performance relative to existing methods, even with some estimated propensity scores close to zero."
"10.1093/biomet/asp039","2009","Use of functionals in linearization and composite estimation with application to two-sample survey data","0","An important problem associated with two-sample surveys is the estimation of nonlinear functions of finite population totals such as ratios, correlation coefficients or measures of income inequality. Computation and estimation of the variance of such complex statistics are made more difficult by the existence of overlapping units. In one-sample surveys, the linearization method based on the influence function approach is a powerful tool for variance estimation. We introduce a two-sample linearization technique that can be viewed as a generalization of the one-sample influence function approach. Our technique is based on expressing the parameters of interest as multivariate functionals of finite and discrete measures and then using partial influence functions to compute the linearized variables. Under broad assumptions, the asymptotic variance of the substitution estimator, derived from Deville (1999), is shown to be the variance of a weighted sum of the linearized variables. The paper then focuses on a general class of composite substitution estimators, and from this class the optimal estimator for minimizing the asymptotic variance is obtained. The efficiency of the optimal composite estimator is demonstrated through an empirical study."
"10.1093/biomet/asp034","2009","Optimal repeated measurement designs for a model with partial interactions","0","We consider crossover designs for a model with partial interactions. In this model, the carryover effect depends on whether the treatment is preceded by itself or not. When the aim of the experiment is to study the total effects corresponding to a single treatment, we obtain approximate optimal symmetric designs, within the competing class of circular designs, by generalizing the method introduced by Kushner (1997) and Kunert & Martin (2000). This generalization places the method proposed by Bailey & Druilhet (2004) into Kushner's context. The optimal designs obtained are not binary, as in Kunert & Martin (2000). We also propose efficient designs generated by only one sequence."
"10.1093/biomet/asp028","2009","Gaussian process emulation of dynamic computer codes","0","Computer codes are used in scientific research to study and predict the behaviour of complex systems. Their run times often make uncertainty and sensitivity analyses impractical because of the thousands of runs that are conventionally required, so efficient techniques have been developed based on a statistical representation of the code. The approach is less straightforward for dynamic codes, which represent time-evolving systems. We develop a novel iterative system to build a statistical model of dynamic computer codes, which is demonstrated on a rainfall-runoff simulator."
"10.1093/biomet/asp027","2009","Pseudo-partial likelihood estimators for the {C}ox regression model with missing covariates","1","By embedding the missing covariate data into a left-truncated and right-censored survival model, we propose a new class of weighted estimating functions for the Cox regression model with missing covariates. The resulting estimators, called the pseudo-partial likelihood estimators, are shown to be consistent and asymptotically normal. A simulation study demonstrates that, compared with the popular inverse-probability weighted estimators, the new estimators perform better when the observation probability is small and improve efficiency of estimating the missing covariate effects. Application to a practical example is reported."
"10.1093/biomet/asp026","2009","Pseudo-partial likelihood for proportional hazards models with biased-sampling data","4","We obtain a pseudo-partial likelihood for proportional hazards models with biased-sampling data by embedding the biased-sampling data into left-truncated data. The log pseudo-partial likelihood of the biased-sampling data is the expectation of the log partial likelihood of the left-truncated data conditioned on the observed data. In addition, asymptotic properties of the estimator that maximize the pseudo-partial likelihood are derived. Applications to length-biased data, biased samples with right censoring and proportional hazards models with missing covariates are discussed."
"10.1093/biomet/asp032","2009","Weighted {B}reslow-type and maximum likelihood estimation in semiparametric transformation models","1","A semiparametric transformation model comprises a parametric component for covariate effects and a nonparametric component for the baseline hazard/intensity. The Breslow-type estimator has been proposed for estimating the nonparametric component in some inefficient estimation procedures. We show that introducing weights into this estimator leads to nonparametric maximum likelihood estimation, with the weights depending on the martingale residuals. The weighted Breslow-type estimator suggests an iterative reweighting algorithm for nonparametric maximum likelihood estimation, which can be implemented by a weighted variant of the existing algorithms for inefficient estimation, and can be computationally more efficient than an em-type algorithm. The weighting idea is further extended to semiparametric transformation models with mismeasured covariates."
"10.1093/biomet/asp025","2009","Induced smoothing for the semiparametric accelerated failure time model: asymptotics and extensions to clustered data","0","This paper extends the induced smoothing procedure of Brown & Wang (2006) for the semiparametric accelerated failure time model to the case of clustered failure time data. The resulting procedure permits fast and accurate computation of regression parameter estimates and standard errors using simple and widely available numerical methods, such as the Newton-Raphson algorithm. The regression parameter estimates are shown to be strongly consistent and asymptotically normal; in addition, we prove that the asymptotic distribution of the smoothed estimator coincides with that obtained without the use of smoothing. This establishes a key claim of Brown & Wang (2006) for the case of independent failure time data and also extends such results to the case of clustered data. Simulation results show that these smoothed estimates perform as well as those obtained using the best available methods at a fraction of the computational cost."
"10.1093/biomet/asp030","2009","Improving point and interval estimators of monotone functions by rearrangement","2","Suppose that a target function is monotonic and an available original estimate of this target function is not monotonic. Rearrangements, univariate and multivariate, transform the original estimate to a monotonic estimate that always lies closer in common metrics to the target function. Furthermore, suppose an original confidence interval, which covers the target function with probability at least 1-alpha, is defined by an upper and lower endpoint functions that are not monotonic. Then the rearranged confidence interval, defined by the rearranged upper and lower endpoint functions, is monotonic, shorter in length in common norms than the original interval, and covers the target function with probability at least 1-alpha. We illustrate the results with a growth chart example."
"10.1093/biomet/asp024","2009","Empirical {B}ayes estimation for additive hazards regression models","1","We develop a novel empirical Bayesian framework for the semiparametric additive hazards regression model. The integrated likelihood, obtained by integration over the unknown prior of the nonparametric baseline cumulative hazard, can be maximized using standard statistical software. Unlike the corresponding full Bayes method, our empirical Bayes estimators of regression parameters, survival curves and their corresponding standard errors have easily computed closed-form expressions and require no elicitation of hyperparameters of the prior. The method guarantees a monotone estimator of the survival function and accommodates time-varying regression coefficients and covariates. To facilitate frequentist-type inference based on large-sample approximation, we present the asymptotic properties of the semiparametric empirical Bayes estimates. We illustrate the implementation and advantages of our methodology with a reanalysis of a survival dataset and a simulation study."
"10.1093/biomet/asp017","2009","Objective {B}ayesian model selection in {G}aussian graphical models","2","This paper presents a default model-selection procedure for Gaussian graphical models that involves two new developments. First, we develop a default version of the hyper-inverse Wishart prior for restricted covariance matrices, called the hyper-inverse Wishart g-prior, and show how it corresponds to the implied fractional prior for selecting a graph using fractional Bayes factors. Second, we apply a class of priors that automatically handles the problem of multiple hypothesis testing. We demonstrate our methods on a variety of simulated examples, concluding with a real example analyzing covariation in mutual-fund returns. These studies reveal that the combined use of a multiplicity-correction prior on graphs and fractional Bayes factors for computing marginal likelihoods yields better performance than existing Bayesian methods."
"10.1093/biomet/asp009","2009","Dimension reduction in time series and the dynamic factor model","0","This note shows that the dimension reduction method proposed by Li & Shedden (2002) is equivalent to the dynamic factor model introduced by Pena & Box (1987)."
"10.1093/biomet/asp022","2009","Saddlepoint approximation for mixture models","1","Two-component mixture distributions with one component a point mass and the other a continuous density may be used as priors for Bayesian inference when sparse representation of an underlying signal is required. We show how saddlepoint approximation in such models can yield highly accurate quantiles for posterior distributions, and illustrate this numerically, using wavelet regression with point mass/Laplace and point mass/normal prior distributions."
"10.1093/biomet/asp007","2009","Scale adjustments for classifiers in high-dimensional, low sample size settings","2","Distance-based classifiers are generally considered to be effective at discriminating between populations that differ in location. Indeed, nearest-neighbour methods and the support vector machine are frequently used in very high-dimensional problems involving gene expression data, where it is believed that elevated levels of expression convey much of the information for classification. However, one problem inherent to distance-based classifiers is that scale differences can mask location differences. In consequence, such classifiers can have poor performance if the information for classification accumulates through a large number of relatively small location differences in data components, rather than via large differences. In this paper, we show that a simple adjustment for scale, applicable to a variety of distance-based classifiers, can remedy the problem. For some classifiers, such as those based on the support vector machine or the centroid method, scale corrections are important primarily in the case of small training-sample sizes. However, for other classifiers, including those based on nearest-neighbour and average-distance methods, scale adjustments are helpful more generally."
"10.1093/biomet/asp010","2009","Marginal analysis of panel counts through estimating functions","0","We develop nonparametric estimation procedures for the marginal mean function of a counting process based on periodic observations, using two types of self-consistent estimating equations. The first is derived from the likelihood studied by Wellner & Zhang (2000), assuming a Poisson counting process. It gives a nondecreasing estimator, which equals the nonparametric maximum likelihood estimator of Wellner & Zhang and is consistent without the Poisson assumption. Motivated by the construction of parametric generalized estimating equations, the second type is a set of data-adaptive quasi-score functions, which are likelihood estimating functions under a mixed-Poisson assumption. We evaluate the procedures using simulation, and illustrate them with the data from a bladder cancer study."
"10.1093/biomet/asp018","2009","Double block bootstrap confidence intervals for dependent data","0","The block bootstrap confidence interval for dependent data can outperform the conventional normal approximation only with nontrivial studentization which, in the case of complicated statistics, calls for specialist treatment and often results in unstable endpoints. We propose two double block bootstrap approaches for improving the accuracy of the block bootstrap confidence interval under very general conditions. The first approach calibrates the nominal coverage level and the second calculates studentizing factors directly from a block bootstrap series without the need for nontrivial analytical treatment. We prove that the two approaches reduce the coverage error of the block bootstrap interval by an order of magnitude with simple tuning of block lengths at the two block bootstrapping levels. Empirical properties of the procedures are investigated by simulations and application to an econometric time series."
"10.1093/biomet/asp011","2009","Non-finite {F}isher information and homogeneity: an {EM} approach","1","Even simple examples of finite mixture models can fail to fulfil the regularity conditions that are routinely assumed in standard parametric inference problems. Many methods have been investigated for testing for homogeneity in finite mixture models, for example, but all rely on regularity conditions including the finiteness of the Fisher information and the space of the mixing parameter being a compact subset of some Euclidean space. Very simple examples where such assumptions fail include mixtures of two geometric distributions and two exponential distributions, and, more generally, mixture models in scale distribution families. To overcome these difficulties, we propose and study an em-test statistic, which has a simple limiting distribution for examples in this paper. Simulations show that the em-test has accurate Type I errors and is more efficient than existing methods when they are applicable. A real example is included."
"10.1093/biomet/asp006","2009","Optimal testing of multiple hypotheses with common effect direction","1","We present a theoretical basis for testing related endpoints. Typically, it is known how to construct tests of the individual hypotheses, but not how to combine them into a multiple test procedure that controls the familywise error rate. Using the closure method, we emphasize the role of consonant procedures, from an interpretive as well as a theoretical viewpoint. Surprisingly, even if each intersection test has an optimality property, the overall procedure obtained by applying closure to these tests may be inadmissible. We introduce a new procedure, which is consonant and has a maximin property under the normal model. The results are then applied to PROactive, a clinical trial designed to investigate the effectiveness of a glucose-lowering drug on macrovascular outcomes among patients with type 2 diabetes."
"10.1093/biomet/asp002","2009","Adjusting for covariate effects on classification accuracy using the covariate-adjusted receiver operating characteristic curve","0","Recent scientific and technological innovations have produced an abundance of potential markers that are being investigated for their use in disease screening and diagnosis. In evaluating these markers, it is often necessary to account for covariates associated with the marker of interest. Covariates may include subject characteristics, expertise of the test operator, test procedures or aspects of specimen handling. In this paper, we propose the covariate-adjusted receiver operating characteristic curve, a measure of covariate-adjusted classification accuracy. Nonparametric and semiparametric estimators are proposed, asymptotic distribution theory is provided and finite sample performance is investigated. For illustration we characterize the age-adjusted discriminatory accuracy of prostate-specific antigen as a biomarker for prostate cancer."
"10.1093/biomet/asp020","2009","A group bridge approach for variable selection","0","In multiple regression problems when covariates can be naturally grouped, it is important to carry out feature selection at the group and within-group individual variable levels simultaneously. The existing methods, including the lasso and group lasso, are designed for either variable selection or group selection, but not for both. We propose a group bridge approach that is capable of simultaneous selection at both the group and within-group individual variable levels. The proposed approach is a penalized regularization method that uses a specially designed group bridge penalty. It has the oracle group selection property, in that it can correctly select important groups with probability converging to one. In contrast, the group lasso and group least angle regression methods in general do not possess such an oracle property in group selection. Simulation studies indicate that the group bridge has superior performance in group and individual variable selection relative to several existing methods."
"10.1093/biomet/asp013","2009","A generalized {D}antzig selector with shrinkage tuning","1","The Dantzig selector performs variable selection and model fitting in linear regression. It uses an L(1) penalty to shrink the regression coefficients towards zero, in a similar fashion to the lasso. While both the lasso and Dantzig selector potentially do a good job of selecting the correct variables, they tend to overshrink the final coefficients. This results in an unfortunate trade-off. One can either select a high shrinkage tuning parameter that produces an accurate model but poor coefficient estimates or a low shrinkage parameter that produces more accurate coefficients but includes many irrelevant variables. We extend the Dantzig selector to fit generalized linear models while eliminating overshrinkage of the coefficient estimates, and develop a computationally efficient algorithm, similar in nature to least angle regression, to compute the entire path of coefficient estimates. A simulation study illustrates the advantages of our approach relative to others. We apply the methodology to two datasets."
"10.1093/biomet/asp005","2009","Generalized method of moments estimation for linear regression with clustered failure time data","0","We propose a generalized method of moments approach to the accelerated failure time model with correlated survival data. We study the semiparametric rank estimator using martingale-based moments. We circumvent direct estimation of correlation parameters by concatenating the moments and minimizing a quadratic objective function. We establish the consistency and asymptotic normality of the parameter estimators, and derive the limiting distribution of the objective function. We carry out simulation studies to examine the finite-sample properties of the method, and demonstrate its substantial efficiency gain over the conventional method. Finally, we illustrate the new proposal with an example from a diabetic retinopathy study."
"10.1093/biomet/asp014","2009","Mixtures of {P}olya trees for flexible spatial frailty survival modelling","0","Mixtures of Polya trees offer a very flexible nonparametric approach for modelling time-to-event data. Many such settings also feature spatial association that requires further sophistication, either at the point level or at the lattice level. In this paper, we combine these two aspects within three competing survival models, obtaining a data analytic approach that remains computationally feasible in a fully hierarchical Bayesian framework using Markov chain Monte Carlo methods. We illustrate our proposed methods with an analysis of spatially oriented breast cancer survival data from the Surveillance, Epidemiology and End Results program of the National Cancer Institute. Our results indicate appreciable advantages for our approach over competing methods that impose unrealistic parametric assumptions, ignore spatial association or both."
"10.1093/biomet/asn063","2009","A note on cause-specific residual life","0","In medical research, investigators often wish to characterize the distributions of remaining lifetimes. While nonparametric analyses of residual life distributions have been widely studied with independently right-censored data, residual life analysis has not been examined in the competing risks setting, with multiple, potentially dependent, failure types. We define the cause-specific residual life distribution as the residual cumulative incidence function conditionally on survival to a given time. Because of the improper form of the cause-specific distribution, the mean cause-specific residual lifetime does not exist, theoretically. We develop nonparametric inferences for the cause-specific residual life function and its corresponding quantiles, which may exist. Theoretical justification, including uniform consistency and weak convergence, is established. Simulation studies and a breast cancer data analysis demonstrate the practical utility of the methods."
"10.1093/biomet/asn059","2009","A note on profile likelihood for exponential tilt mixture models","2","Suppose that independent observations are drawn from multiple distributions, each of which is a mixture of two component distributions such that their log density ratio satisfies a linear model with a slope parameter and an intercept parameter. Inference for such models has been studied using empirical likelihood, and mixed results have been obtained. The profile empirical likelihood of the slope and intercept has an irregularity at the null hypothesis so that the two component distributions are equal. We derive a profile empirical likelihood and maximum likelihood estimator of the slope alone, and obtain the usual asymptotic properties for the estimator and the likelihood ratio statistic regardless of the null. Furthermore, we show the maximum likelihood estimator of the slope and intercept jointly is consistent and asymptotically normal regardless of the null. At the null, the joint maximum likelihood estimator falls along a straight line through the origin with perfect correlation asymptotically to the first order."
"10.1093/biomet/asn073","2009","A note on semiparametric efficient inference for two-stage outcome-dependent sampling with a continuous outcome","1","Outcome-dependent sampling designs have been shown to be a cost-effective way to enhance study efficiency. We show that the outcome-dependent sampling design with a continuous outcome can be viewed as an extension of the two-stage case-control designs to the continuous-outcome case. We further show that the two-stage outcome-dependent sampling has a natural link with the missing-data and biased-sampling frameworks. Through the use of semiparametric inference and missing-data techniques, we show that a certain semiparametric maximum-likelihood estimator is computationally convenient and achieves the semiparametric efficient information bound. We demonstrate this both theoretically and through simulation."
"10.1093/biomet/asn072","2009","Fast block variance estimation procedures for inhomogeneous spatial point processes","1","We introduce two new variance estimation procedures that use non-overlapping and overlapping blocks, respectively. The non-overlapping blocks estimator can be viewed as the limit of the thinned block bootstrap estimator recently proposed in Guan Loh (2007), by letting the number of thinned processes and bootstrap samples therein both increase to infinity. The non-overlapping blocks estimator can be obtained quickly since it does not require any thinning or bootstrap steps, and it is more stable. The overlapping blocks estimator further improves the performance of the non-overlapping blocks with a modest increase in computation time. A simulation study demonstrates the superiority of the proposed estimators over the thinned block bootstrap estimator."
"10.1093/biomet/asn061","2009","On fuzzy familywise error rate and false discovery rate procedures for discrete distributions","0","Fuzzy multiple comparisons procedures are introduced as a solution to the problem of multiple comparisons for discrete test statistics. The critical function of the randomized p-values is proposed as a measure of evidence against the null hypotheses. The classical concept of randomized tests is extended to multiple comparisons. This approach makes all theory of multiple comparisons developed for continuously distributed statistics automatically applicable to the discrete case. Examples of familywise error rate and false discovery rate procedures are discussed and an application to linkage disequilibrium testing is given. Software for implementing the procedures is available."
"10.1093/biomet/asn055","2009","Dealing with limited overlap in estimation of average treatment effects","0","Estimation of average treatment effects under unconfounded or ignorable treatment assignment is often hampered by lack of overlap in the covariate distributions between treatment groups. This lack of overlap can lead to imprecise estimates, and can make commonly used estimators sensitive to the choice of specification. In such cases researchers have often used ad hoc methods for trimming the sample. We develop a systematic approach to addressing lack of overlap. We characterize optimal subsamples for which the average treatment effect can be estimated most precisely. Under some conditions, the optimal selection rules depend solely on the propensity score. For a wide range of distributions, a good approximation to the optimal rule is provided by the simple rule of thumb to discard all units with estimated propensity scores outside the range [0.1,0.9]."
"10.1093/biomet/asn068","2009","Reducing variability of crossvalidation for smoothing-parameter choice","0","One of the attractions of crossvalidation, as a tool for smoothing-parameter choice, is its applicability to a wide variety of estimator types and contexts. However, its detractors comment adversely on the relatively high variance of crossvalidatory smoothing parameters, noting that this compromises the performance of the estimators in which those parameters are used. We show that the variability can be reduced simply, significantly and reliably by employing bootstrap aggregation or bagging. We establish that in theory, when bagging is implemented using an adaptively chosen resample size, the variability of crossvalidation can be reduced by an order of magnitude. However, it is arguably more attractive to use a simpler approach, based for example on half-sample bagging, which can reduce variability by approximately 50%."
"10.1093/biomet/asn060","2009","Wilcoxon-type generalized {B}ayesian information criterion","0","We develop a generalized Bayesian information criterion for regression model selection. The new criterion relaxes the usually strong distributional assumption associated with Schwarz's bic by adopting a Wilcoxon-type dispersion function and appropriately adjusting the penalty term. We establish that the Wilcoxon-type generalized bic preserves the consistency of Schwarz's bic without the need to assume a parametric likelihood. We also show that it outperforms Schwarz's bic with heavier-tailed data in the sense that asymptotically it can yield substantially smaller L(2) risk. On the other hand, when the data are normally distributed, both criteria have similar L(2) risk. The new criterion function is convex and can be conveniently computed using existing statistical software. Our proposal provides a flexible yet highly efficient alternative to Schwarz's bic; at the same time, it broadens the scope of Wilcoxon inference, which has played a fundamental role in classical nonparametric analysis."
"10.1093/biomet/asn074","2009","Model checking in regression via dimension reduction","0","Lack-of-fit checking for parametric and semiparametric models is essential in reducing misspecification. The efficiency of most existing model-checking methods drops rapidly as the dimension of the covariates increases. We propose to check a model by projecting the fitted residuals along a direction that adapts to the systematic departure of the residuals from the desired pattern. Consistency of the method is proved for parametric and semiparametric regression models. A bootstrap implementation is also discussed. Simulation comparisons with several existing methods are made, suggesting that the proposed methods are more efficient than the existing methods when the dimension increases. Air pollution data from Chicago are used to illustrate the procedure."
"10.1093/biomet/asn067","2009","Confidence intervals for spectral mean and ratio statistics","0","We propose a new method, to construct confidence intervals for spectral mean and related ratio statistics of a stationary process, that avoids direct estimation of their asymptotic variances. By introducing a bandwidth, a self-normalization procedure is adopted and the distribution of the new statistic is asymptotically nuisance-parameter free. The bandwidth is chosen using information criteria and a moving average sieve approximation. Through a simulation study, we demonstrate good finite sample performance of our method when the sample size is moderate, while a comparison with an empirical likelihood-based method for ratio statistics is made, confirming a wider applicability of our method."
"10.1093/biomet/asn062","2009","Bayesian-inspired minimum aberration two- and four-level designs","0","Motivated by a Bayesian framework, we propose a new minimum aberration-type criterion for designing experiments with two- and four-level factors. The Bayesian approach helps in overcoming the ad hoc nature of effect ordering in the existing minimum aberration-type criteria. The approach is also capable of distinguishing between qualitative and quantitative factors. Numerous examples are given to demonstrate its advantages."
"10.1093/biomet/asn070","2009","D-optimal design of split-split-plot experiments","0","In industrial experimentation, there is growing interest in studies that span more than one processing step. Convenience often dictates restrictions in randomization in passing from one processing step to another. When the study encompasses three processing steps, this leads to split-split-plot designs. We provide an algorithm for computing D-optimal split-split-plot designs and several illustrative examples."
"10.1093/biomet/asn069","2009","Partial and latent ignorability in missing-data problems","2","When an assumption of missing at random is untenable, it becomes necessary to model missing-data indicators, which carry information about the parameters of the complete-data population. Within a given application, however, researchers may believe that some aspects of missingness are ignorable but others are not. We argue that there are two different ways to formalize the notion that only part of the missingness is ignorable. These approaches correspond to assumptions that we call partially missing at random and latently missing at random. We explain these concepts and apply them in a latent-class analysis of survey questions with item nonresponse."
"10.1093/biomet/asn056","2009","Efficient nonparametric estimation of causal effects in randomized trials with noncompliance","1","Causal approaches based on the potential outcome framework provide a useful tool for addressing noncompliance problems in randomized trials. We propose a new estimator of causal treatment effects in randomized clinical trials with noncompliance. We use the empirical likelihood approach to construct a profile random sieve likelihood and take into account the mixture structure in outcome distributions, so that our estimator is robust to parametric distribution assumptions and provides substantial finite-sample efficiency gains over the standard instrumental variable estimator. Our estimator is asymptotically equivalent to the standard instrumental variable estimator, and it can be applied to outcome variables with a continuous, ordinal or binary scale. We apply our method to data from a randomized trial of an intervention to improve the treatment of depression among depressed elderly patients in primary care practices."
"10.1093/biomet/asp058","2009","Construction of orthogonal {L}atin hypercube designs","1","Latin hypercube designs have found wide application. Such designs guarantee uniform samples for the marginal distribution of each input variable. We propose a method for constructing orthogonal Latin hypercube designs in which all the linear terms are orthogonal not only to each other, but also to the quadratic terms. This construction method is convenient and flexible, and the resulting designs can accommodate many more factors than can existing ones."
"10.1093/biomet/asp042","2009","Some design properties of a rejective sampling procedure","0","Occasionally, a selected probability sample may appear undesirable with respect to the available auxiliary information. In such a situation, the practitioner might consider rejecting the sample and selecting a new set of sample elements. We consider a procedure in which the probability sample is rejected unless the sample mean of an auxiliary vector is within a specified distance of the population mean. It is proven that the large sample mean and variance of the regression estimator for the rejective sample are the same as those of the regression estimator for the original selection procedure. Likewise, the usual estimator of variance for the regression estimator is appropriate for the rejective sample. In a Monte Carlo experiment, the large sample properties hold for relatively small samples and the Monte Carlo results are in agreement with the theoretical orders of approximation. The efficiency effect of the described rejective sampling is o(n(N)(-1), where n(N) is the expected sample size, but the effect can be important for particular samples. For example, rejective sampling can be used to eliminate those samples that give negative weights for the regression estimator."
"10.1093/biomet/asp047","2009","Bayesian lasso regression","5","The lasso estimate for linear regression corresponds to a posterior mode when independent, double-exponential prior distributions are placed on the regression coefficients. This paper introduces new aspects of the broader Bayesian treatment of lasso regression. A direct characterization of the regression coefficients' posterior distribution is provided, and computation and inference under this characterization is shown to be straightforward. Emphasis is placed on point estimation using the posterior mean, which facilitates prediction of future observations via the posterior predictive distribution. It is shown that the standard lasso prediction method does not necessarily agree with model-based, Bayesian predictions. A new Gibbs sampler for Bayesian lasso regression is introduced."
"10.1093/biomet/asp049","2009","Bayesian analysis of matrix normal graphical models","1","We present Bayesian analyses of matrix-variate normal data with conditional independencies induced by graphical model structuring of the characterizing covariance matrix parameters. This framework of matrix normal graphical models includes prior specifications, posterior computation using Markov chain Monte Carlo methods, evaluation of graphical model uncertainty and model structure search. Extensions to matrix-variate time series embed matrix normal graphs in dynamic models. Examples highlight questions of graphical model uncertainty, search and comparison in matrix data contexts. These models may be applied in a number of areas of multivariate analysis, time series and also spatial modelling."
"10.1093/biomet/asp051","2009","Inference on population size in binomial detectability models","2","Many models for biological populations, including simple mark-recapture models and distance sampling models, involve a binomially distributed number, n, of observations x(1), ..., x(n) on members of a population of size N. Two popular estimators of (N, theta), where theta is a vector parameter, are the maximum likelihood estimator ((N) over cap, (theta) over cap) and the conditional maximum likelihood estimator ((N) over cap (c) (theta) over cap (c)) based on the conditional distribution of x(1), ..., x(n) given n. We derive the large-N asymptotic distributions of (((N) over cap, (theta) over cap) and ((N) over cap (c) (theta) over cap (c)), and give formulae for the biases of log (N) over cap and log (N) over cap (c). We show that the difference (N) over capc -(N) over cap is, remarkably, of order 1 and we give a simple formula for the leading part of this difference. Simulations indicate that in many cases this formula is very accurate and that confidence intervals based on the asymptotic distribution have excellent coverage. An extension to product-binomial models is given."
"10.1093/biomet/asp055","2009","Bias reduction in exponential family nonlinear models","1","In Firth (1993, Biometrika) it was shown how the leading term in the asymptotic bias of the maximum likelihood estimator is removed by adjusting the score vector, and that in canonical-link generalized linear models the method is equivalent to maximizing a penalized likelihood that is easily implemented via iterative adjustment of the data. Here a more general family of bias-reducing adjustments is developed for a broad class of univariate and multivariate generalized nonlinear models. The resulting formulae for the adjusted score vector are computationally convenient, and in univariate models they directly suggest implementation through an iterative scheme of data adjustment. For generalized linear models a necessary and sufficient condition is given for the existence of a penalized likelihood interpretation of the method. An illustrative application to the Goodman row-column association model shows how the computational simplicity and statistical benefits of bias reduction extend beyond generalized linear models."
"10.1093/biomet/asp057","2009","A new look at time series of counts","0","This paper proposes a simple new model for stationary time series of integer counts. Previous work has focused on thinning methods and classical time series autoregressive moving-average difference equations; in contrast, our methods use a renewal process to generate a correlated sequence of Bernoulli trials. By superpositioning independent copies of such processes, stationary series with binomial, Poisson, geometric or any other discrete marginal distribution can be readily constructed. The model class proposed is parsimonious, non-Markov and readily generates series with either short- or long-memory autocovariances. The model can be fitted with linear prediction techniques for stationary series. As an example, a stationary series with binomial marginal distributions is fitted to the number of rainy days in 210 consecutive weeks at Key West, Florida."
"10.1093/biomet/asp053","2009","Sinh-arcsinh distributions","0","We introduce the sinh-arcsinh transformation and hence, by applying it to a generating distribution with no parameters other than location and scale, usually the normal, a new family of sinh-arcsinh distributions. This four-parameter family has symmetric and skewed members and allows for tailweights that are both heavier and lighter than those of the generating distribution. The central place of the normal distribution in this family affords likelihood ratio tests of normality that are superior to the state-of-the-art in normality testing because of the range of alternatives against which they are very powerful. Likelihood ratio tests of symmetry are also available and are very successful. Three-parameter symmetric and asymmetric subfamilies of the full family are also of interest. Heavy-tailed symmetric sinh-arcsinh distributions behave like Johnson S-U distributions, while their light-tailed counterparts behave like sinh-normal distributions, the sinh-arcsinh family allowing a seamless transition between the two, via the normal, controlled by a single parameter. The sinh-arcsinh family is very tractable and many properties are explored. Likelihood inference is pursued, including an attractive reparameterization. Illustrative examples are given. A multivariate version is considered. Options and extensions are discussed."
"10.1093/biomet/asp029","2009","A negative binomial model for time series of counts","0","We study generalized linear models for time series of counts, where serial dependence is introduced through a dependent latent process in the link function. Conditional on the covariates and the latent process, the observation is modelled by a negative binomial distribution. To estimate the regression coefficients, we maximize the pseudolikelihood that is based on a generalized linear model with the latent process suppressed. We show the consistency and asymptotic normality of the generalized linear model estimator when the latent process is a stationary strongly mixing process. We extend the asymptotic results to generalized linear models for time series, where the observation variable, conditional on covariates and a latent process, is assumed to have a distribution from a one-parameter exponential family. Thus, we unify in a common framework the results for Poisson log-linear regression models of Davis et al. (2000), negative binomial logit regression models and other similarly specified generalized linear models."
"10.1093/biomet/asp037","2009","Effects of data dimension on empirical likelihood","3","We evaluate the effects of data dimension on the asymptotic normality of the empirical likelihood ratio for high-dimensional data under a general multivariate model. Data dimension and dependence among components of the multivariate random vector affect the empirical likelihood directly through the trace and the eigenvalues of the covariance matrix. The growth rates to infinity we obtain for the data dimension improve the rates of Hjort et al. (2008)."
"10.1093/biomet/asp023","2009","Markov models for accumulating mutations","0","We introduce and analyze a waiting time model for the accumulation of genetic changes. The continuous-time conjunctive Bayesian network is defined by a partially ordered set of mutations and by the rate of fixation of each mutation. The partial order encodes constraints on the order in which mutations can fixate in the population, shedding light on the mutational pathways underlying the evolutionary process. We study a censored version of the model and derive equations for an em algorithm to perform maximum likelihood estimation of the model parameters. We also show how to select the maximum likelihood partially ordered set. The model is applied to genetic data from cancer cells and from drug resistant human immunodeficiency viruses, indicating implications for diagnosis and treatment."
"10.1093/biomet/asp036","2009","Approximating the {$\alpha$}-permanent","0","The standard matrix permanent is the solution to a number of combinatorial and graph-theoretic problems, and the alpha-weighted permanent is the density function for a class of Cox processes called boson processes. The exact computation of the ordinary permanent is known to be #P-complete, and the same appears to be the case for the alpha-permanent for most values of alpha. At present, the lack of a satisfactory algorithm for approximating the alpha-permanent is a formidable obstacle to the use of boson processes in applied work. This paper proposes an importance-sampling estimator using nonuniform random permutations generated in a cycle format. Empirical investigation reveals that the estimator works well for the sorts of matrices that arise in point-process applications, involving up to a few hundred points. We conclude with a numerical illustration of the Bayes estimate of the intensity function of a boson point process, which is a ratio of alpha-permanents."
"10.1093/biomet/asp035","2009","Asymptotic properties of penalized spline estimators","2","We study the class of penalized spline estimators, which enjoy similarities to both regression splines, without penalty and with fewer knots than data points, and smoothing splines, with knots equal to the data points and a penalty controlling the roughness of the fit. Depending on the number of knots, sample size and penalty, we show that the theoretical properties of penalized regression spline estimators are either similar to those of regression splines or to those of smoothing splines, with a clear breakpoint distinguishing the cases. We prove that using fewer knots results in better asymptotic rates than when using a large number of knots. We obtain expressions for bias and variance and asymptotic rates for the number of knots and penalty parameter."
"10.1093/biomet/asp038","2009","Adaptive regularization using the entire solution surface","1","Several sparseness penalties have been suggested for delivery of good predictive performance in automatic variable selection within the framework of regularization. All assume that the true model is sparse. We propose a penalty, a convex combination of the L-1- and L-infinity-norms, that adapts to a variety of situations including sparseness and nonsparseness, grouping and nongrouping. The proposed penalty performs grouping and adaptive regularization. In addition, we introduce a novel homotopy algorithm utilizing subgradients for developing regularization solution surfaces involving multiple regularizers. This permits efficient computation and adaptive tuning. Numerical experiments are conducted using simulation. In simulated and real examples, the proposed penalty compares well against popular alternatives."
"10.1093/biomet/asp004","2009","Some results on {$D$}-optimal designs for nonlinear models with applications","0","Sufficient conditions are established for the locally D$-optimal design for a nonlinear model to have a minimal number of support points. The conditions are applied to obtain locally D-optimal designs for a one-compartment pharmacokinetic model and a Poisson regression model."
"10.1093/biomet/asp003","2009","Jackknife estimation of mean squared error of small area predictors in nonlinear mixed models","1","Empirical Bayes predictors of small area parameters of interest are often obtained under a linear mixed model for continuous response data or a generalized linear mixed model for binary responses or count data. However, estimation of the unconditional mean squared error of prediction is complicated, particularly for a nonlinear mixed model. Jiang et al. (2002) proposed a jackknife method for estimating the unconditional mean squared error and showed that the resulting estimator is nearly unbiased. The leading term of this estimator does not depend on the area-specific responses in the nonlinear case, whereas the posterior variance of the small area parameter given the model parameters is area-specific. Rao (2003) proposed an alternative method that leads to a computationally simpler jackknife estimator with an area-specific leading term. We show that a modification of Rao's method leads to a nearly unbiased area-specific jackknife estimator, which is also nearly unbiased for the conditional mean squared error given the area-specific responses. We examine the relative performances of the jackknife estimators, conditionally as well as unconditionally, in a simulation study, and apply the proposed method to estimate small area mean squared errors in disease mapping problems."
"10.1093/biomet/asp015","2009","Nonparametric additive regression for repeatedly measured data","1","We develop an easily computed smooth backfitting algorithm for additive model fitting in repeated measures problems. Our methodology easily copes with various settings, such as when some covariates are the same over repeated response measurements. We allow for a working covariance matrix for the regression errors, showing that our method is most efficient when the correct covariance matrix is used. The component functions achieve the known asymptotic variance lower bound for the scalar argument case. Smooth backfitting also leads directly to design-independent biases in the local linear case. Simulations show our estimator has smaller variance than the usual kernel estimator. This is also illustrated by an example from nutritional epidemiology."
"10.1093/biomet/asp012","2009","Covariate-adjusted generalized linear models","0","We propose covariate adjustment methodology for a situation where one wishes to study the dependence of a generalized response on predictors while both predictors and response are distorted by an observable covariate. The distorting covariate is thought of as a size measurement that affects predictors in a multiplicative fashion. The generalized response is modelled by means of a random threshold, where the subject-specific thresholds are affected by a multiplicative factor that is a function of the distorting covariate. While the various factors are modelled as smooth unknown functions of the distorting covariate, the underlying relationship between response and covariates is assumed to be governed by a generalized linear model with a known link function. This model provides an extension of a covariate-adjusted regression approach to the case of a generalized linear model. We demonstrate that this contamination model leads to a semiparametric varying-coefficient model. Numerical implementation is straightforward by combining binning, quasilikelihood, and smoothing steps. The asymptotic distribution of the proposed estimators for the regression coefficients of the latent generalized linear model is derived by means of a martingale central limit theorem. Combining this result with consistent estimators for the asymptotic variance makes it then possible to obtain asymptotic inference for the targeted parameters. Both real and simulated data are used in illustrating the proposed methodology."
"10.1093/biomet/asp016","2009","Hierarchically penalized {C}ox regression with grouped variables","0","In many biological and other scientific applications, predictors are often naturally grouped. For example, in biological applications, assayed genes or proteins are grouped by biological roles or biological pathways. When studying the dependence of survival outcome on these grouped predictors, it is desirable to select variables at both the group level and the within-group level. In this article, we develop a new method to address the group variable selection problem in the Cox proportional hazards model. Our method not only effectively removes unimportant groups, but also maintains the flexibility of selecting variables within the identified groups. We also show that the new method offers the potential for achieving the asymptotic oracle property."
"10.1093/biomet/asp008","2009","Gamma frailty transformation models for multivariate survival times","0","We propose a class of transformation models for multivariate failure times. The class of transformation models generalize the usual gamma frailty model and yields a marginally linear transformation model for each failure time. Nonparametric maximum likelihood estimation is used for inference. The maximum likelihood estimators for the regression coefficients are shown to be consistent and asymptotically normal, and their asymptotic variances attain the semiparametric efficiency bound. Simulation studies show that the proposed estimation procedure provides asymptotically efficient estimates and yields good inferential properties for small sample sizes. The method is illustrated using data from a cardiovascular study."
"10.1093/biomet/asp021","2009","Nonparametric {B}ayes local partition models for random effects","1","This paper focuses on the problem of choosing a prior for an unknown random effects distribution within a Bayesian hierarchical model. The goal is to obtain a sparse representation by allowing a combination of global and local borrowing of information. A local partition process prior is proposed, which induces dependent local clustering. Subjects can be clustered together for a subset of their parameters, and one learns about similarities between subjects increasingly as parameters are added. Some basic properties are described, including simple two-parameter expressions for marginal and conditional clustering probabilities. A slice sampler is developed which bypasses the need to approximate the countably infinite random measure in performing posterior computation. The methods are illustrated using simulation examples, and an application to hormone trajectory data."
"10.1093/biomet/asn065","2009","A note on time-ordered classification [\refcno 0368317]","0",""
"10.1093/biomet/asn064","2009","Construction of orthogonal and nearly orthogonal {L}atin hypercubes","3","We propose a method for constructing orthogonal or nearly orthogonal Latin hypercubes. The method yields a large Latin hypercube by coupling an orthogonal array of index unity with a small Latin hypercube. It is shown that the large Latin hypercube inherits the exact or near orthogonality of the small Latin hypercube. Thus, effort for searching for large Latin hypercubes, that are exactly or nearly orthogonal, can be focussed on finding small Latin hypercubes with the same property. We obtain a useful collection of orthogonal or nearly orthogonal Latin hypercubes, which have a large factor-to-run ratio and the results are often much more economical than existing methods."
"10.1093/biomet/asn054","2009","Bayesian nonparametric functional data analysis through density estimation","0","In many modern experimental settings, observations are obtained in the form of functions and interest focuses on inferences about a collection of such functions. We propose a hierarchical model that allows us simultaneously to estimate multiple curves nonparametrically by using dependent Dirichlet process mixtures of Gaussian distributions to characterize the joint distribution of predictors and outcomes. Function estimates are then induced through the conditional distribution of the outcome given the predictors. The resulting approach allows for flexible estimation and clustering, while borrowing information across curves. We also show that the function estimates we obtain are consistent on the space of integrable functions. As an illustration, we consider an application to the analysis of conductivity and temperature at depth data in the north Atlantic."
"10.1093/biomet/asn071","2009","Tapered empirical likelihood for time series data in time and frequency domains","0","We investigate data tapering in two formulations of empirical likelihood for time series. One empirical likelihood is formed from tapered data blocks in the time domain and a second is based on the tapered periodogram in the frequency domain. Limiting distributions are provided for both empirical likelihood versions under tapering. Theoretical and simulation evidence indicates that a data taper improves the coverage accuracy of empirical likelihood confidence intervals for time series parameters, such as means and correlations."
"10.1093/biomet/asn066","2009","Optimal two-level regular fractional factorial block and split-plot designs","0","We propose a general and unified approach to the selection of regular fractional factorial designs, which can be applied to experiments that are unblocked, blocked or have a split-plot structure. Our criterion is derived as a good surrogate for the model-robustness criterion of information capacity. In the case of random block effects, it takes the ratio of intra- and interblock variances into account. In most of the cases we have examined, there exist designs that are optimal for all values of that ratio. Examples of optimal designs that depend on the ratio are provided. We also demonstrate that our criterion can further discriminate designs that cannot be distinguished by the existing minimum-aberration criteria."
"10.1093/biomet/asn057","2009","Orthogonal and nearly orthogonal designs for computer experiments","5","We introduce a method for constructing a rich class of designs that are suitable for use in computer experiments. The designs include Latin hypercube designs and two-level fractional factorial designs as special cases and fill the vast vacuum between these two familiar classes of designs. The basic construction method is simple, building a series of larger designs based on a given small design. If the base design is orthogonal, the resulting designs are orthogonal; likewise, if the base design is nearly orthogonal, the resulting designs are nearly orthogonal. We present two generalizations of our basic construction method. The first generalization improves the projection properties of the basic method; the second generalization gives rise to designs that have smaller correlations. Sample constructions are presented and properties of these designs are discussed."
"10.1093/biomet/asp001","2009","Modelling pairwise dependence of maxima in space","0","We model pairwise dependence of temporal maxima, such as annual maxima of precipitation, that have been recorded in space, either on a regular grid or at irregularly spaced locations. The construction of our estimators stems from the variogram concept. The asymptotic properties of our pairwise dependence estimators are established through properties of empirical processes. The performance of our approach is illustrated by simulations and by the treatment of a real dataset. In addition to bringing new results about the asymptotic behaviour of copula estimators, the latter being linked to first-order variograms, one main advantage of our approach is to propose a simple connection between extreme value theory and geostatistics."
"10.1093/biomet/asn044","2008","A note on nonparametric quantile inference for competing risks and more complex multistate models","0","Nonparametric quantile inference for competing risks has recently been studied by Peng & Fine (2007). Their key result establishes uniform consistency and weak convergence of the inverse of the Aalen-Johansen estimator of the cumulative incidence function, using the representation of the cumulative incidence estimator as a sum of independent and identically distributed random variables. The limit process is of a form similar to that of the standard survival result, but with the cause-specific hazard of interest replacing the all-causes hazard. We show that this fact is not a coincidence, but can be derived from a general Hadamard differentiation result. We discuss a simplified proof and extensions of the approach to more complex multistate models. As a further consequence, we find that the bootstrap works."
"10.1093/biomet/asn040","2008","On an internal method for deriving a summary measure","0","Some preliminary comments are made about the reasons for combining component observations into composite or derived variables. A method for forming derived variables sensitive to specified changes in the underlying multivariate distribution is described and illustrated by an issue in a study of animal pathology."
"10.1093/biomet/asn037","2008","On consistency of {K}endall's tau under censoring","0","Necessary and sufficient conditions for consistency of a simple estimator of Kendall's tau under bivariate censoring are presented. The results are extended to data subject to bivariate left truncation as well as right censoring."
"10.1093/biomet/asn039","2008","On the consequences of overstratification","1","It is common, in particular in observational studies in epidemiology, to impose stratification to adjust for possible effects of age and other variables on the binary outcome of interest. Overstratification may lower the precision of the estimated effects of interest. Understratification risks bias. These issues are studied analytically. Asymptotic results show that loss of efficiency depends on the true effect and on a measure of the average imbalance across strata between exposed and unexposed individuals. Bias depends on the correlation between stratum-specific size imbalances and event rates in the unexposed. Approximate results are also given. An example is used."
"10.1093/biomet/asn038","2008","Forecasting with the age-period-cohort model and the extended chain-ladder model","1","We consider forecasting from age-period-cohort models, as well as from the extended chain-ladder model. The parameters of these models are known only to be identified up to linear trends. Forecasts from such models may therefore depend on arbitrary linear trends. A condition for invariant forecasts is proposed. A number of standard forecast models are analysed."
"10.1093/biomet/asn026","2008","Identification of the age-period-cohort model and the extended chain-ladder model","1","We consider the identification problem that arises in the age-period-cohort models as well as in the extended chain-ladder model. We propose a canonical parameterization based on the accelerations of the trends in the three factors. This parameterization is exactly identified and eases interpretation, estimation and forecasting. The canonical parameterization is applied to a class of index sets which have trapezoidal shapes, including various Lexis diagrams and the insurance-reserving triangles."
"10.1093/biomet/asn036","2008","Estimating the false discovery rate using the stochastic approximation algorithm","0","Testing of multiple hypotheses involves statistics that are strongly dependent in some applications, but most work on this subject is based on the assumption of independence. We propose a new method for estimating the false discovery rate of multiple hypothesis tests, in which the density of test scores is estimated parametrically by minimizing the Kullback-Leibler distance between the unknown density and its estimator using the stochastic approximation algorithm, and the false discovery rate is estimated using the ensemble averaging method. Our method is applicable under general dependence between test statistics. Numerical comparisons between our method and several competitors, conducted on simulated and real data examples, show that our method achieves more accurate control of the false discovery rate in almost all scenarios."
"10.1093/biomet/asn049","2008","Semiparametric maximum likelihood estimation in normal transformation models for bivariate survival data","1","We consider a class of semiparametric normal transformation models for right-censored bivariate failure times. Nonparametric hazard rate models are transformed to a standard normal model and a joint normal distribution is assumed for the bivariate vector of transformed variates. A semiparametric maximum likelihood estimation procedure is developed for estimating the marginal survival distribution and the pairwise correlation parameters. This produces an efficient estimator of the correlation parameter of the semiparametric normal transformation model, which characterizes the dependence of bivariate survival outcomes. In addition, a simple positive-mass-redistribution algorithm can be used to implement the estimation procedures. Since the likelihood function involves infinite-dimensional parameters, empirical process theory is utilized to study the asymptotic properties of the proposed estimators, which are shown to be consistent, asymptotically normal and semiparametric efficient. A simple estimator for the variance of the estimates is derived. Finite sample performance is evaluated via extensive simulations."
"10.1093/biomet/asn042","2008","Multiple imputation when records used for imputation are not used or disseminated for analysis","2","When some of the records used to estimate the imputation models in multiple imputation are not used or available for analysis, the usual multiple imputation variance estimator has positive bias. We present an alternative approach that enables unbiased estimation of variances and, hence, calibrated inferences in such contexts. First, using all records, the imputer samples m values of the parameters of the imputation model. Second, for each parameter draw, the imputer simulates the missing values for all records n times. From these mn completed datasets, the imputer can analyse or disseminate the appropriate subset of records. We develop methods for interval estimation and significance testing for this approach. Methods are presented in the context of multiple imputation for measurement error."
"10.1093/biomet/asn048","2008","Small area estimation when auxiliary information is measured with error","1","Small area estimation methods typically combine direct estimates from a survey with predictions from a model in order to obtain estimates of population quantities with reduced mean squared error. When the auxiliary information used in the model is measured with error, using a small area estimator such as the Fay-Herriot estimator while ignoring measurement error may be worse than simply using the direct estimator. We propose a new small area estimator that accounts for sampling variability in the auxiliary information, and derive its properties, in particular showing that it is approximately unbiased. The estimator is applied to predict quantities measured in the U.S. National Health and Nutrition Examination Survey, with auxiliary information from the U.S. National Health Interview Survey."
"10.1093/biomet/asn051","2008","Model diagnostic tests for selecting informative correlation structure in correlated data","0","In the generalized method of moments approach to longitudinal data analysis, unbiased estimating functions can be constructed to incorporate both the marginal mean and the correlation structure of the data. Increasing the number of parameters in the correlation structure corresponds to increasing the number of estimating functions. Thus, building a correlation model is equivalent to selecting estimating functions. This paper proposes a chi-squared test to choose informative unbiased estimating functions. We show that this methodology is useful for identifying which source of correlation it is important to incorporate when there are multiple possible sources of correlation. This method can also be applied to determine the optimal working correlation for the generalized estimating equation approach."
"10.1093/biomet/asn045","2008","A goodness-of-fit test for inhomogeneous spatial {P}oisson processes","0","We introduce a formal testing procedure to assess the fit of an inhomogeneous spatial Poisson process model, based on a discrepancy measure function Dc( t; theta) that is constructed from residuals obtained from the fitted model. We derive the asymptotic distributional properties of Dc( t; theta) and develop a test statistic based on them. Our test statistic has a limiting standard normal distribution, so that the test can be performed by simply comparing the test statistic with readily available critical values. We perform a simulation study to assess the performance of the proposed method and apply it to a real data example."
"10.1093/biomet/asn023","2008","A note on conditional {AIC} for linear mixed-effects models","2","The conventional model selection criterion, the Akaike information criterion, AIC, has been applied to choose candidate models in mixed-effects models by the consideration of marginal likelihood. Vaida & Blanchard (2005) demonstrated that such a marginal AIC and its small sample correction are inappropriate when the research focus is on clusters. Correspondingly, these authors suggested the use of conditional AIC. Their conditional AIC is derived under the assumption that the variance-covariance matrix or scaled variance-covariance matrix of random effects is known. This note provides a general conditional AIC but without these strong assumptions. Simulation studies show that the proposed method is promising."
"10.1093/biomet/asn034","2008","Extended {B}ayesian information criteria for model selection with large model spaces","7","The ordinary Bayesian information criterion is too liberal for model selection when the model space is large. In this paper, we re-examine the Bayesian paradigm for model selection and propose an extended family of Bayesian information criteria, which take into account both the number of unknown parameters and the complexity of the model space. Their consistency is established, in particular allowing the number of covariates to increase to infinity with the sample size. Their performance in various situations is evaluated by simulation studies. It is demonstrated that the extended Bayesian information criteria incur a small loss in the positive selection rate but tightly control the false discovery rate, a desirable property in many applications. The extended Bayesian information criteria are extremely useful for variable selection in problems with a moderate sample size but with a huge number of covariates, especially in genome-wide association studies, which are now an active area in genetics research."
"10.1093/biomet/asn011","2008","Conditional properties of unconditional parametric bootstrap procedures for inference in exponential families","1","Higher-order inference about a scalar parameter in the presence of nuisance parameters can be achieved by bootstrapping, in circumstances where the parameter of interest is a component of the canonical parameter in a full exponential family. The optimal test, which is approximated, is a conditional one based on conditioning on the sufficient statistic for the nuisance parameter. A bootstrap procedure that ignores the conditioning is shown to have desirable conditional properties in providing third-order relative accuracy in approximation of p-values associated with the optimal test, in both continuous and discrete models. The bootstrap approach is equivalent to third-order analytical approaches, and is demonstrated in a number of examples to give very accurate approximations even for very small sample sizes."
"10.1093/biomet/asn029","2008","Conditionally specified continuous distributions","0","A distribution is conditionally specified when its model constraints are expressed conditionally. For example, Besag's (1974) spatial model was specified conditioned on the neighbouring states, and pseudolikelihood is intended to approximate the likelihood using conditional likelihoods. There are three issues of interest: existence, uniqueness and computation of a joint distribution. In the literature, most results and proofs are for discrete probabilities; here we exclusively study distributions with continuous state space. We examine all three issues using the dependence functions derived from decomposition of the conditional densities. We show that certain dependence functions of the joint density are shared with its conditional densities. Therefore, two conditional densities involving the same set of variables are compatible if their overlapping dependence functions are identical. We prove that the joint density is unique when the set of dependence functions is both compatible and complete. In addition, a joint density, apart from a constant, can be computed from the dependence functions in closed form. Since all of the results are expressed in terms of dependence functions, we consider our approach to be dependence-based, whereas methods in the literature are generally density-based. Applications of the dependence-based formulation are discussed."
"10.1093/biomet/asn033","2008","The {B}enjamini-{H}ochberg method with infinitely many contrasts in linear models","0","Benjamini and Hochberg's method for controlling the false discovery rate is applied to the problem of testing infinitely many contrasts in linear models. Exact, easily calculated critical values are derived, defining a new multiple comparisons method for testing contrasts in linear models. The method is adaptive, depending on the data through the F-statistic, like the Waller-Duncan Bayesian multiple comparisons method. Comparisons with Scheffe's method are given, and the method is extended to the simultaneous confidence intervals of Benjamini and Yekutieli."
"10.1093/biomet/asn025","2008","Supremum weighted log-rank test and sample size for comparing two-stage adaptive treatment strategies","1","In two-stage adaptive treatment strategies, patients receive an induction treatment followed by a maintenance therapy, given that the patient responded to the induction treatment they received. To test for a difference in the effects of different induction and maintenance treatment combinations, a modified supremum weighted log-rank test is proposed. The test is applied to a dataset from a two-stage randomized trial and the results are compared to those obtained using a standard weighted log-rank test. A sample-size formula is proposed based on the limiting distribution of the supremum weighted log-rank statistic. The sample-size formula reduces to Eng and Kosorok's sample-size formula for a two-sample supremum log-rank test when there is no second randomization. Monte Carlo studies show that the proposed test provides sample sizes that are close to those obtained by standard weighted log-rank test under a proportional hazards alternative. However, the proposed test is more powerful than the standard weighted log-rank test under non-proportional hazards alternatives."
"10.1093/biomet/asn003","2008","Improving the efficiency of the log-rank test using auxiliary covariates","0","Under the assumption of proportional hazards, the log-rank test is optimal for testing the null hypothesis H(0) : beta = 0, where beta denotes the logarithm of the hazard ratio. However, if there are additional covariates that correlate with survival times, making use of their information will increase the efficiency of the log-rank test. We apply the theory of semiparametrics to characterize a class of regular and asymptotically linear estimators for beta when auxiliary covariates are incorporated into the model, and derive estimators that are more efficient. The Wald tests induced by these estimators are shown to be more powerful than the log-rank test. Simulation studies are used to illustrate the gains in efficiency."
"10.1093/biomet/asn024","2008","Additive partial linear models with measurement errors","1","We consider statistical inference for additive partial linear models when the linear covariate is measured with error. We propose attenuation-to-correction and simulation-extrapolation, SIMEX, estimators of the parameter of interest. It is shown that the first resulting estimator is asymptotically normal and requires no undersmoothing. This is an advantage of our estimator over existing backfitting-based estimators for semiparametric additive models which require undersmoothing of the nonparametric component in order for the estimator of the parametric component to be root-n consistent. This feature stems from a decrease of the bias of the resulting estimator, which is appropriately derived using a profile procedure. A similar characteristic in semiparametric partially linear models was obtained by Wang et al. ( 2005). We also discuss the asymptotics of the proposed SIMEX approach. Finite-sample performance of the proposed estimators is assessed by simulation experiments. The proposed methods are applied to a dataset from a semen study."
"10.1093/biomet/asn006","2008","Generalized varying coefficient models for longitudinal data","1","We propose a generalization of the varying coefficient model for longitudinal data to cases where not only current but also recent past values of the predictor process affect current response. More precisely, the targeted regression coefficient functions of the proposed model have sliding window supports around current time t. A variant of a recently proposed two-step estimation method for varying coefficient models is proposed for estimation in the context of these generalized varying coefficient models, and is found to lead to improvements, especially for the case of additive measurement errors in both response and predictors. The proposed methodology for estimation and inference is also applicable for the case of additive measurement error in the common versions of varying coefficient models that relate only current observations of predictor and response processes to each other. Asymptotic distributions of the proposed estimators are derived, and the model is applied to the problem of predicting protein concentrations in a longitudinal study. Simulation studies demonstrate the efficacy of the proposed estimation procedure."
"10.1093/biomet/asn015","2008","Adjustment uncertainty in effect estimation","0","Often there is substantial uncertainty in the selection of confounders when estimating the association between an exposure and health. We define this type of uncertainty as 'adjustment uncertainty'. We propose a general statistical framework for handling adjustment uncertainty in exposure effect estimation for a large number of confounders, we describe a specific implementation, and we develop associated visualization tools. Theoretical results and simulation studies show that the proposed method provides consistent estimators of the exposure effect and its variance. We also show that, when the goal is to estimate an exposure effect accounting for adjustment uncertainty, Bayesian model averaging with posterior model probabilities approximated using information criteria can fail to estimate the exposure effect and can over- or underestimate its variance. We compare our approach to Bayesian model averaging using time series data on levels of fine particulate matter and mortality."
"10.1093/biomet/asn021","2008","Pointwise testing with functional data using the {W}estfall-{Y}oung randomization method","0","We consider hypothesis testing with smooth functional data by performing pointwise tests and applying a multiple comparisons procedure. Methods based on general inequalities, such as Bonferroni's method, do not perform well because of the high correlation between observations at nearby points. We consider the multiple comparison procedure proposed by Westfall & Young (1993) and show that it approximates a multiple comparison correction for a continuum of comparisons as the grid for pointwise comparisons becomes finer. Simulations and an application verify that this result applies in practical settings."
"10.1093/biomet/asn035","2008","Joint modelling of paired sparse functional data using principal components","4","We propose a modelling framework to study the relationship between two paired longitudinally observed variables. The data for each variable are viewed as smooth curves measured at discrete time-points plus random errors. While the curves for each variable are summarized using a few important principal components, the association of the two longitudinal variables is modelled through the association of the principal component scores. We use penalized splines to model the mean curves and the principal component curves, and cast the proposed model into a mixed-effects model framework for model fitting, prediction and inference. The proposed method can be applied in the difficult case in which the measurement times are irregular and sparse and may differ widely across individuals. Use of functional principal components enhances model interpretation and improves statistical and numerical stability of the parameter estimates."
"10.1093/biomet/asn022","2008","Using calibration weighting to adjust for nonresponse under a plausible model","0","When we estimate the population total for a survey variable or variables, calibration forces the weighted estimates of certain covariates to match known or alternatively estimated population totals called benchmarks. Calibration can be used to correct for sample-survey nonresponse, or for coverage error resulting from frame undercoverage or unit duplication. The quasi-randomization theory supporting its use in nonresponse adjustment treats response as an additional phase of random sampling. The functional form of a quasi-random response model is assumed to be known, its parameter values estimated implicitly through the creation of calibration weights. Unfortunately, calibration depends upon known benchmark totals while the covariates in a plausible model for survey response may not be the benchmark covariates. Moreover, it may be prudent to keep the number of covariates in a response model small. We use calibration to adjust for nonresponse when the benchmark model and covariates may differ, provided the number of the former is at least as great as that of the latter. We discuss the estimation of a total for a vector of survey variables that do not include the benchmark covariates, but that may include some of the model covariates. We show how to measure both the additional asymptotic variance due to the nonresponse in a calibration-weighted estimator and the full asymptotic variance of the estimator itself. All variances are determined with respect to the randomization mechanism used to select the sample, the response model generating the subset of sample respondents, or both. Data from the U.S. National Agricultural Statistical Service's 2002 Census of Agriculture and simulations are used to illustrate alternative adjustments for nonresponse. The paper concludes with some remarks about adjustment for coverage error."
"10.1093/biomet/asn028","2008","A new approach to weighting and inference in sample surveys","0","The validity of design-based inference is not dependent on any model assumption. However, it is well known that estimators derived through design-based theory may be inefficient for the estimation of population totals when the design weights are weakly related to the variables of interest and have widely dispersed values. We propose estimators that have the potential to improve the efficiency of any estimator derived under the design-based theory. Our main focus is limited to the improvement of the Horvitz-Thompson estimator, but we also discuss the extension to calibration estimators. The new estimators are obtained by smoothing design or calibration weights using an appropriate model. Our approach to inference requires the modelling of only one variable, the weight, and it leads to a single set of smoothed weights in multipurpose surveys. This is to be contrasted with other model-based approaches, such as the prediction approach, in which it is necessary to postulate and validate a model for each variable of interest leading potentially to variable-specific sets of weights. Our proposed approach is first justified theoretically and then evaluated through a simulation study."
"10.1093/biomet/asn008","2008","A new class of average moment matching priors","0","We derive a new class of priors for the variance component in the Fay - Herriot model, a mixed regression model widely used in small area estimation. This class includes the well-known uniform or superharmonic prior. Through simulation we illustrate the use of our class of priors."
"10.1093/biomet/asn019","2008","A note on deletion diagnostics for estimating equations","0","We describe an algorithm based upon the Sherman - Morrison - Woodbury formula for the inversion of matrices with special structure that occur in formulae for deletion diagnostics. Substantial computational savings relative to a method based upon Cholesky's decomposition are illustrated. The result has broad application to regression diagnostics for clustered data."
"10.1093/biomet/asm094","2008","Diagnostic measures for empirical likelihood of general estimating equations","1","We develop diagnostic measures for assessing the influence of individual observations when using empirical likelihood with general estimating equations, and we use these measures to construct goodness-of-fit statistics for testing possible misspecification in the estimating equations. Our diagnostics include case-deletion measures, local influence measures and pseudo-residuals. Our goodness-of-fit statistics include the sum of local influence measures and the processes of pseudo-residuals. Simulation studies are conducted to evaluate our methods, and real datasets are analyzed to illustrate the use of our diagnostic measures and goodness-of-fit statistics."
"10.1093/biomet/asn004","2008","The prognostic analogue of the propensity score","2","The propensity score collapses the covariates of an observational study into a single measure summarizing their joint association with treatment conditions; prognostic scores summarize covariates' association with potential responses. As with propensity scores, stratification on prognostic scores brings to uncontrolled studies a concrete and desirable form of balance, a balance that is more familiar as an objective of experimental control. Like propensity scores, prognostic scores can reduce the dimension of the covariate, yet causal inferences conditional on them are as valid as are inferences conditional only on the unreduced covariate. As a method of adjustment unto itself, prognostic scoring has limitations not shared with propensity scoring, but it holds promise as a complement to the propensity score, particularly in certain designs for which unassisted propensity adjustment is difficult or infeasible."
"10.1093/biomet/asn002","2008","Determining the dimension of the central subspace and central mean subspace","0","The central subspace and central mean subspace are two important targets of sufficient dimension reduction. We propose a weighted chi-squared test to determine their dimensions based on matrices whose column spaces are exactly equal to the central subspace or the central mean subspace. The asymptotic distribution of the test statistic is obtained. Simulation examples are used to demonstrate the performance of this test."
"10.1093/biomet/asm095","2008","Model diagnosis for parametric regression in high-dimensional spaces","0","We study tools for checking the validity of a parametric regression model. When the dimension of the regressors is large, many of the existing tests face the curse of dimensionality or require some ordering of the data. Our tests are based on the residual empirical process marked by proper functions of the regressors. They are able to detect local alternatives converging to the null at parametric rates. Parametric and nonparametric alternatives are considered. In the latter case, through a proper principal component decomposition, we are able to derive smooth directional tests which are asymptotically distribution-free under the null model. The new tests take into account precisely the 'geometry of the model'. A simulation study is carried through and an application to a real dataset is illustrated."
"10.1093/biomet/asn010","2008","On the asymptotics of penalized splines","7","We study the asymptotic behaviour of penalized spline estimators in the univariate case. We use B-splines and a penalty is placed on mth-order differences of the coefficients. The number of knots is assumed to converge to infinity as the sample size increases. We show that penalized splines behave similarly to Nadaraya - Watson kernel estimators with 'equivalent' kernels depending upon m. The equivalent kernels we obtain for penalized splines are the same as those found by Silverman for smoothing splines. The asymptotic distribution of the penalized spline estimator is Gaussian and we give simple expressions for the asymptotic mean and variance. Provided that it is fast enough, the rate at which the number of knots converges to infinity does not affect the asymptotic distribution. The optimal rate of convergence of the penalty parameter is given. Penalized splines are not design-adaptive."
"10.1093/biomet/asn014","2008","Least absolute deviation estimation for fractionally integrated autoregressive moving average time series models with conditional heteroscedasticity","1","We consider a unified least absolute deviation estimator for stationary and nonstationary fractionally integrated autoregressive moving average models with conditional heteroscedasticity. Its asymptotic normality is established when the second moments of errors and innovations are finite. Several other alternative estimators are also discussed and are shown to be less efficient and less robust than the proposed approach. A diagnostic tool, consisting of two portmanteau tests, is designed to check whether or not the estimated models are adequate. The simulation experiments give further support to our model and the results for the absolute returns of the Dow Jones Industrial Average Index daily closing price demonstrate their usefulness in modelling time series exhibiting the features of long memory, conditional heteroscedasticity and heavy tails."
"10.1093/biomet/asn009","2008","Modelling multiple time series via common factors","6","We propose a new method for estimating common factors of multiple time series. One distinctive feature of the new approach is that it is applicable to some nonstationary time series. The unobservable, nonstationary factors are identified by expanding the white noise space step by step, thereby solving a high-dimensional optimization problem by several low-dimensional sub-problems. Asymptotic properties of the estimation are investigated. The proposed methodology is illustrated with both simulated and real datasets."
"10.1093/biomet/asn016","2008","Multi-parameter automodels and their applications","0","Motivated by the modelling of non-Gaussian data or positively correlated data on a lattice, extensions of Besag's automodels to exponential families with multi-dimensional parameters have been proposed recently. We provide a multiple-parameter analogue of Besag's one-dimensional result that gives the necessary form of the exponential families for the Markov random field's conditional distributions. We propose estimation of parameters by maximum pseudolikelihood and give a proof of the consistency of the estimators for the multi-parameter automodel. The methodology is illustrated with examples, in particular the building of a cooperative system with beta conditional distributions. We also indicate future applications of these models to the analysis of mixed-state spatial data."
"10.1093/biomet/asn001","2008","Objective {B}ayesian analysis for the {S}tudent-{$t$} regression model","0","We develop a Bayesian analysis based on two different Jeffreys priors for the Student-t regression model with unknown degrees of freedom. It is typically difficult to estimate the number of degrees of freedom: improper prior distributions may lead to improper posterior distributions, whereas proper prior distributions may dominate the analysis. We show that Bayesian analysis with either of the two considered Jeffreys priors provides a proper posterior distribution. Finally, we show that Bayesian estimators based on Jeffreys analysis compare favourably to other Bayesian estimators based on priors previously proposed in the literature."
"10.1093/biomet/asn012","2008","Kernel stick-breaking processes","9","We propose a class of kernel stick-breaking processes for uncountable collections of dependent random probability measures. The process is constructed by first introducing an infinite sequence of random locations. Independent random probability measures and beta-distributed random weights are assigned to each location. Predictor-dependent random probability measures are then constructed by mixing over the locations, with stick-breaking probabilities expressed as a kernel multiplied by the beta weights. Some theoretical properties of the process are described, including a covariate-dependent prediction rule. A retrospective Markov chain Monte Carlo algorithm is developed for posterior computation, and the methods are illustrated using a simulated example and an epidemiological application."
"10.1093/biomet/asn013","2008","A family of {B}ayes multiple testing procedures","0","Under the model of independent test statistics, we propose a two-parameter family of Bayes multiple testing procedures. The two parameters can be viewed as tuning parameters. Using the Benjamini - Hochberg step-up procedure for controlling false discovery rate as a baseline for conservativeness, we choose the tuning parameters to compromise between the operating characteristics of that procedure and a less conservative procedure that focuses on alternatives that a priori might be considered likely or meaningful. The Bayes procedures do not have the theoretical and practical shortcomings of the popular stepwise procedures. In terms of the number of mistakes, simulations for two examples indicate that over a large segment of the parameter space, the Bayes procedure is preferable to the step-up procedure. Another desirable feature of the procedures is that they are computationally feasible for any number of hypotheses."
"10.1093/biomet/asn018","2008","On weighted {H}ochberg procedures","0","We consider different ways of constructing weighted Hochberg-type step-up multiple test procedures including closed procedures based on weighted Simes tests and their conservative step-up short-cuts, and step-up counterparts of two weighted Holm procedures. It is shown that the step-up counterparts have some serious pitfalls such as lack of familywise error rate control and lack of monotonicity in rejection decisions in terms of p-values. Therefore an exact closed procedure appears to be the best alternative, its only drawback being lack of simple stepwise structure. A conservative step-up short-cut to the closed procedure may be used instead, but with accompanying loss of power. Simulations are used to study the familywise error rate and power properties of the competing procedures for independent and correlated p-values. Although many of the results of this paper are negative, they are useful in highlighting the need for caution when procedures with similar pitfalls may be used."
"10.1093/biomet/asn007","2008","Hierarchical testing of variable importance","3","A frequently encountered challenge in high-dimensional regression is the detection of relevant variables. Variable selection suffers from instability and the power to detect relevant variables is typically low if predictor variables are highly correlated. When taking the multiplicity of the testing problem into account, the power diminishes even further. To gain power and insight, it can be advantageous to look for influence not at the level of individual variables but rather at the level of clusters of highly correlated variables. We propose a hierarchical approach. Variable importance is first tested at the coarsest level, corresponding to the global null hypothesis. The method then tries to attribute any effect to smaller subclusters or even individual variables. The smallest possible clusters, which still exhibit a significant influence on the response variable, are retained. It is shown that the proposed testing procedure controls the familywise error rate at a prespecified level, simultaneously over all resolution levels. The method has power comparable to the Bonferroni - Holm procedure on the level of individual variables and dramatically larger power for coarser resolution levels. The best resolution level is selected adaptively."
"10.1093/biomet/asm084","2008","Asymptotic inference for a nonstationary double {$\rm AR(1)$} model","0","We investigate the nonstationary double AR(1) model,y(t) = phi y(t-1) + eta t root(omega + alpha y(t-1)(2)),where omega > 0, alpha > 0, the eta(t) are independent standard normal random variables and E log vertical bar phi + eta(t) root alpha vertical bar >= 0. We show that the maximum likelihood estimator of (phi, alpha) is consistent and asymptotically normal. Combination of this result with that in Ling ( 2004) for the stationary case gives the asymptotic normality of the maximum likelihood estimator of phi for any phi in the real line, with a root-n rate of convergence. This is in contrast to the results for the classical AR(1) model, corresponding to alpha = 0."
"10.1093/biomet/asm080","2008","A note on repeated {$p$}-values for group sequential designs","0","One-sided confidence intervals and overall p-values for group-sequential designs are typically based on a sample space ordering which determines both the overall p-value and the corresponding confidence bound. Accordingly, the strength of evidence against the null hypothesis is consistently measured by both quantities such that the order of the p-values of two distinct sample points is consistent with the order of the respective confidence bounds. An exception is the commonly used repeated p-values and repeated confidence intervals. We show that they are not ordering-consistent in the above sense and propose an alternative repeated p-value which is ordering-consistent and has the monitoring property of the classical repeated p-value in being valid even when deviating from the prefixed stopping rule."
"10.1093/biomet/asm085","2008","Testing hypotheses in order","2","In certain circumstances, one wishes to test one hypothesis only if certain other hypotheses have been rejected. This ordering of hypotheses simplifies the task of controlling the probability of rejecting any true hypothesis. In an example from an observational study, a treated group is shown to be further from both of two control groups than the two control groups are from each other."
"10.1093/biomet/asm083","2008","A note on path-based variable selection in the penalized proportional hazards model","2","We propose an efficient and adaptive shrinkage method for variable selection in the Cox model. The method constructs a piecewise-linear regularization path connecting the maximum partial likelihood estimator and the origin. Then a model is selected along the path. We show that the constructed path is adaptive in the sense that, with a proper choice of regularization parameter, the fitted model works as well as if the true underlying submodel were given in advance. A modified algorithm of the least-angle-regression type efficiently computes the entire regularization path of the new estimator. Furthermore, we show that, with a proper choice of shrinkage parameter, the method is consistent in variable selection and efficient in estimation. Simulation shows that the new method tends to outperform the lasso and the smoothly-clipped-absolute-deviation estimators with moderate samples. We apply the methodology to data concerning nursing homes."
"10.1093/biomet/asm089","2008","Nonparametric estimation of cause-specific cross hazard ratio with bivariate competing risks data","3","We propose an alternative representation of the cause-specific cross hazard ratio for bivariate competing risks data. The representation leads to a simple plug-in estimator, unlike an existing ad hoc procedure. The large sample properties of the resulting inferences are established. Simulations and a real data example demonstrate that the proposed methodology may substantially reduce the computational burden of the existing procedure, while maintaining similar efficiency properties."
"10.1093/biomet/asm091","2008","Nonparametric estimation of bivariate failure time associations in the presence of a competing risk","4","Most research on the study of associations among paired failure times has either assumed time invariance or been based on complex measures or estimators. Little has accommodated competing risks. This paper targets the conditional cause-specific hazard ratio, henceforth called the cause-specific cross ratio, a recent modification of the conditional hazard ratio designed to accommodate competing risks data. Estimation is accomplished by an intuitive, nonparametric method that localizes Kendall's tau. Time variance is accommodated through a partitioning of space into 'bins' between which the strength of association may differ. Inferential procedures are developed, small-sample performance is evaluated, and the methods are applied to the investigation of familial association in dementia onset."
"10.1093/biomet/asm096","2008","Predicting cumulative incidence probability by direct binomial regression","2","We suggest a new simple approach for estimation and assessment of covariate effects for the cumulative incidence curve in the competing risks model. We consider a semiparametric regression model where some effects may be time-varying and some may be constant over time. Our estimator can be implemented by standard software. Our simulation study shows that the estimator works well and has finite-sample properties comparable with the subdistribution approach. We apply the method to bone marrow transplant data and estimate the cumulative incidence of death in complete remission following a bone marrow transplantation. Here death in complete remission and relapse are two competing events."
"10.1093/biomet/asm098","2008","Two-stage sampling from a prediction point of view when the cluster sizes are unknown","0","We consider the problem of estimating the population total in two-stage cluster sampling when cluster sizes are known only for the sampled clusters, making use of a population model arising from a variance component model. The problem can be considered as one of predicting the unobserved part Z of the total, and the concept of predictive likelihood is studied. Prediction intervals and a predictor for the population total are derived for the normal case, based on predictive likelihood. For a more general distribution-free model, by application of an analysis of variance approach instead of maximum likelihood for parameter estimation, the predictor obtained from the predictive likelihood is shown to be approximately uniformly optimal for large sample size and large number of clusters, in the sense of uniformly minimizing the mean-squared error in a partially linear class of model-unbiased predictors. Three prediction intervals for Z based on three similar predictive likelihoods are studied. For a small number n(0) of sampled clusters, they differ significantly, but for large n(0), the three intervals are practically identical. Model-based and design-based coverage properties of the prediction intervals are studied based on a comprehensive simulation study. The simulation study indicates that for large sample sizes, the coverage measures achieve approximately the nominal level 1 - alpha and are slightly less than 1 - alpha for moderately large sample sizes. For small sample sizes, the coverage measures are about 1 - 2 alpha, being raised to 1 - alpha for a modified interval based on the t(n0-2) distribution."
"10.1093/biomet/asm086","2008","Retrospective {M}arkov chain {M}onte {C}arlo methods for {D}irichlet process hierarchical models","13","Inference for Dirichlet process hierarchical models is typically performed using Markov chain Monte Carlo methods, which can be roughly categorized into marginal and conditional methods. The former integrate out analytically the infinite-dimensional component of the hierarchical model and sample from the marginal distribution of the remaining variables using the Gibbs sampler. Conditional methods impute the Dirichlet process and update it as a component of the Gibbs sampler. Since this requires imputation of an infinite-dimensional process, implementation of the conditional method has relied on finite approximations. In this paper, we show how to avoid such approximations by designing two novel Markov chain Monte Carlo algorithms which sample from the exact posterior distribution of quantities of interest. The approximations are avoided by the new technique of retrospective sampling. We also show how the algorithms can obtain samples from functionals of the Dirichlet process. The marginal and the conditional methods are compared and a careful simulation study is included, which involves a non-conjugate model, different datasets and prior specifications."
"10.1093/biomet/asm077","2008","Probability estimation for large-margin classifiers","3","Large margin classifiers have proven to be effective in delivering high predictive accuracy, particularly those focusing on the decision boundaries and bypassing the requirement of estimating the class probability given input for discrimination. As a result, these classifiers may not directly yield an estimated class probability, which is of interest itself. To overcome this difficulty, this article proposes a novel method for estimating the class probability through sequential classifications, by using features of interval estimation of large-margin classifiers. The method uses sequential classifications to bracket the class probability to yield an estimate up to the desired level of accuracy. The method is implemented for support vector machines and psi-learning, in addition to an estimated Kullback-Leibler loss for tuning. A solution path of the method is derived for support vector machines to reduce further its computational cost. Theoretical and numerical analyses indicate that the method is highly competitive against alternatives, especially when the dimension of the input greatly exceeds the sample size. Finally, an application to leukaemia data is described."
"10.1093/biomet/asm088","2008","Bayesian and frequentist confidence intervals arising from empirical-type likelihoods","1","For a general class of empirical-type likelihoods for the population mean, higher-order asymptotics are developed with a view to characterizing its members which allow, for any given prior, the existence of a confidence interval that has approximately correct posterior as well as frequentist coverage. In particular, it is seen that the usual empirical likelihood always allows such a confidence interval, while many of its variants proposed in the literature do not enjoy this property. An explicit form of the confidence interval is also given."
"10.1093/biomet/asm081","2008","Nonparametric regression using local kernel estimating equations for correlated failure time data","0","We study nonparametric regression for correlated failure time data. Kernel estimating equations are used to estimate nonparametric covariate effects. Independent and weighted-kernel estimating equations are studied. The derivative of the nonparametric function is first estimated and the nonparametric function is then estimated by integrating the derivative estimator. We show that the nonparametric kernel estimator is consistent for any arbitrary working correlation matrix and that its asymptotic variance is minimized by assuming working independence. We evaluate the performance of the proposed kernel estimator using simulation studies, and apply the proposed method to the western Kenya parasitaemia data."
"10.1093/biomet/asm079","2008","Flexible generalized {$t$}-link models for binary response data","0","A critical issue in modelling binary response data is the choice of the links. We introduce a new link based on the generalized t-distribution. There are two parameters in the generalized t-link: one parameter purely controls the heaviness of the tails of the link and the second parameter controls the scale of the link. Two major advantages are offered by the generalized t-links. First, a symmetric generalized t-link with an unknown shape parameter is much more identifiable than a Student t-link with unknown degrees of freedom and a known scale parameter. Secondly, skewed generalized t-links with both unknown shape and scale parameters provide much more flexible and improved skewed link regression models than the existing skewed links. Various theoretical properties and attractive features of the proposed links are examined and explored in detail. An efficient Markov chain Monte Carlo algorithm is developed for sampling from the posterior distribution. The deviance information criterion measure is used for guiding the choice of links. The proposed methodology is motivated and illustrated by prostate cancer data."
"10.1093/biomet/asm078","2008","Predicting future responses based on possibly misspecified working models","0","Under a general regression setting, we propose an optimal unconditional prediction procedure for future responses. The resulting prediction intervals or regions have a desirable average coverage level over a set of covariate vectors of interest. When the working model is not correctly specified, the traditional conditional prediction method is generally invalid. On the other hand, one can empirically calibrate the above unconditional procedure and also obtain its crossvalidated counterpart. Various large and small sample properties of these unconditional methods are examined analytically and numerically. We find that the K-fold crossvalidated procedure performs exceptionally well even for cases with rather small sample sizes. The new proposals are illustrated with two real examples, one with a continuous response and the other with a binary outcome."
"10.1093/biomet/asm087","2008","Shared parameter models under random effects misspecification","1","A common objective in longitudinal studies is the investigation of the association structure between a longitudinal response process and the time to an event of interest. An attractive paradigm for the joint modelling of longitudinal and survival processes is the shared parameter framework, where a set of random effects is assumed to induce their interdependence. In this work, we propose an alternative parameterization for shared parameter models and investigate the effect of misspecifying the random effects distribution in the parameter estimates and their standard errors."
"10.1093/biomet/asm090","2008","Empirical and counterfactual conditions for sufficient cause interactions","5","Sufficient-component causes are discussed within the deterministic potential outcomes framework so as to formalize notions of sufficient causes, synergism and sufficient cause interactions. Doing so allows for the derivation of counterfactual and empirical conditions for detecting the presence of sufficient cause interactions. The conditions are novel in that, unlike other conditions in the literature, they make no assumption about monotonicity. The conditions can also be generalized and the conditions for three-way sufficient cause interactions are given explicitly. The statistical tests derived for sufficient cause interactions are compared with and contrasted to interaction terms in standard statistical models."
"10.1093/biomet/asm097","2008","Population intervention models in causal inference","0","We propose a new causal parameter, which is a natural extension of existing approaches to causal inference such as marginal structural models. Modelling approaches are proposed for the difference between a treatment-specific counterfactual population distribution and the actual population distribution of an outcome in the target population of interest. Relevant parameters describe the effect of a hypothetical intervention on such a population and therefore we refer to these models as population intervention models. We focus on intervention models estimating the effect of an intervention in terms of a difference and ratio of means, called risk difference and relative risk if the outcome is binary. We provide a class of inverse-probability-of-treatment-weighted and doubly-robust estimators of the causal parameters in these models. The finite-sample performance of these new estimators is explored in a simulation study."
"10.1093/biomet/asm092","2008","Distortion of effects caused by indirect confounding","1","Undetected confounding may severely distort the effect of an explanatory variable on a response variable, as defined by a stepwise data-generating process. The best known type of distortion, which we call direct confounding, arises from an unobserved explanatory variable common to a response and its main explanatory variable of interest. It is relevant mainly for observational studies, since it is avoided by successful randomization. By contrast, indirect confounding, which we identify in this paper, is an issue also for intervention studies. For general stepwise-generating processes, we provide matrix and graphical criteria to decide which types of distortion may be present, when they are absent and how they are avoided. We then turn to linear systems without other types of distortion, but with indirect confounding. For such systems, the magnitude of distortion in a least-squares regression coefficient is derived and shown to be estimable, so that it becomes possible to recover the effect of the generating process from the distorted coefficient."
"10.1093/biomet/asm093","2008","Studentization and deriving accurate {$p$}-values","0","We have a statistic for assessing an observed data point relative to a statistical model but find that its distribution function depends on the parameter. To obtain the corresponding p-value, we require the minimally modified statistic that is ancillary; this process is called Studentization. We use recent likelihood theory to develop a maximal third-order ancillary; this gives immediately a candidate Studentized statistic. We show that the corresponding p-value is higher-order Un(0, 1), is equivalent to a repeated bootstrap version of the initial statistic and agrees with a special Bayesian modification of the original statistic. More importantly, the modified statistic and p-value are available by Markov chain Monte Carlo simulations and, in some cases, by higher-order approximation methods. Examples, including the Behrens-Fisher problem, are given to indicate the ease and flexibility of the approach."
"10.1093/biomet/asn041","2008","On the asymptotics of marginal regression splines with longitudinal data","1","There have been studies on how the asymptotic efficiency of a nonparametric function estimator depends on the handling of the within-cluster correlation when nonparametric regression models are used on longitudinal or cluster data. In particular, methods based on smoothing splines and local polynomial kernels exhibit different behaviour. We show that the generalized estimation equations based on weighted least squares regression splines for the nonparametric function have an interesting property: the asymptotic bias of the estimator does not depend on the working correlation matrix, but the asymptotic variance, and therefore the mean squared error, is minimized when the true correlation structure is specified. This property of the asymptotic bias distinguishes regression splines from smoothing splines."
"10.1093/biomet/asn047","2008","Pairwise curve synchronization for functional data","1","Data collected by scientists are increasingly in the form of trajectories or curves. Often these can be viewed as realizations of a composite process driven by both amplitude and time variation. We consider the situation in which functional variation is dominated by time variation, and develop a curve-synchronization method that uses every trajectory in the sample as a reference to obtain pairwise warping functions in the first step. These initial pairwise warping functions are then used to create improved estimators of the underlying individual warping functions in the second step. A truncated averaging process is used to obtain robust estimation of individual warping functions. The method compares well with other available time-synchronization approaches and is illustrated with Berkeley growth data and gene expression data for multiple sclerosis."
"10.1093/biomet/asn043","2008","Bayesian nonparametric inference on stochastic ordering","3","We consider Bayesian inference about collections of unknown distributions subject to a partial stochastic ordering. To address problems in testing of equalities between groups and estimation of group-specific distributions, we propose classes of restricted dependent Dirichlet process priors. These priors have full support in the space of stochastically ordered distributions, and can be used for collections of unknown mixture distributions to obtain a flexible class of mixture models. Theoretical properties are discussed, efficient methods are developed for posterior computation using Markov chain Monte Carlo simulation and the methods are illustrated using data from a study of DNA damage and repair."
"10.1093/biomet/asn046","2008","Estimating equations for spatially correlated data in multi-dimensional space","0","We use the quasilikelihood concept to propose an estimating equation for spatial data with correlation across the study region in a multi-dimensional space. With appropriate mixing conditions, we develop a central limit theorem for a random field under various L-p metrics. The consistency and asymptotic normality of quasilikelihood estimators can then be derived. We also conduct simulations to evaluate the performance of the proposed estimating equation, and a dataset from East Lansing Woods is used to illustrate the method."
"10.1093/biomet/asn053","2008","Testing the covariance structure of multivariate random fields","2","There is an increasing wealth of multivariate spatial and multivariate spatio-temporal data appearing. For such data, an important part of model building is an assessment of the properties of the underlying covariance function describing variable, spatial and temporal correlations. In this paper, we propose a methodology to evaluate the appropriateness of several types of common assumptions on multivariate covariance functions in the spatio-temporal context. The methodology is based on the asymptotic joint normality of the sample space-time cross-covariance estimators. Specifically, we address the assumptions of symmetry, separability and linear models of coregionalization. We conduct simulation experiments to evaluate the sizes and powers of our tests and illustrate our methodology on a trivariate spatio-temporal dataset of pollutants over California."
"10.1093/biomet/asn052","2008","Covariance reducing models: an alternative to spectral modelling of covariance matrices","0","We introduce covariance reducing models for studying the sample covariance matrices of a random vector observed in different populations. The models are based on reducing the sample covariance matrices to an informational core that is sufficient to characterize the variance heterogeneity among the populations. They possess useful equivariance properties and provide a clear alternative to spectral models for covariance matrices."
"10.1093/biomet/asn050","2008","A multi-dimensional scaling approach to shape analysis","2","We propose an alternative to Kendall's shape space for reflection shapes of configurations in Rm with k labelled vertices, where reflection shape consists of all the geometric information that is invariant under compositions of similarity and reflection transformations. The proposed approach embeds the space of such shapes into the space P( k - 1) of ( k - 1) x ( k - 1) real symmetric positive semidefinite matrices, which is the closure of an open subset of a Euclidean space, and defines mean shape as the natural projection of Euclidean means in P( k - 1) on to the embedded copy of the shape space. This approach has strong connections with multi- dimensional scaling, and the mean shape so defined gives good approximations to other commonly used definitions of mean shape. We also use standard perturbation arguments for eigenvalues and eigenvectors to obtain a central limit theorem which then enables the application of standard statistical techniques to shape analysis in two or more dimensions."
"10.1093/biomet/asn032","2008","Semiparametric model-based inference in the presence of missing responses","1","We consider a semiparametric model that parameterizes the conditional density of the response, given covariates, but allows the marginal distribution of the covariates to be completely arbitrary. Responses may be missing. A likelihood-based imputation estimator and a semi-empirical-likelihood-based estimator for the parameter vector describing the conditional density are defined and proved to be asymptotically normal. Semi-empirical loglikelihood functions for the parameter vector and the response mean are derived. It is shown that the two semi-empirical loglikelihood functions are distributed asymptotically as weighted chi(2) and scaled chi(2), respectively."
"10.1093/biomet/asn031","2008","Robust functional estimation using the median and spherical principal components","3","We present robust estimators for the mean and the principal components of a stochastic process in L(2)(R). Robustness and asymptotic properties of the estimators are studied theoretically, by simulation and by example. It is shown that the proposed estimators are generally more robust to outliers than the commonly used sample mean and principal components, although their properties depend on the spacings of the eigenvalues of the covariance function."
"10.1093/biomet/asn030","2008","Influence functions and robust {B}ayes and empirical {B}ayes small area estimation","0","We introduce new robust small area estimation procedures based on area-level models. We first find influence functions corresponding to each individual area-level observation by measuring the divergence between the posterior density functions of regression coefficients with and without that observation. Next, based on these influence functions, properly standardized, we propose some new robust Bayes and empirical Bayes small area estimators. The mean squared errors and estimated mean squared errors of these estimators are also found. A small simulation study compares the performance of the robust and the regular empirical Bayes estimators. When the model variance is larger than the sample variance, the proposed robust empirical Bayes estimators are superior."
"10.1093/biomet/asn027","2008","Optimal sampling and estimation strategies under the linear model","0","In some cases model-based and model-assisted inferences can lead to very different estimators. These two paradigms are not so different if we search for an optimal strategy rather than just an optimal estimator, a strategy being a pair composed of a sampling design and an estimator. We show that, under a linear model, the optimal model-assisted strategy consists of a balanced sampling design with inclusion probabilities that are proportional to the standard deviations of the errors of the model and the Horvitz-Thompson estimator. If the heteroscedasticity of the model is ""fully explainable"" by the auxiliary variables, then this strategy is also optimal in a model-based sense. Moreover, under balanced sampling and with inclusion probabilities that are proportional to the standard deviation of the model, the best linear unbiased estimator and the Horvitz-Thompson estimator are equal. Finally, it is possible to construct a single estimator for both the design and model variance. The inference can thus be valid under the sampling design and under the model."
"10.1093/biomet/asn017","2008","Nonparametric variance estimation in the analysis of microarray data: a measurement error approach","1","We investigate the effects of measurement error on the estimation of nonparametric variance functions. We show that either ignoring measurement error or direct application of the simulation extrapolation, SIMEX, method leads to inconsistent estimators. Nevertheless, the direct SIMEX method can reduce bias relative to a naive estimator. We further propose a permutation SIMEX method that leads to consistent estimators in theory. The performance of both the SIMEX methods depends on approximations to the exact extrapolants. Simulations show that both the SIMEX methods perform better than ignoring measurement error. The methodology is illustrated using microarray data from colon cancer patients."
"10.1093/biomet/asn005","2008","Simultaneous confidence bands in spectral density estimation","0","We propose a method for the construction of simultaneous confidence bands for a smoothed version of the spectral density of a Gaussian process based on nonparametric kernel estimators obtained by smoothing the periodogram. A studentized statistic is used to determine the width of the band at each frequency and a frequency-domain bootstrap approach is employed to estimate the distribution of the supremum of this statistic over all frequencies. We prove by means of strong approximations that the bootstrap estimates consistently the distribution of the supremum deviation of interest and, consequently, that the proposed confidence bands achieve asymptotically the desired simultaneous coverage probability. The behaviour of our method in finite-sample situations is investigated by simulations and a real-life data example demonstrates its applicability in time series analysis."
"10.1093/biomet/asn020","2008","Estimating functions for inhomogeneous spatial point processes with incomplete covariate data","0","The R package spatstat provides a very flexible and useful framework for analysing spatial point patterns. A fundamental feature is a procedure for fitting spatial point process models depending on covariates. However, in practice one often faces incomplete observation of the covariates and this leads to parameter estimation error which is difficult to quantify. In this paper, we introduce a Monte Carlo version of the estimating function used in spatstat for fitting inhomogeneous Poisson processes and certain inhomogeneous cluster processes. For this modified estimating function, it is feasible to obtain the asymptotic distribution of the parameter estimators in the case of incomplete covariate information. This allows a study of the loss of efficiency due to the missing covariate data."
"10.1093/biomet/asm082","2008","Analysis of least absolute deviation","2","We develop a unified L-1-based analysis-of-variance-type method for testing linear hypotheses. Like the classical L-2-based analysis of variance, the method is coordinate-free in the sense that it is invariant under any linear transformation of the covariates or regression parameters. Moreover, it allows singular design matrices and heterogeneous error terms. A simple approximation using stochastic perturbation is proposed to obtain cut-off values for the resulting test statistics. Both test statistics and distributional approximations can be computed using standard linear programming. An asymptotic theory is derived for the method. Special cases of one-and multi-way analysis of variance and analysis of covariance models are worked out in detail. The main results of this paper can be extended to general quantile regression. Extensive simulations show that the method works well in practical settings. The method is also applied to a dataset from General Social Surveys."
"10.1093/biomet/asm075","2007","Positive association among three binary variables and cross-product ratios","0","We show that, when the three-way association level among the three binary variables, X, U-1 and U-2 is fixed, D-P = pr( X = 1 | U-1 = 1) - pr( X = 1 | U-1 = 0) increases as the cross-product ratio of U-1 and U-2 increases under the assumption that X is positively associated with U-1 and U-2. We then discuss some implications of this property."
"10.1093/biomet/asm065","2007","Use of the {G}ibbs sampler to obtain conditional tests, with applications","0","A random sample is drawn from a distribution which admits a minimal sufficient statistic for the parameters. The Gibbs sampler is proposed to generate samples, called conditionally sufficient or co-sufficient samples, from the conditional distribution of the sample given its value of the sufficient statistic. The procedure is illustrated for the gamma distribution. Co-sufficient samples may be used to give exact tests of fit; for the gamma distribution these are compared for size and power with approximate tests based on the parametric bootstrap."
"10.1093/biomet/asm076","2007","Importance sampling via the estimated sampler","0","Monte Carlo importance sampling for evaluating numerical integration is discussed. We consider a parametric family of sampling distributions and propose the use of the sampling distribution estimated by maximum likelihood. The proposed method of importance sampling using the estimated sampling distribution is shown to improve the asymptotic variance of the ordinary method using the true sampling distribution. The argument is closely related to the discussion of the paradox in Henmi & Eguchi (2004). We focus on a condition under which the estimated integration value obtained by the proposed method has asymptotic zero variance."
"10.1093/biomet/asm068","2007","Kernel-type density estimation on the unit interval","0","We consider kernel-type methods for the estimation of a density on [0, 1] which eschew explicit boundary correction. We propose using kernels that are symmetric in their two arguments; these kernels are conditional densities of bivariate copulas. We give asymptotic theory for the version of the new estimator using Gaussian copula kernels and report on simulation comparisons of it with the beta-kernel density estimator of Chen (1999). We also provide automatic bandwidth selection in the form of 'rule-of-thumb' bandwidths for both estimators. As well as its competitive integrated squared error performance, advantages of the new approach include its greater range of possible values at 0 and 1, the fact that it is a bona fide density and that the individual kernels and resulting estimator are comprehensible in terms of a single simple picture."
"10.1093/biomet/asm067","2007","Hochberg's step-up method: cutting corners off {H}olm's step-down method","1","Holm's method and Hochberg's method for multiple testing can be viewed as step-down and step-up versions of the Bonferroni test. We show that both are special cases of partition testing. The difference is that, while Holm's method tests each partition hypothesis using the largest order statistic, setting a critical value based on the Bonferroni inequality, Hochberg's method tests each partition hypothesis using all the order statistics, setting a series of critical values based on Simes' inequality. Geometrically, Hochberg's step-upmethod 'cuts corners' off the acceptance regions of Holm's step-down method by making assumptions on the joint distribution of the test statistics. As can be expected, partition testing making use of the joint distribution of the test statistics is more powerful than partition testing using probabilistic inequalities. Thus, if the joint distribution of the test statistics is available, through modelling for example, we recommend partition step-down testing, setting exact critical values based on the joint distribution."
"10.1093/biomet/asm072","2007","A jackknife variance estimator for unistage stratified samples with unequal probabilities","0","Existing jackknife variance estimators used with sample surveys can seriously overestimate the true variance under unistage stratified sampling without replacement with unequal probabilities. A novel jackknife variance estimator is proposed which is as numerically simple as existing jackknife variance estimators. Under certain regularity conditions, the proposed variance estimator is consistent under stratified sampling without replacement with unequal probabilities. The high entropy regularity condition necessary for consistency is shown to hold for the Rao-Sampford design. An empirical study of three unequal probability sampling designs supports our findings."
"10.1093/biomet/asm051","2007","A hybrid pairwise likelihood method","1","A modification to the pairwise likelihood method is proposed, which aims to improve the estimation of the marginal distribution parameters. This is achieved by replacing the pairwise likelihood score equations, for estimating such parameters, by the optimal linear combinations of the marginal score functions. A further advantage of the proposed estimator of marginal parameters, over pairwise likelihood, is that it is robust to misspecification of the bivariate distributions as long as the univariate marginal distributions are correctly specified. While alternating logistic regression can be seen as a special case of the proposed method, it is shown that an existing generalization of alternating logistic regression applicable to ordinal data is not the same as and is inferior to the proposed method because it replaces certain conditional densities by pseudodensities that assume working independence. The fitting of the multivariate negative binomial distribution is another scenario involving intractable likelihood that calls for the use of pairwise likelihood methods, and the superiority of the modified method is demonstrated in a simulation study. Two examples, based on the analyses of salamander mating and patient-controlled analgesia data, demonstrate the usefulness of the proposed method. The possibility of combining optimally the pairwise, rather than marginal, scores is also considered and its difficulty and potential are discussed."
"10.1093/biomet/asm066","2007","Empirical likelihood semiparametric regression analysis for longitudinal data","2","A semiparametric regression model for longitudinal data is considered. The empirical likelihood method is used to estimate the regression coefficients and the baseline function, and to construct confidence regions and intervals. It is proved that the maximum empirical likelihood estimator of the regression coefficients achieves asymptotic efficiency and the estimator of the baseline function attains asymptotic normality when a bias correction is made. Two calibrated empirical likelihood approaches to inference for the baseline function are developed. We propose a groupwise empirical likelihood procedure to handle the inter-series dependence for the longitudinal semiparametric regression model, and employ bias correction to construct the empirical likelihood ratio functions for the parameters of interest. This leads us to prove a nonparametric version of Wilks' theorem. Compared with methods based on normal approximations, the empirical likelihood does not require consistent estimators for the asymptotic variance and bias. A simulation compares the empirical likelihood and normal-based methods in terms of coverage accuracies and average areas/lengths of confidence regions/intervals."
"10.1093/biomet/asm064","2007","The role of pseudo data for robust smoothing with application to wavelet regression","1","We propose a robust curve and surface estimator based on M-type estimators and penalty-based smoothing. This approach also includes an application to wavelet regression. The concept of pseudo data, a transformation of the robust additive model to the one with bounded errors, is used to derive some theoretical properties and also motivate a computational algorithm. The resulting algorithm, termed the ES-algorithm, is computationally fast and provides a simple way of choosing the amount of smoothing. Moreover, it is easily described, straightforwardly implemented and can be extended to other wavelet regression settings such as irregularly spaced data and image denoising. Results from a simulation study and real data examples demonstrate the promising empirical properties of the proposed approach."
"10.1093/biomet/asm062","2007","A general approach to the predictability issue in survival analysis with applications","1","Very often in survival analysis one has to study martingale integrals where the integrand is not predictable and where the counting process theory of martingales is not directly applicable, as for example in nonparametric and semiparametric applications where the integrand is based on a pilot estimate. We call this the predictability issue in survival analysis. The problem has been resolved by approximations of the integrand by predictable functions which have been justified by ad hoc procedures. We present a general approach to the solution of this problem. The usefulness of the approach is shown in three applications. In particular, we argue that earlier ad hoc procedures do not work in higher-dimensional smoothing problems in survival analysis."
"10.1093/biomet/asm054","2007","Aalen additive hazards change-point model","1","We study a test comparing the full Aalen additive hazards model and the change-point model, and suggest how to estimate the parameters of the change-point model. We also study a test for no change-point effect. Both tests are provided with large sample properties and a resampling method is applied to obtain p-values. The finite-sample properties of the proposed inference procedures and estimators are assessed through a simulation study. The methods are further applied to a dataset concerning myocardial infarction."
"10.1093/biomet/asm070","2007","Estimation of regression models for the mean of repeated outcomes under nonignorable nonmonotone nonresponse","2","We propose a new class of models for making inference about the mean of a vector of repeated outcomes when the outcome vector is incompletely observed in some study units and missingness is nonmonotone. Each model in our class is indexed by a set of unidentified selection-bias functions which quantify the residual association of the outcome at each occasion t and the probability that this outcome is missing after adjusting for variables observed prior to time t and for the past nonresponse pattern. In particular, selection-bias functions equal to zero encode the investigator's a priori belief that nonresponse of the next outcome does not depend on that outcome after adjusting for the observed past. We call this assumption sequential explainability. Since each model in our class is nonparametric, it fits the data perfectly well. As such, our models are ideal for conducting sensitivity analyses aimed at evaluating the impact that different degrees of departure from sequential explainability have on inference about the marginal means of interest. Although the marginal means are identified under each of our models, their estimation is not feasible in practice because it requires the auxiliary estimation of conditional expectations and probabilities given high-dimensional variables. We henceforth discuss the estimation of the marginal means under each model in our class assuming, additionally, that at each occasion either one of the following two models holds: a parametric model for the conditional probability of nonresponse given current outcomes and past recorded data or a parametric model for the conditional mean of the outcome on the nonrespondents given the past recorded data. We call the resulting procedure 2(T)-multiply robust as it protects at each of the T time points against misspecification of one of these two working models, although not against simultaneous misspecification of both. We extend our proposed class of models and estimators to incorporate data configurations which include baseline covariates and a parametric model for the conditional mean of the vector of repeated outcomes given the baseline covariates."
"10.1093/biomet/asm071","2007","Generalized spatial {D}irichlet process models","4","Many models for the study of point-referenced data explicitly introduce spatial random effects to capture residual spatial association. These spatial effects are customarily modelled as a zero-mean stationary Gaussian process. The spatial Dirichlet process introduced by Gelfand et al. (2005) produces a random spatial process which is neither Gaussian nor stationary. Rather, it varies about a process that is assumed to be stationary and Gaussian. The spatial Dirichlet process arises as a probability-weighted collection of random surfaces. This can be limiting for modelling and inferential purposes since it insists that a process realization must be one of these surfaces. We introduce a random distribution for the spatial effects that allows different surface selection at different sites. Moreover, we can specify the model so that the marginal distribution of the effect at each site still comes from a Dirichlet process. The development is offered constructively, providing a multivariate extension of the stick-breaking representation of the weights. We then introduce mixing using this generalized spatial Dirichlet process. We illustrate with a simulated dataset of independent replications and note that we can embed the generalized process within a dynamic model specification to eliminate the independence assumption."
"10.1093/biomet/asm069","2007","Population-based reversible jump {M}arkov chain {M}onte {C}arlo","0","We present an extension of population-based Markov chain Monte Carlo to the transdimensional case. A major challenge is that of simulating from high- and transdimensional target measures. In such cases, Markov chain Monte Carlo methods may not adequately traverse the support of the target; the simulation results will be unreliable. We develop population methods to deal with such problems, and give a result proving the uniform ergodicity of these population algorithms, under mild assumptions. This result is used to demonstrate the superiority, in terms of convergence rate, of a population transition kernel over a reversible jump sampler for a Bayesian variable selection problem. We also give an example of a population algorithm for a Bayesian multivariate mixture model with an unknown number of components. This is applied to gene expression data of 1000 data points in six dimensions and it is demonstrated that our algorithm outperforms some competing Markov chain samplers. In this example, we show how to combine the methods of parallel chains (Geyer, 1991), tempering (Geyer & Thompson, 1995), snooker algorithms (Gilks et al., 1994), constrained sampling and delayed rejection (Green & Mira, 2001)."
"10.1093/biomet/asm061","2007","Bayesian nonparametric estimation of the probability of discovering new species","1","We consider the problem of evaluating the probability of discovering a certain number of new species in a new sample of population units, conditional on the number of species recorded in a basic sample. We use a Bayesian nonparametric approach. The different species proportions are assumed to be random and the observations from the population exchangeable. We provide a Bayesian estimator, under quadratic loss, for the probability of discovering new species which can be compared with well-known frequentist estimators. The results we obtain are illustrated through a numerical example and an application to a genomic dataset concerning the discovery of new genes by sequencing additional single-read sequences of cDNA fragments."
"10.1093/biomet/asm050","2007","The high-dimension, low-sample-size geometric representation holds under mild conditions","7","High-dimension, low-small-sample size datasets have different geometrical properties from those of traditional low-dimensional data. In their asymptotic study regarding increasing dimensionality with a fixed sample size, Hall et al. ( 2005) showed that each data vector is approximately located on the vertices of a regular simplex in a high-dimensional space. A perhaps unappealing aspect of their result is the underlying assumption which requires the variables, viewed as a time series, to be almost independent. We establish an equivalent geometric representation under much milder conditions using asymptotic properties of sample covariance matrices. We discuss implications of the results, such as the use of principal component analysis in a high-dimensional space, extension to the case of nonindependent samples and also the binary classification problem."
"10.1093/biomet/asm046","2007","On a generalization of a result of {W}. {G}. {C}ochran","0","A relationship due to W. G. Cochran showing the effect on least squares regression coefficients of marginalizing over or conditioning on an explanatory variable is generalized to quantile regression coefficients. The condition under which conditioning does not induce interaction or effect reversal is shown. Examples are given. The discussion is simplest when all variables are continuous; the extension to discrete variables is outlined."
"10.1093/biomet/asm045","2007","On the approximation of the quadratic exponential distribution in a latent variable context","0","Following Cox & Wermuth ( 1994, 2002), we show that the distribution of a set of binary observable variables, induced by a certain discrete latent variable model, may be approximated by a quadratic exponential distribution. This discrete latent variable model is equivalent to the latent-class version of the two-parameter logistic model of Birnbaum ( 1968), which may be seen as a generalized version of the Rasch model ( Rasch, 1960, 1961). On the basis of this result, we develop an approximate maximum likelihood estimator of the item parameters of the two-parameter logistic model which is very simply implemented. The proposed approach is illustrated through an example based on a dataset on educational assessment."
"10.1093/biomet/asm059","2007","Nonparametric quantile inference with competing-risks data","3","A conceptually simple quantile inference procedure is proposed for cause-specific failure probabilities with competing risks data. The quantiles are defined using the cumulative incidence function, which is intuitively meaningful in the competing-risks set-up. We establish the uniform consistency and weak convergence of a nonparametric estimator of this quantile function. These results form the theoretical basis for extensions of standard one-sample and two-sample quantile inference for independently censored data. This includes the construction of confidence intervals and bands for the quantile function, and two-sample tests. Simulation studies and a real data example illustrate the practical utility of the methodology."
"10.1093/biomet/asm058","2007","Survival analysis with temporal covariate effects","3","We propose a natural generalization of the Cox regression model, in which the regression coefficients have direct interpretations as temporal covariate effects on the survival function. Under the conditionally independent censoring mechanism, we develop a smoothing-free estimation procedure with a set of martingale-based equations. Our estimator is shown to be uniformly consistent and to converge weakly to a Gaussian process. A simple resampling method is proposed for approximating the limiting distribution of the estimated coefficients. Second-stage inferences with time-varying coefficients are developed accordingly. Simulations and a real example illustrate the practical utility of the proposed method. Finally, we extend this proposal of temporal covariate effects to the general class of linear transformation models and also establish a connection with the additive hazards model."
"10.1093/biomet/asm057","2007","Estimation of the mean function with panel count data using monotone polynomial splines","1","We study nonparametric likelihood-based estimators of the mean function of counting processes with panel count data using monotone polynomial splines. The generalized Rosen algorithm, proposed by Zhang & Jamshidian ( 2004), is used to compute the estimators. We show that the proposed spline likelihood-based estimators are consistent and that their rate of convergence can be faster than n(1/3). Simulation studies with moderate samples show that the estimators have smaller variances and mean squared errors than their alternatives proposed by Wellner & Zhang ( 2000). A real example from a bladder tumour clinical trial is used to illustrate this method."
"10.1093/biomet/asm037","2007","Adaptive {L}asso for {C}ox's proportional hazards model","13","We investigate the variable selection problem for Cox's proportional hazards model, and propose a unified model selection and estimation procedure with desired theoretical properties and computational convenience. The new method is based on a penalized log partial likelihood with the adaptively weighted L-1 penalty on regression coefficients, providing what we call the adaptive Lasso estimator. The method incorporates different penalties for different coefficients: unimportant variables receive larger penalties than important ones, so that important variables tend to be retained in the selection process, whereas unimportant variables are more likely to be dropped. Theoretical properties, such as consistency and rate of convergence of the estimator, are studied. We also show that, with proper choice of regularization parameters, the proposed estimator has the oracle properties. The convex optimization nature of the method leads to an efficient algorithm. Both simulated and real examples show that the method performs competitively."
"10.1093/biomet/asm049","2007","Optimal adaptive randomized designs for clinical trials","0","Optimal decision-analytic designs are deterministic. Such designs are appropriately criticized in the context of clinical trials because they are subject to assignment bias. On the other hand, balanced randomized designs may assign an excessive number of patients to a treatment arm that is performing relatively poorly. We propose a compromise between these two extremes, one that achieves some of the good characteristics of both. We introduce a constrained optimal adaptive design for a fully sequential randomized clinical trial with k arms and n patients. An r-design is one for which, at each allocation, each arm has probability at least r of being chosen, 0 <= r <= 1/k. An optimal design among all r-designs is called r-optimal. An r(1)-design is also an r(2)-design if r(1) >= r(2). A design without constraint is the special case r = 0 and a balanced randomized design is the special case r = 1/k. The optimization criterion is to maximize the expected overall utility in a Bayesian decision-analytic approach, where utility is the sum over the utilities for individual patients over a 'patient horizon' N. We prove analytically that there exists an r-optimal design such that each patient is assigned to a particular one of the arms with probability 1 - ( k - 1) r, and to the remaining arms with probability r. We also show that the balanced design is asymptotically r-optimal for any given r, 0 <= r < 1/k, as N/n -> infinity. This implies that every r-optimal design is asymptotically optimal without constraint. Numerical computations using backward induction for k = 2 arms show that, in general, this asymptotic optimality feature for r-optimal designs can be accomplished with moderate trial size n if the patient horizon N is large relative to n. We also show that, in a trial with an r-optimal design, r < 1/2, fewer patients are assigned to an inferior arm than when following a balanced design, even for r-optimal designs having the same statistical power as a balanced design. We discuss extensions to various clinical trial settings."
"10.1093/biomet/asm052","2007","Recursive computing and simulation-free inference for general factorizable models","1","We illustrate how the recursive algorithm of Reeves & Pettitt ( 2004) for general factorizable models can be extended to allow exact sampling, maximization of distributions and computation of marginal distributions. All of the methods we describe apply to discrete-valued Markov random fields with nearest neighbour integrations defined on regular lattices; in particular we illustrate that exact inference can be performed for hidden autologistic models defined on moderately sized lattices. In this context we offer an extension of this methodology which allows approximate inference to be carried out for larger lattices without resorting to simulation techniques such as Markov chain Monte Carlo. In particular our work offers the basis for an automatic inference machine for such models."
"10.1093/biomet/asm056","2007","Simulation of hyper-inverse {W}ishart distributions in graphical models","5","We introduce and exemplify an efficient method for direct sampling from hyper-inverse Wishart distributions. The method relies very naturally on the use of standard junction-tree representation of graphs, and couples these with matrix results for inverse Wishart distributions. We describe the theory and resulting computational algorithms for both decomposable and nondecomposable graphical models. An example drawn from financial time series demonstrates application in a context where inferences on a structured covariance model are required. We discuss and investigate questions of scalability of the simulation methods to higher-dimensional distributions. The paper concludes with general comments about the approach, including its use in connection with existing Markov chain Monte Carlo methods that deal with uncertainty about the graphical model structure."
"10.1093/biomet/asm048","2007","Simulation and inference for stochastic volatility models driven by {L}\'evy processes","0","We study Ornstein-Uhlenbeck stochastic processes driven by Levy processes, and extend them to more general non-Ornstein-Uhlenbeck models. In particular, we investigate the means of making the correlation structure in the volatility process more flexible. For one model, we implement a method for introducing quasi long-memory into the volatility model. We demonstrate that the models can be fitted to real share price returns data."
"10.1093/biomet/asm043","2007","Partial inverse regression","2","In regression with a vector of quantitative predictors, sufficient dimension reduction methods can effectively reduce the predictor dimension, while preserving full regression information and assuming no parametric model. However, all current reduction methods require the sample size n to be greater than the number of predictors p. It is well known that partial least squares can deal with problems with n < p. We first establish a link between partial least squares and sufficient dimension reduction. Motivated by this link, we then propose a new dimension reduction method, entitled partial inverse regression. We show that its sample estimator is consistent, and that its performance is similar to or superior to partial least squares when n < p, especially when the regression model is nonlinear or heteroscedastic. An example involving the spectroscopy analysis of biscuit dough is also given."
"10.1093/biomet/asm044","2007","Sparse sufficient dimension reduction","4","Existing sufficient dimension reduction methods suffer from the fact that each dimension reduction component is a linear combination of all the original predictors, so that it is difficult to interpret the resulting estimates. We propose a unified estimation strategy, which combines a regression-type formulation of sufficient dimension reduction methods and shrinkage estimation, to produce sparse and accurate solutions. The method can be applied to most existing sufficient dimension reduction methods such as sliced inverse regression, sliced average variance estimation and principal Hessian directions. We demonstrate the effectiveness of the proposed method by both simulations and real data analysis."
"10.1093/biomet/asm055","2007","Implications of influence function analysis for sliced inverse regression and sliced average variance estimation","0","Sliced inverse regression, sliced inverse regression II and sliced average variance estimation are three related dimension-reduction methods that require relatively mild model assumptions. As an approximation for the relative influence of single observations from large samples, the influence function is used to compare the sensitivity of the three methods to particular observational types. The analysis carried out here helps to explain why there is a lack of agreement concerning the preferability of these dimension-reduction procedures in general. An efficient sample version of the influence function is also developed and evaluated."
"10.1093/biomet/asm038","2007","Dimension reduction in regression without matrix inversion","2","Regressions in which the fixed number of predictors p exceeds the number of independent observational units n occur in a variety of scientific fields. Sufficient dimension reduction provides a promising approach to such problems, by restricting attention to d < n linear combinations of the original p predictors. However, standard methods of sufficient dimension reduction require inversion of the sample predictor covariance matrix. We propose a method for estimating the central subspace that eliminates the need for such inversion and is applicable regardless of the ( n, p) relationship. Simulations show that our method compares favourably with standard large sample techniques when the latter are applicable. We illustrate our method with a genomics application."
"10.1093/biomet/asm053","2007","Tuning parameter selectors for the smoothly clipped absolute deviation method","18","The penalized least squares approach with smoothly clipped absolute deviation penalty has been consistently demonstrated to be an attractive regression shrinkage and selection method. It not only automatically and consistently selects the important variables, but also produces estimators which are as efficient as the oracle estimator. However, these attractive features depend on appropriate choice of the tuning parameter. We show that the commonly used generalized crossvalidation cannot select the tuning parameter satisfactorily, with a nonignorable overfitting effect in the resulting model. In addition, we propose a BIC tuning parameter selector, which is shown to be able to identify the true model consistently. Simulation studies are presented to support theoretical findings, and an empirical example is given to illustrate its use in the Female Labor Supply data."
"10.1093/biomet/asm060","2007","The weighted log-rank class of permutation tests: {$P$}-values and confidence intervals using saddlepoint methods","0","Test statistics from the weighted log-rank class are commonly used to compare treatment with control when there is right censoring. This paper uses saddlepoint methods to determine mid-p-values from the null permutation distributions of tests from the weighted log-rank class. Analytical saddlepoint computations replace the permutation simulations and provide mid-p-values that are virtually exact for all practical purposes. The speed of these saddlepoint computations makes it practicable to invert the weighted log-rank tests to determine nominal 95% confidence intervals for the treatment effect with right-censored data. Such analytical inversions lead to permutation confidence intervals that are easily computed and virtually identical to the exact intervals that would normally require massive amounts of simulation."
"10.1093/biomet/asm040","2007","Integrated likelihood functions for non-{B}ayesian inference","2","Consider a model with parameter theta = (psi, lambda), where psi is the parameter of interest, and let L(psi, lambda) denote the likelihood function. One approach to likelihood inference for psi is to use an integrated likelihood function, in which lambda is eliminated from L(psi, lambda) by integrating with respect to a density function pi(lambda|psi). The goal of this paper is to consider the problem of selecting pi(lambda|psi) so that the resulting integrated likelihood function is useful for non- Bayesian likelihood inference. The desirable properties of an integrated likelihood function are analyzed and these suggest that pi(lambda|psi) should be chosen by finding a nuisance parameter phi that is unrelated to psi and then taking the prior density for phi to be independent of psi. Such an unrelated parameter is constructed and the resulting integrated likelihood is shown to be closely related to the modified profile likelihood."
"10.1093/biomet/asm047","2007","Shape-space smoothing splines for planar landmark data","4","A method is developed for fitting smooth curves through a series of shapes of landmarks in two dimensions using unrolling and unwrapping procedures in Riemannian manifolds. An explicit method of calculation is given which is analogous to that of Jupp & Kent ( 1987) for spherical data. The resulting splines are called shape-space smoothing splines. The method resembles that of fitting smoothing splines in real spaces in that, if the smoothing parameter is zero, the resulting curve interpolates the data points, and if it is infinitely large the curve is a geodesic line. The fitted path to the data is defined such that its unrolled version at the tangent space of the starting point is a cubic spline fitted to the unwrapped data with respect to that path. Computation of the fitted path consists of an iterative procedure which converges quickly, and the resulting path is given in a discretised form in terms of a piecewise geodesic path. The procedure is applied to the analysis of some human movement data, and a test for the appropriateness of a mean geodesic curve is given."
"10.1093/biomet/asm032","2007","Adjusting estimative prediction limits","0","This note presents a direct adjustment of the estimative prediction limit to reduce the coverage error from a target value to third-order accuracy. The adjustment is asymptotically equivalent to those of Barndorff-Nielsen & Cox (1994, 1996) and Vidoni (1998). It has a simpler form with a plug-in estimator of the coverage probability of the estimative limit at the target value."
"10.1093/biomet/asm028","2007","Small-sample degrees of freedom for multi-component significance tests for multiple imputation for missing data","2","When performing multi-component significance tests with multiply-imputed datasets, analysts can use a Wald-like test statistic and a reference F-distribution. The currently employed degrees of freedom in the denominator of this F-distribution are derived assuming an infinite sample size. For modest complete-data sample sizes, this degrees of freedom can be unrealistic; for example, it may exceed the complete-data degrees of freedom. This paper presents an alternative denominator degrees of freedom that is always less than or equal to the complete-data denominator degrees of freedom, and equals the currently employed denominator degrees of freedom for infinite sample sizes. Its advantages over the currently employed degrees of freedom are illustrated with a simulation."
"10.1093/biomet/asm029","2007","Identifiability of single-index models and additive-index models","2","We provide a proof for the identifiability for both single-index models and partially linear single-index models assuming only the continuity of the regression function, a condition much weaker than the differentiability conditions assumed in the existing literature. Our discussion is then extended to the identifiability of the additive-index models."
"10.1093/biomet/asm033","2007","Testing goodness-of-fit in logistic case-control studies","1","We present a goodness-of-fit test for the logistic regression model under case-control sampling. The test statistic is constructed via a discrepancy between two competing kernel density estimators of the underlying conditional distributions given case-control status. The proposed goodness-of-fit test is shown to compare very favourably with previously proposed tests for case-control sampling in terms of power. The test statistic can be easily computed as a quadratic form in the residuals from a prospective logistic regression maximum likelihood fit. In addition, the proposed test is affine invariant and has an alternative representation in terms of empirical characteristic functions."
"10.1093/biomet/asm035","2007","Resampling-based empirical prediction: an application to small area estimation","0","Best linear unbiased prediction is well known for its wide range of applications including small area estimation. While the theory is well established for mixed linear models and under normality of the error and mixing distributions, the literature is sparse for nonlinear mixed models under nonnormality of the error distribution or of the mixing distributions. We develop a resampling-based unified approach for predicting mixed effects under a generalized mixed model set-up. Second-order-accurate nonnegative estimators of mean squared prediction errors are also developed. Given the parametric model, the proposed methodology automatically produces estimators of the small area parameters and their mean squared prediction errors, without requiring explicit analytical expressions for the mean squared prediction errors."
"10.1093/biomet/asm017","2007","Bayesian predictive information criterion for the evaluation of hierarchical {B}ayesian and empirical {B}ayes models","0","The problem of evaluating the goodness of the predictive distributions of hierarchical Bayesian and empirical Bayes models is investigated. A Bayesian predictive information criterion is proposed as an estimator of the posterior mean of the expected loglikelihood of the predictive distribution when the specified family of probability distributions does not contain the true distribution. The proposed criterion is developed by correcting the asymptotic bias of the posterior mean of the loglikelihood as an estimator of its expected loglikelihood. In the evaluation of hierarchical Bayesian models with random effects, regardless of our parametric focus, the proposed criterion considers the bias correction of the posterior mean of the marginal loglikelihood because it requires a consistent parameter estimator. The use of the bootstrap in model evaluation is also discussed."
"10.1093/biomet/asm031","2007","Uncertainty in prior elicitations: a nonparametric approach","0","A key task in the elicitation of expert knowledge is to construct a distribution from the. finite, and usually small, number of statements that have been elicited from the expert. These statements typically specify some quantiles or moments of the distribution. Such statements are not enough to identify the expert's probability distribution uniquely, and the usual approach is to fit some member of a convenient parametric family. There are two clear deficiencies in this solution. First, the expert's beliefs are forced to fit the parametric family. Secondly, no account is then taken of the many other possible distributions that might have fitted the elicited statements equally well. We present a nonparametric approach which tackles both of these deficiencies. We also consider the issue of the imprecision in the elicited probability judgements."
"10.1093/biomet/asm030","2007","Aster models for life history analysis","0","We present a new class of statistical models, designed for life history analysis of plants and animals, that allow joint analysis of data on survival and reproduction over multiple years, allow for variables having different probability distributions, and correctly account for the dependence of variables on earlier variables. We illustrate their utility with an analysis of data taken from an experimental study of Echinacea angustifolia sampled from remnant prairie populations in western Minnesota. These models generalize both generalized linear models and survival analysis. The joint distribution is factorized as a product of conditional distributions, each an exponential family with the conditioning variable being the sample size of the conditional distribution. The model may be heterogeneous, each conditional distribution being from a different exponential family. We show that the joint distribution is from a flat exponential family and derive its canonical parameters, Fisher information and other properties. These models are implemented in an R package 'aster' available from the Comprehensive R Archive Network, CRAN."
"10.1093/biomet/asm027","2007","Nonparametric estimation of age-at-onset distributions from censored kin-cohort data","0","We present a nonparametric estimator of genotype-specific age-at-onset distributions from kin-cohort data. Standard error calculations. are derived and the methodology is illustrated through an analysis of the influence of mutations of the Parkin gene on Parkinson's disease. Semiparametric efficiency considerations are briefly discussed."
"10.1093/biomet/asm019","2007","Estimating a treatment effect with repeated measurements accounting for varying effectiveness duration","0","To assess treatment efficacy in clinical trials, certain clinical outcomes are repeatedly measured over time for the same subject. The difference in their means may characterize a treatment effect. Since treatment effectiveness lag and saturation times may exist, erosion of treatment effect often occurs during the observation period. Instead of using models based on ad hoc parametric or purely nonparametric time-varying coefficients, we model the treatment effectiveness durations, which are the time intervals between the lag and saturation times. Then we use some mean response models to include such treatment effectiveness durations. Our methodology is demonstrated by simulations and analysis of a landmark HIV/AIDS clinical trial of short-course nevirapine against mother-to-child HIV vertical transmission during labour and delivery."
"10.1093/biomet/asm024","2007","Pairwise dependence diagnostics for clustered failure-time data","3","Frailty and copula models specify a parametric dependence structure for multivariate failure-time data. Estimation of some joint quantities can be highly sensitive to the assumed parametric form, and hence model fit is an important issue. This paper lays out a general diagnostic framework for evaluating and selecting frailty and copula models. The approach is based on the cumulative sum of residuals that are calculated in bivariate time. The residuals reflect the difference between the observed and expected bivariate association structures. The proposed model-checking process is interpretable with a limiting distribution which can be approximated using the bootstrap. Simulations and a data example illustrate the practical application of the method."
"10.1093/biomet/asm016","2007","Additive hazard regression with auxiliary covariates","1","We consider the additive hazard model when some of the true covariates are measured only on a randomly selected validation set whereas auxiliary covariates are observed for all study subjects. An updated pseudoscore estimation approach is proposed for the parameters of the additive hazard model. It allows one to fit the model with auxiliary covariates, while leaving the baseline hazard unspecified. Asymptotic properties of the proposed estimators are established, and consistent standard error estimators are developed. Simulations demonstrate that the asymptotic approximations of the proposed estimates are adequate for practical use. A real example is used to illustrate the performance of the proposed method."
"10.1093/biomet/asm023","2007","On the alignment of multiple time series fragments","0","We consider a local least-squares criterion for aligning multiple time series fragments differing by locations and show the consistency of the time-lag estimator and the asymptotic normality of the location estimator. We apply the criterion to the problem of aligning 50 glacial varve fragments and construct a 3000-year surrogate for global temperature."
"10.1093/biomet/asm036","2007","Model evaluation based on the sampling distribution of estimated absolute prediction error","1","The construction of a reliable, practically useful prediction rule for future responses is heavily dependent on the 'adequacy' of the fitted regression model. In this article, we consider the absolute prediction error, the expected value of the absolute difference between the future and predicted responses, as the model evaluation criterion. This prediction error is easier to interpret than the average squared error and is equivalent to the misclassification error for a binary outcome. We show that the prediction error can be consistently estimated via the resubstitution and crossvalidation methods even when the fitted model is not correctly specified. Furthermore, we show that the resulting estimators are asymptotically normal. When the prediction rule is 'nonsmooth', the variance of the above normal distribution can be estimated well with a perturbation-resampling method. With two real examples and an extensive simulation study, we demonstrate that the interval estimates obtained from the above normal approximation for the prediction errors provide much more information about model adequacy than their point-estimate counterparts."
"10.1093/biomet/asm073","2007","Cholesky decompositions and estimation of a covariance matrix: orthogonality of variance-correlation parameters","3","Chen & Dunson (2003) have proposed a modified Cholesky decomposition of the form Sigma = DLL'D for a covariance matrix where D is a diagonal matrix with entries proportional to the square roots of the diagonal entries of Sigma and L is a unit lower-triangular matrix solely determining its correlation matrix. This total separation of variance and correlation is definitely a major advantage over the more traditional modified Cholesky decomposition of the form (LDL)-L-2' (Pourahmadi, 1999). We show that, though the variance and correlation parameters of the former decomposition are separate, they are not asymptotically orthogonal and that the estimation of the new parameters could be more demanding computationally. We also provide statistical interpretation for the entries of L and D as certain moving average parameters and innovation variances and indicate how the existing likelihood procedures can be employed to estimate the new parameters."
"10.1093/biomet/asm063","2007","Using hierarchical likelihood for missing data problems","0","Most statistical solutions to the problem of statistical inference with missing data involve integration or expectation. This can be done in many ways: directly or indirectly, analytically or numerically, deterministically or stochastically. Missing-data problems can be formulated in terms of latent random variables, so that hierarchical likelihood methods of Lee & Nelder (1996) can be applied to missing-value problems to provide one solution to the problem of integration of the likelihood. The resulting methods effectively use a Laplace approximation to the marginal likelihood with an additional adjustment to the measures of precision to accommodate the estimation of the fixed effects parameters. We first consider missing at random cases where problems are simpler to handle because the integration does not need to involve the missing-value mechanism and then consider missing not at random cases. We also study tobit regression and refit the missing not at random selection model to the antidepressant trial data analyzed in Diggle & Kenward (1994)."
"10.1093/biomet/asm074","2007","Monte {C}arlo estimation for nonlinear non-{G}aussian state space models","1","We develop a proposal or importance density for state space models with a nonlinear non-Gaussian observation vector y similar to p(y|theta) and an unobserved linear Gaussian signal vector theta similar to p(theta). The proposal density is obtained from the Laplace approximation of the smoothing density p(theta|y). We present efficient algorithms to calculate the mode of p(theta|y) and to sample from the proposal density. The samples can be used for importance sampling and Markov chain Monte Carlo methods. The new results allow the application of these methods to state space models where the observation density p(y|theta) is not log-concave. Additional results are presented that lead to computationally efficient implementations. We illustrate the methods for the stochastic volatility model with leverage."
"10.1093/biomet/asm012","2007","Plant-capture estimation of the size of a homogeneous population","0","We consider maximum likelihood estimation of the size of a target population to which has been added a known number of planted individuals. The standard equal-catchability model used in mark-recapture is assumed to be applicable to the augmented population. After proving the unimodality of the profile likelihood for the target population size, we obtain both the maximum likelihood estimator of this size and interval estimators based on its asymptotic distribution."
"10.1093/biomet/asm003","2007","Optimal sufficient dimension reduction for the conditional mean in multivariate regression","0","The aim of this article is to develop optimal sufficient dimension reduction methodology for the conditional mean in multivariate regression. The context is roughly the same as that of a related method by Cook & Setodji (2003), but the new method has several advantages. It is asymptotically optimal in the sense described herein and its test statistic for dimension always has a chi-squared distribution asymptotically under the null hypothesis. Additionally, the optimal method allows tests of predictor effects. A comparison of the two methods is provided."
"10.1093/biomet/asm008","2007","Variable selection for the single-index model","3","We consider variable selection in the single-index model. We prove that the popular leave-m-out crossvalidation method has different behaviour in the single-index model from that in linear regression models or nonparametric regression models. A new consistent variable selection method, called separated crossvalidation, is proposed. Further analysis suggests that the method has better finite-sample performance and is computationally easier than leave-m-out crossvalidation. Separated crossvalidation, applied to the Swiss banknotes data and the ozone concentration data, leads to single-index models with selected variables that have better prediction capability than models based on all the covariates."
"10.1093/biomet/asm015","2007","Inference for clustered data using the independence loglikelihood","1","We use the properties of independence estimating equations to adjust the 'independence' loglikelihood function in the presence of clustering. The proposed adjustment relies on the robust sandwich estimator of the parameter covariance matrix, which is easily calculated. The methodology competes favourably with established techniques based on independence estimating equations; we provide some insight as to why this is so. The adjustment is applied to examples relating to the modelling of wind speed in Europe and annual maximum temperatures in the U.K."
"10.1093/biomet/asm009","2007","Modelling the effects of partially observed covariates on {P}oisson process intensity","3","We propose an estimating function for parameters in a model for Poisson process intensity when time- or space-varying covariates are observed for both the events of the process and at sample times or locations selected from a probability-based sampling design. We investigate the large-sample properties of the proposed estimator under increasing domain asymptotics, demonstrating that it is consistent and asymptotically normally distributed. We illustrate our approach using data from an ecological momentary assessment of smoking."
"10.1093/biomet/asm014","2007","Extending conventional priors for testing general hypotheses in linear models","1","We consider that observations come from a general normal linear model and that it is desirable to test a simplifying null hypothesis about the parameters. We approach this problem from an objective Bayesian, model-selection perspective. Crucial ingredients for this approach are 'proper objective priors' to be used for deriving the Bayes factors. Jeffreys-Zellner-Siow priors have good properties for testing null hypotheses defined by specific values of the parameters in full-rank linear models. We extend these priors to deal with general hypotheses in general linear models, not necessarily of full rank. The resulting priors, which we call 'conventional priors', are expressed as a generalization of recently introduced 'partially informative distributions'. The corresponding Bayes factors are fully automatic, easily computed and very reasonable. The methodology is illustrated for the change-point problem and the equality of treatments effects problem. We compare the conventional priors derived for these problems with other objective Bayesian proposals like the intrinsic priors. It is concluded that both priors behave similarly although interesting subtle differences arise. We adapt the conventional priors to deal with nonnested model selection as well as multiple-model comparison. Finally, we briefly address a generalization of conventional priors to nonnormal scenarios."
"10.1093/biomet/asm006","2007","A generalized threshold mixed model for analyzing nonnormal nonlinear time series, with application to plague in {K}azakhstan","1","We introduce the generalized threshold mixed model for piecewise-linear stochastic regression with possibly nonnormal time-series data. It is assumed that the conditional probability distribution of the response variable belongs to the exponential family, and the conditional mean response is linked to some piecewise-linear stochastic regression function. We study the particular case where the response variable equals zero in the lower regime. Some large-sample properties of a likelihood-based estimation scheme are derived. Our approach is motivated by the need for modelling nonlinearity in serially correlated epizootic events. Data coming from monitoring conducted in a natural plague focus in Kazakhstan are used to illustrate this model by obtaining biologically meaningful conclusions regarding the threshold relationship between prevalence of plague and some covariates including past abundance of great gerbils and other climatic variables."
"10.1093/biomet/asm013","2007","The unobserved heterogeneity distribution in duration analysis","0","In a large class of hazard models with proportional unobserved heterogeneity, the distribution of the heterogeneity among survivors converges to a gamma distribution. This convergence is often rapid. We derive this result as a general result for exponential mixtures and explore its implications for the specification and empirical analysis of univariate and multivariate duration models."
"10.1093/biomet/asm005","2007","Graphical identifiability criteria for causal effects in studies with an unobserved treatment/response variable","0","We consider the problem of using data in studies with an unobserved treatment/response variable in order to evaluate average causal effects, when cause-effect relationships between variables can be described by a directed acyclic graph and the corresponding recursive factorization of a joint distribution. The paper proposes graphical criteria to test whether average causal effects are identifiable even if a treatment/response variable is unobserved. If the answer is affirmative, we provide further formulations for average causal effects from the observed data. The graphical criteria enable us to evaluate average causal effects when it is difficult to observe a treatment/response variable."
"10.1093/biomet/asm018","2007","Model selection and estimation in the {G}aussian graphical model","18","We propose penalized likelihood methods for estimating the concentration matrix in the Gaussian graphical model. The methods lead to a sparse and shrinkage estimator of the concentration matrix that is positive definite, and thus conduct model selection and estimation simultaneously. The implementation of the methods is nontrivial because of the positive definite constraint on the concentration matrix, but we show that the computation can be done effectively by taking advantage of the efficient maxdet algorithm developed in convex optimization. We propose a BIC-type criterion for the selection of the tuning parameter in the penalized likelihood methods. The connection between our methods and existing methods is illustrated. Simulations and real examples demonstrate the competitive performance of the new methods."
"10.1093/biomet/asm039","2007","Optimal nested row-column designs with specified components","1","We consider nested row-column designs where each of the row and column component designs is specified. For the case that each of the component designs has second-order balance, we define such a nested row-column design to be special if it is generally balanced, with the smallest possible number of canonical treatment contrasts having the lower canonical efficiency factor in both components. We show that if any special row-column design exists then it is A-optimal over all nested row-column designs with the given components."
"10.1093/biomet/asm022","2007","Automatic estimation of multivariate spectra via smoothing splines","0","The classical method for estimating the spectral density of a multivariate time series is first to calculate the periodogram, and then to smooth it to obtain a consistent estimator. Typically, to ensure the estimate is positive definite, all the elements of the periodogram. are smoothed the same way. There are, however, many situations for which different components of the spectral matrix have different degrees of smoothness. We propose a Bayesian approach that uses Markov chain Monte Carlo techniques to fit smoothing splines to each component, real and imaginary, of the Cholesky decomposition of the periodogram matrix. The spectral estimator is then obtained-by reconstructing the spectral estimator from the smoothed Cholesky decomposition components. Our technique produces an automatically smoothed spectral matrix estimator along with samples from the posterior distributions of the parameters to facilitate inference."
"10.1093/biomet/asm025","2007","Inference on fractal processes using multiresolution approximation","0","We consider Bayesian inference via Markov chain Monte Carlo for a variety of fractal Gaussian processes on the real line. These models have unknown parameters in the covariance matrix, requiring inversion of a new covariance matrix at each Markov chain Monte Carlo iteration. The processes have no suitable independence properties so this becomes computationally prohibitive. We surmount these difficulties by developing a computational algorithm for likelihood evaluation based on a 'multiresolution approximation' to the original process. The method is computationally very efficient and widely applicable, making likelihood-based inference feasible for large datasets. A simulation study indicates that this approach leads to accurate estimates for underlying parameters in fractal models, including fractional Brownian motion and fractional Gaussian noise, and functional parameters in the recently introduced multifractional Brownian motion. We apply the method to a variety of real datasets and illustrate its application to prediction and to model selection."
"10.1093/biomet/asm021","2007","Marginal tests with sliced average variance estimation","2","We present a new computationally feasible test for the dimension of the central subspace in a regression problem based on sliced average variance estimation. We also provide a marginal coordinate test. Under the null hypothesis, both the test of dimension and the marginal coordinate test involve test statistics that asymptotically have chi-squared distributions given normally distributed predictors, and have a distribution that is a linear combination of chi-squared distributions in general."
"10.1093/biomet/asm026","2007","A weighted multivariate sign test for cluster-correlated data","2","We consider the multivariate location problem with cluster-correlated data. A family of multivariate weighted sign tests is introduced for which observations from different clusters can receive different weights. Under weak assumptions, the test statistic is asymptotically distributed as a chi-squared random variable as the number of clusters goes to infinity. The asymptotic distribution of the test statistic is also given for a local alternative model under multivariate normality. Optimal weights maximizing Pitman asymptotic efficiency are provided. These weights depend on the cluster sizes and on the intracluster correlation. Several approaches for estimating these weights are presented. Using Pitman asymptotic efficiency, we show that appropriate weighting can increase substantially the efficiency compared to a test that gives the same weight to each cluster. A multivariate weighted t-test is also introduced. The finite-sample performance of the weighted sign test is explored through a simulation study which shows that the proposed approach is very competitive. A real data example illustrates the practical application of the methodology."
"10.1093/biomet/asm034","2007","An asymptotic theory for model selection inference in general semiparametric problems","4","Hjort & Claeskens (2003) developed an asymptotic theory. for model selection, model averaging and subsequent inference using likelihood methods in parametric models, along with associated confidence statements. In this article, we consider a semiparametric version of this problem, wherein the likelihood depends on parameters and an unknown function, and model selection/averaging is to be applied to the parametric parts of the model. We show that all the results of Hjort & Claeskens hold in the semiparametric context, if the Fisher information matrix for parametric models is replaced by the semiparametric information bound for semiparametric models, and if maximum likelihood estimators for parametric models are replaced by semiparametric efficient profile estimators. Our methods of proof employ Le Cam's contiguity lemmas, leading to transparent results. The results also describe the behaviour of semiparametric model estimators when the parametric component is misspecified, and also have implications for pointwise-consistent model selectors."
"10.1093/biomet/asm007","2007","Estimation of a covariance matrix with zeros","4","We consider estimation of the covariance matrix of a multivariate random vector under the constraint that certain covariances are zero. We first present an algorithm, which we call iterative conditional fitting, for computing the maximum likelihood estimate of the constrained covariance matrix, under the assumption of multivariate normality. In contrast to previous approaches, this algorithm has guaranteed convergence properties. Dropping the assumption of multivariate normality, we show how to estimate the covariance matrix in an empirical likelihood approach. These approaches are then compared via simulation and on an example of gene expression."
"10.1093/biomet/asm010","2007","Partially linear models with missing response variables and error-prone covariates","4","We consider partially linear models of the form Y = X-T beta + nu(Z) + epsilon when the response variable Y is sometimes missing with missingness probability pi depending on (X, Z), and the covariate X is measured with error, where nu(z) is an unspecified smooth function. The missingness structure is therefore missing not at random, rather than the usual missing at random. We propose a class of semiparametric estimators for the parameter of interest beta, as well as for the population mean E(Y). The resulting estimators are shown to be consistent and asymptotically normal under general assumptions. To construct a confidence region for beta, we also propose an empirical-likelihood-based statistic, which is shown to have a chi-squared distribution asymptotically. The proposed methods are applied to an AIDS clinical trial dataset. A simulation study is also reported."
"10.1093/biomet/asm020","2007","Constrained local likelihood estimators for semiparametric skew-normal distributions","0","A local likelihood estimator for a nonparametric nuisance function is proposed in the context of semiparametric skew-normal distributions. Constraints imposed on such functions result in a nonparametric estimator with a different target function for maximization from classical local likelihood estimators. The optimal asymptotic semiparametric efficiency bound on parameters of interest is achieved by using this estimator in conjunction with an estimating equation formed by summing efficient scores. A generalized profile likelihood approach is also proposed. This method has the advantage of providing a unique estimate in cases where an estimating equation has multiple solutions. Our nonparametric estimator of the nuisance function leads to an estimator of the semiparametric skew-normal density. Both the estimating equation and profile likelihood approaches are applicable to more general skew-symmetric distributions."
"10.1093/biomet/asm011","2007","A pseudolikelihood method for analyzing interval censored data","0","We introduce a method based on a pseudolikelihood ratio for estimating the distribution function of the survival time in a mixed-case interval censoring model. In a mixed-case model, an individual is observed a random number of times, and at each time it is recorded whether an event has happened or not. One seeks to estimate the distribution of time to event. We use a Poisson process as the basis of a likelihood function to construct a pseudolikelihood ratio statistic for testing the value of the distribution function at a fixed point, and show that this converges under the null hypothesis to a known limit distribution, that can be expressed as a functional of different convex minorants of a two-sided Brownian motion process with parabolic drift. Construction of confidence sets then proceeds by standard inversion. The computation of the confidence sets is simple, requiring the use of the pool-adjacent-violators algorithm or a standard isotonic regression algorithm. We also illustrate the superiority of the proposed method over competitors based on resampling techniques or on the limit distribution of the maximum pseudolikelihood estimator, through simulation studies, and illustrate the different methods on a dataset involving time to HIV seroconversion in a group of haemophiliacs."
"10.1093/biomet/asm002","2007","Interval censoring: identifiability and the constant-sum property","0","The constant-sum property given in Oller et al. (2004) for censoring models justifies the use of a simplified likelihood to obtain the nonparametric maximum likelihood estimator of the lifetime distribution. In this paper we study the relevance of the constant-sum property in the identifiability of the lifetime distribution. We show that the lifetime distribution is not identifiable outside the class of constant-sum models. We also show that the lifetime probabilities assigned to the observable intervals are identifiable inside the class of constant-sum models. We illustrate all these notions with several examples."
"10.1093/biomet/asm001","2007","Fuzzy {$p$}-values in latent variable problems","0","We consider the problem of testing a statistical hypothesis where the scientifically meaningful test statistic is a function of latent variables. In particular, we consider detection of genetic linkage, where the latent variables are patterns of inheritance at specific genome locations. Introduced by Geyer & Meeden (2005), fuzzy p-values are random variables, described by their probability distributions, that are interpreted as p-values. For latent variable problems, we introduce the notion of a fuzzy p-value as having the conditional distribution of the latent p-value given the observed data, where the latent p-value is the random variable that would be the p-value if the latent variables were observed.The fuzzy p-value provides an exact test using two sets of simulations of the latent variables under the null hypothesis, one unconditional and the other conditional on the observed data. It provides not only an expression of the strength of the evidence against the null hypothesis but also an expression of the uncertainty in that expression owing to lack of knowledge of the latent variables. We illustrate these features with an example of simulated data mimicking a real example of the detection of genetic linkage."
"10.1093/biomet/asm004","2007","Maxima of discretely sampled random fields, with an application to `bubbles'","1","A smooth Gaussian random field with zero mean and unit variance is sampled on a discrete lattice, and we are interested in the exceedance probability or P-value of the maximum in a finite region. If the random field is smooth relative to the mesh size, then the P-value can be well approximated by results for the continuously sampled smooth random field (Adler, 1981; Worsley, 1995a; Taylor & Adler, 2003; Adler & Taylor, 2007). If the random field is not smooth, so that adjacent lattice values are nearly independent, then the usual Bonferroni bound is very accurate. The purpose of this paper is to bridge the gap between the two, and derive a simple, accurate upper bound for intermediate mesh sizes. The result uses a new improved Bonferroni-type bound based on discrete local maxima. We give an application to the 'bubbles' technique for detecting areas of the face used to discriminate fear from happiness."
"10.1093/biomet/93.4.1018","2006","Discriminant analysis with common principal components","0","Zhu & Hastie (2003) presented a general criterion for finding discriminant directions. To optimise their criterion, iterative methods are needed unless each class has a Gaussian distribution with a common covariance matrix. In this short paper, we present a slightly more general case where iterative methods can also be avoided."
"10.1093/biomet/93.4.1011","2006","Multivariate logistic models","0","The multivariate logistic transform is a reparameterisation of cell probabilities in terms of marginal logistic contrasts. It is known that an arbitrary set of logistic contrasts may not correspond to a valid joint distribution. In this paper we present an efficient algorithm for detecting whether or not the inverse transform exists, and for computing it if it does."
"10.1093/biomet/93.4.1003","2006","A diagnostic test for the mixing distribution in a generalised linear mixed model","0","We introduce a diagnostic test for the mixing distribution in a generalised linear mixed model. The test is based on the difference between the marginal maximum likelihood and conditional maximum likelihood estimators of a subset of the fixed effects in the model. We derive the asymptotic variance of this difference, and propose a test statistic that has a limiting chi-squared distribution under the null hypothesis that the mixing distribution is correctly specified. This strategy uses an idea presented by Hausman (1978), who considered analogous tests for the linear mixed model. An important advantage of the methods outlined here is that the resulting diagnostic test is easily implemented in commercial software. We illustrate the method by applying it to data from a clinical trial investigating the effect of hormonal contraceptives in women."
"10.1093/biomet/93.4.996","2006","Identification of a competing risks model with unknown transformations of latent failure times","1","This paper is concerned with identification of a competing risks model with unknown transformations of latent failure times. The model includes, as special cases, competing risks versions of proportional hazards, mixed proportional hazards and accelerated failure time models. It is shown that covariate effects on latent failure times, cause-specific link functions and the joint survivor function of the disturbance terms can be identified without relying on modelling the dependence between latent failure times parametrically nor using an exclusion restriction among covariates. As a result, the paper provides an identification result about the joint survivor function of the latent failure times conditional on covariates."
"10.1093/biomet/93.4.989","2006","Studies in the history of probability and statistics. {XLIX}. {O}n the {M}at\'ern correlation family","4","Handcock & Stein (1993) introduced the Matern family of spatial correlations into statistics as a flexible parametric class with one parameter determining the smoothness of the paths of the underlying spatial field. We document the varied history of this family, which includes contributions by eminent physical scientists and statisticians."
"10.1093/biomet/93.4.973","2006","Simple and accurate one-sided inference based on a class of {$M$}-estimators","0","This paper presents a simple method of calculating one-sided p-values which yield accurate inferences for a simple null hypothesis or, equivalently, accurate one-sided confidence limits regarding a scalar parameter in the presence of nuisance parameters. The method, which extends the work of DiCiccio et al. (2001) and Lee & Young (2005) in the context of adjusted likelihood estimation, is based on a test statistic analogous to the signed root of the loglikelihood, but derived from the objective function of an M-estimator from a certain class. Monte Carlo simulation is used to avoid the need for onerous analytical calculations typical of competing procedures based on Edgeworth or saddle-point approximations. The specific class of M-estimators under consideration is characterised by the requirement that the associated test statistic has constant variance to second order. This class contains, among others, the maximum likelihood estimator as well as variants of commonly used M-estimators of location, such as Huber's (1964) Proposal 2."
"10.1093/biomet/93.4.961","2006","Isotonic logistic discrimination","0","We propose an isotonic logistic discrimination procedure which generalises linear logistic discrimination by allowing linear boundaries to be more flexibly shaped as monotone functions of the discriminant variables. Under each of three familiar sampling schemes for obtaining a training dataset, namely prospective, mixture and retrospective, we provide the corresponding likelihood-based inference. An application to a cancer study is given. In addition, we consider theoretical comparisons of our method with two recent algorithmic monotone discrimination procedures."
"10.1093/biomet/93.4.943","2006","Multi-level modelling under informative sampling","1","We consider a model-dependent approach for multi-level modelling that accounts for informative probability sampling of first- and lower-level population units. The proposed approach consists of first extracting the hierarchical model holding for the sample data given the selected sample, as a function of the corresponding population model and the first- and lower-level sample selection probabilities, and then fitting the resulting sample model using Bayesian methods. An important implication of the use of the model holding for the sample is that the sample selection probabilities feature in the analysis as additional data that possibly strengthen the estimators. A simulation experiment is carried out in order to study the performance of this approach and compare it to the use of 'design-based' methods. The simulation study indicates that both approaches perform in general equally well in terms of point estimation, but the model-dependent approach yields confidence/credibility intervals with better coverage properties. Another simulation study assesses the impact of misspecification of the models assumed for the sample selection probabilities. The use of maximum likelihood estimation is also considered."
"10.1093/biomet/93.4.927","2006","Modelling of covariance structures in generalised estimating equations for longitudinal data","5","When used for modelling longitudinal data generalised estimating equations specify a working structure for the within-subject covariance matrices, aiming to produce efficient parameter estimators. However, misspecification of the working covariance structure may lead to a large loss of efficiency of the estimators of the mean parameters. In this paper we propose an approach for joint modelling of the mean and covariance structures of longitudinal data within the framework of generalised estimating equations. The resulting estimators for the mean and covariance parameters are shown to be consistent and asymptotically Normally distributed. Real data analysis and simulation studies show that the proposed approach yields e?cient estimators for both the mean and covariance parameters."
"10.1093/biomet/93.4.911","2006","A functional-based distribution diagnostic for a linear model with correlated outcomes","0","In this paper we present an easy-to-implement graphical distribution diagnostic for linear models with correlated errors. Houseman et al. (2004) constructed quantile-quantile plots for the marginal residuals of such models, suitably transformed. We extend the pointwise asymptotic theory to address the global stochastic behaviour of the corresponding empirical cumulative distribution function, and describe a simulation technique that serves as a computationally efficient parametric bootstrap for generating representatives of its stochastic limit. Thus, continuous functionals of the empirical cumulative distribution function may be used to form global tests of normality. Through the use of projection matrices, we generalised our methods to include tests that are directed at assessing the normality of particular components of the error. Thus, tests proposed by Lange & Ryan (1989) follow as a special case. Our method works well both for models having independent units of sampling and for those in which all observations are correlated."
"10.1093/biomet/93.4.895","2006","Semiparametric estimation of marginal mark distribution","0","In many applications, the outcome of interest is a mark such that its observation is contingent upon occurrence of an event. With incomplete follow-up data, the marginal mark distribution is, however, nonparametrically nowhere identifiable in many practical situations. To address this problem, we suggest a semiparametric model that postulates a normal copula for the association between the mark and survival time, but leaves the marginals unspecified. We show identifiability of the marginal mark distribution under this model, and propose an inference procedure. The estimated marginal distribution function is consistent and asymptotically normal, and it provides a basis for estimating summaries of the mark. Furthermore, we propose graphical model-checking methods and Kolmogorov-Smirnov-type goodness-of-fit tests. Simulation studies demonstrate that the inference procedure performs well in practical settings. The method is applied to the estimation of lifetime medical cost in a lung cancer trial."
"10.1093/biomet/93.4.877","2006","Variable selection in clustering via {D}irichlet process mixture models","4","The increased collection of high-dimensional data in various fields has raised a strong interest in clustering algorithms and variable selection procedures. In this paper, we propose a model-based method that addresses the two problems simultaneously. We introduce a latent binary vector to identify discriminating variables and use Dirichlet process mixture models to define the cluster structure. We update the variable selection index using a Metropolis algorithm and obtain inference on the cluster structure via a split-merge Markov chain Monte Carlo technique. We explore the performance of the methodology on simulated data and illustrate an application with a DNA microarray study."
"10.1093/biomet/93.4.861","2006","Forming post-strata via {B}ayesian treed capture-recapture models","0","For the problem of dual system estimation, we propose a Bayesian treed capture-recapture model to account for heterogeneity of capture probabilities where individual auxiliary information is available. The model uses a binary tree to partition the covariate space into 'homogeneous' regions, within each of which the capture response can be described adequately by a simple model that assumes equal catchability. The attractive features of the proposed model include reduction of correlation bias, robustness and practical flexibility as well as simplicity and interpretability. In addition, it provides a systematic and effective way of forming post-strata for the Sekar-Deming estimator of population size. We compare the performance of estimators based on this model to those of alternative estimators in three scenarios."
"10.1093/biomet/93.4.843","2006","Building mixture trees from binary sequence data","0","We develop a new method for building a hierarchical tree from binary sequence data. It is based on an ancestral mixture model. The sieve parameter in the model plays the role of time in the evolutionary tree of the sequences. By varying the sieve parameter, one can create a hierarchical tree that estimates the population structure at each fixed backward point in time. Application to the clustering of the mitochondrial DNA sequences of Griffiths & Tavare (1994) shows that the approach performs well. Theoretical and computational properties of the ancestral mixture model are further developed."
"10.1093/biomet/93.4.827","2006","Auxiliary mixture sampling for parameter-driven models of time series of counts with applications to state space modelling","1","We consider parameter-driven models of time series of counts, where the observations are assumed to arise from a Poisson distribution with a mean changing over time according to a latent process. Estimation of these models is carried out within a Bayesian framework using data augmentation and Markov chain Monte Carlo methods. We suggest a new auxiliary mixture sampler, which possesses a Gibbsian transition kernel, where we draw from full conditional distributions belonging to standard distribution families only. Emphasis lies on application to state space modelling of time series of counts, but we show that auxiliary mixture sampling may be applied to a wider range of parameter-driven models, including random-effects models and panel data models based on the Poisson distribution."
"10.1093/biomet/93.4.809","2006","Bayesian model selection for partially observed diffusion models","0","We present an approach to Bayesian model selection for finitely observed diffusion processes. We use data augmentation by treating the paths between observed points as missing data. For a fixed model formulation, the strong dependence between the missing paths and the volatility of the diffusion can be broken down by adopting the method of Roberts & Stramer (2001). We describe how this method may be extended to the case of model selection via reversible jump Markov chain Monte Carlo. In addition we extend the formulation of a diffusion model to capture a potential non-Markov state dependence in the drift. Issues of appropriate choices of priors and efficient transdimensional proposal distributions for the reversible jump algorithm are also addressed. The approach is illustrated using simulated data and an example from finance."
"10.1093/biomet/93.4.791","2006","Posterior propriety and computation for the {C}ox regression model with applications to missing covariates","0","In this paper, we carry out an in-depth theoretical investigation of Bayesian inference for the Cox regression model. We establish necessary and sufficient conditions for posterior propriety of the regression coefficient, beta, in Cox's partial likelihood, which can be obtained as the limiting marginal posterior distribution of beta through the specification of a gamma process prior for the cumulative baseline hazard and a uniform improper prior for beta. We also examine necessary and sufficient conditions for posterior propriety of the regression coefficients, beta, using full likelihood Bayesian approaches in which a gamma process prior is specified for the cumulative baseline hazard. We examine characterisation of posterior propriety under completely observed data settings as well as for settings involving missing covariates. Latent variables are introduced to facilitate a straightforward Gibbs sampling scheme in the Bayesian computation. A real dataset is presented to illustrate the proposed methodology."
"10.1093/biomet/93.4.777","2006","Nonparametric {$k$}-sample tests with panel count data","2","We study the nonparametric k-sample test problem with panel count data. The asymptotic normality of a smooth functional of the nonparametric maximum pseudo-likelihood estimator (Wellner & Zhang, 2000) is established under some mild conditions. We construct a class of easy-to-implement nonparametric tests for comparing mean functions of k populations based on this asymptotic normality. We conduct various simulations to validate and compare the tests. The simulations show that the tests perform quite well and generally have good power to detect differences among the mean functions. The method is illustrated with a real-life example."
"10.1093/biomet/93.4.763","2006","Analysing panel count data with informative observation times","0","In this paper, we study panel count data with informative observation times. We assume nonparametric and semiparametric proportional rate models for the underlying event process, where the form of the baseline rate function is left unspecified and a subject-specific frailty variable inflates or deflates the rate function multiplicatively. The proposed models allow the event processes and observation times to be correlated through their connections with the unobserved frailty; moreover, the distributions of both the frailty variable and observation times are considered as nuisance parameters. The baseline rate function and the regression parameters are estimated by maximising a conditional likelihood function of observed event counts and solving estimation equations. Large-sample properties of the proposed estimators are studied. Numerical studies demonstrate that the proposed estimation procedures perform well for moderate sample sizes. An application to a bladder tumour study is presented."
"10.1093/biomet/93.4.747","2006","Censored linear regression for case-cohort studies","1","Right-censored data from a classical case-cohort design and a stratified case-cohort design are considered. In the classical case-cohort design the subcohort is obtained as a simple random sample of the entire cohort, whereas in the stratified design this subcohort is elected by independent Bernoulli sampling with arbitrary selection probabilities. For each design and under a linear regression model, methods for estimating the regression parameters are proposed and analysed. These methods are derived by modifying the linear ranks tests and estimating equations that arise from full-cohort data using methods that are similar to the pseudolikelihood estimating equation that has been used in relative risk regression for these models. The estimators so obtained are shown to be consistent and asymptotically normal. Variance estimation and numerical illustrations are also provided."
"10.1093/biomet/93.2.486","2006","Understanding nonparametric estimation for clustered data","3","In this note we give an alternative formulation of the nonparametric estimators of Wang (2003) with the identity link. This results in a closed form of the estimator that has computational advantages and gives insight into the rationale behind the estimator."
"10.1093/biomet/93.2.481","2006","Models for recurring events with marginal proportional hazards","0","Semiparametric methods were proposed by Wei et al. (1989) to analyse recurring event-time data. They modelled the marginal distribution of each event time with a Cox proportional hazards model without imposing any constraint on the joint distribution of different event times. Therefore, it is unclear whether or not event times can simultaneously satisfy their respective marginal proportional hazards assumptions, while having continuous joint distribution. Often this leads to a difficulty of conducting simulation studies. In this note we construct parametric marginal proportional hazards models for recurring event times with proper joint density functions."
"10.1093/biomet/93.2.472","2006","Local likelihood density estimation based on smooth truncation","0","Two existing density estimators based on local likelihood have properties that are comparable to those of local likelihood regression but they are much less used than their counterparts in regression. We consider truncation as a natural way of localising parametric density estimation. Based on this idea, a third local likelihood density estimator is introduced. Our main result establishes that the three estimators coincide when a free multiplicative constant is used as an extra local parameter."
"10.1093/biomet/93.2.465","2006","Parametric modelling of thresholds across scales in wavelet regression","1","We propose a parametric wavelet thresholding procedure for estimation in the 'function plus independent, identically distributed Gaussian noise' model. To reflect the decreasing sparsity of wavelet coefficients from finer to coarser scales, our thresholds also decrease. They retain the noise-free reconstruction property while being lower than the universal threshold, and are jointly parameterised by a single scalar parameter. We show that our estimator achieves near-optimal risk rates for the usual range of Besov spaces. We propose a crossvalidation technique for choosing the parameter of our procedure. A simulation study demonstrates very good performance of our estimator compared to other state-of-the-art techniques. We discuss an extension to non-Gaussian noise."
"10.1093/biomet/93.2.459","2006","Rank-based regression for analysis of repeated measures","0","We consider rank-based regression models for repeated measures. To account for possible withinsubject correlations, we decompose the total ranks into between- and within-subject ranks and obtain two different estimators based on between- and within-subject ranks. A simple perturbation method is then introduced to generate bootstrap replicates of the estimating functions and the parameter estimates. This provides a convenient way for combining the corresponding two types of estimating function for more efficient estimation."
"10.1093/biomet/93.2.451","2006","An efficient {M}arkov chain {M}onte {C}arlo method for distributions with intractable normalising constants","5","Maximum likelihood parameter estimation and sampling from Bayesian posterior distributions are problematic when the probability density for the parameter of interest involves an intractable normalising constant which is also a function of that parameter. In this paper, an auxiliary variable method is presented which requires only that independent samples can be drawn from the unnormalised density at any particular parameter value. The proposal distribution is constructed so that the normalising constant cancels from the Metropolis-Hastings ratio. The method is illustrated by producing posterior samples for parameters of the Ising model given a particular lattice realisation."
"10.1093/biomet/93.2.439","2006","Estimating a bivariate density when there are extra data on one or both components","0","The objective of this paper is to estimate a bivariate density nonparametrically from a dataset from the joint distribution and datasets from one or both marginal distributions. We develop a copula-based solution, which has potential benefits even when the marginal datasets are empty. For example, if the copula density is sufficiently smooth in the region where we wish to estimate it, the joint density can be estimated with a high degree of accuracy. Similar improvements in performance are available if the marginals are close to being independent. We use wavelet estimators to approximate the copula density, which in cases of statistical interest can be unbounded along boundaries. Our techniques are also useful for solving recently-considered related problems, for example where the marginal distributions are determined by parametric models. The methodology is also readily extended to more general multivariate settings."
"10.1093/biomet/93.2.425","2006","Effects of the reference set on frequentist inferences","0","We employ second-order likelihood asymptotics to investigate how ideal frequentist inferences depend on the probability model for the data through more than the likelihood function, referring to this as the effect of the reference set. There are two aspects of higherorder corrections to first-order likelihood methods, namely (i) that involving effects of fitting nuisance parameters and leading to the modified profile likelihood, and (ii) another part pertaining to limitation in adjusted information. Generally, each of these involves a first-order adjustment depending on the reference set. However, we show that, for some important settings, likelihood-irrelevant model specifications have a second-order effect on both of these adjustments; this result includes specification of the censoring model for survival data. On the other hand, for sequential experiments the likelihood-irrelevant specification of the stopping rule has a second-order effect on adjustment (i) but a firstorder effect on adjustment (ii). These matters raise the issue of what are 'ideal' frequentist inferences, since consideration of 'exact' frequentist inferences will not suffice. We indicate that to second order ideal frequentist inferences may be based on the distribution of the ordinary likelihood ratio statistic, without commonly considered adjustments thereto."
"10.1093/biomet/93.2.411","2006","Using the periodogram to estimate period in nonparametric regression","1","Properties of the periodogram are seldom studied in the setting of nonparametric regression, although that is the context in which the periodogram is widely applied in astronomy. There it is a competitor with more recent least-squares methods. The periodogram has the advantage of providing significant graphical insight into statistical and numerical aspects of the problem. However, as we show in the present paper, it also has drawbacks. The estimator that it produces has somewhat higher variance than its least-squares counterpart, and a periodogram-based approach is more prone to suffer difficulties caused by periodicity of the observation schedule. While the periodogram remains a very attractive tool, the information provided in this paper allows users to assess more readily the extent to which it can be relied upon in a nonparametric setting. This aspect of our contributions is discussed theoretically and illustrated by numerical studies involving a real dataset."
"10.1093/biomet/93.2.399","2006","A test statistic for graphical modelling of multivariate time series","1","A graphical model for multivariate time series is a concept extended by Dahlhaus (2000) from that for a random vector to a multivariate time series. We propose a test statistic for identifying the model based on the Kullback-Leibler divergence between two graphical models. The null distribution is shown to be asymptotically normal with mean and variance which depend just on the dimensions of the graphs."
"10.1093/biomet/93.2.385","2006","Fitting binary regression models with case-augmented samples","1","In a case-augmented study, measurements on a random sample from a population are augmented by information from an independent sample of cases, that is units with some characteristic of interest. We show that inferences about the effect of the covariates on the probability of being a case can be made by fitting a modified prospective likelihood. We also show that this procedure is fully efficient."
"10.1093/biomet/93.2.367","2006","Nonparametric estimation with left-truncated semicompeting risks data","0","Nonparametric estimators for competing risks data can be applied to semicompeting risks data, a type of multi-state data where a terminating event may censor a nonterminating event, after forcing the data into the competing risks format. Complications may arise with left truncation of the terminating event, where the competing risks analysis naively truncates the nonterminating event using the left-truncation time for the terminating event, which may lead to large efficiency losses. We propose nonparametric estimators which use all semicompeting risks information and do not require artificial truncation. The uniform consistency and weak convergence of the estimators are established and variance estimators are provided. Simulation studies and an analysis of a diabetes registry demonstrate large efficiency gains over the naive estimators."
"10.1093/biomet/93.2.357","2006","Confidence bands for hazard rates under random censorship","0","We suggest a completely empirical approach to the construction of confidence bands for hazard functions, based on smoothing the Nelsen-Aalen estimator. In particular, we introduce a local bandwidth-choice method. Our approach uses empirical information about both the survival rate and the censoring rate, and employs undersmoothing to alleviate difficulties caused by bias. We use both Edgeworth expansion and numerical simulation, the former to develop a basic formula and the latter to modify it for general use."
"10.1093/biomet/93.2.343","2006","Estimating the quality-of-life-adjusted gap time distribution of successive events subject to censoring","0","When treatment effects are studied in the context of successive or recurrent life events, separate analyses of the quality-of-life scores and of the inter-event, gap, times might lead to possibly contradictory conclusions. In an attempt to reconcile this, we propose a unitary and more comprehensive nonparametric analysis that combines the two separate analyses by introducing the quality-of-life-adjusted gap time concept. Inverse probability of censoring estimators of the quality-of-life-adjusted gap time joint and conditional distributions are proposed and are shown to be consistent and asymptotically normal. Simulations performed in a variety of scenarios indicate that the joint and conditional quality-of-life-adjusted gap time distribution estimators are virtually unbiased, with properly estimated standard errors and asymptotic normality features. An example from the International Breast Cancer Study Group Trial V illustrates the use of the proposed estimators."
"10.1093/biomet/93.2.329","2006","On the accelerated failure time model for current status and interval censored data","2","This paper introduces a novel approach to making inference about the regression parameters in the accelerated failure time model for current status and interval censored data. The estimator is constructed by inverting a Wald-type test for testing a null proportional hazards model. A numerically efficient Markov chain Monte Carlo based resampling method is proposed for obtaining simultaneously the point estimator and a consistent estimator of its variance-covariance matrix. We illustrate our approach with interval censored datasets from two clinical studies. Extensive numerical studies are conducted to evaluate the finite-sample performance of the new estimators."
"10.1093/biomet/93.2.315","2006","A {$k$}-sample test with interval censored data","0","The problem of testing for the equality of k distribution functions under Case 2 interval censoring is studied and a supremum-type test statistic is proposed based on the differences between the nonparametric maximum likelihood estimator and the so-called leveraged bootstrap estimator of the k underlying distributions. The proposed test is distributionfree and consistent against all alternatives. As the main results hold for a wide range of resampling sizes, a data-driven method is suggested for determining the size of each leveraged bootstrap sample. Another advantage of the test is that it can detect different distributions with equal means or heavy crossover. Simulation studies indicate that the test performs quite well with a moderate sample size. Finally, a slightly modified version of the test is applied to breast cosmesis data."
"10.1093/biomet/93.2.303","2006","Linear life expectancy regression with censored data","2","In the statistical literature, life expectancy is usually characterised by the mean residual life function. Regression models are thus needed to study the association between the mean residual life functions and their covariates. In this paper, we consider a linear mean residual life model and develop inference procedures in the presence of potential censoring. The new model and inference procedures are applied to the Stanford heart transplant data. Semiparametric efficiency calculations and information bounds are also considered."
"10.1093/biomet/93.2.289","2006","Optimal blocking of two-level factorial designs","0","Blocking of two-level factorial designs is considered for block sizes 2 and 4 using the method of fractional partial confounding. A-, D- and E-optimal designs are obtained for block size 2 within the class of orthogonal designs for which main effects and two-factor interactions are all orthogonal to each other before allowing for blocking. A-, D- and E-optimal designs are obtained for block size 4 within the class of orthogonal designs with main effects orthogonal to blocks. The designs obtained also have other favourable properties including orthogonal estimation of effects and orthogonality to superblocks."
"10.1093/biomet/93.2.279","2006","A construction method for orthogonal {L}atin hypercube designs","8","The Latin hypercube design is a popular choice of experimental design when computer simulation is used to study a physical process. These designs guarantee uniform samples for the marginal distribution of each single input. A number of methods have been proposed for extending the uniform sampling to higher dimensions.We show how to construct Latin hypercube designs in which all main effects are orthogonal. Our method can also be used to construct Latin hypercube designs with low correlation of first-order and second-order terms. Our method generates orthogonal Latin hypercube designs that can include many more factors than those proposed by Ye (1998)."
"10.1093/biomet/93.2.269","2006","Applying the {H}orvitz-{T}hompson criterion in complex designs: a computer-intensive perspective for estimating inclusion probabilities","1","A modification of the Horvitz-Thompson estimator is proposed for complex sampling designs. The inclusion probabilities are estimated by means of independent replications of the sampling scheme. The properties of the resulting estimator are derived. Guidelines for choosing the appropriate number of replications are given and some applications are considered."
"10.1093/biomet/93.2.255","2006","{$M$}-quantile models for small area estimation","0","Small area estimation techniques typically rely on regression models that use both covariates and random effects to explain variation between the areas. However, such models also depend on strong distributional assumptions, require a formal specification of the random part of the model and do not easily allow for outlier-robust inference. We describe a new approach to small area estimation that is based on modelling quantilelike parameters of the conditional distribution of the target variable given the covariates. This avoids the problems associated with specification of random effects, allowing inter-area differences to be characterised by area-specific M-quantile coefficients. The proposed approach is easily made robust against outlying data values and can be adapted for estimation of a wide range of area-specific parameters, including quantiles of the distribution of the target variable in the different small areas. The differences between M-quantile and random effects models are discussed and the alternative approaches to small area estimation are compared using both simulated and real data."
"10.1093/biomet/93.2.235","2006","Bayesian alignment using hierarchical models, with applications in protein bioinformatics","0","An important problem in shape analysis is to match configurations of points in space after filtering out some geometrical transformation. In this paper we introduce hierarchical models for such tasks, in which the points in the configurations are either unlabelled or have at most a partial labelling constraining the matching, and in which some points may only appear in one of the configurations. We derive procedures for simultaneous inference about the matching and the transformation, using a Bayesian approach. Our hierarchical model is based on a Poisson process for hidden true point locations; this leads to considerable mathematical simplification and efficiency of implementation of EM and Markov chain Monte Carlo algorithms. We find a novel use for classical distributions from directional statistics in a conditionally conjugate specification for the case where the geometrical transformation includes an unknown rotation. Throughout, we focus on the case of affine or rigid motion transformations. Under a broad parametric family of loss functions, an optimal Bayesian point estimate of the matching matrix can be constructed that depends only on a single parameter of the family. Our methods are illustrated by two applications from bioinformatics. The first problem is of matching protein gels in two dimensions, and the second consists of aligning active sites of proteins in three dimensions. In the latter case, we also use information related to the grouping of the amino acids, as an example of a more general capability of our methodology to include partial labelling information. We discuss some open problems and suggest directions for future work."
"10.1093/biomet/93.1.228","2006","A note on kernel polygons","0","Jones (1989) has pointed out that piecewise linear interpolated kernel density estimators on a sufficiently fine grid can be visually indistinguishable from the true density. A simple device, the kernel polygon, is proposed for eliminating the evaluation of the normalisation constant of the estimator while retaining its property of being a density function as well as providing practical advantages. The class of uniform and linear kernels of the kernel polygons is given. Finally, we present a simulation study and a real data example in which we compare bandwidth selectors for the kernel polygons."
"10.1093/biomet/93.1.221","2006","A note on time-reversibility of multivariate linear processes","1","We derive some readily verifiable necessary and sufficient conditions for a multivariate non-Gaussian linear process to be time-reversible, under two sets of conditions on the contemporaneous dependence structure of the innovations. One set of conditions concerns the case of independent-component innovations, in which case a multivariate non-Gaussian linear process is time-reversible if and only if the coefficients consist of essentially asymmetric columns with column-specific origins of symmetry or symmetric pairs of columns with pair-specific origins of symmetry. On the other hand, for dependent-component innovations plus other regularity conditions, a multivariate non-Gaussian linear process is time-reversible if and only if the coefficients are essentially symmetric about some origin."
"10.1093/biomet/93.1.215","2006","On {B}artlett correction of empirical likelihood in the presence of nuisance parameters","5","Lazar & Mykland (1999) showed that an empirical likelihood defined by two estimating equations with a nuisance parameter need not be Bartlett-correctable. This paper shows that Bartlett correction of empirical likelihood in the presence of a nuisance parameter depends critically on the way the nuisance parameter is removed when formulating the likelihood for the parameter of interest. We establish in the broad framework of estimating functions that the empirical likelihood is still Bartlett-correctable if the nuisance parameter is profiled out given the value of the parameter of interest."
"10.1093/biomet/93.1.207","2006","Semiparametric transformation models for the case-cohort study","0","A general class of semiparametric transformation models is studied for analysing survival data from the case-cohort design, which was introduced by Prentice (1986). Weighted estimating equations are proposed for simultaneous estimation of the regression parameters and the transformation function. It is shown that the resulting regression estimators are asymptotically normal, with variance-covariance matrix that has a closed form and can be consistently estimated by the usual plug-in method. Simulation studies show that the proposed approach is appropriate for practical use. An application to a case-cohort dataset from the Atherosclerosis Risk in Communities study is also given to illustrate the methodology."
"10.1093/biomet/93.1.197","2006","Range of correlation matrices for dependent {B}ernoulli random variables","0","We say that a pair (p, R) is compatible if there exists a multivariate binary distribution with mean vector p and correlation matrix R. In this paper we study necessary and sufficient conditions for compatibility for structured and unstructured correlation matrices. We give examples of correlation matrices that are incompatible with any p. Using our results we show that the parametric binary models of Emrich & Piedmonte (1991) and Qaqish (2003) allow a good range of correlations between the binary variables. We also obtain necessary and sufficient conditions for a matrix of odds ratios to be compatible with a given p. Our findings support the popular belief that the odds ratios are less constrained and more flexible than the correlations."
"10.1093/biomet/93.1.179","2006","A shrinkage estimator for spectral densities","0","We propose a shrinkage estimator for spectral densities based on a multilevel normal hierarchical model. The first level captures the sampling variability via a likelihood constructed using the asymptotic properties of the periodogram. At the second level, the spectral density is shrunk towards a parametric time series model. To avoid selecting a particular parametric model for the second level, a third level is added which induces an estimator that averages over a class of parsimonious time series models. The estimator derived from this model, the model averaged shrinkage estimator, is consistent, is shown to be highly competitive with other spectral density estimators via simulations, and is computationally inexpensive."
"10.1093/biomet/93.1.163","2006","Semiparametric efficient estimation of survival distributions in two-stage randomisation designs in clinical trials with censored data","3","Two-stage randomisation designs are useful in the evaluation of combination therapies where patients are initially randomised to an induction therapy and then, depending upon their response and consent, are randomised to a maintenance therapy. In this paper we derive the best regular asymptotically linear estimator for the survival distribution and related quantities of treatment regimes. We propose an estimator which is easily computable and is more efficient than existing estimators. Large-sample properties of the proposed estimator are derived and comparisons with other estimators are made using simulation."
"10.1093/biomet/93.1.147","2006","On least-squares regression with censored data","4","The semiparametric accelerated failure time model relates the logarithm of the failure time linearly to the covariates while leaving the error distribution unspecified. The present paper describes simple and reliable inference procedures based on the least-squares principle for this model with right-censored data. The proposed estimator of the vector-valued regression parameter is an iterative solution to the Buckley-James estimating equation with a preliminary consistent estimator as the starting value. The estimator is shown to be consistent and asymptotically normal. A novel resampling procedure is developed for the estimation of the limiting covariance matrix. Extensions to marginal models for multivariate failure time data are considered. The performance of the new inference procedures is assessed through simulation studies. Illustrations with medical studies are provided."
"10.1093/biomet/93.1.137","2006","Orthogonal arrays robust to nonnegligible two-factor interactions","4","Regular fractional factorial designs with clear two-factor interactions provide a useful class of designs that are robust to nonnegligible two-factor interactions. In this paper, the concept of clear two-factor interactions is generalised to orthogonal arrays. The new concept leads to a much wider class of designs robust to nonnegligible two-factor interactions. We study the existence and construction of such designs. The designs we construct have a structure that render themselves particularly attractive in the robust parameter design setting. We also discuss an interesting connection between designs with clear two-factor interactions and mixed orthogonal arrays."
"10.1093/biomet/93.1.127","2006","Efficient designs for one-sided comparisons of two or three treatments with a control in a one-way layout","0","The problem of providing lower confidence bounds for the mean improvements of p >= 2 test treatments over a control treatment is considered. The expected average and expected maximum allowances are two criteria for comparing different systems of confidence intervals or bounds. In this paper, lower bounds are derived for the expected average allowance and the expected maximum allowance of Dunnett's simultaneous lower confidence bounds for the p mean improvements. These lower bounds hold for any p >= 2 and any allocation of sample sizes. For p = 2 test treatments, sample allocations are given for which the bounds are achievable. For p = 3 test treatments, a tighter set of bounds is derived which enables easy determination of the sample allocation required to achieve highly efficient designs. A table of the bounds for the expected average and expected maximum allowances and the sample allocation that achieves these bounds is given for p = 2, 3. The theoretical results can easily be adapted to cover upper confidence bounds."
"10.1093/biomet/93.1.113","2006","Spatially adaptive smoothing splines","0","We use a reproducing kernel Hilbert space representation to derive the smoothing spline solution when the smoothness penalty is a function lambda(t) of the design space t, thereby allowing the model to adapt to various degrees of smoothness in the structure of the data. We propose a convenient form for the smoothness penalty function and discuss computational algorithms for automatic curve fitting using a generalised crossvalidation measure."
"10.1093/biomet/93.1.99","2006","Robust and efficient estimation under data grouping","0","The minimum Hellinger distance estimator is known to have desirable properties in terms of robustness and efficiency. We propose an approximate minimum Hellinger distance estimator by adapting the approach to grouped data from a continuous distribution. It is easier to compute the approximate version for either the continuous data or the grouped data. Given certain conditions on the model distribution and reasonable grouping rules, the approximate minimum Hellinger distance estimator is shown to be consistent and asymptotically normal. Furthermore, it is robust and can be asymptotically as efficient as the maximum likelihood estimator. The merit of the estimator is demonstrated through simulation studies and real data examples."
"10.1093/biomet/93.1.85","2006","Covariance matrix selection and estimation via penalised normal likelihood","23","We propose a nonparametric method for identifying parsimony and for producing a statistically efficient estimator of a large covariance matrix. We reparameterise a covariance matrix through the modified Cholesky decomposition of its inverse or the one-step-ahead predictive representation of the vector of responses and reduce the nonintuitive task of modelling covariance matrices to the familiar task of model selection and estimation for a sequence of regression models. The Cholesky factor containing these regression coefficients is likely to have many off-diagonal elements that are zero or close to zero. Penalised normal likelihoods in this situation with L-1 and L-2 penalities are shown to be closely related to Tibshirani's (1996) LASSO approach and to ridge regression. Adding either penalty to the likelihood helps to produce more stable estimators by introducing shrinkage to the elements in the Cholesky factor, while, because of its singularity, the L-1 penalty will set some elements to zero and produce interpretable models. An algorithm is developed for computing the estimator and selecting the tuning parameter. The proposed maximum penalised likelihood estimator is illustrated using simulation and a real dataset involving estimation of a 102 x 102 covariance matrix."
"10.1093/biomet/93.1.75","2006","Efficient semiparametric estimator for heteroscedastic partially linear models","2","We study the heteroscedastic partially linear model with an unspecified partial baseline component and a nonparametric variance function. An interesting finding is that the performance of a naive weighted version of the existing estimator could deteriorate when the smooth baseline component is badly estimated. To avoid this, we propose a family of consistent estimators and investigate their asymptotic properties. We show that the optimal semiparametric efficiency bound can be reached by a semiparametric kernel estimator in this family. Building upon our theoretical findings and heuristic arguments about the equivalence between kernel and spline smoothing, we conjecture that a weighted partial-spline estimator could also be semiparametric efficient. Properties of the proposed estimators are presented through theoretical illustration and numerical simulations."
"10.1093/biomet/93.1.65","2006","Using intraslice covariances for improved estimation of the central subspace in regression","4","Popular methods for estimating the central subspace in regression require slicing a continuous response. However, slicing can result in loss of information and in some cases that loss can be substantial. We use intraslice covariances to construct improved inference methods for the central subspace. These methods are optimal within a class of quadratic inference functions and permit chi-squared tests of conditional independence hypotheses involving the predictors. Our experience gained through simulation is that the new method is never worse than existing methods, and can be substantially better."
"10.1093/biomet/93.1.53","2006","Latent-model robustness in structural measurement error models","1","We present methods for diagnosing the effects of model misspecification of the true-predictor distribution in structural measurement error models. We first formulate latent-model robustness theoretically. Then we provide practical techniques for examining the adequacy of an assumed latent predictor model. The methods are illustrated via analytical examples, application to simulated data and with data from a study of coronary heart disease."
"10.1093/biomet/93.1.41","2006","Efficient {B}ayes factor estimation from the reversible jump output","2","We propose a class of estimators of the Bayes factor which is based on an extension of the bridge sampling identity of Meng & Wong (1996) and makes use of the output of the reversible jump algorithm of Green (1995). Within this class we give the optimal estimator and also a suboptimal one which may be simply computed on the basis of the acceptance probabilities used within the reversible jump algorithm for jumping between models. The proposed estimators are very easily computed and lead to a substantial gain of efficiency in estimating the Bayes factor over the standard estimator based on the reversible jump output. This is illustrated through a series of Monte Carlo simulations involving a linear and a logistic regression model."
"10.1093/biomet/93.1.23","2006","Reference priors for discrete graphical models","1","The combination of graphical models and reference analysis represents a powerful tool for Bayesian inference in highly multivariate settings. It is typically difficult to derive reference priors in complex problems. In this paper we present a suitable mixed parameterisation for a discrete decomposable graphical model and derive the corresponding reference prior."
"10.1093/biomet/93.1.1","2006","Adaptive and nonadaptive group sequential tests","1","Methods have been proposed for redesigning a clinical trial at an interim stage in order to increase power. In order to preserve the type I error rate, methods for unplanned design-change have to be defined in terms of nonsufficient statistics, and this calls into question their efficiency and the credibility of conclusions reached. We evaluate schemes for adaptive redesign, extending the theoretical arguments for use of sufficient statistics of Tsiatis & Mehta (2003) and assessing the possible benefits of preplanned adaptive designs by numerical computation of optimal tests; these optimal adaptive designs are concrete examples of optimal sequentially planned sequential tests proposed by Schmitz (1993). We conclude that the flexibility of unplanned adaptive designs comes at a price and we recommend that the appropriate power for a study should be determined as thoroughly as possible at the outset. Then, standard error-spending tests, possibly with unevenly spaced analyses, provide efficient designs, but it is still possible to fall back on flexible methods for redesign should study objective change unexpectedly once the trial is under way."
"10.1093/biomet/93.3.742","2006","Simes' procedure is `valid on average'","0","Although Simes' modification of the Bonferroni procedure tends to perform very well, albeit often being slightly liberal for negatively dependent hypotheses, there are special cases where it fails more dramatically. We prove that these special cases are indeed special, applying only to specific significance levels, and obtain a strong bound on the average deviation of the Simes corrected P-value from the true probability over any interval of P-values. From this, it is argued that Simes' procedure should be expected to perform well except for pathological examples."
"10.1093/biomet/93.3.735","2006","Prospective survival analysis with a general semiparametric shared frailty model: a pseudo full likelihood approach","1","We provide a simple estimation procedure for a general frailty model for the analysis of prospective correlated failure times. The large-sample properties of the proposed estimators of both the regression coefficient vector and the dependence parameter are described, and consistent variance estimators are given. A brief outline of the proofs is given. In a simulation study under the widely used gamma frailty model, our proposed approach was found to have essentially the same efficiency as the EM-based maximum likelihood approach considered by other authors, with negligible difference between the standard errors of the two estimators. However, the proposed approach provides a framework capable of handling general frailty distributions with finite moments and yields an explicit consistent variance estimator."
"10.1093/biomet/93.3.723","2006","Empirical-type likelihoods allowing posterior credible sets with frequentist validity: higher-order asymptotics","3","With reference to a general class of empirical-type likelihoods, we develop higher-order asymptotics for the frequentist coverage of Bayesian credible sets based on posterior quantiles and highest posterior density. These asymptotics, in turn, characterise members of the class that allow approximate frequentist validity of such sets. It is seen that the usual empirical likelihood does not enjoy this property up to the order of approximation considered here."
"10.1093/biomet/93.3.705","2006","Empirical {B}ayes block shrinkage of wavelet coefficients via the noncentral {$\chi^2$} distribution","0","Empirical Bayes approaches to the shrinkage of empirical wavelet coefficients have generated considerable interest in recent years. Much of the work to date has focussed on shrinkage of individual wavelet coefficients in isolation. In this paper we propose an empirical Bayes approach to simultaneous shrinkage of wavelet coefficients in a block, based on the block sum of squares. Our approach exploits a useful identity satisfied by the noncentral chi(2) density and provides some tractable Bayesian block shrinkage procedures. Our numerical results indicate that the new procedures perform very well."
"10.1093/biomet/93.3.687","2006","A {H}aar-{F}isz technique for locally stationary volatility estimation","2","We consider a locally stationary model for financial log-returns whereby the returns are independent and the volatility is a piecewise-constant function with jumps of an unknown number and locations, defined on a compact interval to enable a meaningful estimation theory. We demonstrate that the model explains well the common characteristics of log-returns. We propose a new wavelet thresholding algorithm for volatility estimation in this model, in which Haar wavelets are combined with the variance-stabilising Fisz transform. The resulting volatility estimator is mean-square consistent with a near-parametric rate, does not require any pre-estimates, is rapidly computable and is easily implemented. We also discuss important variations on the choice of estimation parameters. We show that our approach both gives a very good fit to selected currency exchange datasets, and achieves accurate long- and short-term volatility forecasts in comparison to the GARCH(1, 1) and moving window techniques."
"10.1093/biomet/93.3.671","2006","Models for interval censoring and simulation-based inference for lifetime distributions","0","Interval-censored lifetime data arise when individuals in a study are inspected intermittently so that a lifetime is observed to lie between two successive times. In settings where only these two times are available, methods exist for nonparametric or parametric estimation of lifetime distributions. However, there has been virtually no discussion of how inspection processes may be estimated or identified. Such estimates are needed if one is to generate interval-censored data by simulation. This paper identifies which aspects of an independent inspection process are estimable from interval-censored data, and shows how to obtain nonparametric estimates. The results allow interval-censored data from any specified distribution to be generated, and give new simulation procedures for estimation or testing. A new omnibus goodness-of-fit test is introduced."
"10.1093/biomet/93.3.655","2006","Estimating survival under a dependent truncation","0","The product-limit estimator calculated from data subject to random left-truncation relies on the testable assumption of quasi-independence between the failure time and the truncation time. In this paper, we propose a model for a truncated sample of pairs (X-i,Y-i) satisfying Y-i > X-i. A possible dependency between the truncation time and the variable of interest is modelled with a parametric family of copulas. The model also features a distribution function F-X(.) and a survival distribution S-Y(.) associated with the marginal behaviours of X and Y in the observable region Y > X. Semiparametric estimators for these two functions are proposed; they do not make any parametric assumption about either F-X(.) or S-Y(.). We derive an estimator for the copula parameter alpha based on the conditional Kendall's tau. We generalise the copula-graphic estimators of Zheng & Klein (1995) to truncated variables. The asymptotic distributions of all these estimators are then investigated. The methods are illustrated with a real dataset on HIV infection by transfusion and by simulations."
"10.1093/biomet/93.3.641","2006","Confidence intervals in group sequential trials with random group sizes and applications to survival analysis","1","A new ordering scheme for defining quantiles of the multivariate distribution of a stopping time and a stopped stochastic process is introduced. This ordering scheme is used in conjunction with resampling methods to construct confidence intervals for a population mean following a group sequential test with random group sizes, and for the regression parameter of a proportional hazards model following a time-sequential clinical trial with censored survival data. It is shown that this approach resolves the long-standing difficulties in inference due to two different time scales in time-sequential trials, and that the confidence intervals thus constructed have coverage probabilities close to the nominal values and provide marked improvements over those based on alternative ordering schemes and normal approximations."
"10.1093/biomet/93.3.627","2006","Efficient estimation of semiparametric transformation models for counting processes","8","A class of semiparametric transformation models is proposed to characterise the effects of possibly time-varying covariates on the intensity functions of counting processes. The class includes the proportional intensity model and linear transformation models as special cases. Nonparametric maximum likelihood estimators are developed for the regression parameters and cumulative intensity functions of these models based on censored data. The estimators are shown to be consistent and asymptotically normal. The limiting variances for the estimators of the regression parameters achieve the semi-parametric efficient bounds and can be consistently estimated. The limiting variances for the estimators of smooth functionals of the cumulative intensity function can also be consistently estimated. Simulation studies reveal that the proposed inference procedures perform well in practical settings. Two medical studies are provided."
"10.1093/biomet/93.3.613","2006","On optimal crossover designs when carryover effects are proportional to direct effects","2","There are a number of different models for crossover designs which take account of carryover effects. Since it seems plausible that a treatment with a large direct effect should generally have a larger carryover effect, Kempton et al. (2001) considered a model where the carryover effects are proportional to the direct effects. The advantage of this model lies in the fact that there are fewer parameters to be estimated. Its problem lies in the nonlinearity of the estimators. Kempton et al. (2001) considered the least squares estimator. They point out that this estimator is asymptotically equivalent to the estimator in a linear model which assumes the true parameters to be known. For this estimator they determine optimal designs numerically for some cases. The present paper generalises some of their results. Our results are derived with the help of a generalisation of the methods used in Kunert & Martin (2000)."
"10.1093/biomet/93.3.601","2006","On recovering a population covariance matrix in the presence of selection bias","0","This paper considers the problem of using observational data in the presence of selection bias to identify causal effects in the framework of linear structural equation models. We propose a criterion for testing whether or not observed statistical dependencies among variables are generated by conditioning on a common response variable. When the answer is affirmative, we further provide formulations for recovering the covariance matrix of the whole population from that of the selected population. The results of this paper provide guidance for reliable causal inference, based on the recovered covariance matrix obtained from the statistical information with selection bias."
"10.1093/biomet/93.3.587","2006","A computationally tractable multivariate random effects model for clustered binary data","0","We consider a multivariate random effects model for clustered binary data that is useful when interest focuses on the association structure among clustered observations. Based on a vector of gamma random effects and a complementary log-log link function, the model yields a likelihood that has closed form, making a frequentist approach to model-fitting straightforward. This closed form yields several advantages over existing methods, including easy inspection of model identifiability and straightforward adjustment for nonrandom ascertainment of subjects, such as that which occurs in family studies of disease aggregation. We use the proposed model to analyse two different binary datasets concerning disease outcome data from a familial aggregation study of breast and ovarian cancer in women and loss of heterozygosity outcomes from a brain tumour study."
"10.1093/biomet/93.3.573","2006","Differential effects and generic biases in observational studies","0","There are two treatments, each of which may be applied or withheld, yielding a 2 x 2 factorial arrangement with three degrees of freedom between groups. The differential effect of the two treatments is the effect of applying one treatment in lieu of the other. In randomised experiments, the differential effect is of no more or less interest than other treatment contrasts. Differential effects play a special role in certain observational studies in which treatments are not assigned to subjects at random, where differing outcomes may reflect biased assignments rather than effects caused by the treatments. Differential effects are immune to certain types of unobserved bias, called generic biases, which are associated with both treatments in a similar way. This is explored using several examples and models. Differential effects are not immune to differential biases, whose possible consequences are examined by sensitivity analysis."
"10.1093/biomet/93.3.555","2006","Structured multicategory support vector machines with analysis of variance decomposition","1","The support vector machine has been a popular choice of classification method for many applications in machine learning. While it often outperforms other methods in terms of classification accuracy, the implicit nature of its solution renders the support vector machine less attractive in providing insights into the relationship between covariates and classes. Use of structured kernels can remedy the drawback. Borrowing the flexible model-building idea of functional analysis of variance decomposition, we consider multicategory support vector machines with analysis of variance kernels in this paper. An additional penalty is imposed on the sum of weights of functional subspaces, which encourages a sparse representation of the solution. Incorporation of the additional penalty enhances the interpretability of a resulting classifier with often improved accuracy. The proposed method is demonstrated through simulation studies and an application to real data."
"10.1093/biomet/93.3.537","2006","Efficient {B}ayesian inference for {G}aussian copula regression models","5","A Gaussian copula regression model gives a tractable way of handling a multivariate regression when some of the marginal distributions are non-Gaussian. Our paper presents a general Bayesian approach for estimating a Gaussian copula model that can handle any combination of discrete and continuous marginals, and generalises Gaussian graphical models to the Gaussian copula framework. Posterior inference is carried out using a novel and efficient simulation method. The methods in the paper are applied to simulated and real data."
"10.1093/biomet/93.3.525","2006","Forensic identification of relatives of individuals included in a database of {DNA} profiles","0","In this paper we evaluate the characteristics observed both on a crime sample and on individuals included in a database to assess the probability of alternative hypotheses concerning identification. The problem is first addressed by considering a generic characteristic and we demonstrate the problem via a computationally efficient Bayesian network. Then we turn our attention to a heritable DNA trait to show how to evaluate the hypotheses that some individuals, genetically related to the members of the database, are the donors of the crime sample. Then the network is extended to cope with many loci. Applications of the method are provided as well as details of computational requirements."
"10.1093/biomet/93.3.509","2006","False discovery control with {$p$}-value weighting","6","We present a method for multiple hypothesis testing that maintains control of the false discovery rate while incorporating prior information about the hypotheses. The prior information takes the form of p-value weights. If the assignment of weights is positively associated with the null hypotheses being false, the procedure improves power, except in cases where power is already near one. Even if the assignment of weights is poor, power is only reduced slightly, as long as the weights are not too large. We also provide a similar method for controlling false discovery exceedance."
"10.1093/biomet/93.3.491","2006","Adaptive linear step-up procedures that control the false discovery rate","17","The linear step-up multiple testing procedure controls the false discovery rate at the desired level q for independent and positively dependent test statistics. When all null hypotheses are true, and the test statistics are independent and continuous, the bound is sharp. When some of the null hypotheses are not true, the procedure is conservative by a factor which is the proportion m(0)/m of the true null hypotheses among the hypotheses. We provide a new two-stage procedure in which the linear step-up procedure is used in stage one to estimate m(0), providing a new level q ' which is used in the linear step-up procedure in the second stage. We prove that a general form of the two-stage procedure controls the false discovery rate at the desired level q. This framework enables us to study analytically the properties of other procedures that exist in the literature. A simulation study is presented that shows that two-stage adaptive procedures improve in power over the original procedure, mainly because they provide tighter control of the false discovery rate. We further study the performance of the current suggestions, some variations of the procedures, and previous suggestions, in the case where the test statistics are positively dependent, a case for which the original procedure controls the false discovery rate. In the setting studied here the newly proposed two-stage procedure is the only one that controls the false discovery rate. The procedures are illustrated with two examples of biological importance."
"10.1093/biomet/92.4.975","2005","An alternative derivation of the distributions of the maximum likelihood estimators of the parameters in an inverse {G}aussian distribution","0","We provide a simpler derivation of the sampling properties of the maximum likelihood estimators of the parameters in an inverse Gaussian distribution."
"10.1093/biomet/92.4.971","2005","A note on reducing the bias of the approximate {B}ayesian bootstrap imputation variance estimator","0","Rubin & Schenker (1986) proposed the approximate Bayesian bootstrap, a two-stage resampling procedure, as a method of creating multiple imputations when missing data are ignorable. Kim (2002) showed that the multiple imputation variance estimator is biased for moderate sample sizes when this method is used. To reduce the bias, Kim (2002) proposed modifying the number of samples drawn at the first stage of the Bayesian bootstrap procedure. In this note, we suggest an alternative method for reducing the bias via a simple correction factor applied to the standard multiple imputation variance estimate. The proposed correction is more easily implemented and more efficient than the procedure proposed by Kim (2002)."
"10.1093/biomet/92.4.965","2005","Concordance probability and discriminatory power in proportional hazards regression","0","The concordance probability is used to evaluate the discriminatory power and the predictive accuracy of nonlinear statistical models. We derive an analytical expression for the concordance probability in the Cox proportional hazards model. The proposed estimator is a function of the regression parameters and the covariate distribution only and does not use the observed event and censoring times. For this reason it is asymptotically unbiased, unlike Harrell's c-index based on informative pairs. The asymptotic distribution of the concordance probability estimate is derived using U-statistic theory and the methodology is applied to a predictive model in lung cancer."
"10.1093/biomet/92.4.957","2005","The optimal confidence region for a random parameter","1","Suppose that, under a two-level hierarchical model, the distribution of the vector of random parameters is known or can be estimated well. The data are generated via a fixed, but unobservable, realisation of the vector. We derive the smallest confidence region for a specific component of this random vector under a joint Bayesian/frequentist paradigm. On average this optimal region can be much smaller than the corresponding Bayesian highest posterior density region. The new estimation procedure is especially appealing when one deals with data generated under a highly parallel structure. The new proposal is illustrated with a dataset from a multi-centre clinical study and also with one from a typical microarray experiment. The performance of our procedure is examined via simulation studies."
"10.1093/biomet/92.4.951","2005","Testing for complete independence in high dimensions","2","A simple statistic is proposed for testing the complete independence of random variables having a multivariate normal distribution. The asymptotic null distribution of this statistic, as both the sample size and the number of variables go to infinity, is shown to be normal. Consequently, this test can be used when the number of variables is not small relative to the sample size and, in particular, even when the number of variables exceeds the sample size. The finite sample size performance of the normal approximation is evaluated in a simulation study and the results are compared to those of the likelihood ratio test."
"10.1093/biomet/92.4.937","2005","Can the strengths of {AIC} and {BIC} be shared? {A} conflict between model indentification and regression estimation","10","A traditional approach to statistical inference is to identify the true or best model first with little or no consideration of the specific goal of inference in the model identification stage. Can the pursuit of the true model also lead to optimal regression estimation? In model selection, it is well known that BIC is consistent in selecting the true model, and AIC is minimax-rate optimal for estimating the regression function. A recent promising direction is adaptive model selection, in which, in contrast to AIC and BIC, the penalty term is data-dependent. Some theoretical and empirical results have been obtained in support of adaptive model selection, but it is still not clear if it can really share the strengths of AIC and BIC. Model combining or averaging has attracted increasing attention as a means to overcome the model selection uncertainty. Can Bayesian model averaging be optimal for estimating the regression function in a minimax sense? We show that the answers to these questions are basically in the negative: for any model selection criterion to be consistent, it must behave suboptimally for estimating the regression function in terms of minimax rate of covergence; and Bayesian model averaging cannot be minimax-rate optimal for regression estimation."
"10.1093/biomet/92.4.921","2005","Towards reconciling two asymptotic frameworks in spatial statistics","4","Two asymptotic frameworks, increasing domain asymptotics and infill asymptotics, have been advanced for obtaining limiting distributions of maximum likelihood estimators of covariance parameters in Gaussian spatial models with or without a nugget effect. These limiting distributions are known to be different in some cases. It is therefore of interest to know, for a given finite sample, which framework is more appropriate. We consider the possibility of making this choice on the basis of how well the limiting distributions obtained under each framework approximate their finite-sample counterparts. We investigate the quality of these approximations both theoretically and empirically, showing that, for certain consistently estimable parameters of exponential covariograms, approximations corresponding to the two frameworks perform about equally well. For those parameters that cannot be estimated consistently, however, the infill asymptotic approximation is preferable."
"10.1093/biomet/92.4.909","2005","First-order intrinsic autoregressions and the de {W}ijs process","2","We discuss intrinsic autoregressions for a first-order neighbourhood on a two-dimensional rectangular lattice and give an exact formula for the variogram that extends known results to the asymmetric case. We obtain a corresponding asymptotic expansion that is more accurate and more general than previous ones and use this to derive the de Wijs variogram under appropriate averaging, a result that can be interpreted as a two-dimensional spatial analogue of Brownian motion obtained as the limit of a random walk in one dimension. This provides a bridge between geostatistics, where the de Wijs process was once the most popular formulation, and Markov random fields, and also explains why statistical analysis using intrinsic autoregressions is usually robust to changes of scale. We briefly describe corresponding calculations in the frequency domain, including limiting results for higher-order autoregressions. The paper closes with some practical considerations, including applications to irregularly-spaced data."
"10.1093/biomet/92.4.893","2005","Lower bounds for the number of false null hypotheses for multiple testing of associations under general dependence structures","2","We propose probabilistic lower bounds for the number of false null hypotheses when testing multiple hypotheses of association simultaneously. The bounds are valid under general and unknown dependence structures between the test statistics. The power of the proposed estimator to detect the full proportion of false null hypotheses is discussed and compared to other estimators. The proposed estimator is shown to deliver a tight probabilistic lower bound for the number of false null hypotheses in a multiple testing situation even under strong dependence between test statistics."
"10.1093/biomet/92.4.875","2005","Semiparametric estimators for the regression coefficients in the linear transformation competing risks model with missing cause of failure","1","We consider the problem of estimating the regression coefficients in a competing risks model, where the relationship between the cause-specific hazard for the cause of interest and covariates is described using linear transformation models and when cause of failure is missing at random for a subset of individuals. Using the theory of Robins et al. (1994) for missing data problems and the approach of Chen et al. (2002) for estimating regression coefficients for linear transformation models, we derive augmented inverse probability weighted complete-case estimators for the regression coefficients that are doubly robust. Simulations demonstrate the relevance of the theory in finite samples."
"10.1093/biomet/92.4.863","2005","Coherence principles in dose-finding studies","2","This paper studies the coherence conditions of dose-finding methods in the context of phase I clinical trials, where the objective is to estimate a targeted quantile of the unknown dose-toxicity curve. Most phase I methods are outcome-adaptive, and thus escalate or de-escalate doses for future patients based on the previous observations. An escalation for a new patient is said to be coherent only when the previous patient does not show sign of toxicity. Likewise, a de-escalation is coherent only when a toxic outcome has just been seen. The coherence conditions, motivated by ethical concerns in trial conduct, are satisfied by many statistical designs in the literature, but not by some commonly used modifications of the methods. This paper shows examples in which coherence is violated, and discusses how the coherence principles may be applied to calibrate a two-stage design and to deal with situations with delayed toxicity."
"10.1093/biomet/92.4.847","2005","Bivariate current status data with univariate monitoring times","0","For bivariate current status data with univariate monitoring times, the identifiable part of the joint distribution is three univariate cumulative distribution functions, namely the two marginal distributions and the bivariate cumulative distribution function evaluated on the diagonal. We show that smooth functionals of these univariate cumulative distribution functions can be efficiently estimated with easily computed nonparametric maximum likelihood estimators based on reduced data consisting of univariate current status observations. This theory is then applied to functionals that address independence of the two survival times and the goodness-of-fit of a copula model used by Wang & Ding (2000). Some brief simulations are provided along with an illustration based on data on HIV transmission. Extension of the ideas to incorporate covariates, possibly time-dependent, are discussed."
"10.1093/biomet/92.4.831","2005","Model-assisted estimation for complex surveys using penalised splines","1","Estimation of finite population totals in the presence of auxiliary information is considered. A class of estimators based on penalised spline regression is proposed. These estimators are weighted linear combinations of sample observations, with weights calibrated to known control totals. They allow straightforward extensions to multiple auxiliary variables and to complex designs. Under standard design conditions, the estimators are design consistent and asymptotically normal, and they admit consistent variance estimation using familiar design-based methods. Data-driven penalty selection is considered in the context of unequal probability sampling designs. Simulation experiments show that the estimators are more efficient than parametric regression estimators when the parametric model is incorrectly specified, while being approximately as efficient when the parametric specification is correct. An example using Forest Health Monitoring survey data from the U.S. Forest Service demonstrates the applicability of the methodology in the context of a two-phase survey with multiple auxiliary variables."
"10.1093/biomet/92.4.821","2005","Estimating residual variance in nonparametric regression using least squares","0","We propose a new estimator for the error variance in a nonparametric regression model. We estimate the error variance as the intercept in a simple linear regression model with squared differences of paired observations as the dependent variable and squared distances between the paired covariates as the regressor. For the special case of a one-dimensional domain with equally spaced design points, we show that our method reaches an asymptotic optimal rate which is not achieved by some existing methods. We conduct extensive simulations to evaluate finite-sample performance of our method and compare it with existing methods. Our method can be extended to nonparametric regression models with multivariate functions defined on arbitrary subsets of normed spaces, possibly observed on unequally spaced or clustered designed points."
"10.1093/biomet/92.4.801","2005","Nonparametric maximum likelihood estimation of the structural mean of a sample of curves","4","A random sample of curves can be usually thought of as noisy realisations of a compound stochastic process X(t) = Z{W(t)}, where Z(t) produces random amplitude variation and W(t) produces random dynamic or phase variation. In most applications it is more important to estimate the so-called structural mean mu(t) = E{Z(t)} than the crosssectional mean E{X(t)}, but this estimation problem is difficult because the process Z(t) is not directly observable. In this paper we propose a nonparametric maximum likelihood estimator of mu(t). This estimator is shown to be root n-consistent and asymptotically normal under the assumed model and robust to model misspecification. Simulations and a realdata example show that the proposed estimator is competitive with landmark registration, often considered the benchmark, and has the advantage of avoiding time-consuming and often infeasible individual landmark identification."
"10.1093/biomet/92.4.787","2005","Symmetric diagnostics for the analysis of the residuals in regression models","0","Typical alternative hypotheses in the analysis of residuals of a standard regression model are considered, and for each one a Bayesian diagnostic based on a symmetric form of the Kullback-Leibler divergence is determined. The results include an explicit expression for the diagnostic when the alternative hypothesis is that the errors are generated by an unknown distribution function with a Dirichlet process prior. This expression is immediately interpretable, exactly computable and endowed with important asymptotic connections. A linear approximation of the diagnostic reveals close links with the class of Lagrange multiplier test statistics. When the alternative hypothesis is that the errors are generated by an autoregressive process the linear approximation is proportional to the Box-Pierce statistic or to the Ljung-Box statistic, according to the characteristics of the prior, if the observations have zero mean; it depends on the Durbin-Watson statistic if the errors are first-order autoregressive, and it is related to the Cliff-Ord statistic if they are generated by a first-order spatial autoregression. The sensitivity to the prior of the diagnostic and of its linear approximation is also discussed."
"10.1093/biomet/92.4.779","2005","Covariance decomposition in undirected {G}aussian graphical models","1","The covariance between two variables in a multivariate Gaussian distribution is decomposed into a sum of path weights for all paths connecting the two variables in an undirected independence graph. These weights are useful in determining which variables are important in mediating correlation between the two path endpoints. The decomposition arises in undirected Gaussian graphical models and does not require or involve any assumptions of causality. This covariance decomposition is derived using basic linear algebra. The decomposition is feasible for very large numbers of variables if the corresponding precision matrix is sparse, a circumstance that arises in examples such as gene expression studies in functional genomics. Additional computational efficiences are possible when the undirected graph is derived from an acyclic directed graph."
"10.1093/biomet/92.4.765","2005","Data tracking and the understanding of {B}ayesian consistency","0","We deal with strong consistency for Bayesian density estimation. An awkward consequence of inconsistency is described. It is pointed out that consistency at some density f(0) depends on the prior mass assigned to the 'pathological' set of those densities that are close to f(0), in a weak sense, and far apart from f(0), in a Hellinger sense. An analysis of these sets leads to the identification of the notion of 'data tracking'. Specific examples in which this phenomenon cannot occur are discussed. When it can happen, we show how and where things can go wrong, thus providing more intuition about the sources of inconsistency."
"10.1093/biomet/92.4.747","2005","Adaptive sampling for {B}ayesian variable selection","4","Our paper proposes adaptive Monte Carlo sampling schemes for Bayesian variable selection in linear regression that improve on standard Markov chain methods. We do so by considering Metropolis-Hastings proposals that make use of accumulated information about the posterior distribution obtained during sampling. Adaptation needs to be done carefully to ensure that sampling is from the correct ergodic distribution. We give conditions for the validity of an adaptive sampling scheme in this problem, and for simulating from a distribution on a finite state space in general, and suggest a class of adaptive proposal densities which uses best linear prediction to approximate the Gibbs sampler. Our sampling scheme is computationally much faster per iteration than the Gibbs sampler, and when this is taken into account the efficiency gains when using our sampling scheme compared to alternative approaches are substantial in terms of precision of estimation of posterior quantities of interest for a given amount of computation time. We compare our method with other sampling schemes for examples involving both real and simulated data. The methodology developed in the paper can be extended to variable selection in more general problems."
"10.1093/biomet/92.3.737","2005","The {S}tein-{J}ames estimator for short- and long-memory {G}aussian processes","0","We investigate the mean squared error of the Stein-James estimator for the mean when the observations are generated from a Gaussian vector stationary process with dimension greater than two. First, assuming that the process is short-memory, we evaluate the mean squared error, and compare it with that for the sample mean. Then a sufficient condition for the Stein-James estimator to improve upon the sample mean is given in terms of the spectral density matrix around the origin. We repeat the analysis for Gaussian vector long-memory processes. Numerical examples clearly illuminate the Stein-James phenomenon for dependent samples. The results have the potential to improve the usual trend estimator in time series regression models."
"10.1093/biomet/92.3.732","2005","Estimating a nonlinear function of a normal mean","0","We derive a Monte-Carlo-amenable, minimum variance unbiased estimator of a nonlinear function of a normal mean and the variance of the estimator. Applications to problems arising in the analysis of data measured with error are described."
"10.1093/biomet/92.3.724","2005","Minimum distance estimation for the logistic regression model","3","It is well known that the maximum likelihood fit of the logistic regression parameters can be greatly affected by atypical observations. Several robust alternatives have been proposed. However, if we consider the model from the case-control viewpoint, it is clear that current techniques can exhibit poor behaviour in many common situations. A new robust class of estimation procedures is introduced. The estimators are constructed via a minimum distance approach after identifying the model with a semiparametric biased sampling model. The approach is developed under the case-control sampling scheme, yet is shown to be applicable under prospective sampling as well. A weighted Cramer-von Mises distance is used as an illustrative example of the methodology."
"10.1093/biomet/92.3.717","2005","Comparison of hierarchical likelihood versus orthodox best linear unbiased predictor approaches for frailty models","1","Hierarchical likelihood provides a statistically efficient procedure for frailty models. Recently, a method using the computationally attractive orthodox best linear unbiased predictor has been proposed; this uses Pearson-type estimation. We compare both approaches and discuss their relative merits. With semiparametric frailty models difficulties can arise for the orthodox method, if the number of nuisance parameters increases with the sample size. This difficulty is avoided by the use of the hierarchical-likelihood method."
"10.1093/biomet/92.3.703","2005","Estimation of order-restricted means from correlated data","0","In many applications, researchers are interested in estimating the mean of a multivariate normal random vector whose components are subject to order restrictions. Various authors have demonstrated that the likelihood-based methodology may perform poorly under certain conditions for such problems. The problem is much harder when the underlying covariance matrix is nondiagonal. In this paper a simple iterative algorithm is introduced that can be used for estimating the mean of a multivariate normal population when the components are subject to any order restriction. The proposed methodology is illustrated through an application to human reproductive hormone data."
"10.1093/biomet/92.3.691","2005","Diagnostic checking for time series models with conditional heteroscedasticity estimated by the least absolute deviation approach","2","The recent paper by Peng & Yao (2003) gave an interesting extension of least absolute deviation estimation to generalised autoregressive conditional heteroscedasticity, GARCH, time series models. The asymptotic distributions of absolute residual autocorrelations and squared residual autocorrelations from the GARCH model estimated by the least absolute deviation method are derived in this paper. These results lead to two useful diagnostic tools which can be used to check whether or not a GARCH model fitted by using the least absolute deviation method is adequate. Some simulation experiments give further support to the asymptotic theory and a real data example is also reported."
"10.1093/biomet/92.3.679","2005","Orthogonal bases approach for comparing nonnormal continuous distributions","0","We present an orthonormal bases approach for detecting general differences among continuous distributions. An unknown density function is represented by a finite vector of its estimated Fourier coefficients with respect to a suitable orthonormal basis. For a wide class of orthonormal bases, we establish asymptotic normality of the vector of estimated Fourier coefficients and propose an unbiased and consistent estimator of its asymptotic covariance matrix. Fourier coeffients are modelled as functions of fixed and possibly random effects. This approach allows simultaneous detection of distributional differences attributable to various factors in clustered and correlated data with suffciently large numbers of observations per each cluster with the same fixed and random effects realisations. This work was motivated by multi-level clustered non-Gaussian datasets from genetic studies."
"10.1093/biomet/92.3.667","2005","Nonparametric inference in multivariate mixtures","2","We consider mixture models in which the components of data vectors from any given subpopulation are statistically independent, or independent in blocks. We argue that if, under this condition of independence, we take a nonparametric view of the problem and allow the number of subpopulations to be quite general, the distributions and mixing proportions can often be estimated root-n consistently. Indeed, we show that, if the data are k-variate and there are p subpopulations, then for each p >= 2 there is a minimal value of k, k(p) say, such that the mixture problem is always nonparametrically identifiable, and all distributions and mixture proportions are nonparametrically identifiable when k >= k(p). We treat the case p = 2 in detail, and there we show how to construct explicit distribution, density and mixture-proportion estimators, converging at conventional rates. Other values of p can be addressed using a similar approach, although the methodology becomes rapidly more complex as p increases."
"10.1093/biomet/92.3.647","2005","The accelerated gap times model","3","This paper develops a new semiparametric model for the effect of covariates on the conditional intensity of a recurrent event counting process. The model is a transparent extension of the accelerated failure time model for univariate survival data. Estimation of the regression parameter is motivated by semiparametric efficiency considerations, extending the class of weighted log-rank estimating functions originally proposed in Prentice (1978) and subsequently studied in detail by Tsiatis (1990) and Ritov (1990). A novel rank-based one-step estimator for the regression parameter is proposed. An Aalen-type estimator for the baseline intensity function is obtained. Asymptotics are handled with empirical process methods, and finite sample properties are studied via simulation. Finally, the new model is applied to the bladder tumour data of Byar (1980)."
"10.1093/biomet/92.3.633","2005","Bayesian adaptive designs for clinical trials","1","A Bayesian adaptive design is proposed for a comparative two-armed clinical trial using decision-theoretic approaches. A loss function is specified, based on the cost for each patient and the costs of making incorrect decisions at the end of a trial. At each interim analysis, the decision to terminate or to continue the trial is based on the expected loss function while concurrently incorporating efficacy, futility and cost. The maximum number of interim analyses is determined adaptively by the observed data. We derive explicit connections between the loss function and the frequentist error rates, so that the desired frequentist properties can be maintained for regulatory settings. The operating characteristics of the design can be evaluated on frequentist grounds. Extensive simulations are carried out to compare the proposed design with existing ones. The design is general enough to accommodate both continuous and discrete types of data. We illustrate the methods with an animal study evaluating a medical treatment for cardiac arrest."
"10.1093/biomet/92.3.619","2005","Semiparametric {B}ox-{C}ox power transformation models for censored survival observations","6","The accelerated failure time model specifies that the logarithm of the failure time is linearly related to the covariate vector without assuming a parametric error distribution. In this paper, we consider the semiparametric Box-Cox transformation model, which includes the above regression model as a special case, to analyse possibly censored failure time observations. Inference procedures for the transformation and regression parameters are proposed via a resampling technique. Prediction of the survival function of future subjects with a specific covariate vector is also provided via pointwise and simultaneous interval estimates. All the proposals are illustrated with datasets from two clinical studies."
"10.1093/biomet/92.3.605","2005","Semiparametric inference in observational duration-response studies, with duration possibly right-censored","1","Once treatment is found to be effective in clinical studies, attention often focuses on optimum or efficacious treatment delivery. In treatment duration-response studies, the optimum treatment delivery refers to the treatment length that optimises the mean response. In many studies, the treatment length is often left to the discretion of an attending investigator or physician but may be abruptly terminated because of treatment-terminating events. Thus, a recommended treatment length often delineates a 'treatment duration policy' which prescribes that treatment be given for a specified length of time or until a treatment-terminating event occurs, whichever comes first. Estimating a functional relationship between the response and a treatment duration policy, continuously in time, is the focus of this paper."
"10.1093/biomet/92.3.587","2005","Joint modelling of accelerated failure time and longitudinal data","0","The accelerated failure time model is an attractive alternative to the Cox model when the proportionality assumption fails to capture the relationship between the survival time and longitudinal covariates. Several complications arise when the covariates are measured intermittently at different time points for different subjects, possibly with measurement errors, or measurements are not available after the failure time. Joint modelling of the failure time and longitudinal data offers a solution to such complications. We explore the joint modelling approach under the accelerated failure time assumption when covariates are assumed to follow a linear mixed effects model with measurement errors. The procedure is based on maximising the joint likelihood function with random effects treated as missing data. A Monte Carlo EM algorithm is used to estimate all the unknown parameters, including the unknown baseline hazard function. The performance of the proposed procedure is checked in simulation studies. A case study of reproductive egg-laying data for female Mediterranean fruit flies and their relationship to longevity demonstrate the effectiveness of the new procedure."
"10.1093/biomet/92.3.573","2005","A semiparametric regression cure model with current status data","1","This paper considers the analysis of current status data with a cured proportion in the population using a mixture model that combines a logistic regression formulation for the probability of cure with a semiparametric regression model for the time to occurrence of the event. The semiparametric regression model belongs to the flexible class of partly linear models that allows one to explore the possibly nonlinear effect of a certain covariate on the response variable. A sieve maximum likelihood estimation method is proposed and the asymptotic properties of the proposed estimators are discussed. Under some mild conditions, the estimators are shown to be strongly consistent. The convergence rate of the estimator for the unknown smooth function is obtained and the estimator for the unknown parameter is shown to be asymptotically efficient and normally distributed. Simulation studies were carried out to investigate the performance of the proposed method and the model is fitted to a dataset from a study of calcification of the hydrogel intraocular lenses, a complication of cataract treatment."
"10.1093/biomet/92.3.559","2005","Locally-efficient robust estimation of haplotype-disease association in family-based studies","0","Modelling human genetic variation is critical to understanding the genetic basis of complex disease. The Human Genome Project has discovered millions of binary DNA sequence variants, called single nucleotide polymorphisms, and millions more may exist. As coding for proteins takes place along chromosomes, organisation of polymorphisms along each chromosome, the haplotype phase structure, may prove to be most important in discovering genetic variants associated with disease. As haplotype phase is often uncertain, procedures that model the distribution of parental haplotypes can, if this distribution is misspecified, lead to substantial bias in parameter estimates even when complete genotype information is available. Using a geometric approach to estimation in the presence of nuisance parameters, we address this problem and develop locally-efficient estimators of the effect of haplotypes on disease that are robust to incorrect estimates of haplotype frequencies. The methods are demonstrated with a simulation study of a case-parent design."
"10.1093/biomet/92.3.543","2005","Smooth quantile ratio estimation","2","We propose a novel approach to estimating the mean difference between two highly skewed distributions. The method, which we call smooth quantile ratio estimation, smooths, over percentiles, the ratio of the quantiles of the two distributions. The method defines a large class of estimators, including the sample mean difference, the maximum likelihood estimator under log-normal samples and the L-estimator. We derive asymptotic properties such as consistency and asymptotic normality, and also provide a closed-form expression for the asymptotic variance. In a simulation study, we show that smooth quantile ratio estimation has lower mean squared error than several competitors, including the sample mean difference and the log-normal parametric estimator in several realistic situations. We apply the method to the 1987 National Medicare Expenditure Survey to estimate the difference in medical expenditures between persons suffering from the smoking attributable diseases, lung cancer and chronic obstructive pulmonary disease, and persons without these diseases."
"10.1093/biomet/92.3.529","2005","Frequentist prediction intervals and predictive distributions","1","We consider parametric frameworks for the prediction of future values of a random variable Y, based on previously observed data X. Simple pivotal methods for obtaining calibrated prediction intervals are presented and illustrated. Frequentist predictive distributions are defined as confidence distributions, and their utility is demonstrated. A simple pivotal-based approach that produces prediction intervals and predictive distributions with well-calibrated frequentist probability interpretations is introduced, and efficient simulation methods for producing predictive distributions are considered. Properties related to an average Kullback-Leibler measure of goodness for predictive or estimated distributions are given. The predictive distributions here are shown to be optimal in certain settings with invariance structure, and to dominate plug-in distributions under certain conditions."
"10.1093/biomet/92.3.519","2005","A note on composite likelihood inference and model selection","6","A composite likelihood consists of a combination of valid likelihood objects, usually related to small subsets of data. The merit of composite likelihood is to reduce the computational complexity so that it is possible to deal with large datasets and very complex models, even when the use of standard likelihood or Bayesian methods is not feasible. In this paper, we aim to suggest an integrated, general approach to inference and model selection using composite likelihood methods. In particular, we introduce an information criterion for model selection based on composite likelihood. We also describe applications to the modelling of time series of counts through dynamic generalised linear models and to the analysis of the well-known Old Faithful geyser dataset."
"10.1093/biomet/92.3.507","2005","Likelihood ratio tests in curved exponential families with nuisance parameters present only under the alternative","0","For submodels of an exponential family, we consider likelihood ratio tests for hypotheses that render some parameters nonidentifiable. First, we establish the asymptotic equivalence between the likelihood ratio test and the score test. Secondly, the score-test representation is used to derive the asymptotic distribution of the likelihood ratio test. These results are derived for general submodels of an exponential family without assuming compactness of the parameter space. We then exemplify the results on a class of multivariate normal models, where null hypotheses concerning the covariance structure lead to loss of identifiability of a parameter. Our motivating problem throughout the paper is to test a random intercepts model against an alternative covariance structure allowing for serial correlation."
"10.1093/biomet/92.2.499","2005","Expected lengths of confidence intervals based on empirical discrepancy statistics","3","We consider a very general class of empirical discrepancy statistics that includes the Cressie-Read discrepancy statistics and, in particular, the empirical likelihood ratio statistic. Higher-order asymptotics for expected lengths of associated confidence intervals are investigated. An explicit formula is worked out and its use for comparative purposes is discussed. It is seen that the empirical likelihood ratio statistic, which enjoys interesting second-order power properties, loses much of its edge under the present criterion."
"10.1093/biomet/92.2.492","2005","Empirical likelihood analysis of the rank estimator for the censored accelerated failure time model","1","We use the empirical likelihood method to derive a test and thus a confidence interval based on the rank estimators of the regression coefficient in the accelerated failure time model. Standard chi-squared distributions are used to calculate the p-value and to construct the confidence interval. Simulations and examples show that the chi-squared approximation to the distribution of the log empirical likelihood ratio performs well, and has some advantages over the existing methods."
"10.1093/biomet/92.2.485","2005","Generalised minimum aberration construction results for symmetrical orthogonal arrays","0","Generalised minimum aberration is a recently-established design criterion for the whole class of orthogonal arrays and fractional factorial designs. The criterion is, as its name suggests, a generalisation of minimum aberration for regular designs and of minimum G(2)-aberration for two-level designs. The aim of the criterion is to find designs which minimise in a certain sense the aliasing between main effects and interactions. In this paper, theoretical results are developed for finding symmetrical orthogonal arrays with generalised minimum aberration for more than two factor levels."
"10.1093/biomet/92.2.477","2005","Weighted least absolute deviations estimation for an {$\rm AR(1)$} process with {$\rm ARCH(1)$} errors","2","The weighted least absolute deviations estimator is studied for an AR(1) process with ARCH(1) errors c, Unlike for the quasi maximum likelihood estimator, the estimator's limiting distribution is shown to be normal even when E(epsilon(t)(4)) = infinity. Furthermore, the estimator can be applied to examine the symmetry of the density of epsilon(t) and to estimate the quantity E(log vertical bar alpha + lambda(1/2)epsilon(t)vertical bar), which are of crucial importance for conducting asymptotic inference for quasi maximum likelihood estimators and weighted least absolute deviations estimators."
"10.1093/biomet/92.2.465","2005","Saddlepoint approximations for the {B}ingham and {F}isher-{B}ingham normalising constants","2","The Fisher-Bingham distribution is obtained when a multivariate normal random vector is conditioned to have unit length. Its normalising constant can be expressed as an elementary function multiplied by the density, evaluated at 1, of a linear combination of independent noncentral chi(1)(2) random variables. Hence we may approximate the normalising constant by applying a saddlepoint approximation to this density. Three such approximations, implementation of each of which is straightforward, are investigated: the first-order saddlepoint density approximation, the second-order saddlepoint density approximation and a variant of the second-order approximation which has proved slightly more accurate than the other two. The numerical and theoretical results we present show that this approach provides highly accurate approximations in a broad spectrum of cases."
"10.1093/biomet/92.2.451","2005","Monte {C}arlo conditioning on a sufficient statistic","1","In this paper we derive general formulae suitable for Monte Carlo computation of conditional expectations of functions of a random vector given a sufficient statistic. The problem of direct sampling from the conditional distribution is considered in particular. It is shown that this can be done by a simple parameter adjustment of the original statistical model, provided the model has a certain pivotal structure. A connection with a classical problem regarding fiducial and posterior distributions is pointed out."
"10.1093/biomet/92.2.435","2005","Mean estimating equation approach to analysing cluster-correlated data with nonignorable cluster sizes","2","Most methods for analysing cluster-correlated biological data implicitly assume the ignorability of cluster sizes. When this assumption fails, the resulting inferences may be asymptotically invalid. Hoffman et al. (2001) proposed a simple but computationally intensive method, based on a large number of within-cluster resamples and associated separate estimating equations, that leads to asymptotically valid inferences whether the cluster sizes are ignorable or not. We study a simple method, based on a single inverse cluster size-weighted estimating equation, that avoids resampling and yet leads to asymptotically valid inferences. Simulation results are presented to assess the performance of the proposed method. We also propose Wald tests for ignorability of cluster sizes."
"10.1093/biomet/92.2.419","2005","Hierarchical models for assessing variability among functions","1","In many applications of functional data analysis, summarising functional variation based on fits, without taking account of the estimation process, runs the risk of attributing the estimation variation to the functional variation, thereby overstating the latter. For example, the first eigenvalue of a sample covariance matrix computed from estimated functions may be biased upwards. We display a set of estimated neuronal Poisson-process intensity functions where this bias is substantial, and we discuss two methods for accounting for estimation variation. One method uses a random-coefficient model, which requires all functions to be fitted with the same basis functions. An alternative method removes the same-basis restriction by means of a hierarchical Gaussian process model. In a small simulation study the hierarchical Gaussian process model outperformed the random-coefficient model and greatly reduced the bias in the estimated first eigenvalue that would result from ignoring estimation variability. For the neuronal data the hierarchical Gaussian process estimate of the first eigenvalue was much smaller than the naive estimate that ignored variability due to function estimation. The neuronal setting also illustrates the benefit of incorporating alignment parameters into the hierarchical scheme."
"10.1093/biomet/92.2.399","2005","Semiparametric maximum likelihood estimation exploiting gene-environment independence in case-control studies","3","We consider the problem of maximum-likelihood estimation in case-control studies of gene-environment associations with disease when genetic and environmental exposures can be assumed to be independent in the underlying population. Traditional logistic regression analysis may not be efficient in this setting. We study the semiparametric maximum likelihood estimates of logistic regression parameters that exploit the gene-environment independence assumption and leave the distribution of the environmental exposures to be nonparametric. We use a profile-likelihood technique to derive a simple algorithm for obtaining the estimator and we study the asymptotic theory. The results are extended to situations where genetic and environmental factors are independent conditional on some other factors. Simulation studies investigate small-sample properties. The method is illustrated using data from a case-control study designed to investigate the interplay of BRCA1/2 mutations and oral contraceptive use in the aetiology of ovarian cancer."
"10.1093/biomet/92.2.385","2005","Some nonregular designs from the {N}ordstrom-{R}obinson code and their statistical properties","2","The Nordstrom-Robinson code is a well-known nonlinear code in coding theory. This paper explores the statistical properties of this nonlinear code. Many nonregular designs with 32, 64, 128 and 256 runs and 7-16 factors are derived from it. It is shown that these nonregular designs are better than regular designs of the same size in terms of resolution, aberration and projectivity. Furthermore, many of these nonregular designs are shown to have generalised minimum aberration among all possible designs. Seven orthogonal arrays are shown to have unique word-length pattern and four of them are shown to be unique up to isomorphism."
"10.1093/biomet/92.2.371","2005","Direction estimation in single-index regressions","8","We propose a general dimension-reduction method that combines the ideas of likelihood, correlation, inverse regression and information theory. We do not require that the dependence be confined to particular conditional moments, nor do we place restrictions on the predictors or on the regression that are necessary for methods like ordinary least squares and sliced-inverse regression. Although we focus on single-index regressions, the underlying idea is applicable more generally. Illustrative examples are presented."
"10.1093/biomet/92.2.351","2005","Conditional {A}kaike information for mixed-effects models","6","This paper focuses on the Akaike information criterion, AIC, for linear mixed-effects models in the analysis of clustered data. We make the distinction between questions regarding the population and questions regarding the particular clusters in the data. We show that the AIC in current use is not appropriate for the focus on clusters, and we propose instead the conditional Akaike information and its corresponding criterion, the conditional AIC, cAIC. The penalty term in cAIC is related to the effective degrees of freedom p for a linear mixed model proposed by Hodges & Sargent (2001); p reflects an intermediate level of complexity between a fixed-effects model with no cluster effect and a corresponding model with fixed cluster effects. The cAIC is defined for both maximum likelihood and residual maximum likelihood estimation. A pharmacokinetics data application is used to illuminate the distinction between the two inference settings, and to illustrate the use of the conditional AIC in model selection."
"10.1093/biomet/92.2.337","2005","On the identification of path analysis models with one hidden variable","2","We study criteria for identifiability of path analysis models with one hidden variable. We first derive sufficient criteria for identification of models in which marginalisation is carried out over the hidden variable. The sufficient criteria are based on the structure of the directed acyclic graph associated with the path analysis model and can be derived from the graph. We treat further the identification of models when the hidden variable is conditioned on and establish connections with the extended skew-normal distribution. Finally it is shown that the derived conditions extend the existing graphical criteria for identification."
"10.1093/biomet/92.2.317","2005","A {M}onte {C}arlo method for computing the marginal likelihood in nondecomposable {G}aussian graphical models","3","A centred Gaussian model that is Markov with respect to an undirected graph G is characterised by the parameter set of its precision matrices which is the cone M+(G) of positive definite matrices with entries corresponding to the missing edges of G constrained to be equal to zero. In a Bayesian framework, the conjugate family for the precision parameter is the distribution with Wishart density with respect to the Lebesgue measure restricted to M+(G). We call this distribution the G-Wishart. When G is nondecomposable, the normalising constant of the G-Wishart cannot be computed in closed form. In this paper, we give a simple Monte Carlo method for computing this normalising constant. The main feature of our method is that the sampling distribution is exact and consists of a product of independent univariate standard normal and chi-squared distributions that can be read off the graph G. Computing this normalising constant is necessary for obtaining the posterior distribution of G or the marginal likelihood of the corresponding graphical Gaussian model. Our method also gives a way of sampling from the posterior distribution of the precision matrix."
"10.1093/biomet/92.2.303","2005","Variable selection for multivariate failure time data","5","In this paper, we propose a penalised pseudo-partial likelihood method for variable selection with multivariate failure time data with a growing number of regression coefficients. Under certain regularity conditions, we show the consistency and asymptotic normality of the penalised likelihood estimators. We further demonstrate that, for certain penalty functions with proper choices of regularisation parameters, the resulting estimator can correctly identify the true model, as if it were known in advance. Based on a simple approximation of the penalty function, the proposed method can be easily carried out with the Newton-Raphson algorithm. We conduct extensive Monte Carlo simulation studies to assess the finite sample performance of the proposed procedures. We illustrate the proposed method by analysing a dataset from the Framingham Heart Study."
"10.1093/biomet/92.2.283","2005","Additive hazards {M}arkov regression models illustrated with bone marrow transplant data","0","When there are covariate effects to be considered, multi-state survival analysis is dominated either by parametric Markov regression models or by semiparametric Markov regression models using Cox's (1972) proportional hazards models for transition intensities between the states. The purpose of this research work is to study alternatives to Cox's model in a general finite-state Markov process setting. We shall look at two alternative models, Aalen's (1989) nonparametric additive hazards model and Lin & Ying's (1994) semiparametric additive hazards model. The former allows the effects of covariates to vary freely over time, while the latter assumes that the regression coefficients are constant over time. With the basic tools of the product integral and the functional delta-method, we present an estimator of the transition probability matrix and develop the large-sample theory for the estimator under each of these two models. Data on 1459 HLA identical sibling transplants for acute leukaemia from the International Bone Marrow Transplant Registry serve as illustration."
"10.1093/biomet/92.2.271","2005","Empirical-likelihood-based semiparametric inference for the treatment effect in the two-sample problem with censoring","2","To compare two samples of censored data, we propose a unified method of semiparametric inference for the parameter of interest when the model for one sample is parametric and that for the other is nonparametric. The parameter of interest may represent, for example, a comparison of means, or survival probabilities. The confidence interval derived from the semiparametric inference, which is based on the empirical likelihood principle, improves its counterpart constructed from the common estimating equation. The empirical likelihood ratio is shown to be asymptotically chi-squared. Simulation experiments illustrate that the method based on the empirical likelihood substantially outperforms the method based on the estimating equation. A real dataset is analysed."
"10.1093/biomet/92.2.251","2005","Marginal likelihood, conditional likelihood and empirical likelihood: connections and applications","2","Marginal likelihood and conditional likelihood are often used for eliminating nuisance parameters. For a parametric model, it is well known that the full likelihood can be decomposed into the product of a conditional likelihood and a marginal likelihood. This property is less transparent in a nonparametric or semiparametric likelihood setting. In this paper we show that this nice parametric likelihood property can be carried over to the empirical likelihood world. We discuss applications in case-control studies, genetical linkage analysis, genetical quantitative traits analysis, tuberculosis infection data and unordered-paired data, all of which can be treated as semiparametric finite mixture models. We consider the estimation problem in detail in the simplest case of unordered-paired data where we can only observe the minimum and maximum values of two random variables; the identities of the minimum and maximum values are lost. The profile empirical likelihood approach is used for maximum semiparametric likelihood estimation. We present some large-sample results along with a simulation study."
"10.1093/biomet/92.1.242","2005","A note on shrinkage sliced inverse regression","5","We employ Lasso shrinkage within the context of sufficient dimension reduction to obtain a shrinkage sliced inverse regression estimator, which provides easier interpretations and better prediction accuracy without assuming a parametric model. The shrinkage sliced inverse regression approach can be employed for both single-index and multiple-index models. Simulation studies suggest that the new estimator performs well when its tuning parameter is selected by either the Bayesian information criterion or the residual information criterion."
"10.1093/biomet/92.1.234","2005","Calibrated interpolated confidence intervals for population quantiles","0","Beran B Hall's (1993) simple linear interpolation provides a very convenient approach for constructing nonparametric confidence intervals for population quantiles based on a random sample of size n. We show that the coverage error of the interpolated interval, which is of order O(n(-1)), can be improved upon by calibrating the nominal coverage level. Three distinct methods of calibration are considered. The analytical and Monte Carlo methods succeed in reducing the order of coverage error to O(n(-3/2)), while the smoothed bootstrap method reduces it further to O(n(-25/14)). We provide guidelines for practical implementation of the calibration methods. Their performance is compared with the simple linear interpolated interval in a simulation study which confirms superiority of the calibrated intervals."
"10.1093/biomet/92.1.229","2005","An examination of the effect of heterogeneity on the estimation of population size using capture-recapture data","0","Part of the folklore of capture-recapture experiments is that ignoring heterogeneity of capture probabilities results in a downward bias. This has been based on experience and simulation studies but is often interpreted as being due to individuals with lower capture probabilities. Here estimating equation arguments are used to show that the effect on Horvitz-Thompson-type estimators of ignoring heterogeneity in capture-recapture experiments is to introduce a downward bias. The arguments are extended to continuous-time experiments and to an influence function constructed to determine the effect of a small number of individuals with heterogeneous capture probabilities in an otherwise homogeneous population and the influence function is shown to be negative. The downward bias holds even if the small number of heterogeneous individuals have capture probabilities larger than the homogeneous majority, and this is confirmed by simulations."
"10.1093/biomet/92.1.213","2005","Exploiting occurrence times in likelihood inference for componentwise maxima","0","Multivariate extreme value distributions arise as the limiting distributions of normalised componentwise maxima. They are often used to model multivariate data that can be regarded as the componentwise maxima of some unobserved underlying multivariate process. In many applications we have extra information. We often know the locations of the maxima within the underlying process. If the process is temporal this knowledge is frequently available through the dates on which the maxima are recorded. We show how to incorporate this extra information into maximum likelihood procedures. Asymptotic and small-sample efficiency results are presented for the dependence parameter in the logistic parametric sub-class of bivariate extreme value distributions. We conclude with an application to sea levels."
"10.1093/biomet/92.1.197","2005","Adaptive two-stage test procedures to find the best treatment in clinical trials","2","A main objective in clinical trials is to find the best treatment in a given finite class of competing treatments and then to show superiority of this treatment against a control treatment. The traditional procedure estimates the best treatment in a first trial. Then in an independent second trial superiority of this treatment, estimated as best in the first trial, is to be shown against the control treatment by a size alpha test. In this paper we investigate these two trials of this traditional procedure as a two-stage test procedure. Additionally we introduce competing two-stage group-sequential test procedures. Then we derive formulae for the expected number of patients. These formulae depend on unknown parameters. When we have a prior for the unknown parameters we can determine the two-stage test procedure of size a and power beta that is optimal, in that it needs a minimal number of observations. The results are illustrated by a numerical example, which indicates the superiority of the group-sequential procedures."
"10.1093/biomet/92.1.183","2005","On measuring the variability of small area estimators under a basic area level model","8","In this paper based on a basic area level model we obtain second-order accurate approximations to the mean squared error of model-based small area estimators, using the Fay I Herriot (1979) iterative method of estimating the model variance based on weighted residual sum of squares. We also obtain mean squared error estimators unbiased to second order. Based on simulations, we compare the finite-sample performance of our mean squared error estimators with those based on method-of-moments, maximum likelihood and residual maximum likelihood estimators of the model variance. Our results suggest that the Fay-Herriot method performs better, in terms of relative bias of mean squared error estimators, than the other methods across different combinations of number of areas, pattern of sampling variances and distribution of small area effects. We also derive a noninformative prior on the model parameters for which the posterior variance of a small area mean is second-order unbiased for the mean squared error. The posterior variance based on such a prior possesses both Bayesian and frequentist interpretations."
"10.1093/biomet/92.1.173","2005","Power of edge exclusion tests in graphical {G}aussian models","0","Asymptotic multivariate normal approximations to the joint distributions of edge exclusion test statistics for saturated graphical Gaussian models are derived. Non-signed and signed square-root versions of the likelihood ratio, Wald and score test statistics are considered. Noncentral chi-squared approximations are also considered for the non-signed versions. These approximations are used to estimate the power of edge exclusion tests and an example is presented."
"10.1093/biomet/92.1.159","2005","Nonparametric tests for and against likelihood ratio ordering in the two-sample problem","0","We derive nonparametric procedures for testing for and against likelihood ratio ordering in the two-population setting with continuous distributions. We account for this ordering by examining the least concave majorant of the ordinal dominance curve formed from the nonparametric maximum likelihood estimators of the continuous distribution functions F and G. In particular, we focus on testing equality of F and G versus likelihood ratio ordering and testing for a violation of likelihood ratio ordering. For both testing problems, we propose area-based and sup-norm-based test statistics, derive appropriate limiting distributions, and provide simulation results that characterise the performance of our procedures. We illustrate our methods using data from a controlled experiment involving the effects of radiation on mice."
"10.1093/biomet/92.1.149","2005","Standard errors and covariance matrices for smoothed rank estimators","3","A 'pseudo-Bayesian' interpretation of standard errors yields a natural induced smoothing of statistical estimating functions. When applied to rank estimation, the lack of smoothness which prevents standard error estimation is remedied. Efficiency and robustness are preserved, while the smoothed estimation has excellent computational properties. In particular, convergence of the iterative equation for standard error is fast, and standard error calculation becomes asymptotically a one-step procedure. This property also extends to covariance matrix calculation for rank estimates in multi-parameter problems. Examples, and some simple explanations, are given."
"10.1093/biomet/92.1.135","2005","Discrete-transform approach to deconvolution problems","4","If Fourier series are used as the basis for inference in deconvolution problems, the effects of the errors factorise out in a way that is easily exploited empirically. This property is the consequence of elementary addition formulae for sine and cosine functions, and is not readily available when one is using methods based on other orthogonal series or on continuous Fourier transforms. It allows relatively simple estimators to be constructed, founded on the addition of finite series rather than on integration. The performance of these methods can be particularly effective when edge effects are involved, since cosine-series estimators are quite resistant to boundary problems. In this context we point to the advantages of trigonometric-series methods for density deconvolution; they have better mean squared error performance when edge effects are involved, they are particularly easy to code, and they admit a simple approach to empirical choice of smoothing parameter, in which a version of thresholding, familiar in wavelet-based inference, is used in place of conventional smoothing. Applications to other deconvolution problems are briefly discussed."
"10.1093/biomet/92.1.119","2005","Multiscale generalised linear models for nonparametric function estimation","1","We present a method for extracting information about both the scale and trend of local components of an inhomogeneous function in a nonparametric generalised linear model. Our multiscale framework combines recursive partitions, which allow for the incorporation of scale in a natural manner, with systems of piecewise polynomials supported on the partition intervals, which serve to summarise the smooth trend within each interval. Our estimators are formulated as solutions of complexity-penalised likelihood optimisations, where the penalty seeks to limit the number of intervals used to model the data. The actual calculation of the estimators may be accomplished using standard software routines for generalised linear models, within the context of efficient, tree-based, polynomial-time algorithms. A risk analysis shows that these estimators achieve the same asymptotic rates in the nonparametric generalised linear model as the classical wavelet-based estimators in the Gaussian 'function plus noise' model, for suitably defined ranges of Besov spaces. Numerical simulations show that the method tends to perform at least as well as, and often better than, alternative wavelet-based methodologies in the context of finite samples, while applications to gamma-ray burst data in astronomy and packet loss data in computer network traffic analysis confirm its practical relevance."
"10.1093/biomet/92.1.105","2005","Theory for penalised spline regression","7","Penalised spline regression is a popular new approach to smoothing, but its theoretical properties are not yet well understood. In this paper, mean squared error expressions and consistency results are derived by using a white-noise model representation for the estimator. The effect of the penalty on the bias and variance of the estimator is discussed, both for general splines and for the case of polynomial splines. The penalised spline regression estimator is shown to achieve the optimal nonparametric convergence rate established by Stone (1982)."
"10.1093/biomet/92.1.91","2005","Exact likelihood ratio tests for penalised splines","2","Penalised-spline-based additive models allow a simple mixed model representation where the variance components control departures from linear models. The smoothing parameter is the ratio of the random-coefficient and error variances and tests for linear regression reduce to tests for zero random-coefficient variances. We propose exact likelihood and restricted likelihood ratio tests for testing polynomial regression versus a general alternative modelled by penalised splines. Their spectral decompositions are used as the basis of fast simulation algorithms. We derive the asymptotic local power properties of the tests under weak conditions. In particular we characterise the local alternatives that are detected with asymptotic probability one. Confidence intervals for the smoothing parameter are obtained by inverting the tests for a fixed smoothing parameter versus a general alternative. We discuss F and R tests and show that ignoring the variability in the smoothing parameter estimator can have a dramatic effect on their null distributions. The powers of several known tests are investigated and a small set of tests with good power properties is identified. The restricted likelihood ratio test is among the best in terms of power."
"10.1093/biomet/92.1.75","2005","Covariate-adjusted regression","3","We introduce covariate-adjusted regression for situations where both predictors and response in a regression model are not directly observable, but are contaminated with a multiplicative factor that is determined by the value of an unknown function of an observable covariate. We demonstrate how the regression coefficients can be estimated by establishing a connection to varying-coefficient regression. The proposed covariate-adjustment method is illustrated with an analysis of the regression of plasma fibrinogen concentration as response on serum transferrin level as predictor for 69 haemodialysis patients. In this example, both response and predictor are thought to be influenced in a multiplicative fashion by body mass index. A bootstrap hypothesis test enables us to test the significance of the regression parameters. We establish consistency and convergence rates of the parameter estimators for this new covariate-adjusted regression model. Simulation studies demonstrate the efficacy of the proposed method."
"10.1093/biomet/92.1.59","2005","Local polynomial regression analysis of clustered data","3","This paper proposes a classical weighted least squares type of local polynomial smoothing for the analysis of clustered data, with the key idea of using generalised inverses of correlation matrices. The estimator has a simple closed-form expression. Simplicity is achieved also for nonparametric generalised linear models with arbitrary link function via a transformation. Our approach can be characterised by 'local observations with local variances', which yields intuitively correct results in the sense that correct/incorrect specification of within-cluster correlation has respective positive/negative effects. The approach is a natural extension of classical local polynomial smoothing. Consequently, existing theory can be largely carried over and important issues such as bandwidth selection can be tackled in the classical fashion. Moreover, the approach can handle various types of covariate, such as cluster-level, subject-level or partially cluster-level. Numerical studies support the theoretical results. The method is illustrated with a real example on luteinising hormone levels in cows."
"10.1093/biomet/92.1.47","2005","On the implementation of local probability matching priors for interest parameters","4","Probability matching priors are priors for which the posterior probabilities of certain specified sets are exactly or approximately equal to their coverage probabilities. These priors arise as solutions of partial differential equations that may be difficult to solve, either analytically or numerically. Recently Levine & Casella (2003) presented an algorithm for the implementation of probability matching priors for an interest parameter in the presence of a single nuisance parameter. In this paper we develop a local implementation that is very much more easily computed. A local probability matching prior is a data-dependent approximation to a probability matching prior and is such that the asymptotic order of approximation of the frequentist coverage probability is not degraded. We illustrate the theory with a number of examples, including three discussed in Levine & Casella (2003)."
"10.1093/biomet/92.1.31","2005","Bayesian exponentially tilted empirical likelihood","8","While empirical likelihood has been shown to exhibit many of the properties of conventional parametric likelihoods, a formal probabilistic interpretation has so far been lacking. We show that a likelihood function very closely related to empirical likelihood naturally arises from a nonparametric Bayesian procedure which places a type of noninformative prior on the space of distributions. This prior gives preference to distributions having a small support and, among those sharing the same support, it favours entropy-maximising distributions. The resulting nonparametric Bayesian procedure admits a computationally convenient representation as an empirical-likelihood-type likelihood where the probability weights are obtained via exponential tilting. The proposed methodology provides an attractive alternative to the Bayesian bootstrap as a nonparametric limit of a Bayesian procedure for moment condition models."
"10.1093/biomet/92.1.19","2005","Semiparametric regression analysis of mean residual life with censored survival data","3","As function of time t, a mean residual life is the remaining life expectancy of a subject given survival up to t. The proportional mean residual life model, proposed by Oakes A Dasu (1990), provides an alternative to the Cox proportional hazards model for studying the association between survival times and covariates. In the presence of censoring, we use counting process theory to develop semiparametric inference procedures for the regression coefficients of the Oakes-Dasu model. Simulation studies and an application to the well-known Veterans' Administration lung cancer survival data are presented."
"10.1093/biomet/92.1.1","2005","Semiparametric analysis of short-term and long-term hazard ratios with two-sample survival data","0","Standard approaches to semiparametric modelling of two-sample survival data are not appropriate when the two survival curves cross. We introduce a two-sample model that accommodates crossing survival curves. The two scalar parameters of the model have the interpretations of being the short-term and long-term hazard ratios respectively. The time-varying hazard ratio is expressed semiparametrically by the two scalar parameters and an unspecified baseline distribution. The new model includes the Cox model and the proportional odds model as submodels. For inference we use a pseudo maximum likelihood approach that can be expressed via some simple estimating equations, analogous to that for the maximum partial likelihood estimator of the Cox model, that provide consistent and asymptotically normal estimators. Simulation studies show that the estimators perform well for moderate sample sizes. We also illustrate the methods with a real-data example. The new model can be extended easily to the regression setting."
"10.1093/biomet/91.4.987","2004","The distribution of the difference between two {$t$}-variates","0","In this paper, the difference between two correlated t variables is divided by a function of their sample correlation and the distribution of the resulting quantity is examined. Functions of the sample correlation are found for which this quantity is approximately pivotal and has a t distribution, asymptotically. Simulations show that the asymptotic results hold well for small sample sizes. The results yield a useful test for comparing the difference in standardised scores of an individual with those of a group of controls. The test assumes that sampling is from a bivariate normal distribution and robustness of the test to departure from normality is examined."
"10.1093/biomet/91.4.975","2004","Multivariate distributions with support above the diagonal","0","A general family of distributions for the empirical modelling of ordered multivariate data is proposed. The family is based on, but greatly extends, the joint distribution of order statistics from an independent and identically distributed univariate sample. General properties, including marginal and conditional distributions, bivariate dependence, limiting distributions and links to the Dirichlet distribution are described. Univariate and bivariate special cases of the multivariate distributions, the latter including an equivalent rotated version, are considered. Two particular tractable special cases are stressed. The models are successfully and usefully fitted, by maximum likelihood, to meteorological data. The models are also applicable to data in which one variable is unconstrained and the other are all nonnegative."
"10.1093/biomet/91.4.955","2004","`{A}nalytic' wavelet thresholding","0","We introduce so-called analytic stationary wavelet transform thresholding where, using the discrete Hilbert transform, we create a complex-valued 'analytic' vector from which an amplitude vector is defined. Thresholding of a real-valued wavelet coefficient at some transform level is carried out according to the corresponding value in this amplitude vector; relevant statistical results follow from properties of the discrete Hilbert transform. Analytic stationary wavelet transform thresholding is found to produce consistently a reduced mean squared error compared to using standard stationary wavelet transform, or 'cycle spinning', thresholding. For signals with extensive oscillations at some transform levels, this improvement is very marked. Furthermore we show that our thresholding test is invariant to phase shifts in the data, whereas, if complex wavelet filters are being used, the filters must be analytic or anti-analytic at each level of the wavelet transform."
"10.1093/biomet/91.4.943","2004","Statistical inference based on non-smooth estimating functions","3","When the estimating function for a vector of parameters is not smooth, it is often rather difficult, if not impossible, to obtain a consistent estimator by solving the corresponding estimating equation using standard numerical techniques. In this paper, we propose a simple inference procedure via the importance sampling technique, which provides a consistent root of the estimating equation and also an approximation to its distribution without solving any equations or involving nonparametric function estimates. The new proposal is illustrated and evaluated via two extensive examples with real and simulated datasets."
"10.1093/biomet/91.4.929","2004","A paradox concerning nuisance parameters and projected estimating functions","4","This paper is concerned with a paradox associated with parameter estimation in the presence of nuisance parameters. In a statistical model with unknown nuisance parameters, the efficiency of an estimator of a parameter usually increases when the nuisance parameters are known. However the opposite phenomenon can sometimes occur. In this paper, we elucidate the occurrence of this paradox by examining estimating functions. In particular, we focus on the projected estimating function, which is defined by the projection of the score function on to a given estimating function. A sufficient condition for the paradox to occur is the orthogonality of the two components of the projected estimating functions corresponding to parameters of interest and nuisance parameters. In addition, a numerical assessment is conducted in the context of a simple model to investigate the improvement of the asymptotic efficiency of estimators."
"10.1093/biomet/91.4.913","2004","Coordination, combination and extension of balanced samples","0","The cube method allows the selection of balanced samples on several auxiliary variables with equal or unequal inclusion probabilities. Practical implementation of the cube method has raised questions concerning the selection of a multi-phase balanced sampling design, the rebalancing of an unbalanced sampling design by completing it with another sample, the selection of a balanced sample from an unbalanced sample and the coordination of balanced samples. This paper provides a complete solution of all these problems."
"10.1093/biomet/91.4.893","2004","Efficient balanced sampling: the cube method","4","A balanced sampling design is defined by the property that the Horvitz-Thompson estimators of the population totals of a set of auxiliary variables equal the known totals of these variables. Therefore the variances of estimators of totals of all the variables of interest are reduced, depending on the correlations of these variables with the controlled variables. In this paper, we develop a general method, called the cube method, for selecting approximately balanced samples with equal or unequal inclusion probabilities and any number of auxiliary variables."
"10.1093/biomet/91.4.877","2004","Adaptive cluster double sampling","0","We present a multi-phase variant of adaptive cluster sampling which allows the sampler to control the number of measurements of the variable of interest. A first-phase sample is selected using an adaptive cluster sampling design based on an inexpensive auxiliary variable associated with the survey variable. Then the network structure of the adaptive cluster sample is used to select an ordinary one-phase or two-phase subsample of units and the values of the survey variable associated with those units are recorded. The population mean is estimated by either a regression-type estimator or a Horvitz-Thompson-type estimator. The results of a simulation study show good performance of the proposed design, and suggest that in many real situations this design might be preferred to the ordinary adaptive cluster sampling design."
"10.1093/biomet/91.4.863","2004","Adjusting for covariate errors with nonparametric assessment of the true covariate distribution","0","A well-known and useful method for generalised regression analysis when a linear covariate x is available only through some approximation z is to carry out more or less the usual analysis with E(x\z) substituted for x. Sometimes, but not always, the quantity var (x\z) should be used to allow for overdispersion introduced by this substitution. These quantities involve the distribution of true covariates x, and with some exceptions this requires assessment of that distribution through the distribution of observed values z. It is often desirable to take a nonparametric approach to this, which inherently involves a deconvolution that is difficult to carry our directly. However, if covariate errors are assumed to be multiplicative and log-normal, simple but accurate approximations are available for the quantities E (x(k)\z) (k = 1, 2,...). In particular, the approximations depend only on the first two derivatives of the logarithm of the density of z at the point under consideration and the coefficient of variation of z\x. The methods will thus be most useful in large-scale observational studies where the distribution of z can be assessed well enough in an essentially nonparametric manner to approximate adequately those derivatives. We consider both the classical and Berkson error models. This approach is applied to radiation dose estimates for atomic-bomb survivors."
"10.1093/biomet/91.4.849","2004","A semiparametric changepoint model","0","A semiparametric changepoint model is considered and the empirical likelihood method is applied to detect the change from a distribution to a weighted distribution in a sequence of independent random variables. The maximum likelihood changepoint estimator is shown to be consistent. The empirical likelihood ratio test statistic is proved to have the same limit null distribution as that with parametric models. A data-based test for the validity of the models is also proposed. Simulation shows the sensitivity and robustness of the semiparametric approach. The methods are applied to some classical datasets such as the Nile River data and stock price data."
"10.1093/biomet/91.4.835","2004","Locally efficient semiparametric estimators for functional measurement error models","10","A class of semiparametric estimators are proposed in the general setting of functional measurement error models. The estimators follow from estimating equations that are based on the serniparametric efficient score derived under a possibly incorrect distributional assumption for the unobserved 'measured with error' covariates. It is shown that such estimators are consistent and asymptotically normal even with misspecification and are efficient if computed under the truth. The methods are demonstrated with a simulation study of a quadratic logistic regression model with measurement error."
"10.1093/biomet/91.4.819","2004","A crossvalidation method for estimating conditional densities","8","We extend the idea of crossvalidation to choose the smoothing parameters of the 'double-kernel' local linear regression for estimating a conditional density. Our selection rule optimises the estimated conditional density function by minimising the integrated squared error. We also discuss three other bandwidth selection rules, an ad hoc method used by Fan et al. (1996), a bootstrap method of Hall et al. (1999) for bandwidth selection in the estimation of conditional distribution functions, modified by Bashtannyk & Hyndman (2001) to cover conditional density functions, and finally a simple approach proposed by Hyndman & Yao (2002). The performance of the new approach is compared with these three methods by simulation studies, and our method performs outstandingly well. The method is illustrated by an application to estimating the transition density and the Value-at-Risk of treasury-bill data."
"10.1093/biomet/91.4.801","2004","Additive hazards model with multivariate failure time data","1","Marginal additive hazards models are considered for multivariate survival data in which individuals may experience events of several types and there may also be correlation between individuals. Estimators are proposed for the parameters of such models and for the baseline hazard functions. The estimators of the regression coefficients are shown asymptotically to follow a multivariate normal distribution with a sandwich-type covariance matrix that can be consistently estimated. The estimated baseline and subject-specific cumulative hazard processes are shown to converge weakly to a zero-mean Gaussian random field. The weak convergence properties for the corresponding survival processes are established. A resampling technique is proposed for constructing simultaneous confidence bands for the survival curve of a specific subject. The methodology is extended to a multivariate version of a class of partly parametric additive hazards model. Simulation studies are conducted to assess finite sample properties, and the method is illustrated with an application to development of coronary heart diseases and cardiovascular accidents in the Framingham Heart Study."
"10.1093/biomet/91.4.785","2004","Model selection in irregular problems: applications to mapping quantitative trait loci","2","Two methods of model selection are discussed for changepoint-like problems, especially those arising in genetic linkage analysis. The first is a method that selects the model with the smallest p-value, while the second is a modification of the Bayes information criterion. The methods are compared theoretically and on examples from the literature. For these examples, they are roughly comparable although the p-value-based method is somewhat more liberal in selecting a high-dimensional model."
"10.1093/biomet/91.4.763","2004","Estimation of treatment effects in randomised trials with non-compliance and a dichotomous outcome using structural mean models","3","We consider estimation of the received treatment effect on a dichotomous outcome in randomised trials with non-compliance. We explore inference about the parameters of the structural mean models of Robins (1994, 1997) and Robins et al. (1999). We show that, in contrast to the additive and multiplicative structural mean models for continuous and count outcomes, unbiased estimating functions for a nonzero (structural) treatment effect parameter do not exist in the presence of many continuous and discrete baseline covariates, even when the randomisation probabilities are known. The best that can be hoped for are estimators, such as those proposed in this paper, that are guaranteed both to estimate consistently the (null) treatment effect when the null hypothesis of no treatment effect is true and to have small bias when the true treatment effect is close to but riot equal to zero."
"10.1093/biomet/91.3.758","2004","Permutation invariance of alternating logistic regression for multivariate binary data","1","A practically important but not so obvious result is that alternating logistic regression is invariant to permutations of the response variables within clusters. In this note, we give a short proof of the invariance result using a pairwise likelihood argument. The same proof can be used to establish invariance for a more general class of estimating equations based on conditional residuals. As it stands, the invariance theory is incomplete because existing standard error estimates are not invariant to permutations. To solve this problem we present a symmetrised version of the estimating equation and use it to obtain permutation-invariant standard errors."
"10.1093/biomet/91.3.751","2004","Efficient recursions for general factorisable models","3","Let n S-valued categorical variables be jointly distributed according to a distribution known only up to an unknown normalising constant. For an unnormalised joint likelihood expressible as a product of factors, we give an algebraic recursion which can be used for computing the normalising constant and other summations. A saving in computation is achieved when each factor contains a lagged subset of the components combining in the joint distribution, with maximum computational efficiency as the subsets attain their minimum size. If each subset contains at most r + 1 of the n components in the joint distribution, we term this a lag-r model, whose normalising constant can be computed using a forward recursion in O(Sr+1) computations, as opposed to 0 (S-n) for the direct computation. We show how a lag-r model represents a Markov random field and allows a neighbourhood structure to be related to the unnormalised joint likelihood. We illustrate the method by showing how the normalising constant of the Ising or autologistic model can be computed."
"10.1093/biomet/91.3.743","2004","Nonparametric confidence intervals for receiver operating characteristic curves","1","We study methods for constructing confidence intervals and confidence bands for estimators of receiver operating characteristics. Particular emphasis is placed on the way in which smoothing should be implemented, when estimating either the characteristic itself or its variance. We show that substantial undersmoothing is necessary if coverage properties are not to be impaired. A theoretical analysis of the problem suggests an empirical, plug-in rule for bandwidth choice, optimising the coverage accuracy of interval estimators. The performance of this approach is explored. Our preferred technique is based on asymptotic approximation, rather than a more sophisticated approach using the bootstrap, since the latter requires a multiplicity of smoothing parameters all of which must be chosen in nonstandard ways. It is shown that the asymptotic method can give very good performance."
"10.1093/biomet/91.3.738","2004","Measurement exchangeability and normal one-factor models","0","The one-factor model restricts the covariance structure of the observed variables on the basis of assumptions about their relationship with an unobserved variable. It is hard to justify these assumptions on substantive or empirical grounds. In this paper, alternative measurement models are proposed that are based on exchangeability of variables after admissible scale transformations. They provide an alternative interpretation of the model and do not involve unobserved variables. They also yield a new one-factor model for sum scales."
"10.1093/biomet/91.3.729","2004","A note on pseudolikelihood constructed from marginal densities","10","For likelihood-based inference involving distributions in which high-dimensional dependencies are present it may be useful to use approximate likelihoods based, for example, on the univariate or bivariate marginal distributions. The asymptotic properties of formal maximum likelihood estimators in such cases are outlined. In particular, applications in which only a single q x 1 vector of observations is observed are examined. Conditions under which consistent estimators of parameters result from the approximate likelihood using only pairwise joint distributions are studied. Some examples are analysed in detail."
"10.1093/biomet/91.3.715","2004","A superiority-equivalence approach to one-sided tests on multiple endpoints in clinical trials","0","This paper considers the problem of comparing a new treatment with a control based on multiple endpoints. The hypotheses are formulated with the goal of showing that the treatment is equivalent, i.e. not inferior, on all endpoints and superior on at least one endpoint compared to the control, where thresholds for equivalence and superiority are specified for each endpoint. Roy's (1953) union-intersection and Berger's (1982) intersection-union principles are employed to derive the basic test. It is shown that the critical constants required for the union-intersection test of superiority can be sharpened by a careful analysis of its type I error rate. The composite UI-IU test is illustrated by an example and compared in a simulation study to alternative tests proposed by Bloch et al. (2001) and Perlman & Wu (2004). The Bloch et al. test does not control the type I error rate because of its nonmonotone nature, and is hence not recommended. The UI-IU and the Perlman & Wu tests both control the type I error rate, but the latter test generally has a slightly higher power."
"10.1093/biomet/91.3.705","2004","The geometry of biplot scaling","0","A simple geometry allows the main properties of matrix approximations used in biplot displays to be developed. It establishes orthogonal components of an analysis of variance, from which different contributions to approximations may be assessed. Particular attention is paid to approximations that share the same singular vectors, in which case the solution space is a convex cone. Two- and three-dimensional approximations are examined in detail and then the geometry is interpreted for different forms of the matrix being approximated."
"10.1093/biomet/91.3.683","2004","Temporal process regression","5","We consider regression for response and covariates which are temporal processes observed over intervals. A functional generalised linear model is proposed which includes extensions of standard models in multi-state survival analysis. Simple nonparametric estimators of time-indexed parameters are developed using 'working independence' estimating equations and are shown to be uniformly consistent and to converge weakly to Gaussian processes. The procedure does not require smoothing or a Markov assumption, unlike approaches based on transition intensities. The usual definition of optimal estimating equations for parametric models is then generalised to the functional model and the optimum is identified in a class of functional generalised estimating equations. Simulations demonstrate large efficiency gains relative to working independence at times where censoring is heavy. The estimators are the basis for new tests of the covariate effects and for the estimation of models in which greater structure is imposed on the parameters, providing novel goodness-of-fit tests. The methodology's practical utility is illustrated in a data analysis."
"10.1093/biomet/91.3.661","2004","Efficient estimation for semivarying-coefficient models","4","Motivated by two practical problems, we propose a new procedure for estimating a semivarying-coefficient model. Asymptotic properties are established which show that the bias of the parameter estimator is of order h(3) when a symmetric kernel is used, where h is the bandwidth, and the variance is of order n(-1) and efficient in the semiparametric sense. Undersmoothing is unnecessary for the root-n consistency of the estimators. Therefore, commonly used bandwidth selection methods can be employed. A model selection method is also developed. Simulations demonstrate how the proposed method works. Some insights are obtained into the two motivating problems by using the proposed models."
"10.1093/biomet/91.3.645","2004","On multiple regression models with nonstationary correlated errors","0","We consider the estimation of parameters of a multiple regression model with nonstationary errors. We assume the nonstationary errors satisfy a time-dependent autoregressive process and describe a method for estimating the parameters of the regressors and the time-dependent autoregressive parameters. The parameters are rescaled as in nonparametric regression to obtain the asymptotic sampling properties of the estimators. The method is illustrated with an example taken from global temperature anomalies."
"10.1093/biomet/91.3.629","2004","Multivariate spectral analysis using {C}holesky decomposition","3","We propose to smooth the Cholesky decomposition of a raw estimate of a multivariate spectrum, allowing different degrees of smoothness for different elements. The final spectral estimate is reconstructed from the smoothed Cholesky elements, and is consistent and positive definite. More importantly, the Cholesky decomposition matrix of the spectrum can be used as a transfer function in generating time series whose spectrum is identical to the given spectrum at the Fourier frequencies. This not only provides us with much flexibility in simulations, but also allows us to construct bootstrap confidence intervals for the multivariate spectrum by generating bootstrap samples using the Cholesky decomposition of the spectral estimate. A numerical example and an application to electroencephalogram data are used as illustrations."
"10.1093/biomet/91.3.613","2004","Large-sample properties of the periodogram estimator of seasonally persistent processes","0","Seasonally persistent models were first introduced by Andel (1986) and Gray et al. (1989) to extend autoregressive moving-average and fractionally differenced models and to encompass long-memory quasi-periodic behaviour. These models are, for certain ranges of parameters, stationary, and we prove here that the behaviour of the periodogram and other tapered estimators cannot be simply extended from the work of Kunsch (1986) and Hurvich & Beltrao (1993) on long memory induced by a pole at the origin. We demonstrate that potentially large both positive and negative bias can be found from the same value of the long-memory parameter, and that the new distribution can be easily written down in the case of Gaussian processes. We also consider using both the cosine taper and the sine taper. The extended least squares estimator is also considered in this context."
"10.1093/biomet/91.3.603","2004","A modified likelihood ratio statistic for some nonregular models","0","Higher-order approximations to the distribution of the likelihood ratio statistic are considered for a class of nonregular models in which the maximum likelihood estimator of the parameter of interest is asymptotically distributed according to an exponential, rather than a normal, distribution. Asymptotic behaviour of this type often arises when the boundary of the support of the distributions under consideration depends on theta. A modified likelihood ratio statistic is proposed that follows its asymptotic distribution to a high degree of approximation, and this statistic is illustrated on several examples."
"10.1093/biomet/91.3.591","2004","Model selection for {G}aussian concentration graphs","4","A multivariate Gaussian graphical Markov model for an undirected graph G, also called a covariance selection model or concentration graph model, is defined in terms of the Markov properties, i.e. conditional independences associated with G, which in turn are equivalent to specified zeros among the set of pairwise partial correlation coefficients. By means of Fisher's z-transformation and Sidak's correlation inequality, conservative simultaneous confidence intervals for the entire set of partial correlations can be obtained, leading to a simple method for model selection that controls the overall error rate for incorrect edge inclusion. The simultaneous p-values corresponding to the partial correlations are partitioned into three disjoint sets, a significant set S, an indeterminate set I and a nonsignificant set N. Our model selection method selects two graphs, a graph G(SI) whose edges correspond to the set S boolean OR I, and a more conservative graph G(S) whose edges correspond to S only. Similar considerations apply to covariance graph models, which are defined in terms of marginal independence rather than conditional independence. The method is applied to some well-known examples and to simulated data."
"10.1093/biomet/91.3.579","2004","A diagnostic procedure based on local influence","3","Cook's (1986) normal curvature measure is useful for sensitivity analysis of model assumptions in statistical models. However, there is no rigorous approach based on the normal curvature for addressing two fundamental issues: to assess the extent of discrepancy between an assumed model and the underlying model from which the data are generated, and to identify suspicious data points for which the discrepancy is most evident. Our purpose is to establish a theoretically sound procedure for resolving these issues for case-weight perturbation under the framework of independent distributions. We show that the local influence measure, Cook's distance and likelihood distance are asymptotically equivalent. A diagnostic procedure, based on local influence, is proposed for evaluating model misspecification and for detecting influential points simultaneously. We analyse two real datasets."
"10.1093/biomet/91.3.559","2004","Fractional hot deck imputation","6","To compensate for item nonresponse, hot deck imputation procedures replace missing values with values that occur in the sample. Fractional hot deck imputation replaces each missing observation with a set of imputed values and assigns a weight to each imputed value. Under the model in which observations in an imputation cell are independently and identically distributed, fractional hot deck imputation is shown to be an effective imputation procedure. A consistent replication variance estimation procedure for estimators computed with fractional imputation is suggested. Simulations show that fractional imputation and the suggested variance estimator are superior to multiple imputation estimators in general, and much superior to multiple imputation for estimating the variance of a domain mean."
"10.1093/biomet/91.3.543","2004","Inference based on the {EM} algorithm for the competing risks model with masked causes of failure","1","In this paper we propose inference methods based on the Em algorithm for estimating the parameters of a weakly parameterised competing risks model with masked causes of failure and second-stage data. With a carefully chosen definition of complete data, the maximum likelihood estimation of the cause-specific hazard functions and of the masking probabilities is performed via an Em algorithm. Both the E- and m-steps can be solved in closed form under the full model and under some restricted models of interest. We illustrate the flexibility of the method by showing how grouped data and tests of common hypotheses in the literature on missing cause of death can be handled. The method is applied to a real dataset and the asymptotic and robustness properties of the estimators are investigated through simulation."
"10.1093/biomet/91.3.529","2004","Case-control current status data","0","In this paper, we show that the distribution function of survival times is identified, up to a one-parameter family of distribution functions, based on information from case-control current status data. With supplementary information on the population frequency of cases relative to controls, a simple weighted version of the nonparametric maximum likelihood estimator for prospective current status data provides a natural estimator for case-control samples. Following the parametric results of Scott & Wild (1997), we show that this estimator is, in fact, the nonparametric"
"10.1093/biomet/91.3.507","2004","Power, sample size and adaptation considerations in the design of group sequential clinical trials","0","A class of flexible and asymptotically efficient group sequential designs is developed herein for one-sided and two-sided tests of the parameter of an exponential family. Efficiency is measured in terms of the expected sample size and power function, under prespecified constraints on the maximum sample size and Type I error probability. It is shown how these designs achieve asymptotic efficiency by adapting to the information about the unknown parameter during the course of the experiment. Moreover, they are very flexible and can circumvent the difficulties due to 'information time' versus 'calendar time' that arise in more complex settings such as time-sequential clinical trials comparing the failure times of two treatments."
"10.1093/biomet/91.2.497","2004","Posterior probability intervals in {B}ayesian wavelet estimation","2","We use saddlepoint approximation to derive credible intervals for Bayesian wavelet regression estimates. Simulations show that the resulting intervals perform better than the best existing method."
"10.1093/biomet/91.2.491","2004","Nonparametric detection of correlated errors","0","In regression problems it is hard to detect correlated errors since the errors are not observed. In this paper, a nonparametric method is proposed for the detection of correlated errors when the design points are equally spaced. It turns out that the first-order sample autocovariance of the residuals from the kernel regression estimates provides essential information about correlated errors and its bootstrap is quite effective in implementing such information."
"10.1093/biomet/91.2.471","2004","Efficient importance sampling for events of moderate deviations with applications","0","We propose a method for finding the alternative distribution in importance sampling. The alternative distribution is optimal in the sense that the asymptotic variance is minimised for estimating tail probabilities of asymptotically normal statistics. Our contribution to importance sampling is three-fold. To begin with, we obtain an explicit expression for the mean of the optimal alternative distribution and the expression motivates a recursive approximation algorithm. Secondly, a new multi-dimensional exponential tilting formula is presented. Lastly, a conservative estimator of the variance is given to facilitate a quick comparison among different stratified sampling schemes in conjunction with importance sampling. Several numerical examples illustrating the efficacy of the proposed method are also included. These results indicate that the proposed method is considerably more efficient than the method based on large deviations theory and the efficiency gain is more significant in higher dimensions."
"10.1093/biomet/91.2.461","2004","Efficient {R}obbins-{M}onro procedure for binary data","2","The Robbins-Monro procedure does not perform well in the estimation of extreme quantiles, because the procedure is implemented using asymptotic results, which are not suitable for binary data. Here we propose a modification of the Robbins-Monro procedure and derive the optimal procedure for binary data under some reasonable approximations. The improvement obtained by using the optimal procedure for the estimation of extreme quantiles is substantial."
"10.1093/biomet/91.2.447","2004","Assessing robustness of generalised estimating equations and quadratic inference functions","1","In the presence of data contamination or outliers, some empirical studies have indicated that the two methods of generalised estimating equations and quadratic inference functions appear to have rather different robustness behaviour. This paper presents a theoretical investigation from the perspective of the influence function to identify the causes for the difference. We show that quadratic inference functions lead to bounded influence functions and the corresponding M-estimator has a redescending property, but the generalised estimating equation approach does not. We also illustrate that, unlike generalised estimating equations, quadratic inference functions can still provide consistent estimators even if part of the data is contaminated. We conclude that the quadratic inference function is a preferable method to the generalised estimating equation as far as robustness is concerned. This conclusion is supported by simulations and real-data examples."
"10.1093/biomet/91.2.425","2004","Bayes linear kinematics and {B}ayes linear {B}ayes graphical models","0","Probability kinematics (Jeffrey, 1965, 1983) furnishes a method for revising a prior probability specification based upon new probabilities over a partition. We develop a corresponding Bayes linear kinematic for a Bayes linear analysis given information which changes our beliefs about a random vector in some generalised way. We derive necessary and sufficient conditions for commutativity of successive Bayes linear kinematics which depend upon the eigenstructure of the Joint kinematic resolution transform. As an application we introduce the Bayes linear Bayes graphical model, which is a mixture of fully Bayesian and Bayes linear graphical models, combining the simplicity of Gaussian graphical models with the ability to allow conditioning on marginal distributions of any form, and exploit Bayes linear kinematics to embed full conditional updates within Bayes linear belief adjustments. The theory is illustrated with a treatment of partition testing for software reliability."
"10.1093/biomet/91.2.409","2004","Principal {H}essian directions for regression with measurement error","2","We consider a nonlinear regression problem with predictors with measurement error. We assume that the response is related to unknown linear combinations of a p-dimensional predictor vector through an unknown link function. Instead of observing the predictors, we observe a surrogate vector with the property that its expectation is linearly related to the predictor vector with constant variance. We use an important linear transformation of the surrogates. Based on the transformed variables, we develop the modified Principal Hessian Directions method for estimating the subspace of the effective dimension-reduction space. We derive the asymptotic variances of the modified Principal Hessian Directions estimators. Several examples are reported and comparisons are made with the sliced inverse regression method of Carroll Li (1992)."
"10.1093/biomet/91.2.393","2004","Nonparametric inference for stochastic linear hypotheses: application to high-dimensional data","1","The Mann-Whitney-Wilcoxon rank sum test is limited to comparison of two groups with univariate responses. In this paper, we introduce a class of stochastic linear hypotheses that addresses these limitations within a nonparametric setting. We formulate hypotheses for simultaneous comparisons of several, multivariate response groups, without modelling the response distributions. Inference is developed based on U-statistics theory and an exchangeability assumption. The latter condition is required to identify testable hypotheses for high-dimensional response vectors, such as those arising in genomic and psychosocial research. The methodology is illustrated with two real-data applications."
"10.1093/biomet/91.2.383","2004","Multimodality of the likelihood in the bivariate seemingly unrelated regressions model","4","We analyse the simplest two-equation seemingly unrelated regressions model and demonstrate that its likelihood may have up to five stationary points, and thus there may be up to three local modes. Consequently the estimates obtained via iterative estimation methods may depend on starting values. We further show that the probability of multi-modality vanishes asymptotically. Monte Carlo simulations suggest that multimodality rarely occurs if the seemingly unrelated regressions model is true, but can become more frequent if the model is misspecified. The existence of multimodality in the likelihood for seemingly unrelated regressions models contradicts several claims in the literature."
"10.1093/biomet/91.2.363","2004","Estimating vaccine efficacy from small outbreaks","0","Let C-V and C-0 denote the number of cases among vaccinated and unvaccinated individuals, respectively, and let v be the proportion of individuals vaccinated. The quantity (e) over cap = 1 - (1 - v)C-V/(vC(0)) = 1 - (relative attack rate) is the most used estimator of the effectiveness of a vaccine to protect against infection. For a wide class of vaccine responses, a family of transmission models and three types of community settings, this paper investigates what actually estimates. It does so under the assumption that the community is large and the vaccination coverage is adequate to prevent major outbreaks of the infectious disease, so that only data on minor outbreaks are available. For a community of homogeneous individuals who mix uniformly, it is found that estimates a quantity with the interpretation of1 - (mean susceptibility, per contact, of vaccinees relative to unvaccinated individuals).We provide a standard error for in this setting. For a community with some heterogeneity can be a very misleading estimator of the effectiveness of the vaccine. When individuals have inherent differences, estimates a quantity that depends also on the inherent susceptibilities of different types of individual and on the vaccination coverage for different types. For a community of households, estimates a quantity that depends on the rate of transmission within households and on the reduction in infectivity induced by the vaccine. In communities that are structured, into households or age-groups, it is possible that estimates a value that is negative even when the vaccine reduces both susceptibility and infectivity."
"10.1093/biomet/91.2.345","2004","Stochastic multitype epidemics in a community of households: estimation of threshold parameter {$R_*$} and secure vaccination coverage","1","This paper is concerned with estimation of the threshold parameter R-* for a stochastic model for the spread of a susceptible --> infective --> removed epidemic among a closed, finite population that contains several types of individual and is partitioned into households. It turns out that R-* cannot be estimated consistently from final outcome data, so a Perron-Frobenius argument is used to obtain sharp lower and upper bounds for R-*, which can be estimated consistently. Determining the allocation of vaccines that reduces the upper bound for R-* to its threshold value of one, thus preventing the occurrence of a major outbreak, with minimum vaccine coverage is shown to be a linear programming problem. The estimates of R-*, before and after vaccination, and of the secure vaccination coverage, i.e. the proportion of individuals that have to be vaccinated to reduce the upper bound for R-* to 1 assuming an optimal vaccination scheme, are equipped with standard errors, thus yielding conservative confidence bounds for these key epidemiological parameters. The methodology is illustrated by application to data on influenza outbreaks in Tecumseh, Michigan."
"10.1093/biomet/91.2.331","2004","On semiparametric transformation cure models","6","A general class of semiparametric transformation cure models is studied for the analysis of survival data with long-term survivors. It combines a logistic regression for the probability of event occurrence with the class of transformation models for the time of occurrence. Included as special cases are the proportional hazards cure model (Farewell, 1982; Kuk Chen, 1992; Sy Taylor, 2000; Peng & Dear, 2000) and the proportional odds cure model. Generalised estimating equations are proposed for parameter estimation. It is shown that the resulting estimators are asymptotically normal, with variance-covariance matrix that has a closed form and can be consistently estimated by the usual plug-in method. Simulation studies show that the proposed approach is appropriate for practical use. An application to data from a breast cancer study is given to illustrate the methodology."
"10.1093/biomet/91.2.321","2004","Analysis of longitudinal data in case-control studies","1","Case-control studies for longitudinal data are considered. Among repeated binary measurements of disease status in each subject, the exposure levels of risk factors for all diseased cases are identified and the exposure levels for only a small fraction of disease-free cases, to be regarded as controls, are identified. Case-control studies for longitudinal data bring about economies in cost and time when the disease is rare and when assessing the exposure level of risk factors is difficult. We propose a way of using an ordinary logistic model to analyse case-control longitudinal data. We prove that the proposed estimator is consistent and asymptotically normally distributed provided that the choice of control observations is independent of the covariates for those subjects. We also discuss the validity of the generalised estimating equation method for case-control longitudinal data. Simulation results are provided, and a real example is presented."
"10.1093/biomet/91.2.305","2004","Weighted estimating equations for semiparametric transformation models with censored data from a case-cohort design","2","In a case-cohort design introduced by Prentice (1986), covariates are assembled only for a subcohort randomly selected from the entire cohort, and any additional cases outside the subcohort. Semiparametric transformation models are considered here for failure time data from the case-cohort design. Weighted estimating equations are proposed for estimation of the regression parameters. The estimation procedure of survival probability at given covariate levels is also provided. Asymptotic properties are derived for the estimators using finite population sampling theory, U-statistics theory and martingale convergence results. The finite-sample properties of the proposed estimators, as well as the efficiency relative to the full cohort estimators, are assessed via simulation studies. A case-cohort dataset from the Atherosclerosis Risk in Communities study is used to illustrate the estimating procedure."
"10.1093/biomet/91.2.291","2004","Regression methods for gap time hazard functions of sequentially ordered multivariate failure time data","3","Sequentially ordered multivariate failure time data are often observed in biomedical studies and inter-event, or gap, times are often of interest. Generally, standard hazard regression methods cannot be applied to the gap times because of identifiability issues and induced dependent censoring. We propose estimating equations for fitting proportional hazards regression models to the gap times. Model parameters are shown to be consistent and asymptotically normal. Simulation studies reveal the appropriateness of the asymptotic approximations in finite samples. The proposed methods are applied to renal failure data to assess the association between demographic covariates and both time until wait-listing and time from wait-listing to kidney transplantation."
"10.1093/biomet/91.2.277","2004","Semiparametric regression analysis for doubly censored data","0","We analyse doubly censored data using semiparametric transformation models. We provide inference procedures for the regression parameters and derive the asymptotic distributions of the proposed estimators. Procedures for model checking and model selection are also discussed. We illustrate our approach with a viral-load dataset from a recent AIDS clinical trial."
"10.1093/biomet/91.2.263","2004","Sample-size formula for clustered survival data using weighted log-rank statistics","0","We present a simple sample-size formula for weighted log-rank statistics applied to clustered survival data with variable cluster sizes and arbitrary treatment assignments within clusters. This formula is based on the asymptotic normality of weighted log-rank statistics under certain local alternatives in the clustered data context. We also provide consistent variance estimators. The derived sample-size formula reduces to Schoenfeld's (1983) formula for cases of no clustering or independence within clusters. Simulation results verify control of the Type I error and accuracy of the sample-size formula. Use of the sample-size formula in an event-driven clinical trial design is illustrated using data from the Early Treatment Diabetic Retinopathy Study."
"10.1093/biomet/91.2.251","2004","Profile-kernel versus backfitting in the partially linear models for longitudinal/clustered data","5","We study the profile-kernel and backfitting methods in partially linear models for clustered/longitudinal data. For independent data, despite the potential root-n inconsistency of the backfitting estimator noted by Rice (1986), the two estimators have the same asymptotic variance matrix, as shown by Opsomer & Ruppert (1999). In this paper, theoretical comparisons of the two estimators for multivariate responses are investigated. We show that, for correlated data, backfitting often produces a larger asymptotic variance than the profile-kernel method; that is, for clustered data, in addition to its bias problem, the backfitting estimator does not have the same asymptotic efficiency as the profile-kernel estimator. Consequently, the common practice of using the backfitting method to compute profile-kernel estimates is no longer advised. We illustrate this in detail by following Zeger & Diggle (1994) and Lin & Carroll (2001) with a working independence covariance structure for nonparametric estimation and a correlated covariance structure for parametric estimation. Numerical performance of the two estimators is investigated through a simulation study. Their application to an ophthalmology dataset is also described."
"10.1093/biomet/91.1.246","2004","A multi-move sampler for estimating non-{G}aussian time series models. {C}omments on: ``{L}ikelihood analysis of non-{G}aussian measurement time series'' [{B}iometrika {\bf 84} (1997), no. 3, 653--667; MR1603940] by {N}.\ {S}hephard and {M}. {K}.\ {P}itt","0","This note points out a problem in the multi-move sampler as proposed by Shephard & Pitt (1997) and provides an alternative correct formulation."
"10.1093/biomet/91.1.240","2004","Revisiting simple linear regression with autocorrelated errors","0","This paper studies properties of ordinary and generalised least squares estimators in a simple linear regression with stationary autocorrelated errors. Explicit expressions for the variances of the regression parameter estimators are derived for some common time series autocorrelation structures, including a first-order autoregression and general moving averages. Applications of the results include confidence intervals and an example where the variance of the trend slope estimator does not increase with increasing autocorrelation."
"10.1093/biomet/91.1.234","2004","Compatibility among marginal densities","0","In the Lancaster representation a joint density is decomposed into a sum of additive interactions. Using these interactions, we derive conditions for checking compatibility among a collection of marginal densities. The representation also shows bow to construct an all-positive joint density additively from a given set of compatible marginals. An algorithm is proposed for reducing the dimension of the marginal densities so that compatibility can be checked in sequential increments. The representation may yield insights into the construction and simulation of models represented by undirected graphs."
"10.1093/biomet/91.1.226","2004","Boosting kernel density estimates: a bias reduction technique?","0","This paper proposes an algorithm for boosting kernel density estimates. We show that boosting is closely linked to a previously proposed method of bias reduction and indicate how it should enjoy similar properties. Numerical examples and simulations are used to illustrate the findings, and we also suggest further areas of research."
"10.1093/biomet/91.1.219","2004","Estimating genetic association parameters from family data","1","We consider the problem of estimating a parameter theta reflecting association between a disease and genotypes of a genetic polymorphism, using nuclear family data. In many applications, some parental genotypes are missing, and the distribution of these genotypes is unknown. Since misspecification of this distribution can bias estimators for theta, we consider estimating functions that are unbiased, regardless of how the distribution is specified. We call the resulting estimators parental-genotype-robust. Rabinowitz (2002) has proposed a constrained optimisation method for obtaining locally optimal unbiased tests of the null hypothesis of no association. We use a similar method to derive estimating functions that yield parental-genotype-robust estimators with minimum variance in the class of all such estimators. We extend the estimating functions to obtain parental-genotype-robust estimators when theta is a vector of unknown parameters, and show that the estimating functions enjoy a certain optimality property."
"10.1093/biomet/91.1.211","2004","Contiguity of the {W}hittle measure for a {G}aussian time series","3","For a stationary time series, Whittle constructed a likelihood for the spectral density based on the approximate independence of the discrete Fourier transforms of the data at certain frequencies. Whittle's likelihood has been widely used in the literature for constructing estimators. In this paper, we show that, for a Gaussian time series, the Whittle measure is mutually contiguous with the actual distribution of the data. As a consequence, most asymptotic properties of estimators and test statistics derived under the Whittle measure can be carried over to the actual distribution."
"10.1093/biomet/91.1.195","2004","Generalised likelihood ratio tests for spectral density","2","There are few techniques available for testing whether or not a family of parametric times series models fits a set of data reasonably well without serious restrictions on the forms of alternative models. In this paper, we consider generalised likelihood ratio tests of whether or not the spectral density function of a stationary time series admits certain parametric forms. We propose a bias correction method for the generalised likelihood ratio test of Fan et al. (2001). In particular, our methods can be applied to test whether or not a residual series is white noise. Sampling properties of the proposed tests are established. A bootstrap approach is proposed for estimating the null distribution of the test statistics. Simulation studies investigate the accuracy of the proposed bootstrap estimate and compare the power of the various ways of constructing the generalised likelihood ratio tests as well as some classic methods like the Cramer-von Mises and Ljung-Box tests. Our results favour the newly proposed bias reduction method using the local likelihood estimator."
"10.1093/biomet/91.1.177","2004","Equivalent kernels of smoothing splines in nonparametric regression for clustered/longitudinal data","12","For independent data, it is well known that kernel methods and spline methods are essentially asymptotically equivalent (Silverman, 1984). However, recent work of Welsh et al. (2002) shows that the same is not true for clustered/longitudinal data. Splines and conventional kernels are different in localness and ability to account for the within-cluster correlation. We show that a smoothing spline estimator is asymptotically equivalent to a recently proposed seemingly unrelated kernel estimator of Wang (2003) for any working covariance matrix. We show that both estimators can be obtained iteratively by applying conventional kernel or spline smoothing to pseudo-observations. This result allows us to study the asymptotic properties of the smoothing spline estimator by deriving its asymptotic bias and variance. We show that smoothing splines are consistent for an arbitrary working covariance and have the smallest variance when assuming the true covariance. We further show that both the seemingly unrelated kernel estimator and the smoothing spline estimator are nonlocal unless working independence is assumed but have asymptotically negligible bias. Their finite sample performance is compared through simulations. Our results justify the use of efficient, non-local estimators such as smoothing splines for clustered/longitudinal data."
"10.1093/biomet/91.1.165","2004","A comparison of sequential and non-sequential designs for discrimination between nested regression models","0","Classical regression analysis is usually performed in two steps. In a first step an appropriate model is identified to describe the data-generating process and in a second step statistical inference is performed in the identified model. In this paper we investigate a sequential and a non-sequential design strategy, which take into account these different goals of the analysis for a class of nested models. It is demonstrated that non-sequential designs usually identify the 'correct' model with a higher probability than sequential methods, Although non-sequential designs can never be guaranteed to achieve the best possible efficiency in the 'correct' model, it is demonstrated by means of a simulation study that for realistic sample sizes the efficiencies of the non-sequential designs for the estimation of the parameters in the 'correct' model are at least as high as the corresponding efficiencies of the sequential methods."
"10.1093/biomet/91.1.153","2004","Design sensitivity in observational studies","8","Outside the field of statistics, the literature on observational studies offers advice about research designs or strategies for judging whether or not an association is causal, such as multiple operationalism or a dose-response relationship. These useful suggestions are typically informal and qualitative. A quantitative measure, design sensitivity, is proposed for measuring the contribution such strategies make in distinguishing causal effects from hidden biases. Several common strategies are then evaluated in terms of their contribution to design sensitivity. A related method for computing the power of a sensitivity analysis is also developed."
"10.1093/biomet/91.1.141","2004","On identification of multi-factor models with correlated residuals","6","We specify some conditions for the identification of a multi-factor model with correlated residuals, uncorrelated factors and zero restrictions in the factor loadings. These conditions are derived from the results of Stanghellini (1997) and Vicard (2000) which deal with single-factor models with zero restrictions in the concentration matrix. Like these authors, we make use of the complementary graph of residuals and the conditions build on the role of odd cycles in this graph. However, in contrast to these authors, we consider the case where the conditional dependencies of the residuals are expressed in terms of a covariance matrix rather than its inverse, the concentration matrix. We first derive the corresponding condition for identification of single-factor models with structural zeros in the covariance matrix of the residuals. This is extended to the case where some factor loadings are constrained to be zero. We use these conditions to obtain a sufficient and a necessary condition for identification of multi-factor models."
"10.1093/biomet/91.1.125","2004","Data-informed influence analysis","1","The likelihood-based influence analysis methodology introduced in Cook (1986) uses a parameterised space of local perturbations of a base model. It is frequently the case that such perturbation schemes involve more parameters of interest and perturbation parameters than there are observations, and hence the perturbation space is often explored rather than estimated, where exploration means discovering the effect on inference of putatively choosing values of perturbation parameters. This paper considers the question of what can be learned about the perturbation parameters through the data. It extends Cook's methodology to take account of information available in the data regarding the perturbations, the general philosophy of the approach being that of learn what you can and explore what you cannot learn. Both local and global analyses are possible, as indicated by the data, while the eigenvector sign indeterminacy of local analysis is removed. Numerical examples are given and further developments are briefly indicated."
"10.1093/biomet/91.1.113","2004","Testing for multimodality with dependent data","0","We propose a test for multimodality with dependent data by resampling from a suitably constructed transition probability kernel, which includes Silverman's test with Independent data as a special case. We extend some theoretical properties of Silverman's test with independent and identically distributed data to weakly dependent data, and also discuss the robustness of Silverman's test against departure from independence."
"10.1093/biomet/91.1.95","2004","Small-area estimation based on natural exponential family quadratic variance function models and survey weights","3","We propose pseudo empirical best linear unbiased estimators of small-area means based on natural exponential family quadratic variance function models when the basic data consist of survey-weighted estimators of these means, area-specific covariates and certain summary measures involving the weights. We also provide explicit approximate mean squared errors of these estimators in the spirit of Prasad & Rao (1990), and these estimators can be readily evaluated. A simulation study is undertaken to evaluate the performance of the proposed inferential procedure. We estimate also the proportion of poor children in the 5-17 years age-group for the different counties in one of the states in the United States."
"10.1093/biomet/91.1.81","2004","Statistical inference for infinite-dimensional parameters via asymptotically pivotal estimating functions","2","Suppose that a consistent estimator for an infinite-dimensional parameter can be readily obtained via a set of estimating functions which has a 'good' local linear approximation around the true value of the parameter. However, it may be difficult to estimate the variance function of this estimator well. We show that, if the set of estimating functions evaluated at the true parameter value is 'asymptotically pivotal', then the 'fiducial' distribution of the parameter can be used to approximate the distribution of this consistent estimator. We present three examples to illustrate that the corresponding inference for the parameter can be made via a simple simulation technique without involving complex, high-dimensional nonparametric density estimates."
"10.1093/biomet/91.1.65","2004","Quasi-variances","0","In statistical models of dependence, the effect of a categorical variable is typically described by contrasts among parameters. For reporting such effects, quasi-variances provide an economical and intuitive method which permits approximate inference on any contrast by subsequent readers. Applications include generalised linear models, generalised additive models and hazard models. The present paper exposes the generality of quasi-variances, emphasises the need to control relative errors of approximation, gives simple methods for obtaining quasi-variances and bounds on the approximation error involved, and explores the domain of accuracy of the method. Conditions are identified under which the quasi-variance approximation is exact, and numerical work indicates high accuracy in a variety of settings."
"10.1093/biomet/91.1.45","2004","Bayesian criterion based model assessment for categorical data","0","We propose a general Bayesian criterion for model assessment for categorical data called the weighted L measure, which is constructed from the posterior predictive distribution of the data. The measure is based on weighting the observations according to the sampling variance of their future response vector. The weight component in the weighted L measure plays the role of a penalty term in the criterion, in which a greater weight assigned to covariate values implies a greater penalty term on the dimension of the model. A detailed justification is provided for such a weighting procedure and several theoretical properties of the weighted L measure are presented for a wide variety of discrete data models. For these models, we examine properties of the weighted L measure, and show that it can perform better than the unweighted L measure in a variety of settings. In addition, we show that the weighted quadratic loss L measure is more attractive than the unweighted L measure and the deviance loss L measure for categorical data. Moreover, a calibration for the weighted L measure is motivated and proposed, which allows us to compare formally the L measure values of competing models. A detailed simulation study is presented to examine the performance of the weighted L measure, and it is compared to other established model-selection methods. Finally, the method is applied to a real dataset using a bivariate ordinal response model."
"10.1093/biomet/91.1.27","2004","Bayesian information criteria and smoothing parameter selection in radial basis function networks","3","By extending Schwarz's (1978) basic idea we derive a Bayesian information criterion which enables us to evaluate models estimated by the maximum penalised likelihood method or the method of regularisation. The proposed criterion is applied to the choice of smoothing parameters and the number of basis functions in radial basis function network models. Monte Carlo experiments were conducted to examine the performance of the nonlinear modelling strategy of estimating the weight parameters by regularisation and then determining the adjusted parameters by the Bayesian information criterion. The simulation results show that our modelling procedure performs well in various situations."
"10.1093/biomet/91.1.15","2004","Equivalence of prospective and retrospective models in the {B}ayesian analysis of case-control studies","2","The natural likelihood to use for a case-control study is a 'retrospective' likelihood, i.e. a likelihood based on the probability of exposure given disease status. Prentice & Pyke (1979) showed that, when a logistic regression form is assumed for the probability of disease given exposure, the maximum likelihood estimators and asymptotic covariance matrix of the log odds ratios obtained from the retrospective likelihood are the same as those obtained from the 'prospective' likelihood, i.e. that based on probability of disease given exposure. We prove a similar result for the posterior distribution of the log odds ratios in a Bayesian analysis. This means that the Bayesian analysis of case-control studies may be done using a relatively simple model, the logistic regression model, which treats data as though generated prospectively and which does not involve nuisance parameters for the exposure distribution."
"10.1093/biomet/91.1.1","2004","Bayesian correlation estimation","1","We propose prior probability models for variance-covariance matrices in order to address two important issues. First, the models allow a researcher to represent substantive prior information about the strength of correlations among a set of variables. Secondly, even in the absence of such information, the increased flexibility of the models mitigates dependence on strict parametric assumptions in standard prior models. For example, the model allows a posteriori different levels of uncertainty about correlations among different subsets of variables. We achieve this by including a clustering mechanism in the prior probability model. Clustering is with respect to variables and pairs of variables. Our approach leads to shrinkage towards a mixture structure implied by the clustering. We discuss appropriate posterior simulation schemes to implement posterior inference in the proposed models, including the evaluation of normalising constants that are functions of parameters of interest. The normalising constants result from the restriction that the correlation matrix be positive definite. We discuss examples based on simulated data, a stock return dataset and a population genetics dataset."
"10.1093/biomet/90.4.991","2003","A note on: ``{T}esting the number of components in a normal mixture'' [{B}iometrika {\bf 88} (2001), no. 3, 767--778; MR1859408] by {Y}. {L}o, {N}. {R}. {M}endell, and {D}. {B}. {R}ubin","1","In a recent paper, Lo et al. (2001) propose a test for the likelihood ratio statistic based on the Kullback-Leibler information criterion when testing the null hypothesis that a random sample is drawn from a mixture of k(0) normal components against the alternative hypothesis of a mixture with k(1) normal components with k(0) less than k(1) However, this result requires conditions that are generally not met when the null hypothesis holds. Consequently, the result is not proven and simulations suggest that it may not be correct."
"10.1093/biomet/90.4.985","2003","A note on methods of restoring consistency to the bootstrap","3","We consider the property of consistency and its relevance for determining the performance of the bootstrap. We analyse various parametric bootstrap approximations to the distributions of the Hodges and Stein estimators, whose behaviour is typical of that of super-efficient estimators employed in wavelet regression, kernel density estimation and nonparametric curve fitting. Our results reveal not only some of the difficulties in selecting good modifications to the intuitive bootstrap, but also that inconsistent bootstrap approximations may perform better than consistent versions even in large samples."
"10.1093/biomet/90.4.982","2003","Conditional and marginal association for binary random variables","0","The relationship between marginal and conditional distributions of binary random variables is analysed via a log-linear model. Conditions for the Yule-Simpson effect are established and the implications for latent class analysis examined."
"10.1093/biomet/90.4.976","2003","Conditional likelihood inference under complex ascertainment using data augmentation","0","In many applications, particularly in genetics, samples are drawn under complex ascertainment rules. For example, families may only be selected for study if two or more siblings have trait values exceeding some threshold. The correct likelihood for inference in such situations involves the probabilities of ascertainment, and these are frequently intractable. A consistent, but not fully efficient, method of analysis of such studies is proposed. The main idea is to augment the data with additional pseudo-observations simulated under the ascertainment scheme, and to analyse using a conditional likelihood for discrimination between true observations and pseudo-observations. Ascertainment probabilities cancel in this likelihood. The method is illustrated with a simple example involving left-truncated failure times."
"10.1093/biomet/90.4.967","2003","Least absolute deviations estimation for {ARCH} and {GARCH} models","6","Hall & Yao (2003) showed that, for ARCH/GARCH, i.e. autoregressive conditional heteroscedastic/generalised autoregressive conditional heteroscedastic, models with heavy-tailed errors, the conventional maximum quasilikelihood estimator suffers from complex limit distributions and slow convergence rates. In this paper three types of absolute deviations estimator have been examined, and the one based on logarithmic transformation turns out to be particularly appealing. We have shown that this estimator is asymptotically normal and unbiased. Furthermore it enjoys the standard convergence rate of n(1/2) regardless of whether the errors are heavy-tailed or not. Simulation lends further support to our theoretical results."
"10.1093/biomet/90.4.953","2003","Asymptotic distributions of principal components based on robust dispersions","0","Algebraically, principal components can be defined as the eigenvalues and eigenvectors of a covariance or correlation matrix, but they are statistically meaningful as successive projections of the multivariate data in the direction of maximal variability. An attractive alternative in robust principal component analysis is to replace the classical variability measure, i.e. variance, by a robust dispersion measure. This projection-pursuit approach was first proposed in Li & Chen (1985) as a method of constructing a robust scatter matrix. Recent unpublished work of C. Croux and A. Ruiz-Gazen provided the influence functions of the resulting principal components. The present paper focuses on the asymptotic distributions of robust principal components. In particular, we obtain the asymptotic normality of the principal components that maximise a robust dispersion measure. We also explain the need to use a dispersion functional with a continuous influence function."
"10.1093/biomet/90.4.937","2003","Optimal calibration estimators in survey sampling","2","We show that the model-calibration estimator for the finite population mean, which was proposed by Wu & Sitter (2001) through an intuitive argument, is optimal among a class of calibration estimators. We also present optimal calibration estimators for the finite population distribution function, the population variance, the variance of a linear estimator and other quadratic finite population functions under a unified framework. The proposed calibration estimators are optimal under the true model but remain design consistent even if the working model is misspecified. A limited simulation study shows that the improvement of these optimal estimators over the conventional ones can be substantial. The question of when and how auxiliary information can be used for both the estimation of the population mean using a generalised regression estimator and the estimation of its variance through calibration is addressed clearly under the proposed general methodology. Some fundamental issues in using auxiliary information from survey data are also addressed in the context of optimal estimation."
"10.1093/biomet/90.4.923","2003","Choosing sample size for a clinical trial using decision analysis","2","Consider designing a clinical trial in stages, with two treatments and N exchangeable patients to be treated. Responses are dichotomous. The problem is to decide how large each stage should be and how many patients should be assigned to each treatment during each stage. Information is updated during and after each stage using Bayes' theorem. In planning stage j, responses from selections in stages I to j - I are available, but interim responses in stage j are not available. Our analytical results consider two stages for two scenarios, when one treatment arm is known and when both treatment arms are unknown. The dominant term for the length of the first stage in an optimal design for general N is found explicitly. In both scenarios the order of magnitude of the length of the first stage is the square root of N. The finite-N performance of asymptotically optimal allocation is compared with that of the true optimal allocation. Our numerical study also shows that, for a trial of three stages with one known arm, the optimal first-stage sample size is asymptotically proportional to the cube root of N."
"10.1093/biomet/90.4.913","2003","Testing the proportional odds model under random censoring","0","In practical applications, it is not uncommon for the hazard functions of two groups to converge with time. One approach that allows for converging hazard functions is the proportional odds model. We develop a procedure for testing the proportional odds assumption when the available data consist of two independent random samples of randomly right-censored lifetimes. Asymptotic normality of the test statistic is proved and the procedure is applied to two well-known datasets. The effective significance level and power of the proposed test are assessed through a simulation study."
"10.1093/biomet/90.4.899","2003","Martingale difference residuals as a diagnostic tool for the {C}ox model","0","The proportional hazards model makes two major assumptions: the hazard ratio is constant over time, and the relationship between the hazard and continuous covariates is log-linear. Methods exist for checking and relaxing each of these assumptions, but in both cases the methods rely on the other assumption being true. Problems can occur if neither of the assumptions is appropriate, or even if only one of the assumptions is appropriate but it is not known which. We propose a new kind of residual for checking the two assumptions simultaneously. The smoothed residuals provide a flexible estimate of the hazard ratio, which may deviate from the standard proportional hazards model by having a time-dependent hazard ratio, transformed covariates or both. The methods are illustrated using data from the Medical Research Council's myeloma trials."
"10.1093/biomet/90.4.891","2003","Minimum aberration construction results for nonregular two-level fractional factorial designs","6","Nonregular two-level fractional factorial designs are designs which cannot be specified in terms of a set of defining contrasts. The aliasing properties of nonregular designs can be compared by using a generalisation of the minimum aberration criterion called minimum G(2)-aberration. Until now, the only nontrivial designs that are known to have minimum G(2)-aberration are designs for n runs and m greater than or equal to n - 5 factors. In this paper, a number of construction results are presented which allow minimum G(2)-aberration designs to be found for many of the cases with n = 16, 24, 32, 48, 64 and 96 runs and m greater than or equal to n/2 - 2 factors."
"10.1093/biomet/90.4.881","2003","Second-order power comparisons for a class of nonparametric likelihood-based tests","3","This paper compares the second-order power properties of a broad class of nonparametric likelihood tests recently introduced by Baggerly (1998) as a generalisation of Owen's (1988) empirical likelihood. It is shown that in a multi-parameter setting identity of power up to first order does not imply identity up to second order unless one considers the average power criterion. It is also shown that the empirical likelihood ratio enjoys an optimality property in terms of local maximinity."
"10.1093/biomet/90.4.859","2003","A hybrid estimator in nonlinear and generalised linear mixed effects models","1","A hybrid method that combines Laplace's approximation and Monte Carlo simulations to evaluate integrals in the likelihood function is proposed for estimation of the parameters in nonlinear mixed effects models that assume a normal parametric family for the random effects. Simulations show that these parametric estimates of fixed effects are close to the nonparametric estimates even though the mixing distribution is far from the assumed normal parametric family. An asymptotic theory of this hybrid method for parametric estimation without requiring the true mixing distribution to belong to the assumed parametric family is developed to explain these results. This hybrid method and its asymptotic theory are also extended to generalised linear mixed effects models."
"10.1093/biomet/90.4.845","2003","Adjusted profile estimating function","0","In settings where the full probability model is not specified, consider a general estimating function g(theta, (&lambda;) over cap (theta); y) that involves not only the parameters of interest, theta, but also some nuisance parameters, lambda. We consider methods for reducing the effects on g of fitting nuisance parameters. We propose Cox-Reid-type adjustment to the profile estimating function, g(theta, (&lambda;) over cap (theta); y), that reduces its bias by two orders. Typically, only the first two moments of the response variable are needed to form the adjustment. Important applications of this method include the estimation of the pairwise association and main effects in stratified, clustered data and estimation of the main effects in a matched pair study. A brief simulation study shows that the proposed method considerably reduces the impact of the nuisance parameters."
"10.1093/biomet/90.4.831","2003","Nonparametric estimation of large covariance matrices of longitudinal data","16","Estimation of an unstructured covariance matrix is difficult because of its positive-definiteness constraint. This obstacle is removed by regressing each variable on its predecessors, so that estimation of a covariance matrix is shown to be equivalent to that of estimating a sequence of varying-coefficient and varying-order regression models. Our framework is similar to the use of increasing-order autoregressive models in approximating the covariance matrix or the spectrum of a stationary time series. As an illustration, we adopt Fan & Zhang's (2000) two-step estimation of functional linear models and propose nonparametric estimators of covariance matrices which are guaranteed to be positive definite. For parsimony a suitable order for the sequence of (auto) regression models is found using penalised likelihood criteria like AIC and BIC. Some asymptotic results for the local polynomial estimators of components of a covariance matrix are established. Two longitudinal datasets are analysed to illustrate the methodology. A simulation study reveals the advantage of the nonparametric covariance estimator over the sample covariance matrix for large covariance matrices."
"10.1093/biomet/90.4.809","2003","Efficient estimation of covariance selection models","11","A Bayesian method is proposed for estimating an inverse covariance matrix from Gaussian data. The method is based on a prior that allows the off-diagonal elements of the inverse covariance matrix to be zero, and in many applications results in a parsimonious parameterisation of the covariance matrix. No assumption is made about the structure of the corresponding graphical model, so the method applies to both nondecomposable and decomposable graphs. All the parameters are estimated by model averaging using an efficient Metropolis-Hastings sampling scheme. A simulation study demonstrates that the method produces statistically efficient estimators of the covariance matrix, when the inverse covariance matrix is sparse. The methodology is illustrated by applying it to three examples that are high-dimensional relative to the sample size."
"10.1093/biomet/90.4.791","2003","Exponential functionals and means of neutral-to-the-right priors","4","The mean of a random distribution chosen from a neutral-to-the-right prior can be represented as the exponential functional of an increasing additive process. This fact is exploited in order to give sufficient conditions for the existence of the mean of a neutral-to-the-right prior and for the absolute continuity of its probability distribution. Moreover, expressions for its moments, of any order, are provided. For illustrative purposes we consider a generalisation of the neutral-to-the-right prior based on the gamma process and the beta-Stacy process. Finally, by resorting to the maximum entropy algorithm, we obtain an approximation to the probability density function of the mean of a neutral-to-the-right prior. The arguments are easily extended to examine means of posterior quantities. The numerical results obtained are compared to those yielded by the application of some well-established simulation algorithms."
"10.1093/biomet/90.4.777","2003","Observation-driven models for {P}oisson counts","1","This paper is concerned with a general class of observation-driven models for time series of counts whose conditional distributions given past observations and explanatory variables follow a Poisson distribution. These models provide a flexible framework for modelling a wide range of dependence structures. Conditions for stationarity and ergodicity of these processes are established from which the large-sample properties of the maximum likelihood estimators can be derived. Simulations are provided to give additional insight into the finite-sample behaviour of the estimators. Finally an application to a regression model for daily counts of asthma presentations at a Sydney hospital is described."
"10.1093/biomet/90.4.765","2003","Matching conditional and marginal shapes in binary random intercept models using a bridge distribution function","0","Random effects logistic regression models are often used to model clustered binary response data. Regression parameters in these models have a conditional, subject-specific interpretation in that they quantify regression effects for each cluster. Very often, the logistic functional shape conditional on the random effects does not carry over to the marginal scale. Thus, parameters in these models usually do not have an explicit marginal, population-averaged interpretation. We study a bridge distribution function for the random effect in the random intercept logistic regression model. Under this distributional assumption, the marginal functional shape is still of logistic form, and thus regression parameters have an explicit marginal interpretation. The main advantage of this approach is that likelihood inference can be obtained for either marginal or conditional regression inference within a single model framework. The generality of the results and some properties of the bridge distribution functions are discussed. An example is used for illustration."
"10.1093/biomet/90.4.747","2003","Analysis of multivariate missing data with nonignorable nonresponse","3","We consider multivariate regression analysis with missing data in the outcome variables, when the nonresponse mechanism depends on the underlying values of the responses and hence is nonignorable. Related problems include response-biased sampling where data are sampled with probability depending only on the univariate response. Our methods do not require specification of the form of the nonresponse mechanism. We show that, under certain regularity conditions, all the regression parameters can be identified from a conditional likelihood based on the complete cases, if the marginal distribution of the covariates is known. If the marginal distribution of the covariates is estimated from the data, then the regression parameters are identified from a pseudolikelihood resulting from substituting the estimated marginal distribution of the covariates in the above conditional likelihood. Simulation studies suggest that the pseudolikelihood method is approximately unbiased. In order to identify the model parameters, usually the dimension of the covariates and observed responses is required to be at least as large as the dimension of the missing responses. The method can also be modified to handle partial information about the missing-data mechanism. We also consider the special case where the missing data have a monotone pattern, where better use of the incomplete information can be made under certain assumptions."
"10.1093/biomet/90.3.741","2003","Robust variance estimation for rate ratio parameter estimates from individually matched case-control data","0","The asymptotic variance and robust variance estimators of rate ratios estimated using conditional logistic regression from individually-matched case-control data are derived when the presumed proportional hazards model is misspecified. The robust variance estimators are easily computed using Schoenfeld residuals generated from standard partial likelihood estimation software for failure time data. Simulation studies indicate that the robust variance estimators perform well for typical sizes and that the 'rare disease' version should be adequate for all practical purposes. It was also found that model misspecification must be quite extreme before the model-based, i.e. inverse information, variance is significantly biased and that the robust variance estimators are somewhat more variable than the model-based. We conclude that the model-based variance estimator can be used when model misspecification is not severe. The robust estimator should be used when the presumed model clearly fits the data poorly."
"10.1093/biomet/90.3.732","2003","Rank-based regression with repeated measurements data","1","A rank-based regression method is proposed for repeated measurements data. It is a generalisation of the classical Wilcoxon-Mann-Whitney rank statistic for independent observations. The method is valid under a weak condition on the error terms that can accommodate certain heteroscedasticity and within-subject dependency. The asymptotic normality of the proposed estimator is proved using empirical process theory. A variance estimator, shown to be consistent, is also constructed. The proposed method is illustrated using data from a clinical trial on treating labour pain. Robustness and efficiency of the estimator is demonstrated in simulation studies."
"10.1093/biomet/90.3.728","2003","Studies in the history of probability and statistics. {XLVIII}. {T}he {B}ayesian contributions of {E}rnest {L}hoste","0","The contributions of Ernest Lhoste are largely unknown outside France, and even within that country are not well known. His important contributions were in the two areas of the development of prior distributions that represent little or no information, and a sophisticated posterior analysis for normal and binomial populations. His results are similar to those of Haldane (1948) and Jeffreys (1961), but they appeared much earlier, in 1923. He gave a great deal of thought to how to represent vague prior knowledge and his results represent a significant and unique contribution to Bayesian ideas in the early part of the 20th century."
"10.1093/biomet/90.3.724","2003","Identifiability and censored data","1","It is well known that, without the assumption of independence between two nonnegative random variables X and Y, the survival function of X is not identifiable on the basis of the joint distribution function of Z = min(X, Y) and delta = I(Z = Y). In this paper, we provide a simple condition in the form of conditional distribution of Y given X. We show that our condition is equivalent to the constant-sum condition proposed by Williams & Lagakos (1977). As a result the survival function of X can be identified from the joint distribution of Z and delta and the Kaplan-Meier estimator with Greenwood's formula for its variance remains valid. Examples which satisfy the condition are given."
"10.1093/biomet/90.3.717","2003","Estimating subject-specific survival functions under the accelerated failure time model","7","We use the semiparametric accelerated failure time model to predict the survival function and its related quantities for future subjects with a given set of covariates. We derive the large-sample distribution for the subject-specific cumulative hazard function estimate. We then propose a simple resampling technique for constructing pointwise confidence intervals and simultaneous bands for the corresponding survival function and its quantile function over a properly selected time interval. The new proposals are illustrated with the Mayo primary billary cirrhosis data."
"10.1093/biomet/90.3.703","2003","Testing and estimation of thresholds based on wavelets in heteroscedastic threshold autoregressive models","0","We consider the testing and estimation of thresholds in heteroscedastic threshold auto-regressive models with an unknown number of thresholds. A test statistic based on empirical wavelet coefficients is proposed. The asymptotic distribution of the test statistic is established and consistent estimators of the thresholds and the number of thresholds are given. A Monte Carlo study and a real example are used to assess the performance of our method."
"10.1093/biomet/90.3.679","2003","Principal component models for correlation matrices","0","Distributional theory regarding principal components is less well developed for correlation matrices than it is for covariance matrices. The intent of this paper is to reduce this disparity. Methods are proposed that enable investigators to fit and to make inferences about flexible principal components models for correlation matrices. The models allow arbitrary eigenvalue multiplicities and allow the distinct eigenvalues to be modelled parametrically or nonparametrically. Local parameterisations and implicit functions are used to construct full-rank unconstrained parameterisations. First-order asymptotic distributions are obtained directly from the theory of estimating functions. Second-order accurate distributions for making inferences under normality are obtained directly from likelihood theory. Simulation studies show that the Bartlett correction is effective in controlling the size of the tests and that first-order approximations to nonnull distributions are reasonably accurate. The methods are illustrated on a dataset."
"10.1093/biomet/90.3.669","2003","Assessing landmark influence on shape variation","0","Given two sets of landmark data which differ in shape, it is useful to determine the extent to which shape variation can be explained by the perturbations of individual landmarks. We propose a method for assessing this, based on analysing the relative reduction in distance between the shapes that can be achieved by varying the location of a single landmark. This method is applied to a set of landmark data from the cervical vertebrae of two subspecies of gorillas."
"10.1093/biomet/90.3.655","2003","Spherical regression","1","Methods are introduced for regressing points on the surface of one sphere on points on another. Complex variables and stereographic projection are used to deal with theoretical problems of directional statistics much as they have been used historically to deal with problems in non-Euclidean geometry. The complex plane harbours the group of MobiuS transformations, and stereographic projection is used as a bridge to map these Mobius transforms to regression link functions on the surface of a unit sphere. A special form for these links is introduced which employs the complex plane and stereographic projection to effect angular scale changes on the sphere. The family of special forms is closed under orthogonal transformations of the dependent variable and Mobius transformations of the independent variable, and incorporates independence and proper and improper rotations as special cases. Parameter estimation and inference are exemplified using the von Mises-Fisher spherical distribution and vectorcardiogram data. All statistical results and calculations have been formulated in the real domain."
"10.1093/biomet/90.3.643","2003","A multiple-imputation {M}etropolis version of the {EM} algorithm","0","In this paper we introduce a new stochastic variant of the EM algorithm. The algorithm combines the principle of multiple imputation and the theory of simulated annealing to deal with cases where the E-step and the M-step can be intractable or numerically inefficient."
"10.1093/biomet/90.3.629","2003","A {B}ayesian justification of {C}ox's partial likelihood","2","In this paper, we establish both naive and formal Bayesian Justifications of Cox's ( 1975) partial likelihood and its various modifications. We extend the original work of Kalbfieisch (1978), who showed that the partial likelihood is a limiting marginal posterior under noninformative priors for baseline hazards. We extend the result to scenarios with time-dependent covariates and time-varying regression parameters. We establish results for continuous time as well as grouped survival data. In addition, we present a Bayesian justification of a modified partial likelihood for handling ties. We also present tools for simplification of the Gibbs sampling algorithm for implementing partial likelihood based Bayeslan inference in various practical applications."
"10.1093/biomet/90.3.613","2003","Bayesian inference for {M}arkov processes with diffusion and discrete components","1","Data arising in certain radio-tracking experiments consist of both a continuous spatial component and a discrete component related to behaviour. This leads naturally to stochastic models with a state space which is a product of continuous and discrete components. We consider a class of such models in continuous time, which can be thought of as diffusions in random environments. They are related to switching diffusion or hidden Markov models, but observations are made on both components at discrete time points, so that neither component is completely 'hidden'. We describe and illustrate an approach to fully Bayesian inference for these general models. The algorithm used is a hybrid Markov chain Monte Carlo method. The diffusion parameters, the environment parameters and the sample path of the environment process itself are updated separately, in sequence, and the individual steps are a mixture of Gibbs and random walk Metropolis-Hastings types. Some implementation and model checking issues are discussed, and an example using data arising from a radio-tracking experiment is described."
"10.1093/biomet/90.3.597","2003","Inference about a secondary process following a sequential trial","1","We consider the following sequential testing problem. A group-sequential or fully-sequential test is carried out for a primary parameter, using a score process or an effective score process to eliminate nuisance parameters. After stopping, the possibility of additional parameters is considered, and appropriate tests and estimators are desired that recognise the sequential stopping rule. We formulate an asymptotic multi-dimensional Gaussian process form of such problems, and then construct tests and confidence procedures. Optimality conditions are given, and an example is summarised."
"10.1093/biomet/90.3.585","2003","Using logistic regression procedures for estimating receiver operating characteristic curves","1","Estimation of a receiver operating characteristic, ROC, curve is usually based either on a fully parametric model such as a normal model or on a fully nonparametric model. In this paper, we explore a semiparametric approach by assuming a density ratio model for disease and disease-free densities. This model has a natural connection with the logistic regression model. The proposed semiparametric approach is more robust than a fully parametric approach and is more efficient than a fully nonparametric approach. Two real examples demonstrate that the ROC curve estimated by our semiparametric method is much smoother than that estimated by the nonparametric method."
"10.1093/biomet/90.3.577","2003","Khmaladze-type graphical evaluation of the proportional hazards assumption","0","Khmaladze-type goodness-of-fit tests are based on transforming an appropriate empirical process to one with a known large-sample distribution. Using Donsker's theorem, together with a theorem for proportional hazards generalising a theorem of Xu & O'Quigley (1999), we indicate how to construct a Khmaladze-type graphical test for evaluating the proportional hazards assumption. An illustration is given in which a partially proportional hazards model can be seen to provide a noticeably improved fit over a fully proportional hazards model."
"10.1093/biomet/90.3.567","2003","On the geometry of measurement error models","1","The problem of undertaking inference in the classical linear model when the covariates have been measured with error is investigated from a geometric point of view. Under the assumption that the measurement error is small, relative to the total variation in the data, a. new model is proposed which has good inferential properties. An inference technique which exploits the geometric structure is shown to be computationally simple, efficient and robust to measurement error. The method proposed is illustrated by simulation studies."
"10.1093/biomet/90.3.551","2003","Generalised structured models","4","We present a general class of nonlinear regression and time series models that we call generalised structured models. The class is a natural generalisation of generalised additive models, and it includes generalised interaction models, structured volatility models, visual GARCH, generalised autoregressive conditional heteroscedasticity, models and varying coefficient models. We discuss estimation principles including smoothing splines and a generalisation of the projection approach of Mammen et al. (1999). We finish the paper with some theoretical considerations about the asymptotic performance of the estimator for the general class of generalised structured models."
"10.1093/biomet/90.3.533","2003","Modified profile likelihoods in models with stratum nuisance parameters","4","It is well known, at least through many examples, that when there are many nuisance parameters modified profile likelihoods often perform much better than the profile likelihood. Ordinary asymptotics almost totally fail to deal with this issue. For this reason, we study asymptotic properties of the profile and modified profile likelihoods in models for stratified data in a two-index asymptotics setting. This means that both the sample size of the strata, in, and the dimension of the nuisance parameter, q, may increase to Infinity. It is shown that in this asymptotic setting modified profile likelihoods give improvements, with respect to the profile likelihood, in terms of consistency of estimators and of asymptotic distributional properties. In particular, the modified profile likelihood based statistics have the usual asymptotic distribution, provided that 1/m = o(q(-1/3)), while the analogous condition for the profile likelihood is 1/m = o(q(-1))."
"10.1093/biomet/90.3.517","2003","Conditioning to reduce the sensitivity of general estimating functions to nuisance parameters","0","A conditional method is presented that renders an estimating function insensitive to nuisance parameters. The approach is a generalisation of the conditional score method to a general estimating function context and does not require complete specification of the probability model. We exploit the informal relationship between general estimating functions and score functions to derive simple generalisations of sufficient and partially ancillary statistics, referred to as G-sufficient and G-ancillary statistics, respectively. These two types of statistic are defined in a manner that does not require complete knowledge of the probability model and thus are more suitable for use with estimating functions. If we condition on a G-sufficient statistic for the nuisance parameters, the resulting conditional estimating function is insensitive to nuisance parameters and in particular achieves the plug-in unbiasedness property. Furthermore, if the conditioning argument is also G-ancillary for the parameters of interest, then the conditional estimating function possesses an attractive optimality property."
"10.1093/biomet/90.3.491","2003","Uniform consistency in causal inference","5","There is a long tradition of representing causal relationships by directed acyclic graphs (Wright, 1934). Spirtes (1994), Spirtes et al. ( 1993) and Pearl & Verma (1991) describe procedures for inferring the presence or absence of causal arrows in the graph even if there might be unobserved confounding variables, and/or an unknown time order, and that under weak conditions, for certain combinations of directed acyclic graphs and probability distributions, are asymptotically, in sample size, consistent. These results are surprising since they seem to contradict the standard statistical wisdom that consistent estimators of causal effects do not exist for nonrandomised studies if there are potentially unobserved confounding variables. We resolve the apparent incompatibility of these views by closely examining the asymptotic properties of these causal inference procedures. We show that the asymptotically consistent procedures are 'pointwise consistent', but 'uniformly consistent' tests do not exist. Thus, no finite sample size can ever be guaranteed to approximate the asymptotic results. We also show the nonexistence of valid, consistent confidence intervals for causal effects and the nonexistence of uniformly consistent point estimators. Our results make no assumption about the form of the tests or estimators. In particular, the tests could be classical independence tests, they could be Bayes tests or they could be tests based on scoring methods such as BIC or AIC. The implications of our results for observational studies are controversial and are discussed briefly in the last section of the paper. The results hinge on the following fact: it is possible to find, for each sample size n, distributions P and Q such that P and Q are empirically indistinguishable and yet P and Q correspond to different causal effects."
"10.1093/biomet/90.2.489","2003","A counterexample to a claim about stochastic simulations","2","Engen & Lilleg (a) over circle rd (1997) presented a general method for doing Monte Carlo simulations conditioned on a sufficient statistic. The basic idea was to adjust the parameter values in the corresponding unconditional simulation so that the actual value of the sufficient statistic is obtained, and the claim was that if this adjustment is unique then the modified simulation is from the conditional distribution. Unfortunately the claim is not correct, as shown by a counterexample."
"10.1093/biomet/90.2.482","2003","On sufficient conditions for {B}ayesian consistency","5","This paper contributes to the theory of Bayesian consistency for a sequence of posterior and predictive distributions arising from an independent and identically distributed sample. A new sufficient condition for posterior Hellinger consistency is presented which provides motivation for recent results appearing in the literature. Such motivation is important since current sufficient conditions are not known to be necessary. It also provides new insights into Bayesian consistency. A new consistency theorem for the sequence of predictive densities is given."
"10.1093/biomet/90.2.478","2003","Copula model generated by {D}abrowska's association measure","0","We propose a new archimedean copula model for bivariate. survival data that is motivated by Dabrowska's (1988) measure of association. The model can represent negatively correlated or moderately positively correlated data but not highly positively correlated data. Local and global measures of association are calculated. A generalisation is presented."
"10.1093/biomet/90.2.471","2003","Estimating ordered binomial proportions with the use of group testing","1","This paper considers group testing when the probability of response is increasing across the levels of an observed covariate. We illustrate how previously known results in order-restricted inference can be extended to situations wherein data are collected according to a group-testing protocol, and we derive maximum likelihood estimators for proportions under the increasing order restriction and group-testing model. Finally, we show how the use of group testing can dramatically reduce the bias and mean squared error of isotonic regression estimators obtained from one-at-a-time testing. These proposed methods are illustrated using data from an observational HIV study conducted in Houston, Texas."
"10.1093/biomet/90.2.465","2003","Sufficient conditions for balanced incomplete block designs to be minimal fractional combinatorial treatment designs","0","Sufficient conditions are given for balanced incomplete block designs with block sizes three and four to be saturated minimal fractions of m items taken in mixture sizes of n = 3 and 4 for estimating the contrasts of item means and two-item specific mixing effects. Such fractions are useful for investigations involving mixtures of crops, drugs, marketing practices and other systems using mixtures of items. The balanced incomplete block design with parameters v = 6 and k = 4 is shown to be a saturated minimal fraction for estimating contrasts of item means and two-item and three-item specific mixing effects. This is a continuation of the work of Federer & Raghavarao (1987) and Federer (2001)."
"10.1093/biomet/90.2.455","2003","A family of multivariate binary distributions for simulating correlated binary variables with specified marginal means and correlations","4","We introduce a family of multivariate binary distributions with certain conditional linear property. This family is particularly useful for efficient and easy simulation of correlated binary variables with a given marginal mean vector and correlation matrix."
"10.1093/biomet/90.2.445","2003","The {W}ilson-{H}ilferty transformation is locally saddlepoint","1","Interest in transform methods for normalising test statistics declined with the advent of computers. More recently, small-sample asymptotic methods have been developed to approximate the distributions of complicated test statistics. We propose a generalisation of a classical symmetrising transform to a similar range of statistics. It is shown to behave comparably in a parametric neighbourhood to methods that exploit exponential tilting."
"10.1093/biomet/90.2.435","2003","Closed-form likelihoods for {A}rnason-{S}chwarz models","2","We provide a general framework for the computationally efficient analysis, both Bayesian and classical, of integrated multi-site recovery/recapture models in the presence of individual-level covariates by extending the basic Arnason-Schwarz models and deriving closed-form likelihood expressions, together with corresponding sufficient statistics."
"10.1093/biomet/90.2.423","2003","Measures for designs in experiments with correlated errors","2","In this paper we consider optimal design of experiments in the case of correlated observations. We use and further develop the concept of design measures introduced by Pazman & Milller (1998) for the construction of a simple, quick and elegant design algorithm. We support the construction of this algorithm for a general correlation structure by an interpretation in terms of norms. Examples demonstrate that our results are useful for generating exact designs by sampling from the obtained design measures."
"10.1093/biomet/90.2.411","2003","Weighted chi-squared tests for partial common principal component subspaces","2","We consider tests of the null hypothesis that g covariance matrices have a partial common principal component subspace of dimension s. Our approach uses a dimensionality matrix which has its rank equal to s when the hypothesis holds. The test can then be based on a statistic computed from the eigenvalues of an estimate of this dimensionality matrix. The asymptotic distribution of this' statistic is that of a linear combination of independent one-degree-of-freedom chi-squared random variables. Simulation results indicate that this test yields significance levels that come closer to the nominal level than do those of a previously proposed method. The procedure is also extended to a test that g correlation matrices have a partial common principal component subspace."
"10.1093/biomet/90.2.393","2003","Prepivoting by weighted bootstrap iteration","1","Prepivoting by conventional bootstrap iteration is known to yield a progressively more accurate pivot in certain problems, and has important application in the construction of confidence limits and estimation of null distributions. We investigate the theoretical effects of weighted bootstrap iteration on prepivoting and show that each weighted bootstrap iteration, with weights chosen carefully but empirically, is asymptotically equivalent to two consecutive conventional bootstrap iterations. In terms of reducing the order of error, prepivoting can therefore be carried out much more efficiently if based on weighted bootstrap iterations. This is shown for a variety of problem settings, including the smooth function model, M-estimation and the regression context. A numerical illustration is provided, demonstrating the potential practical usefulness of weighted prepivoting."
"10.1093/biomet/90.2.379","2003","Discriminant analysis through a semiparametric model","1","We consider a semiparametric generalisation of normal-theory discriminant analysis. The semiparametric model assumes that, after unspecified univariate monotone transformations, the class distributions are multivariate normal. We introduce an estimation procedure based on the distribution quantiles, in which the parameters of the sermparametric model are estimated directly without estimating the nonparametric transformations. The procedure is computationally fast and the estimation accuracy is shown to have the usual parametric rate. The relationship between the method and more general nonparametric discriminant analysis is discussed. The semiparametric specification of the class densities is a submodel of the nonparametric log density functional analysis of variance model in which the main effects are completely nonparametric but the interaction terms are specified semiparametrically. Simulations and real examples are used to illustrate the procedure."
"10.1093/biomet/90.2.367","2003","On the inefficiency of the adaptive design for monitoring clinical trials","4","Adaptive designs, which,allow the sample size to be modified based on sequentially computed observed treatment differences, have been advocated recently for monitoring clinical trials. Although such methods have a great deal of appeal on the surface, we show that such methods are inefficient and that one can improve uniformly on such adaptive designs using standard group-sequential tests based on the sequentially computed likelihood ratio test statistic."
"10.1093/biomet/90.2.355","2003","A serially correlated gamma frailty model for longitudinal count data","6","A Poisson-gamma model is introduced to account for between-subjects heterogeneity and within-subjects serial correlation occurring in longitudinal count data. The model extends the usual time-constant shared frailty approach to allow time-varying serially correlated gamma frailty whilst retaining standard marginal assumptions. A composite likelihood approach to estimation and testing for serial correlation is proposed. The work is motivated by a, clinical trial on patient-controlled analgesia where the number of analgesic doses taken, by hospital patients in successive time intervals following abdominal surgery is recorded."
"10.1093/biomet/90.2.341","2003","Rank-based inference for the accelerated failure time model","18","A broad class of rank-based monotone estimating functions is developed for the semi-parametric accelerated failure time model with censored observations. The corresponding estimators can be obtained via linear programming, and are shown to be consistent and asymptotically normal. The limiting covariance matrices can be estimated by a resampling technique, which does not involve nonparametric density estimation or numerical derivatives. The new estimators represent consistent roots of the non-monotone estimating equations based on the familiar weighted log-rank statistics. Simulation studies demonstrate that the proposed methods perform well in practical settings. Two real examples are provided."
"10.1093/biomet/90.2.327","2003","Likelihood for component parameters","5","a statistical model with data, likelihood for the scalar or vector full parameter theta, of dimension p say, is typically well defined and easily computed. In this paper, we investigate likelihood for a component parameter psi(theta) of dimension d < p and make use of the recent likelihood theory that has been successful in producing highly accurate third-order p-values for scalar parameters of continuous models. The theory leads under moderate regularity to a definitive third-order determination of likelihood for a component parameter psi(theta) of dimension d, where 1 less than or equal to d less than or equal to p. We use the simple location model on the plane with standard normal errors to motivate the development. The example exhibits most of the key characteristics of the general case and the recent theory then extends the determination of likelihood to the general context. For the scalar interest parameter case with d = 1, the usual determinations are- typically of second-order accuracy; the example indicates how the new determination achieves 'third-order accuracy. The implementation is straightforward and uses familiar ingredients to other determinations, such as the full maximum likelihood value (&theta;) over cap, the constrained value (&theta;) over tilde (psi) given psi(theta) = psi, and the observed information j(lambdalambda)((&theta;) over cap)(psi) for a complementing nuisance parameter lambda(theta). It does however require a special version of -the nuisance information j(lambdalambda)((&theta;) over cap (psi), a version calibrated relative to a symmetric choice of the exponential-type reparameterisation phi(theta) underlying the recent theory, but this is easily computed. Various examples are given and the motivating example is discussed in detail."
"10.1093/biomet/90.2.319","2003","Bayesian empirical likelihood","9","Research has shown that empirical likelihood tests have many of the same asymptotic properties as those derived from parametric likelihoods. This leads naturally to the possibility of using empirical likelihood as the basis for Bayesian inference. Different ways in which this goal might be accomplished are considered. The validity of the resultant posterior inferences is examined, as are frequentist properties of the Bayesian empirical likelihood intervals."
"10.1093/biomet/90.2.303","2003","Bayesian methods for partial stochastic orderings","2","We discuss two methods of making nonparametric Bayesian inference on probability measures subject to a partial stochastic ordering. The first method involves a nonparametric prior for a measure on partially ordered latent observations, and the second involves rejection sampling. Computational approaches are discussed for each method, and interpretations of prior and posterior information are discussed. An application is presented in which inference is made on the number of independently segregating quantitative trait loci present in an animal population."
"10.1093/biomet/90.2.289","2003","Fully {B}ayesian spline smoothing and intrinsic autoregressive priors","3","There is a well-known Bayesian interpretation for function estimation by spline smoothing using a limit of proper normal priors. The limiting prior and the conditional and intrinsic autoregressive priors popular for spatial modelling have a common form, which we call partially informative normal. We derive necessary and sufficient conditions for the propriety of the posterior for this class of partially informative normal priors with noninformative priors on the variance components, a condition crucial for successful implementation of the Gibbs sampler. The results apply for fully Bayesian smoothing splines, thin-plate splines and L-splines, as well as models using intrinsic autoregressive priors."
"10.1093/biomet/90.2.269","2003","Nonparametric analysis of covariance for censored data","1","The fully nonparametric model for nonlinear analysis of covariance, proposed in Akritas et al. (2000), is considered in the context of censored observations. Under this model, the distributions for each factor level combination and covariate value are not restricted to comply to any parametric or semiparametric model. The data can be continuous or ordinal categorical. The possibility of different shapes of covariate effect in different factor level combinations is also allowed. This generality is useful whenever modelling. assumptions such as additive risks, proportional hazards or proportional odds appear suspect. Test statistics are obtained for the nonparametric hypotheses of no main effect and of no interaction effect which adjusts for the presence of a covariate. They are quadratic forms based on averages over-the covariate values of Beran estimators of the conditional distribution of the survival time given each covariate value, The derivation of the asymptotic chi(2) distribution of the test statistics uses a recently-obtained asymptotic representation of the Bekan estimator as average of independent random variables. A real-data set is analysed and results of simulation studies are reported."
"10.1093/biomet/90.2.251","2003","Decomposability and selection of graphical models for multivariate time series","0","We derive conditions for decomposition and collapsibility of graphical interaction models for multivariate time series. These properties enable us to perform stepwise model selection under certain restrictions. For illustration, we apply the results to a multivariate time series describing the haemodynamic system as monitored in intensive care."
"10.1093/biomet/90.1.245","2003","A proof of the conjecture on positive skewness of generalised inverse {G}aussian distributions","0","We prove the conjecture that the generalised inverse Gaussian distribution is positively skew."
"10.1093/biomet/90.1.239","2003","On modelling mean-covariance structures in longitudinal studies","7","We exploit a reparameterisation of the marginal covariance matrix arising in longitudinal studies (Pourahmadi; 1999, 2000) to model, jointly, the mean and covariance structures in terms of three polynomial functions of time. By reanalysing Kenward's (1987) cattle data, we compare model selection procedures based on regressogram estimation with these based on a global search of the model space. Using a BIC-based model selection criterion to identify the optimum degree triple of the three polynomials, we show that the use of a saturated mean model is not optimal and explain why regressogram-based model estimation may be misleading. We also suggest a new computational method for finding the global optimum based on a criterion involving three pairwise saturated profile likelihoods."
"10.1093/biomet/90.1.233","2003","Some theory for constructing minimum aberration fractional factorial designs","6","Minimum aberration is the most established criterion for selecting a regular fractional factorial design of maximum resolution. Minimum aberration designs for n runs and n/2 less than or equal to m < n factors have previously been constructed using the novel idea of complementary designs. In this paper, an alternative method of construction is developed by relating the wordlength pattern of designs to the so-called 'confounding between experimental runs'. This allows minimum aberration designs to be constructed for n runs and 5n/16 less than or equal to m less than or equal to n/2 factors as well as for n/2 less than or equal to m < n."
"10.1093/biomet/90.1.223","2003","A test for linear versus convex regression function using shape-restricted regression","0","An unbiased test for the appropriateness of the simple linear regression model is presented. The null hypothesis is that the underlying regression function is indeed a line, and the alternative is that it is convex. The exact distribution for a likelihood ratio test statistic is that of a mixture of beta random variables, with the mixing distribution calculated from relative volumes of polyhedral convex cones determined by the convex shape restriction. Simulations show that the power of the test is favourable compared with the usual F-test. against a quadratic model, for some nonquadratic choices of the underlying regression function."
"10.1093/biomet/90.1.209","2003","B\""urmann expansion and test for additivity","0","We propose a Lagrange multiplier test for additivity based on the Burmann expansion of a conditional mean function. The asymptotic null distribution of the test is shown to be chi(2), under some regularity conditions. In contrast; the Lagrange multiplier test proposed by Chen et al. (1995) is based on the Volterra expansion of-the conditional mean function. We discuss some desirable advantages of the Burmann expansion over the Volterra expansion for nonlinear time series modelling. We also reported an empirical study which shows that, in terms of empirical power, the Lagrange multiplier-test motivated by the Burmann expansion outperforms the test of Chen et al. (1995) for the cases for which the Lagrange multiplier test is designed. For other cases for which none of the tests is specifically designed, the empirical powers of the two tests are comparable. Finally, we illustrated the use of the Lagrange multiplier test with a blowfly experimental system."
"10.1093/biomet/90.1.199","2003","A nonparametric test for panel count data","2","Panel count data arise when a recurrent event is under investigation and each study subject is observed only at discrete time points. In this situation, observed data include only the numbers of occurrences of the event of interest between observation time points and no information is. available on subjects between their observation time points. We propose a nonparametric test for comparing the point processes characterising the recurrent event when only panel count data are available. The asymptotic distribution of the test statistic is derived and a simulation study is conducted to evaluate its performance. The method is illustrated using data from a medical follow-up study."
"10.1093/biomet/90.1.183","2003","Nonparametric estimation from current status data with competing risks","4","A great deal of recent attention has focused on the estimation of survival distributions based on current status data, an extreme form of interval censored data. This particular data structure arises in a wide variety of applications where cross-sectional observation either naturally occurs or is preferred to more traditional forms of follow-up. Here we consider current status data in the context of competing risks. We briefly consider simple parametric models as a backdrop to nonparametric procedures. We make some brief comparisons and remarks regarding the nonparametric maximum likelihood estimator. The ideas are illustrated on the data of Krailo & Pike (1983) which considers estimation of the age distribution at both natural and operative menopause. We also consider the case where there is exact observation of failure times due to one of the competing risks when failure occurs prior to the monitoring time."
"10.1093/biomet/90.1.171","2003","Estimation of a failure time distribution based on imperfect diagnostic tests","0","Sequentially-administered diagnostic tests used to determine the occurrence of a silent event are sometimes subject to error, leading to false positive and false negative test results. In such cases, standard methods for interval censored data do not give valid estimates of the distribution of the time to the event. We present methods for estimating the distribution of the time to the event that account for multiple types of imperfect diagnostic test, as well as differing periods at risk. We illustrate the methods with simulated data and results from a clinical trial for the prevention of mother-to-infant transmission of HIV in Tanzania."
"10.1093/biomet/90.1.157","2003","Random effects {C}ox models: a {P}oisson modelling approach","4","We propose a Poisson modelling approach to nested random effects Cox proportional hazards models. An important feature of this approach is that the principal results depend only on the first and second moments of the unobserved random effects. The orthodox best linear unbiased predictor approach to random effects Poisson modelling techniques enables us to justify appropriate consistency and optimality. The explicit expressions for the random effects given by our approach facilitate incorporation of a relatively large number of random effects. The use of the proposed-methods is illustrated through the reanalysis of data from a large-scale cohort study of particulate air pollution and mortality previously reported by Pope et al. (1995)."
"10.1093/biomet/90.1.139","2003","A dependence measure for multivariate and spatial extreme values: properties and inference","6","We present properties of a dependence measure that arises in the study of extreme values in multivariate and spatial problems. For multivariate problems the dependence measure characterises dependence at the bivariate level, for all pairs and all higher orders up to and including the dimension of the variable. Necessary and sufficient conditions are given for subsets of dependence measures to be self-consistent, that is to guarantee the existence of a distribution with such a subset of values for the dependence measure. For pairwise dependence, these conditions are given in terms. of positive semidefinite matrices and non-differentiable, positive definite functions. We construct new nonparametric estimators for the dependence measure which, unlike all naive nonparametric estimators, impose these self-consistency properties. As the new estimators provide an improvement on the naive methods, both in terms of the inferential and interpretability properties; their use in exploratory extreme value analyses should aid the identification of appropriate dependence models. The methods are illustrated through an analysis of simulated multivariate data, which shows that a lack of self-consistency is frequently a problem with the existing estimators, and by a spatial analysis of daily rainfall extremes in south-west England, which finds a smooth decay in extremal dependence with distance."
"10.1093/biomet/90.1.127","2003","Implementing matching priors for frequentist inference","2","Nuisance parameters do not pose any problems in Bayesian inference as marginalisation allows for study of the posterior distribution solely in terms of the parameter of interest. However, no general solution is available for removing nuisance parameters under the frequentist paradigm: In this paper, we merge the two. approaches to construct a general procedure for frequentist elimination of nuisance parameters through the use of matching priors. In particular, we perform Bayesian marginalisation with respect to a prior distribution under which posterior inferences have approximate frequentist validity. Matching priors are constructed as solutions to a partial differential equation. Unfortunately, except in simple cases, these partial differential equations do not yield to analytical nor even standard numerical methods of solution. We present a numerical/Monte Carlo algorithm for obtaining the matching prior, in general, as a solution to the appropriate partial differential equation and draw posterior inferences. To be specific, we develop an automated routine through an implementation of the Metropolis-Hastings algorithm for deriving frequentist valid inferences via the matching prior. We illustrate our results in the contexts of fitting random effects models, fitting logistic regression models and fitting teratological data by beta-binomial models."
"10.1093/biomet/90.1.113","2003","Estimating central subspaces via inverse third moments","5","Modern graphical tools have enhanced our ability-to learn many things from data directly. In recent years, dimension reduction has proven to be an effective tool for generating low-dimensional summary plots without appreciable loss of information. Some well-known inverse regression methods for dimension reduction such as sliced inverse regression (Li; 1991) and sliced average variance estimation (Cook & Weisberg, 1991) have been developed to estimate summary plots for regression and discriminant analysis; In this paper, we suggest a new method that makes use of inverse third moments. This method can find structure beyond that found by sliced inverse regression and sliced average variance estimation, particularly regression mixtures. Illustrative examples are presented."
"10.1093/biomet/90.1.99","2003","Likelihood inference in nearest-neighbour classification models","1","Traditionally the neighbourhood size k in the k-nearest-neighbour algorithm is either fixed at the first nearest neighbour or is selected on the basis of a crossvalidation study. In this paper we present an alternative approach that develops the k-nearest-neighbour algorithm using likelihood-based inference. Our method takes the form of a generalised linear regression on a set of k-nearest-neighbour autocovariates. By defining the k-nearest-neighbour algorithm in this way we are able to extend the method to accommodate the original predictor variables as possible linear effects as well as allowing for the inclusion of multiple nearest-neighbour terms. The choice of the final model proceeds via a stepwise regression procedure. It is shown that our method incorporates a conventional generalised linear model and a conventional k-nearest-neighbour algorithm as special cases. Empirical results suggest that the method out-performs the standard k-nearest-neighbour method in terms of misclassification rate on a wide variety of data-sets."
"10.1093/biomet/90.1.85","2003","Heteroscedastic factor analysis","0","Two moment-based model-fitting procedures for the heteroscedastic factor analysis model are introduced and compared. The procedures produce consistent parameter estimators and asymptotically valid inferences for heteroscedasticity without specifying the distributional forms for the factor and heteroscedastic errors. Also, an individual-specific inference procedure for the factor score is developed. Simulation studies show the practical usefulness of the procedures. An example from a morphological measurement study is described."
"10.1093/biomet/90.1.73","2003","Confidence regions when the {F}isher information is zero","0","We examine the asymptotic behaviour of confidence regions in identifiable one-dimensional parametric models with smooth likelihood function and information equal to zero at a critical point of the parameter space. Confidence regions are based on inversion of the likelihood ratio test statistic and of some common forms of the score and Wald test statistics. For fixed parameter values other than the. critical point, all these statistics have limiting chi((1))(2) distributions, but for most of them the- convergence is not uniform near the critical point: When. it is not, confidence regions based on inverting the tests, using the chi((1))(2) approximation, do not asymptotically have the nominal level. The exception to this lack of locally. uniform convergence occurs with the score test standardised by expected, rather than observed; information. For the regions based. on the score test standardised by observed information and on the likelihood ratio test; conservative procedures that do not rely on the chi((1))(2)-approximation can be developed, but they are much too conservative near the critical parameter value. The regions based on the Wald tests have asymptotic level less than 1/2, regardless of the procedure used. Our results suggest that no procedure based solely. on the likelihood function will be satisfactory. Whether or not this is the case is an open problem. A simulation study illustrates the results of this paper."
"10.1093/biomet/90.1.53","2003","Pattern-mixture models with proper time dependence","3","Recently, pattern-mixture modelling has become a popular tool for modelling incomplete longitudinal data. Such models are under-identified in the sense that, for any dropout pattern; the data provide no direct information on the-distribution of the unobserved outcomes, given the observed ones. One simple way of overcoming this problem, ordinary extrapolation of sufficiently simple pattern-specific-models, often produces rather unlikely descriptions; several authors consider identifying restrictions instead. Molenberghs et al. (1998) have constructed identifying restrictions corresponding to missing at random. In this paper, the family of restrictions where drop-out-does not depend on future, unobserved observations is identified. The ideas are illustrated using a clinical study of Alzheimer patients."
"10.1093/biomet/90.1.43","2003","Marginal nonparametric kernel regression accounting for within-subject correlation","20","There has been substantial recent interest in non- and semiparametric methods for longitudinal or clustered data with dependence within clusters. It has been shown rather inexplicably that, when standard kernel smoothing methods are used in a natural way, higher efficiency is obtained by assuming independence than by using the true correlation structure. It is shown here that this result is a natural consequence of how standard kernel methods incorporate the within-subject correlation in the asymptotic setting considered, where the cluster sizes are fixed and the cluster number increases. In this paper, an alternative kernel smoothing method is proposed. Unlike the standard methods, the smallest variance of the new estimator is achieved when the true correlation is assumed. Asymptotically, the variance of the proposed method is uniformly smaller than that of the most efficient working independence approach. A small simulation study shows that significant improvement is obtained for finite samples."
"10.1093/biomet/90.1.29","2003","Working correlation structure misspecification, estimation and covariate design: implications for generalised estimating equations performance","9","The method of generalised estimating equations for regression modelling of clustered outcomes allows for specification of a working matrix that is intended to approximate the true correlation matrix of the observations. We investigate the asymptotic relative efficiency of the generalised estimating equation for the mean parameters when the correlation parameters are estimated by various methods. The asymptotic relative efficiency depends on three-features of the analysis, namely (i) the discrepancy between the working correlation structure and the unobservable true correlation structure, (ii) the method by which the correlation parameters are estimated and (iii) the 'design', by which we refer to both the structures of the predictor matrices within clusters and distribution of cluster sizes. Analytical and numerical studies of realistic data-analysis scenarios show that choice of working covariance model has a substantial impact on regression estimator efficiency. Protection against avoidable loss of efficiency associated with covariance misspecification is obtained when a 'Gaussian estimation' pseudolikelihood procedure is used with an AR(1) structure."
"10.1093/biomet/90.1.15","2003","Generalised linear models for correlated pseudo-observations, with applications to multi-state models","2","In multi-state-models regression analysis typically involves the modelling of each transition intensity separately. Each probability of interest, namely the probability that a subject will be in a given state at some time, is a complex nonlinear function of the intensity regression coefficients. We present a technique which models the state probabilities directly. This method is based on the pseudo-values from a jackknife statistic constructed from simple summary statistic estimates of the state probabilities. These pseudo-values are then used in a generalised estimating equation to obtain estimates of the model parameters. We illustrate how this technique works by studying examples of common regression problems. We apply the technique to model acute graft-versus-host disease in bone marrow transplants."
"10.1093/biomet/90.1.1","2003","Nonparametric estimation in nonlinear mixed effects models","1","A nonparametric approach is developed herein to estimate parameters in nonlinear mixed effects models. Asymptotic properties of the nonparametric maximum likelihood estimators and associated computational algorithms are provided. Empirical Bayes estimators of functionals of the random effects are also developed. Applications to population pharmacokinetics are given."
"10.1198/016214503000000701","2003","Statistical models and bioterrorism: application to the {U}.{S}.\ anthrax outbreak","0","In the fall of 2001 an outbreak of inhalational anthrax occurred in the United States that was the result of bioterrorism. Letters contaminated with anthrax spores were sent through the postal system. In response to the outbreak, public health officials treated over 10,000 persons with antibiotic prophylaxis in the hopes of preventing further morbidity and mortality. No persons receiving the antibiotics subsequently developed disease. The question arises as to how many cases of disease may actually have been prevented by the public health intervention of antibiotic prophylaxis. A statistical model is developed to answer this question by relating to the incubation period distribution the dates of disease onset, dates of initiation of antibiotic prophylaxis, and dates of exposure to the anthrax spores. An important complication is that the date of exposure to the anthrax spores was unknown for a cluster of cases in Florida because the contaminated letter was never found. A general likelihood function for a multicommon source outbreak is developed where the dates of exposure to the source (e.g., anthrax spores) may or may not be known. Estimates of the incubation period distribution are derived from an outbreak in Sverdlovsk, Russia. The methods are applied to the 2001 U.S. outbreak. The sensitivity of the estimates to the assumed incubation period is investigated. Properties of the estimators, particularly when the outbreak sizes are small, are evaluated by simulation. In the absence of antibiotic prophylaxis, the outbreak could have been about twice as large but unlikely would have been more than 50 cases. The results underscore the importance of early detection of outbreaks together with targeted and effective public health control measures."
"10.1198/016214503000000710","2003","Multidimensional residual analysis of point process models for earthquake occurrences","3","Residual analysis methods for examining the fit of multidimensional point process models are applied to point process models for the space-time-magnitude distribution of earthquake occurrences, using, in particular, the multidimensional version of Ogata's epidemic-type aftershock sequence (ETAS) model and a 30-year catalog of 580 earthquakes occurring in Bear Valley, California. One method involves rescaled residuals, obtained by transforming points along one coordinate to form a homogeneous Poisson process inside a random, irregular boundary. Another method involves thinning the point process according to the conditional intensity to form a homogeneous Poisson process on the original, untransformed space. The thinned residuals suggest that the fit of the model may be significantly improved by using an anisotropic spatial distance function in the estimation of the spatially varying background rate. Using rescaled residuals, it is shown that the temporal-magnitude distribution of aftershock activity is not separable, and that, in particular, in contrast to the ETAS model, the triggering density of earthquakes appears to depend on the magnitude of the secondary events in question. The residual analysis highlights that the fit of the space-time ETAS model may be improved by allowing the parameters governing the triggering density to vary for earthquakes of different magnitudes. Such modifications may be important because the ETAS model is widely used in seismology for hazard analysis."
"10.1198/016214503000000729","2003","Infilling sparse records of spatial fields","3","Historical records of weather, such as monthly precipitation and temperatures from the last century, are an invaluable database to use in studying changes and variability in climate. These data also provide the starting point for understanding and modeling the relationship among climate, ecological processes, and human activities. However, these data are observed irregularly over space and time. The basic statistical problem is to create a complete data record that is consistent with the observed data and is useful for other scientific disciplines. We modify the Gaussian-inverted Wishart spatial field model to accommodate irregular data patterns and to facilitate computations. Novel features of our implementation include the use of cross-validation to determine the relative prior weight given to the regression and geostatistical components and the use of a space-filling subset to reduce the computations for some parameters. We feel that the overall approach has merit, treading a line between computational feasibility and statistical validity. Furthermore, we are able to produce reliable measures of uncertainty for the estimates."
"10.1198/016214503000000738","2003","Multiple edit/multiple imputation for multivariate continuous data","2","Multiple imputation replaces an incomplete dataset with m > 1 simulated complete versions that are analyzed separately by standard methods. We present a natural extension of multiple imputation for handling the dual problems of nonresponse and response error. This extension, which we call multiple edit/multiple imputation (MEMI), replaces an observed dataset containing missing values and errors with m > 1 simulated versions of the ideal dataset that is complete and error-free. These ideal data sets are analyzed separately, and the results are combined using the same rules as for multiple imputation. The resulting inferences simultaneously reflect uncertainty due to nonresponse and response error. MEMI may be an attractive alternative to deterministic or quasi-statistical edit and imputation procedures used by many data-collecting agencies. Producing MEMI's requires assumptions about the distribution of the ideal data, the nature of nonresponse, and a model for the response error mechanism. However, fitting such a model does not necessarily require data from a follow-up study. In this article we develop and implement MEMI for preliminary data from the Third National Health and Nutrition Examination Survey, Phase I (1988-1991). Raw body measurements for 1,345 children age 2-3 years are imputed under a Bayesian model for intermittent or semicontinuous errors. The resulting population estimates are found to be quite insensitive to prior assumptions about the rates and magnitude of errors."
"10.1198/016214503000000756","2003","Assessing variability due to race bridging: application to census counts and vital rates for the year 2000","3","In 1997, the Office of Management and Budget revised the standards for classification of Federal data on race and ethnicity. A key provision of the revised standards is that each respondent in a Federal data collection is now allowed to select more than one race to describe the person in question. The prior standards, published in 1977, specified that each respondent be instructed to select only one race. In the 2000 census, data on race were collected under the new standards. To make the 2000 census data compatible with other data systems that have not yet implemented the new standards as well as with historical data collected under the prior standards, the National Center for Health Statistics of the Centers for Disease Control and Prevention, with assistance from the Bureau of the Census, has produced bridged census counts, that is, estimates of the counts by race that would have been obtained had the prior standards been in effect. This article presents techniques for assessing the variability due to race bridging. The methods developed by Schafer and Schenker for inference with imputed conditional means, which can be considered a first-order approximation to a multiple-imputation analysis with an infinite number of imputations, are adapted to the bridging problem and applied to bridged 2000 census counts as well as to selected vital rates for 2000 computed using bridged census counts as denominators. The relative standard errors of estimated census counts by race under the 1977 standards tend to be higher for finer geographic levels and lower for coarser geographic levels. For each state (or the District of Columbia), the relative standard error of the count for a given race is no greater than .05. For birth and death rates by age group and 1977 race at the national level, on an absolute basis, bridging of the census counts in the denominators does not add substantially to the relative standard errors."
"10.1198/016214503000000765","2003","Forecast uncertainties in macroeconomic modeling: an application to the {U}.{K}.\ economy","3","We argue that probability forecasts convey information on the uncertainties that surround macroeconomic forecasts in a straightforward manner that is preferable to other alternatives, including the use of confidence intervals. Probability forecasts obtained using a small benchmark macroeconometric model and a number of other alternatives are presented and evaluated using recursive forecasts generated over the period 1999q1-2001q1. Out-of-sample probability forecasts of inflation and output growth are also provided over the period 2001q2-2003q1, and their implications are discussed in relation to the Bank of England's inflation target and the need to avoid recessions, both as separate events and jointly. The robustness of the results to parameter and model uncertainties is also investigated using Bayesian model-averaging techniques."
"10.1198/016214503000000774","2003","Bayesian modeling and forecasting of intraday electricity load","1","The advent of wholesale electricity markets has brought renewed focus on intraday electricity load forecasting. This article proposes a multiequation regression model with a diagonal first-order stationary vector autoregresson (VAR) for modeling and forecasting intraday electricity load. The correlation structure of the disturbances to the VAR and the appropriate subset of regressors are explored using Bayesian model selection methodology. The full spectrum of finite-sample inference is obtained using a Bayesian Markov chain Monte Carlo sampling scheme. This includes the predictive distribution of load and the distribution of the time and level of daily peak load, something that is difficult to obtain with other methods of inference. The method is applied to several multiequation models of half-hourly total system load in New South Wales, Australia. A detailed model based on 3 years of data reveals trend, seasonal, bivariate temperature/humidity, and serial correlation components that all vary intraday, justifying the assumption of a multiequation approach. Short-term forecasts from simple models highlight the gains that can be made if accurate temperature predictions are exploited. Bayesian predictive means for half-hourly load compare favorably with point forecasts obtained using iterated generalized least squares estimation of the same models."
"10.1198/016214503000000783","2003","Profiling placebo responders by self-consistent partitioning of functional data","1","Identification of placebo responders among subjects treated with active drug has significant clinical and research implications. In clinical practice, when a patient treated with medication improves, this improvement may be attributed to the chemical component of the drug itself, a ""placebo effect,"" or some combination of these. Determining the proper subsequent treatment and maintenance of the patient may be greatly aided by understanding the mechanism of patient improvement. In a research context, classification of patient response has bearing on how efficacy and effectiveness clinical trials are designed and conducted. This article presents a framework for studying placebo response in diverse areas of medicine. To identify placebo responders among drug-treated patients, a profile of the clinical status over time (outcome profile) is estimated for each subject. Self-consistent partitioning techniques are used to group subjects based on the amount of curvature in the profile as well as the overall trend in the profile. The resulting partitions determine representative profiles for subjects in the drug group that subsequently can be used to classify patients. The proposed method is applied to data from a clinical trial for treatment of depression involving placebo and the active drug phenelzine. Data from the placebo arm of the study is used to help validate the procedure, because the drug-treated and placebo-treated subjects should share common profiles."
"10.1198/016214503000000792","2003","Characterizing the progression of viral mituations over time","2","Development and spread of resistance of human immunodeficiency virus type 1 to antiretroviral therapies is a serious medical and public health concern. A wide variety of mutations have been identified that either singly or in combination reduce the susceptibility of the virus to available therapies. This paper describes methods for understanding the genetic pathways that lead to high-level drug resistance under selective drug pressure, as well as for estimating the rates at which viral populations progress along these pathways. These methods can be used to determine whether the presence of certain mutations among drug-sensitive viruses predispose a patient under a particular treatment to develop patterns of mutations that confer high-level drug resistance. Our approach assumes that viral genotypes can be characterized as belonging to discrete states, defined by patterns of viral mutations, and considers two approaches to modeling the rates of transition between these states. The first approach treats the state at a given time point as known, whereas the second treats this as a latent variable. We apply our methods to genetic sequences of viruses cloned from the plasma of 170 patients who participated in three phase II clinical studies of efavirenz combination therapy (DMP 266-003, -004, -005). Multiple viral clones are available from each plasma sample at each time of measurement, allowing for consideration of the effect of minority species on the evolution of the viral populations infecting patients; the availability of such information motivates the second analytic approach. The sequences can be found in the Stanford HIV RT and Protease Sequence Database."
"10.1198/016214503000000800","2003","A method for normalizing microarrays using genes that are not differentially expressed","0","One of the more challenging, yet easily overlooked, aspects of the analysis of microarrays is how to normalize arrays so that comparisons can be made across arrays. Most studies that utilize microarrays to detect differential gene expression between samples find the data only enable one to conclude that a handful of genes are differentially expressed. The basic idea here is to use the genes that are not differentially expressed to conduct the normalization. Of course, because one cannot determine which genes are diffierentially expressed until the normalization is conducted, this is a nontrivial problem. Here a general framework and computational method (using the Gibbs sampler) is devised to allow for such normalization. We apply the method to a gene expression experiment aimed at furthering our understanding of Porcine reproductive and respiratory syndrome virus, a major source of economic loss in the swine industry."
"10.1198/016214503000000828","2003","Frequentist model average estimators","17","The traditional use of model selection methods in practice is to proceed as if the final selected model had been chosen in advance, without acknowledging the additional uncertainty introduced by model selection. This often means underreporting of variability and too optimistic confidence intervals. We build a general large-sample likelihood apparatus in which limiting distributions and risk properties of estimators post-selection as well as of model average estimators are precisely described, also explicitly taking modeling bias into account. This allows a drastic reduction in complexity, as competing model averaging schemes may be developed, discussed, and compared inside a statistical prototype experiment where only a few crucial quantities matter. In particular, we offer a frequentist view on Bayesian model averaging methods and give a link to generalized ridge estimators. Our work also leads to new model selection criteria. The methods are illustrated with real data applications."
"10.1198/016214503000000819","2003","The focused information criterion","8","A variety of model selection criteria have been developed, of general and specific types. Most of these aim at selecting a single model with good overall properties, for example, formulated via average prediction quality or shortest estimated overall distance to the true model. The Akaike, the Bayesian, and the deviance information criteria, along with many suitable variations, are examples of such methods. These methods are not concerned, however, with the actual use of the selected model, which varies with context and application. The present article takes the view that the model selector should instead focus on the parameter singled out for interest; in particular, a model that gives good precision for one estimand may be worse when used for inference for another estimand. We develop a method that, for a given focus parameter, estimates the precision of any submodel-based estimator. The framework is that of large-sample likelihood inference. Using an unbiased estimate of limiting risk, we propose a focused information criterion for model selection. We investigate and discuss properties of the method, establish some connections to Akaike's information criterion, and illustrate its use in a variety of situations."
"10.1198/C16214503000000909","2003","Directional rates of change under spatial process models","2","Spatial process models are now widely used for inference in many areas of application. In such contexts interest is often in the rate of change of a spatial surface at a given location in a given direction. Examples include temperature or rainfall gradients in meteorology, pollution gradients for environmental data, and surface roughness assessment for digital elevation models. Because the spatial surface is viewed as a random realization, all such rates of change are random as well. We formalize the notions of directional finite difference processes and directional derivative processes building upon the concept of mean square differentiability as developed by Stein and Banerjee and Gelfand. We obtain complete distribution theory results under the assumptions of a stationary Gaussian process model either for the data or for spatial random effects. We present inference under a Bayesian framework which, in this setting, presents several advantages. Finally, we illustrate our methodology with a simulated dataset and also with a real estate dataset consisting of selling prices of individual homes."
"10.1198/016214503000000918","2003","Adaptive estimators and tests of stationary and nonstationary short- and long-memory {ARFIMA}-{GARCH} models","2","This article considers the fractionally autoregressive integrated moving average [ARFIMA(p, d, q)] models with GARCH errors. The process generated by this model is short memory, long memory, stationary, and nonstationary, respectively, when d is an element of (- 1/2, 0), d is an element of (0, 1/2), d is an element of (- 1/2, 1/2), and d is an element of (1/2, infinity). Using a unified approach, the local asymptotic normality of the model is established for d is an element of U-j=0(infinity)(J - 1/2, J + 1/2). The adaptivity and efficiency of the estimating parameters are discussed. In a class of loss functions, the asymptotic minimax bound of the estimators for the model is given when the density f of rescaled residuals is unknown. An adaptive estimator is constructed for the parameters in the ARFIMA part when f is symmetric, and a general form of the efficient estimator is also constructed for all the parameters in the ARFIMA and GARCH parts. When the density f is unknown, Wald tests are constructed for testing the unit root +1 against the class of fractional unit roots. It is shown that these tests asymptotically follow the chi-squared distribution and are locally most powerful. These results are new contributions to the literature, even for the ARFIMA model with iid errors, except for the adaptive estimator in this case with d e (0, 1/2). The performance of the asymptotic results in finite samples is examined through Monte Carlo experiments. An application to the U.S. Consumer Price Index inflation series is given, and a clear conclusion from this is that the series is neither an I(0) nor an I(I), but rather than an I(d) process with d approximate to 0.288."
"10.1198/016214503000000927","2003","Using the bootstrap to select one of a new class of dimension reduction methods","15","Dimension reduction in a regression analysis of response y given a p-dimensional vector of predictors x reduces the dimension of x by replacing it with a lower-dimensional linear combination beta'x of the x's without specifying a parametric model and without loss of information about the conditional distribution of y given x. We unify three existing methods, sliced inverse regression (SIR), sliced average variance estimate (SAVE), and principal Hessian directions (pHd), into a larger class of methods. Each method estimates a particular candidate matrix, essentially a matrix of parameters. We introduce broad classes of dimension reduction candidate matrices, and we distinguish estimators of the matrices from the matrices themselves. Given these classes of methods and several ways to estimate any matrix, we now have the problem of selecting a particular matrix and estimation method. We propose bootstrap methodology to select among candidate matrices, estimators and dimension, and in particular we investigate linear combinations of different methods."
"10.1198/016214503000000936","2003","More efficient local polynomial estimation in nonparametric regression with autocorrelated errors","0","We propose a modification of local polynomial time series regression estimators that improves efficiency when the innovation process is autocorrelated. The procedure is based on a pre-whitening transformation of the dependent variable that must be estimated from the data. We establish the asymptotic distribution of our estimator under weak dependence conditions. We show that the proposed estimation procedure is more efficient than the conventional local polynomial method. We also provide simulation evidence to suggest that gains can be achieved in moderate-sized samples."
"10.1198/016214503000000945","2003","Mobius-like mappings and their use in kernel density estimation","0","It is well known that the manipulation of sample data by means of a parametric function can improve the performance of kernel density estimation. This article proposes a two-parameter Mobius-like function to map sample data drawn from a semi-infinite space into [-1, 1). A standard kernel method is then used to estimate the density. The proposed method is shown to yield effective estimates of density and is computationally more efficient than other well-known transformation methods. The efficacy of the technique is demonstrated in a practical setting by application to two datasets."
"10.1198/016214503000000954","2003","Censored regression quantiles","11","Using quantile regression to analyze survival times offers an valuable complement to traditional Cox proportional hazards modelling. Unfortunately, this approach has been hampered by the lack of a conditional quantile estimator for censored data that is directly analogous to the Kaplan-Meier estimator and applies under standard assumptions for censored regression models. Here a recursively reweighted estimator of the regression quantile process is developed as a direct generalization of the Kaplan-Meier estimator. Specifically, the asymptotic behavior is directly analogous to that of the Kaplan-Meier estimator, and computation is essentially equivalent to current simplex. methods for the quantile process in the uncensored case. Some preliminary examples suggest the strong potential of these methods as a complement to the use of Cox models."
"10.1198/016214503000000963","2003","A lack-of-fit test for quantile regression","10","We propose an omnibus lack-of-fit test for linear or nonlinear quantile regression based on a cusum process of the gradient vector. The test does not involve nonparametric smoothing but is consistent for all nonparametric alternatives without any moment conditions on the regression error. In addition, the test is suitable for detecting the local alternatives of any order arbitrarily close to n(-1/2) from the null hypothesis. The limiting distribution of the proposed test statistic is non-Gaussian but can be characterized by a Gaussian process. We propose a simple sequential resampling scheme to carry out the test whose nominal levels are well approximated in our empirical study for small and modest sample sizes."
"10.1198/016214503000000981","2003","A latent variable model of segregation analysis for ordinal traits","1","Many health conditions, including cancer and psychiatric disorders, are believed to have a complex genetic basis, and genes and environmental factors are likely to interact in the presence and severity of these conditions. Assessing familial aggregation and inheritability of disease is a classic topic of genetic epidemiology, commonly referred to as segregation analysis. Although today it is routine to conduct such analyses for quantitative and dichotomous traits, methods and software that accommodate ordinal traits do not exist. To this end, we propose a latent variable model by extending the work of Zhang and Merikangas, who examined binary traits. The advantage of this latent variable model lies in its flexibility to include environmental factors (usually represented by covariates) and its potential to allow gene-environment interactions. The model building uses the EM algorithm for maximization and a peeling algorithm for computational efficiency. We provide asymptotic theory for statistical inference, and conduct simulation studies to confirm that the asymptotic theory is adequate in practical applications. We also apply our model to examine the familial transmission of alcoholism, which is categorized into three ordinal levels: normal control, alcohol abuse, and alcohol dependence. Our analysis not only confirms that alcoholism is familial, but also suggests that the transmission may have a major gene component not revealed by previous analyses using dichotomous traits."
"10.1198/016214503000000990","2003","Determining inheritance distributions via stochastic penetrances","0","The aim of linkage analysis is to map the position of a gene contributing to an inheritable disease. The statistical model contains the disease allele frequency and penetrance parameters. Here I investigate the inheritance distribution, that is, the conditional distribution of the inheritance vector given phenotypes at the disease locus. Based on this, the likelihood and likelihood score function of Whittemore can be defined. As a result, a general semiparametric methodology of choosing score functions in linkage analysis is proposed. The proposed approach is valid for arbitrary pedigrees, and I treat quantitative, dichotomous (binary), and other phenotypes in a unified framework. The resulting score functions can be easily incorporated into existing software for multipoint linkage analysis. I use the fact that the inheritance distribution depend on unknown founder alleles. These are treated as hidden data and give rise to ""stochastic penetrance factors."" Certain uncorrelated unit variance random variables that are functions of the founder alleles are introduced. I show that the moment-generating function and moments of these play crucial roles in choosing likelihoods and likelihood score functions. Lower-/higher-order moments are more important when the genetic effect is weak/strong, and this corresponds to simultaneous identical by descent (IBD) sharing of few/many individuals. For inbred pedigrees and nonadditive models, the likelihood score function is dominated by individuals homozygous by descent at the disease locus. For outbred pedigrees, the local score function involves pairwise IBD sharing. Relations to existing score functions of nonparametric linkage (S-pairs, S-all, S-robdom) and quantitative trait loci (QTL) are highlighted."
"10.1198/016214503000000972","2003","Information recovery in a study with surrogate endpoints","2","Recently, there has been a lot of interest in statistical methods for analyzing data with surrogate endpoints. In this article, we consider parameter estimation from a model that relates a variable Y to a set of covariates, X, in the presence of a surrogate, S. We assume that the data are made up of two random samples from the population, a validation set where (Y, X, S) are observed on every subject and a nonvalidation set where only (X, S) are measured. We show how information from the nonvalidation set can be incorporated to improve upon estimation of a parameter P using the validation data only. The method we suggest does not require knowledge on the joint distribution between (Y, S), given X. It is based on a two-sample empirical likelihood that simultaneously combines the estimating equations from the validation set and the nonvalidation set. The proposed nonparametric likelihood formulation brings a few attractive features to the inference in P. First, the maximum empirical likelihood estimate is more efficient than that using only the validation sample. Second, confidence regions can be readily constructed without the need to estimate the variance-covariance matrix. Finally, the coverage of the confidence regions can be further improved by an empirical Bartlett correction based on the bootstrap. We show that the method gives favorable results in simulation studies."
"10.1198/01622145030000001007","2003","Estimating cure rates from survival data: an alternative to two-component mixture models","5","This article considers the utility of the bounded cumulative hazard model in cure rate estimation, which is an appealing alternative to the widely used two-component mixture model. This approach has the following distinct advantages: (1) It allows for a natural way to extend the proportional hazards regression model, leading to a wide class of extended hazard regression models. (2) In some settings the model can be interpreted in terms of biologically meaningful parameters. (3) The model structure is particularly suitable for semiparametric and Bayesian methods of statistical inference. Notwithstanding the fact that the model has been around for less than a decade, a large body of theoretical results and applications has been reported to date. This review article is intended to give a big picture of these modeling techniques and associated statistical problems. These issues are discussed in the context of survival data in cancer."
"10.1198/016214503388619030","2003","Statistics: the next generation","0","New technologies benefit our lives in many ways, but they can also bring increased risks, disruptions to our society, and even the malevolence of war. Statisticians can play a critical role in influencing the paths along which technology will take our society. But taking on this role requires changing our discipline, our profession, and our ASA. Change, however, is not an option; if we do not change, then technology may force change on us in detrimental ways. Understanding how to change requires a broader view of statistics as a human activity with a human purpose that must evolve within a social, or human, system. Considered as a technology, statistics is a fundamental and invaluable part of the infrastructure of other sciences. Statistics advances discoveries in other sciences. Universities and foundations must encourage interdisciplinary research as a primary contribution to our field. Our curricula must reflect modern approaches that other sciences require, and students with quantitative talents must be attracted from other disciplines. Statistics should permeate the mathematics curricula at all elementary and secondary levels, and all children should understand variability and uncertainty, how to make sense from data, and the elements involved in making decisions. In industry, we must capitalize on the importance of quality improvement programs to further contributions of statistics. From government, we need timely and accurate statistics that are relevant to the world of the future as well as to today. In particular, we must commit ourselves now to changing our 40-year-old poverty measure and improve access to government statistics with different and stronger ways to protect confidentiality. We must work toward a comprehensive, integrated network of knowledge and information systems for research on individual, social, and organizational change and for decision making by individuals, organizations, and public policy makers at all levels-local, regional, and national. In all of these directions, the ASA must foster and encourage change and, in many cases, lead the way. With the commitment and perseverance of our next generation, statistics can become a leader in the advancement of science and technology to promote human welfare."
"10.1198/016214503388619049","2003","Selection of related multivariate means: monitoring psychiatric care in the {D}epartment of {V}eterans {A}ffairs","2","Quantitative performance monitoring of health care providers has become an increasingly prominent activity in health care delivery. As a consequence, multidimensional ""report cards"" comparing the quality of services provided by diverse health care organizations have begun to appear. We present a case study examining the performance of 22 geographically defined health care systems, known as Veterans Integrated Service Networks; involving the treatment of more than 37,000 veterans discharged from general psychiatry program beds in fiscal year 1998. We describe multilevel multidimensional latent variable models and fit these to a set of related mixed discrete and continuous outcomes to identify networks providing inadequate care to its patients. We use validation sample to assess model fit by comparing observed statistics to the distribution of the statistics across replicated datasets. We propose using posterior tail probabilities associated with functions of the latent variables to select networks with poor outcomes, and finally compare our results with those from current methods."
"10.1198/016214503388619058","2003","The selection or seeding of college basketball or football teams for postseason competition","0","Systems for ranking college basketball or football teams take many forms, ranging from polls of selected coaches or members of the media to so-called computer-ranking systems. Some of these are used in ways that have considerable impact on the teams. The committee responsible for the selection and seeding of teams for the postseason National Collegiate Athletic Association (NCAA) Division I men's basketball tournament is influenced by various rankings, including ones based on the ratings percentage index (RPI). The Bowl Championship Series (BCS) rankings of NCAA Division I-A football teams determine which two teams compete in a postseason national championship game and determine eligibility for other prestigious postseason games. There are certain attributes that seem desirable in any ranking system to be used in selecting or seeding teams for postseason competition or that may have some other tangible or intangible effect on the teams. These attributes include accuracy, appropriateness, impartiality, unobtrusiveness, nondisruptiveness, verifiability, and comprehensibility. The polls, the RPI, and the BCS rankings are notably deficient in several of these attributes. A system having all of the attributes, except for unobtrusiveness, can be achieved by applying least squares to a statistical model in which the expected difference in score in each game is modeled as a difference in team effects plus or minus a home court/field advantage. The potential obtrusiveness of this approach can be circumvented by introducing modifications to reward winning per se and to eliminate any incentive for ""running up the score"" or for deliberately surrendering a lead so as to extend a game into overtime. The modified least squares system was applied to the 1999-2000 basketball and 1999-2001 football seasons. Its accuracy in predicting the outcomes of 73 postseason football games and 93 postseason basketball games was undiminished by the modifications and was comparable to that of the betting line."
"10.1198/016214503388619067","2003","Bayesian modeling of markers of day-specific fertility","0","Cervical mucus hydration increases during the fertile interval before ovulation. Because sperm. can only penetrate mucus having a high water content, cervical secretions provide a reliable marker of the fertile days of the menstrual cycle. This article develops a Bayesian approach for modeling of daily observations of cervical mucus and applies the approach to assess heterogeneity among women and cycles from a given woman with respect to the increase in mucus hydration during the fertile interval. The proposed model relates the mucus observations to an underlying normal mucus hydration score, which varies relative to a peak hydration day. Uncertainty in the timing of the peak is accounted for, and a novel, weighted mixture model is used to characterize heterogeneity in distinct features of the underlying mean function. Prior information on the mucus hydration trajectory is incorporated, and a Markov chain Monte Carlo approach is developed. Based on data from a study of daily fecundability, there appears to be substantial heterogeneity among women in detected preovulatory increases in mucus hydration, but only minimal differences among cycles from a given woman."
"10.1198/016214503388619076","2003","Estimating vaccine efficacy from secondary attack rates","2","Epidemiologists have used secondary attack rates (SARs) to estimate the protective effects of vaccination since the 1930s. SARs can also be used to estimate the effect of vaccination on reducing infectiousness in breakthrough cases. The conventional SAR approach has been to pool the denominators and numerators across transmission units, then to use a confidence interval for a simple relative risk. We demonstrate appropriate model-based methods to estimate vaccine efficacy (VE) from SARs using generalized estimating equations taking correlation within transmission units into account. The model-based procedures require transformation of the parameter estimates to the SAR scale to obtain vaccine efficacy estimates. Appropriate confidence intervals are then based on the bootstrap, with resampling done by transmission unit. We show that the usual confidence intervals are too narrow. We estimated the effect of pertussis vaccination on person-to-person transmission. The results show that pertussis vaccination reduces the ability of a breakthrough clinical case to produce other clinical cases. The methods can be used in evaluating VE for susceptibility and infectiousness from SARs in other infectious diseases."
"10.1198/01621450338861905","2003","The impact of prior distributions for uncontrolled confounding and response bias: a case study of the relation of wire codes and magnetic fields to childhood leukemia","3","This article examines the potential for misleading inferences from conventional analyses and sensitivity analyses of observational data, and describes some proposed solutions based on specifying prior distributions for uncontrolled sources of bias. The issues are illustrated in a sensitivity analysis of confounding in a study of residential wire code and childhood leukemia and in a pooled analysis of 12 studies of magnetic-field measurements and childhood leukemia. Both analyses have been interpreted as evidence in favor of a causal effect of magnetic fields on leukemia risk. This interpretation is contrasted with results from analyses based on prior distributions for the unidentified bias parameters used in the original sensitivity-analysis model. These analyses indicate that accounting for uncontrolled confounding and response bias under a reasonable prior can substantially alter inferences about the existence of a magnetic-field effect. More generally, analyses with informative priors for unidentified bias parameters can help avoid misinterpretation of conventional results and ordinary sensitivity analyses."
"10.1198/016214503388619094","2003","Discovery of conserved sequence patterns using a stochastic dictionary model","0","Detection of unknown patterns from a randomly generated sequence of observations is a problem arising in fields ranging from signal processing to computational biology. Here we focus on the discovery of short recurring patterns (called motifs) in DNA sequences that represent binding sites for certain proteins in the process of gene regulation. What makes this a difficult problem is that these patterns can vary stochastically. We describe a novel data augmentation strategy for detecting such patterns in biological sequences based on an extension of a ""dictionary"" model. In this approach, we treat conserved patterns and individual nucleotides as stochastic words generated according to probability weight matrices and the observed sequences generated by concatenations of these words. By using a missing-data approach to find these patterns, we also address other related problems, including determining widths of patterns, finding multiple motifs, handling low-complexity regions, and finding patterns with insertions and deletions. The issue of selecting appropriate models is also discussed. However, the flexibility of this model is also accompanied by a high degree of computational complexity. We demonstrate how dynamic programming-like recursions can be used to improve computational efficiency."
"10.1198/016214503388619102","2003","Robust indirect inference","3","In this article we develop robust indirect inference for a variety of models in a unified framework. We investigate the local robustness properties of indirect inference and derive the influence function of the indirect estimator, as well as the level and power influence functions of indirect tests. These tools are then used to design indirect inference procedures that are stable in the presence of small deviations from the assumed model. Although indirect inference was originally proposed for statistical models whose likelihood is difficult or even impossible to compute and/or to maximize, we use it here as a device to robustify the estimators and tests for models where this is not possible or is difficult with classical techniques such as M estimators. Examples from financial applications, time series, and spatial statistics are used for illustration."
"10.1198/01621450338861911","2003","Edge detection, spatial smoothing, and image reconstruction with partially observed multivariate data","3","Situations with incomplete multivariate spatial data on a lattice are considered. The goal is to impute the missing data in the presence of edges or boundaries and recover the image. Two methods based on Bayesian hierarchical models that iterate between edge detection and spatial smoothing to impute the missing data within identified homogeneous regions are examined. Their performance is compared with another method that imputes the missing values using edge-preserving spatial smoothers with locally varying weights. The performances of the three methods are compared on artificial and real datasets. It is seen that information from the multivariate data is critical in recovering the images. An application with color images where only one of three primary colors (red, green, or blue) is observed at each pixel is used to illustrate the results."
"10.1198/016214503388619120","2003","Estimates of regression coefficients based on lift rank covariance matrix","0","We introduce a new equivariant estimation method of the parameters of the multivariate regression model with q responses and p regressors. The estimate matrix is derived from the lift rank covariance matrix (LRCM) where the lift rank vectors are based on the Oja criterion function. The k = p + q variate ranks and k + I variate lift ranks are constructed using hyperplanes (or fits) going through k observations. The new LRCM regression estimate and the least squares (LS) estimate are shown to be weighted sums of the elemental estimates based on these hyperplanes. The LRCM regression estimate is equivariant and convergent, has a limiting multinormal distribution, and is highly efficient in the multivariate normal case. For heavy-tailed distributions, it performs better than the standard LS estimate. Estimation of the variance-covariance matrix of the LRCM estimate is briefly discussed. The theory is illustrated by simulations and a real data example."
"10.1198/016214503388619139","2003","Dimension reduction for multivariate response data","10","This article concerns the analysis of multivariate response data with multivariate regressors. Methods for reducing the dimensionality of response variables are developed, with the goal of preserving as much regression information as possible. We note parallels between this goal and the goal of sliced inverse regression, which intends to reduce the regressor dimension in a univariate regression while preserving as much regression information as possible. A detailed discussion is given for the case where the response is a curve measured at fixed points. The problem in this setting is to select basis functions for fitting an aggregate of curves. We propose that instead of focusing on goodness of fit, attention should be shifted to the problem of explaining the variation of the curves in terms of the regressor variables. A data-adaptive basis searching method based on dimension reduction theory is proposed. Simulation results and an application to a climatology problem are given."
"10.1198/016214503388619148","2003","Alternating subspace-spanning resampling to accelerate {M}arkov chain {M}onte {C}arlo simulation","1","This article provides a simple method to accelerate Markov chain Monte Carlo sampling algorithms, such as the data augmentation algorithm and the Gibbs sampler, via alternating subspace-spanning resampling (ASSR). The ASSR algorithm often shares the simplicity of its parent sampler but has dramatically improved efficiency. The methodology is illustrated with Bayesian estimation for analysis of censored data from fractionated experiments. The relationships between ASSR and existing methods are also discussed."
"10.1198/016214503388619157","2003","A reexamination of diffusion estimators with applications to financial model validation","6","Time-homogeneous diffusion models have been widely used for describing the stochastic dynamics of the underlying economic variables. Recently, Stanton proposed drift and diffusion estimators based on a higher-order approximation scheme and kernel regression method. He claimed that ""higher order approximations must outperform lower order approximations"" and concluded nonlinearity in the instantaneous return function of short-term interest rates. To examine the impact of higher-order approximations, we develop general and explicit formulas for the asymptotic behavior of both drift and diffusion estimators. We show that these estimators will reduce the numerical approximation errors in asymptotic biases, but their asymptotic variances escalate nearly exponentially with the order of approximation. Simulation studies also confirm our asymptotic results. This variance inflation problem arises not only from nonparametric fitting, but also from parametric fitting. Stanton's work also postulates the interesting question of whether the short-term rate drift is nonlinear. Based on empirical simulation studies, Chapman and Pearson suggested that the nonlinearity might be spurious, due partially to the boundary effect of kernel regression. This prompts us to use the local linear fit based on the first-order approximation, proposed by Fan and Yao, to ameliorate the boundary effect and to construct formal tests of parametric financial models against the nonparametric alternatives. Our simulation results show that the local linear method indeed outperforms the kernel approach. Furthermore, our nonparametric 11 generalized likelihood ratio tests"" are indeed versatile and powerful in detecting nonparametric alternatives. Using this formal testing procedure, we show that the evidence against the linear drift of the short-term interest rates is weak, whereas evidence against a family of popular models for the volatility function is very strong. Application to Standard & Poor 500 data is also illustrated."
"10.1198/016214503388619166","2003","On additive conditional quantiles with high-dimensional covariates","4","We investigate the estimation of the conditional quantile when many covariates are involved. In particular, we model the conditional quantile of a response as a nonlinear additive function of relevant covariates. For this setup, we propose a nonparametric smoother to estimate the unknown functions. The estimator provides direct computation of the nonlinear functions. Because it does not require any iteration, the estimator allows fast and routine data analysis. On the theoretical front, we also show asymptotic properties of the estimator, including mean squared error and limiting distribution. The theory confirms that for moderate dimension of the covariates, the estimator escapes the ""curse of dimensionality"" problem. Both simulated and real data examples are provided to illustrate the methodology."
"10.1198/016214503388619175","2003","Distribution of rankings for groups exhibiting heteroscedasticity and correlation","0","In one-way analysis of variance, a main interest is in differences among the groups that comprise the population. For a given parameter, such as a mean value, data yield parameter estimates for each group, as well as group rankings based on these statistics. Here ranking probabilities are studied under the assumption that parameter estimates are well approximated by a normal distribution, either in finite samples or asymptotically, with possible intergroup heteroscedasticity and correlation. Particular interest lies in the ranking distribution as a descriptor of experiments on some future dataset. Examples include contract bidding, global economic competition, and rival contests in mating. Ranking distributions are analytically complicated, yet some interesting properties can be derived for them via the symmetry and elliptical geometry of the normal distribution. Some relationships between ranking probabilities and group parameters are described, with attention given to the role of between-group heterogeneity and correlation. For the ranking probabilities, estimators and asymptotically valid standard errors formulas and hypothesis tests are proposed. Simulation is used to describe the sample size needed for accurate asymptotic approximation, and the methods are illustrated with an economic example."
"10.1198/016214503388619184","2003","A pseudoscore estimator for regression problems with two-phase sampling","9","Two-phase stratified sampling designs yield efficient estimates of population parameters in regression models while minimizing the costs of data collection. In measurement error problems, for example, error-free covariates are ascertained only for units selected in a validation sample. Estimators proposed heretofore for such designs require all units to have positive probability of being selected. We describe a new semiparametric estimator that relaxes this assumption and that is applicable to, for example, case-only or control-only validation sampling for binary regression problems. It uses a weighted empirical covariate distribution, with weights determined by the regression model, to estimate the score equations. Implementation is relatively easy for both discrete and continuous outcome data. For designs that are amenable to alternative methods, simulation studies show that the new estimator outperforms the currently available weighted and pseudolikelihood methods and often achieves efficiency comparable to that of semiparametric maximum likelihood. The simulations also demonstrate the vulnerability of the case-only or control-only designs to model misspecification. These results are illustrated by the analysis of data from a population-based case-control study of leprosy."
"10.1198/016214503388619193","2003","Asymptotic variance and convergence rates of nearly-periodic {M}arkov chain {M}onte {C}arlo algorithms","1","This article considers nearly-periodic Markov chains that may have excellent functional estimation properties but poor distributional convergence rate. It shows how simple modifications of the chain (involving using a random number of iterations) can greatly improve the distributional convergence of the chain. Various theoretical results about convergence rates of the modified chains are proven. A number of examples, including a transdimensional Markov chain Monte Carlo example, a card-shuffling example, and several antithetic Metropolis algorithms, are considered."
"10.1198/016214503388619201","2003","Doubly smoothed {EM} algorithm for statistical inverse problems","1","Recent results on discretization effects in unfolding intensity function of an indirectly observed nonhomogeneous Poisson process show that a critical feasibility condition for the strong L-2 consistency of the histogram sieve maximum likelihood estimates requires that the approximation error of the function of interest with step functions tend to 0 faster than the squared singular values of the discretized folding operator. This condition may not be fulfilled in some standard inverse problems like, for example, the deconvolution problem and the Wicksell problem discretized in a standard way. The condition may be satisfied, however, with suitably modified discrete operator and suitably constructed parametric sets for the discrete problems. Motivated by these results, an additional smoothing step is added to the EMS (expectation-maximization-smoothing) algorithm and an automatic procedure for the choice of a smoothing parameter is proposed. An application to the Wicksell problem is presented. In this context, the theory also suggests a special, nonuniform binning in the data space. A simulation study demonstrates that the new approach considerably reduces the L-2 risk when compared to both the standard EMS algorithm and to a spectral procedure specially designed for the Wicksell problem. A priori knowledge of the value of the estimated function at 0 may further reduce the risk very significantly. Theoretical results on the strong L-2 consistency and convergence rates are given."
"10.1198/016214503388619210","2003","Functional inference in frailty measurement error models for clustered survival data using the {SIMEX} approach","3","We consider frailty models for clustered survival data in the presence of measurement errors in covariates. We first show that when the measurement error is accounted for in a full likelihood analysis but the distribution of the unobserved covariate is misspecified, the maximum likelihood estimators are asymptotically biased, especially for the variance component, whose bias can be Substantial. We then discuss making inference using functional estimation via the SIMEX method where no distribution of the unobserved error-prone covariate is assumed. The SIMEX method is easy to implement by repeatedly fitting standard frailty models. We study the asymptotic properties of the SIMEX estimates and show that they are consistent and asymptotically normal. In simulation studies, we compare the SIMEX method and the likelihood method in terms of efficiency and robustness. We also propose a SIMEX score test for the variance component to test for the within-cluster correlation and evaluate its performance through simulation studies. The SIMEX variance component score test does not require specifying distributions for the random effect and the unobserved error-prone covariate, and is easy to implement by repeatedly fitting standard Cox models. The proposed methods are illustrated using the Kenya parasitemia data."
"10.1198/016214503388619229","2003","On optimality properties of the power prior","1","The power prior is a useful general class of priors that can be used for arbitrary classes of regression models, including generalized linear models, generalized linear mixed models semiparametric survival models with censored data, frailty models, multivariate models, and nonlinear models. The power prior specification for the regression coefficients focuses on observable quanities in that the elicitation is based on historical data, D-0, and a scalar quantity, a(0), quantifying the heterogeneity between the current data, D, and the historical data D-0. The power prior distribution is then constructed by raising the likelihood function of the historical data to the power a(0), where 0 less than or equal to a(0) < 1. The scalar a(0) is a precision parameter that can be viewed as a measure of compatibility between the historical and current data. In this article we give a formal justification of the power prior and show that it is an optimal class of informative priors in the sense that it minimizes a convex sum of Kullback-Leibler (KL) divergences between two specific posterior densities, in which one density is based on no incorporation of historical data and the other density is based on pooling the historical and current data. This result provides a strong motivation for using the power prior as an informative prior in Bayesian inference. In addition, we derive a formal relationship between this convex sum of KL divergences and the information-processing rules proposed by others. Specifically, we show that the power prior is a 100% efficient information-processing rule in the sense defined earlier. Several examples involving simulations as well as real datasets are examined to demonstrate the proposed methodology."
"10.1198/016214503388619238","2003","Generalized autoregressive moving average models","4","A class of generalized autoregressive moving average (GARMA) models is developed that extends the univariate Gaussian ARMA time series model to a flexible observation-driven model for non-Gaussian time series data. The dependent variable is assumed to have a conditional exponential family distribution given the past history of the process. The model estimation is carried out using an iteratively reweighted least squares algorithm. Properties of the model, including stationarity and marginal moments, are either derived explicitly or investigated using Monte Carlo simulation. The relationship of the GARMA model to other models is shown, including the autoregressive models of Zeger and Qaqish, the moving average models of Li, and the reparameterized generalized autoregressive conditional heteroscedastic GARCH model (providing the formula for its fourth marginal moment not previously derived). The model is demonstrated by the application of the GARMA model with a negative binomial conditional distribution to a well-known time series dataset of poliomyelitis counts."
"10.1198/01621450338861947","2003","Marginal likelihood and {B}ayes factors for {D}irichlet process mixture models","9","We present a method for comparing semiparametric Bayesian models, constructed under the Dirichlet process mixture (DPM) framework, with alternative semiparameteric or parameteric Bayesian models. A distinctive feature of the method is that it can be applied to semiparametric models containing covariates and hierarchical prior structures, and is apparently the first method of its kind. Formally, the method is based on the marginal likelihood estimation approach of Chib (1995) and requires estimation of the likelihood and posterior ordinates of the DPM model at a single high-density point. An interesting computation is involved in the estimation of the likelihood ordinate, which is devised via collapsed sequential importance sampling. Extensive experiments with synthetic and real data involving semiparametric binary data regression models and hierarchical longitudinal mixed-effects models are used to illustrate the implementation, performance, and applicability of the method."
"10.1198/016214503388619256","2003","Outlier detection and false discovery rates for whole-genome {DNA} matching","1","We define a statistic, called the matching statistic, for locating regions of the genome that exhibit excess similarity among cases when compared to controls. Such regions are reasonable candidates for harboring disease genes. We find the asymptotic distribution of the statistic while accounting for correlations among sampled individuals. We then use the Benjamini and Hochberg false discovery rate (FDR) method for multiple hypothesis testing to find regions of excess sharing. The p values for each region involve estimated nuisance parameters. Under appropriate conditions, we show that the FDR method based on p values and with estimated nuisance parameters asymptotically preserves the FDR property. Finally, we apply the method to a pilot study on schizophrenia."
"10.1198/016214503000000288","2003","Transform estimation of parameters for stage-frequency data","1","We consider multistage development models that occur in the maturation of biological organisms, disease progressions, and industrial processes. The situations that we address are distinguished by the essentially destructive sampling required to assess the stage reached by each individual. We develop robust estimators of stage-dependent maturation rates and overall death rates using method of moments based on Laplace transforms, for which we develop variance estimates under different sampling schemes. We apply these methods to studies of cattle parasite and grasshopper life cycles and show that the Laplace methods compare well with maximum likelihood methods."
"10.1198/016214503000000305","2003","The intrinsic distribution and selection bias of long-period cometary orbits","1","A question that arises in the study of cometary orbits is whether or not the directed normals to the orbits are uniformly distributed on the celestial sphere. Previous studies by statisticians have not taken selection effects into account and have tended to reject uniformity. Here a plausible selection mechanism is proposed that gives rise to a one-parameter family of distributions on the sphere. Data on long-period comets are analyzed using this one-parameter family. A nonzero selection effect is detected, and its size is estimated. Subject to this selection effect, uniformity of the directed normals can no longer be ruled out."
"10.1198/016214503000000323","2003","Estimating the large-scale structure of the universe using quasi-stellar object carbon {IV} absorbers","0","Galaxies have long been known to form large clusters, and cosmologists are interested in characterizing this clustering as a way of studying the large-scale structure of the universe. This work is motivated by a data catalog consisting of information on lines of sight from Earth to distant quasi-stellar objects (QSO's) and the carbon iv absorbers that lie on them. The absorbers are believed to be gas clouds near galaxies too far away to be easily observed. Thus, the absorber catalog provides a unique and interesting way to examine the large-scale structure of the universe. On large scales previous studies have mainly used pairs of absorbers on the same lines of sight to obtain estimates describing the clustering of absorbers. It is clear that absorbers on different lines of sight contain information about the degree of clustering. We develop an adaptation of the rigid motion corrected estimator of the reduced second-moment function for the absorber catalog taking into account the across-line-of-sight information. We show how to compute this estimator efficiently using the weights for an isotropic estimator proposed in a recent study. We also show how the modified versions of the rigid motion estimators can be obtained. Simulations suggest that using the modified rigid motion correction estimator may reduce standard errors by 5-20% on scales from 50 to 250 h(-1) Mpc for a set of 100 lines of sight."
"10.1198/016214503000000341","2003","How much does the far future matter? {A} hierarchical {B}ayesian analysis of the public's willingness to mitigate ecological impacts of climate change","0","How much does the far future matter? This question lies at the heart of many important environmental policy issues, such as global climate change, biodiversity loss, and the disposal of radioactive waste. Although philosophers, experts, and others offer their viewpoints on this deep question, the solution to many environmental problems lies in the willingness of the public to bear significant costs now to make the far future a better place. Short of national plebiscites, the only way to assess the public's willingness to mitigate impacts in the far future is to ask them. Using a unique set of survey data in which respondents were provided with sets of scenarios describing different amounts of forest loss due to climate change, along with associated mitigation methods and costs, we can infer the respondents' willingness to bear additional costs to mitigate future ecological impacts of climate change. The survey also varied the timing of the impacts, which allows us to assess how the willingness to mitigate depends on the timing of the impacts. The responses to the survey questions are a consequence of latent utilities with complex ordinal structures that result in nonrectangular probabilities. Whereas the nonrectangular probabilities complicate standard maximum likelihood-based approaches, we show how the nonrectangular probabilities fit neatly into a hierarchical Bayesian model. We show how to fit these models using the Gibbs sampler, overcoming problems in parameter identification to improve mixing of the induced Markov chain. The results indicate that the public's willingness to incur additional costs to mitigate ecological impacts of climate change is an increasing nonlinear function of the magnitude of the impact, and that they discount future impacts at somewhat less than 1% per year."
"10.1198/016214503000000369","2003","Measurement of higher education in the census and current population survey","0","We examine measurement error in the reporting of higher education in the 1990 Decennial Census and the post-1991 Current Population Survey (CPS). We document that measurement error in the reporting of higher education is prevalent in Census data. Further, these errors violate models of classical measurement error in important ways. The level of education is consistently reported as higher than it is (errors are not mean 0), errors in the reporting of education are correlated with covariates that appear in earnings regressions, and errors in the reporting of education appear correlated with the error term in a model of earnings determination. Thus, neither well-known results on classical measurement error nor recent models of nonclassical measurement error are likely valid when using Census and CPS data. We find some evidence that the measurement error is lower in the CPS than in the Census, presumably because first interviews are generally conducted in person."
"10.1198/016214503000000387","2003","Dynamic latent trait models for multidimensional longitudinal data","5","This article presents a new approach for analysis of multidimensional longitudinal data, motivated by studies using an item response battery to measure traits of an individual repeatedly over time. A general modeling framework is proposed that allows mixtures of count, categorical, and continuous response variables. Each response is related to age-specific latent traits through a generalized linear model that accommodates item-specific measurement errors. A transition model allows the latent traits at a given age to depend on observed predictors and on previous latent traits for that individual. Following a Bayesian approach to inference, a Markov chain Monte Carlo algorithm is proposed for posterior computation. The methods are applied to data from a neurotoxicity study of the pesticide methoxychlor, and evidence of a dose-dependent increase in motor activity is presented."
"10.1198/016214503000000404","2003","A model of the joint distribution of purchase quantity and timing","0","Prediction of purchase timing and quantity decisions of a household is an important element for success of any retailer. This is especially so for an online retailer, as the traditional brick and-mortar retailer would be more concerned with total sales. A number of statistical models have been developed in the marketing literature to aid traditional retailers in predicting sales and analyzing the impact of various marketing activities on sales. However, there are two important differences between traditional retail outlets and the increasingly important online retail/delivery companies, differences that prevent these firms from using models developed for the traditional retailers: (1) the profits of the online retailer/delivery company depend on purchase frequency and on purchase quantity, whereas the profits of traditional retailers are simply tied to total sales, and (2) customers in the tails of the frequency distribution are more important to the delivery company than to the retail outlet. Both of these differences are due to the fact that the delivery companies incur a delivery cost for each sale, whereas customers themselves travel to retail outlets when buying from traditional retailers. These differences in costs translate directly into needs that a model must address. For a model intended to be useful to online retailers, the dependent variable should be a bivariate distribution of frequency and quantity, and the frequency distribution must accurately represent consumers in the tails. In this article we develop such a model and apply it to predicting the consumer's joint decision of when to shop and how much to spend at the store. Our approach is to model the marginal distribution of purchase timing and the distribution of purchase quantity conditional on purchase timing. We propose a hierarchical Bayes model that disentangles the weekly and daily components of the purchase timing. The daily component has a dependence on the weekly component, thereby accounting for strong observed periodicity in the data. For the purchase times, we use the Conway-Maxwell-Poisson distribution, which we find useful to fit data in the tail regions (extremely frequent and infrequent purchasers)."
"10.1198/016214503000000422","2003","Wavelet-based nonparametric modeling of hierarchical functions in colon carcinogenesis","10","In this article we develop new methods for analyzing the data from an experiment using rodent models to investigate the effect of type of dietary fat on O-6-methylguanine-DNA-methyltransferase (MGMT), an important biomarker in early colon carcinogenesis. The data consist of observed profiles over a spatial variable contained within a two-stage hierarchy, a structure that we dub hierarchical functional data. We present a new method providing a unified framework for modeling these data, simultaneously yielding estimates and posterior samples for mean, individual, and subsample-level profiles, as well as covariance parameters at the various hierarchical levels. Our method is nonparametric in that it does not require the prespecification of parametric forms for the functions and involves modeling in the wavelet space, which is especially effective for spatially heterogeneous functions as encountered in the MGMT data. Our approach is Bayesian; the only informative hyperparameters in our model are effectively smoothing parameters. Analysis of this dataset yields interesting new insights into how MGMT operates in early colon carcinogenesis, and how this may depend on diet. Our method is general, so it can be applied to other settings where hierarchical functional data are encountered."
"10.1198/016214503000000512","2003","Order-preserving nonparametric regression, with applications to conditional distribution and quantile function estimation","1","In some regression problems we observe a ""response"" Y-ti to level t of a ""treatment"" applied to an individual with level Xi of a given characteristic, where it has been established that response is monotone increasing in the level of the treatment. A related problem arises when estimating conditional distributions, where the raw data are typically independent and identically distributed pairs (X-i, Z(i)), and Y-ti denotes the proportion of Z(i)'s.that do not exceed t. We expect the regression means g(t)(x) = E(Y-ti\X-i = x) to enjoy the same order relation as the responses, that is, g(t) less than or equal to g(s) whenever s less than or equal to t. This requirement is necessary to obtain bona fide conditional distribution functions, for example. If we estimate g(t) by passing a linear smoother through each dataset chi(t) = {(X-i, Y-ti) : 1 less than or equal to i less than or equal to n}, then the order-preserving property is guaranteed if and only if the smoother has nonnegative weights. However, in such cases the estimators generally have high levels of boundary bias. On the other hand, the order-preserving property usually fails for linear estimators with low boundary bias, such as local linear estimators, or kernel estimators employing boundary kernels. This failure is generally most serious at boundaries of the distribution of the explanatory variables, and ironically it is often in just those places that estimation is, of greatest interest, because responses there imply constraints on the larger population. In this article we suggest nonlinear, order-invariant estimators for nonparametric regression, and discuss their properties. The resulting estimators are applied to the estimation of conditional distribution functions at endpoints and also changepoints. The availability of bona fide distribution function estimators at endpoints also enables the computation of changepoint diagnostics that are based on differences in a suitable norm between two estimated conditional distribution functions, obtained from data that fall into one-sided bins."
"10.1198/016214503000000521","2003","Calibrating the degrees of freedom for automatic data smoothing and effective curve checking","9","Curve fitting and curve checking based on the local polynomial regression technique are commonly used data-analytic methods in statistics. This article examines, in nonparametric settings, both the asymptotic expressions and empirical formulas for degrees of freedom (DF), a notion introduced by Hastie and Tibshirani, of linear smoothers. The asymptotic results give useful insights into the nonparametric modeling complexity. Meanwhile, by substituting the exact DFs by the empirical formula, an empirical version of the generalized cross-validation (EGCV) is obtained. An automatic bandwidth selection method based on minimizing EGCV is proposed for conducting local smoothing. This procedure preserves full benefits of the ordinary and generalized cross-validation, but offers a substantial reduction in computational burden. Furthermore, the EGCV-minimizing bandwidth can be extended in a very simple manner to fit multivariate models, such as the varying-coefficient models. Applications of calibrating DFs to important inferential issues, such as assessing the validity of useful model assumptions and measuring the significance of predictor variables based on the generalized likelihood ratio statistics are also discussed. Simulation studies are presented to illustrate the performance of the proposed procedures in a range of statistical problems."
"10.1198/016214503000000530","2003","Semiparametric estimation of multivariate fractional cointegration","0","We consider the semiparametric estimation of fractional cointegration in a multivariate process of cointegrating rank r > 0. We estimate the cointegrating relationships by the eigenvectors corresponding to the r smallest eigenvalues of an averaged periodogram matrix of tapered, differenced observations. The number of frequencies m used in the periodogram average is held fixed as the sample size grows. We first show that the averaged periodogram matrix converges in distribution to a singular matrix whose null eigenvectors span the space of cointegrating vectors. We then show that the angle between the estimated cointegrating vectors and the space of true cointegrating vectors is O-p (n(du-d)), where d and d(u) are the memory parameters of the observations and cointegrating errors. The proposed estimator is invariant to the labeling of the component series and thus does not require that one of the variables be specified as a dependent variable. We determine the rate of convergence of the r smallest eigenvalues of the periodogram matrix and present a criterion that allows for consistent estimation of r. Finally, we apply our methodology to the analysis of fractional cointegration in interest rates."
"10.1198/016214503000000549","2003","Smoothing spline {ANOVA} for time-dependent spectral analysis","4","In this article we propose a smoothing spline ANOVA model (SS-ANOVA) to estimate and to make inference on the time-varying logspectrum of a locally stationary process. The time-varying spectrum is assumed to be smooth in both time and frequency. This assumption essentially turns a time-frequency spectral estimation problem into a 2-dimensional surface estimation problem. A smooth localized complex exponential (SLEX) basis is used to calculate the initial periodograms, and a SS-ANOVA is fitted to the log-periodograms. This approach allows the time and frequency domains to be modeled in a unified approach and jointly estimated, Inference procedures, such as confidence intervals, and hypothesis tests proposed for the SS-ANOVA can be adopted for the time-varying spectrum. Because of the smoothness assumption of the underlying spectrum, once we have the estimates on a time-frequency grid, we can calculate the estimate at any given time and frequency. This leads to a high computational efficiency, because for large datasets we need only estimate the initial raw periodograms at a much coarser grid. We study a penalized least squares estimator and a penalized Whittle likelihood estimator. The penalized Whittle likelihood estimator has smaller mean squared errors, whereas inference based on the penalized least squares method can adopt existing results. We present simulation results and apply our method to electroencephalogram data recorded during an epileptic seizure."
"10.1198/016214503000000558","2003","Evolutionary similarity among genes","0","An evolutionary history of a set of organisms is a family tree, or topology, with branches of various lengths between vertices that describe how closely the organisms are related to each other. We consider the K evolutionary histories of K genes from a set of N organisms. Evolutionary similarity (ES) occurs when the branching patterns and relative branch lengths in the K evolutionary histories of the genes are the same or nearly the same across the set of organisms. Evolutionary similarity indicates similarity of evolutionary pressures acting on these genes. Current likelihood approaches identify ES conditional on a given topology. For a variety of reasons, different genes may support different topologies when fit independently. We use Bayesian models and reversible-jump Markov chain Monte Carlo to jointly infer topology and branch lengths for multiple genes simultaneously. We test for ES using Bayes factors, conditionally on a consistent topology over the multiple genes, where the topology is either known or unknown. We relax the single topology assumption by employing a dissimilarity measure between evolutionary histories and testing for ES using both prior and posterior predictive p values. We apply our methodology to three genes (DAX1, SOX9, and SRY) believed to be involved in sex determination in primates. We find support in the data for ES between DAX1 and SRY, but not SOX9. These results are consistent with the hypothesized biological roles of these genes."
"10.1198/016214503000000567","2003","Frequency of recurrent events at failure time: modeling and inference","0","Recurrent events arise in many longitudinal medical studies where time to a terminal event or failure is the primary endpoint. With incomplete follow-up data, the analysis of recurrent events is a challenge owing to their association with the failure. One specific quantity of interest rarely addressed in the statistical literature is the recurrence frequency at the failure time; an example is hospitalization frequency, which is often used as a rough measure of lifetime medical cost. In this article we show that a marginal model (e.g., the log-linear model) of the recurrence frequency, although desirable, is typically not identifiable. For this reason, we advocate modeling the recurrent events and the failure time jointly, and propose an approach to forming semiparametric joint models from prespecified marginal ones. We suggest two conceptually simple and nested regression models aiming at the recurrence frequency as a mark of the failure and at the process of recurrent events. We formulate monotone estimating functions and propose novel interval-estimation procedures to accommodate nonsmooth estimating functions. The resulting estimators are consistent and asymptotically normal. Simulation studies and the application to an AIDS clinical trial exhibit that these proposals are easy to implement and reliable for practical use. Finally, we generalize our proposals to marked recurrent events, and also devise a global inference procedure for recurrent events of multiple types."
"10.1198/016214503000000576","2003","Optimality, variability, power: evaluating response-adapative randomization procedures for treatment comparisons","9","We provide a theoretical template for the comparison of response-adaptive randomization procedures for clinical trials. Using a Taylor expansion of the noncentrality parameter of the usual chi-squared test for binary responses, we show explicitly the relationship among the target allocation proportion, the bias of the randomization procedure from that target, and the variability induced by the randomization procedure. We also generalize this relationship for more than two treatments under various multivariate alternatives. This formulation allows us to directly evaluate and compare different response-adaptive randomization procedures and different target allocations in terms of power and expected treatment failure rate without relying on simulation. For K = 2 treatments, we compare four response-adaptive randomization procedures and three target allocations based on multiple objective optimality criteria. We conclude that the drop-the-loser rule and the doubly adaptive biased coin design are clearly superior to sequential maximum likelihood estimation or the randomized play-the-winner rule in terms of decreased variability, but the latter is preferable because it can target any desired allocation. We discuss how the template developed in this article is useful in the design and evaluation of clinical trials using response-adaptive randomization."
"10.1198/016214503000000585","2003","Robust and efficient designs for the {M}ichaelis-{M}enten model","4","For the Michaelis-Menten model, we determine designs that maximize the minimum of the D-efficiencies over a certain interval for the nonlinear parameter. The best two point designs can be found explicitly, and a characterization is given when these designs are optimal within the class of all designs. In most cases of practical interest, the determined designs are highly efficient and robust with respect to misspecification of the nonlinear parameter. The results are illustrated and applied in an example of a hormone receptor assay."
"10.1198/016214503000000594","2003","Characterizing additively closed discrete models by a property of their maximum likelihood estimators, with an application to generalized {H}ermite distributions","1","This article reports on two-parameter count distributions (satisfying very general conditions) that are closed under addition so that their maximum likelihood estimator (MLE) of the population mean is the sample mean. The most important of these in practice, the generalized Hermite distribution, is analyzed, and a necessary and sufficient condition is given to ensure that the MLE is the solution of likelihood equations. Score test to contrast the Poisson assumption is studied, and two examples of applications are given."
"10.1198/016214503000000602","2003","On the comparison of reliability experiments based on the convolution order","0","In this article we study the comparison of experiments in system reliability theory when the component lifetimes are independent and identically distributed random variables that have a common two-parameter exponential distribution with a location parameter theta. For this purpose, we define a new stochastic order, which we call the convolution order, and study some basic properties of it. We then focus our attention on the family of distribution functions that are mixtures of distributions of partial sums of independent exponential random variables, and derive results that identify several conditions under which members of this family are ordered in the convolution stochastic order. We apply the results to order lifetimes of coherent systems, and as a consequence we obtain information inequalities among various lifetimes of coherent systems. We find situations wherein high reliability decreases statistical information."
"10.1198/016214503000000611","2003","Multiple imputation for incomplete data with semicontinuous variables","0","We consider the application of multiple imputation to data containing not only partially missing categorical and continuous variables, but also partially missing 'semicontinuous' variables (variables that take on a single discrete value with positive probability but are otherwise continuously distributed). As an imputation model for data sets of this type, we introduce an extension of the standard general location model proposed by Olkin and Tate; our extension, the blocked general location model, provides a robust and general strategy for handling partially observed semicontinuous variables. In particular, we incorporate a two-level model for the semicontinuous variables into the general location model. The first level models the probability that the semicontinuous variable takes on its point mass value, and the second level models the distribution of the variable given that it is not at its point mass. In addition, we introduce EM and data augmentation algorithms for the blocked general location model with missing data; these can be used to generate imputations under the proposed model and have been implemented in publicly available software. We illustrate our model and computational methods via a simulation study and an analysis of a survey of Massachusetts Megabucks Lottery winners."
"10.1198/016214503000000620","2003","The mean squared error of small area predictors constructed with estimated area variances","2","In the small area estimation literature, the sampling error variances are customarily assumed to be known or to depend on a finite number of parameters. We consider the empirical best linear unbiased predictor (EBLUP) obtained by using the individual directly estimated variance for each small area. An approximation for the mean squared error (MSE) of the EBLUP that recognizes the impact on the predictors of estimation of the variance components is derived. Simulation studies show that the theoretical expressions are good approximations for the MSE of the predictors unless the between-area variance component is very small (relative to the within-area variance). An improved estimator of the MSE is developed that has smaller overestimation than the original estimator when the between-area variance component is small. The robustness of the MSE estimator is studied and predictors for nonnormal sampling errors are proposed. An example from the National Resources Inventory that motivated the development of the theory is described."
"10.1198/016214503000000639","2003","On {$\psi$}-learning","12","The concept of large margins have been recognized as an important principle in analyzing learning methodologies, including boosting, neural networks, and support vector machines (SVMs). However, this concept alone is not adequate for learning in nonseparable cases. We propose a learning methodology, called psi-learning, that is derived from a direct consideration of generalization errors. We provide a theory for psi-learning and show that it essentially attains the optimal rates of convergence in two learning examples. Finally, results from simulation studies and from breast cancer classification confirm the ability of psi-learning to outperform SVM in generalization."
"10.1198/016214503000000648","2003","A {B}ayesian solution for a statistical auditing problem","0","Auditors often consider a stratified finite population where each unit is classified as either acceptable or in error. Based on a random sample, the auditor may be required to give an upper confidence bound for the number of units in the population that are in error. In other cases the auditor may need to give a p value for the hypothesis that at least 5% of the units in the population are in error. Frequentist methods for these problems are not straightforward and can be difficult to compute. Here we give a noninformative Bayesian solution for these problems. This approach is easy to implement and is shown to have good firequentist properties."
"10.1198/016214503000000657","2003","Likelihood-based inference in some continuous exponential families with unknown threshold parameters","0","We study likelihood-based inference in some continuous exponential families with unknown threshold parameters. The introduction of threshold parameters necessitates modification of the standard asymptotic arguments, and some possibly unexpected limiting distributions result."
"10.1198/016214503000000666","2003","Finding the number of clusters in a dataset: an information-theoretic approach","4","One of the most difficult problems in cluster analysis is identifying the number of groups in a dataset. Most previously suggested approaches to this problem are either somewhat ad hoc or require parametric assumptions and complicated calculations. In. this article we develop a simple, yet powerful nonparametric method for choosing the number of clusters based on distortion, a quantity that measures the average distance, per dimension, between each observation and its closest cluster center. Our technique is computationally efficient and straightforward to implement. We demonstrate empirically its effectiveness, not only for choosing the number of clusters, but also for identifying underlying structure, on a wide range of simulated and real world datasets. In addition, we give a rigorous theoretical justification for the method based on information-theoretic ideas. Specifically, results from the subfield of electrical engineering known as rate distortion theory allow us to describe the behavior of the distortion in both the presence and absence of clustering. Finally, we note that these ideas potentially can be extended to a wide range of other statistical model selection problems."
"10.1198/016214503000035","2003","Mixed-model functional {ANOVA} for studying human tactile perception","0","Human tactile perception is studied through digitized images of hand-drawn curves generated by subjects treated with various facial preparations. The drawings represent their subjective assessment of a small brush moving across the faced In this study, exploratory data analysis is carried out through a functional data analog of principal components analysis and curve decomposition based on Fourier representations. In addition, high-dimensional analysis of variance is adapted for mixed-model functional data and applied to test whether preparation effects. are, significant."
"10.1198/016214503000044","2003","Cross-calibration of stroke disability measures: {B}ayesian analysis of longitudinal ordinal categorical data using negative dependence","0","It is common to assess disability of stroke patients using standardized scales, such as the Rankin Stroke Outcome Scale (RS) and the Barthel Index (BI). The RS, which was designed for applications to stroke, is based on assessing directly the global conditions of a patient. The BI, which was designed for more general applications, is based on a series of questions about the patient's ability to carry out 10 basic activities of daily living. Because both scales are commonly used, but few studies use both, translating between scales is important in gaining an overall understanding of the efficacy of alternative treatments, and in developing prognostic models that combine several datasets. The objective of our analysis is to provide a tool for translating between BI and RS. Specifically, we estimate the conditional probability distributions of each given the other. Subjects consisted of 459 individuals who sustained a stroke and who were recruited for the Kansas City Stroke Study from 1995 to 1998. We assessed patients with BI and RS measures 1, 3, and 6 months after stroke. In addition, we included data from the Framingham study, in the form of a table cross-classifying patients by RS and coarsely aggregated BI. Our statistical estimation approach is motivated by several goals: (a) overcoming the difficulty presented by the fact that our two sources report data at different resolutions; (b) smoothing the empirical counts to provide estimates of probabilities in regions of the table that are sparsely populated; (c) avoiding estimates that would conflict with medical knowledge about the relationship between the two measures; and (d) estimating the relationship between RS and BI at three months after the stroke, while borrowing strength from measurements made at I month and 6 months. We address these issues via a Bayesian analysis combining data augmentation and constrained semiparametric inference. Our results provide the basis for comparing and integrating the results of clinical trials using different disability measures, and integrating clinical trials results into a comprehensive decision model for the assessment of long-term implications and cost-effectiveness of stroke prevention and acute treatment interventions. In addition, our results, indicate that the degree of agreement between the two measures is less strong than commonly reported, and emphasize the importance of trial designs that include multiple assessments of outcome."
"10.1198/016214503000053","2003","Hierarchical models for permutations: analysis of auto racing results","1","The popularity of the sport of auto racing is increasing rapidly, but its fans remain less interested in statistics than the fans of other sports. In this article, we propose a new class of models for permutations that closely resembles the behavior of auto racing results. We pose the model in a Bayesian hierarchical framework, which permits hierarchical specification and fully hierarchical estimation of interaction terms. We demonstrate the methodology using several rich datasets that consist of repeated rankings for a collection of drivers. Our models can potentially identify individuals racing in ""minor league"" divisions who have higher potential for competitive performance at higher levels. We also present evidence that one of the sport's more controversial figures, Jeff Gordon, is a statistically dominant figure."
"10.1198/016214503000062","2003","Recounts from undervotes: evidence from the 2000 presidential election","1","The vote recount in the 2000 Presidential election (Broward, Miami-Dade and Palm Beach Counties, Florida) is examined for evidence of bias. A precinct-level dataset is constructed, incorporating the machine-vote tally, the recount vote tally, voter registration demographics, and the ballot review by media sources. A new multivariate beta-logit model is introduced that allows joint modeling of multivariate unobserved latent probabilities. A simple two-step estimator is proposed that approximates the joint maximum likelihood estimator. The estimates are consistent with a strong hypothesis: that the recount vote tally was unbiased. Specifically, it is found that the precinct-level machine-vote probability for a candidate is an unbiased predictor for the hand-recount undervote probability. There is no evidence of bias in the recount."
"10.1198/016214503000071","2003","Principal stratification approach to broken randomized experiments: a case study of school choice vouchers in {N}ew {Y}ork {C}ity","5","The precarious state of the educational system in the inner cities of the United States, as well as its potential causes and solutions, have been popular topics of debate in recent years. Part of the difficulty in resolving this debate is the lack of solid empirical evidence regarding the true impact of educational initiatives. The efficacy of so-called ""school choice"" programs has been a particularly. contentious issue. A current multimillion dollar program,,the School Choice Scholarship Foundation Program in New York, randomized the distribution of vouchers in an attempt to shed some light on this issue. This is an important, time for school choice, because on June. 27, 2002 the U.S. Supreme Court upheld the constitutionality of a voucher program in Cleveland that provides scholarships both to secular and religious private schools. Although this study benefits immensely from a randomized design, it suffers from complications common to such research with human subjects: noncompliance with assigned ""treatments"" and missing data. Recent work has revealed threats to valid estimates of experimental effects that exist in the presence of noncompliance and missing data, even when the goal is to estimate simple intention-to-treat effects. Our goal was to create a better solution when faced with both noncompliance and missing data. This article presents a model that accommodates these complications that is based on the general framework of ""principal stratification"" and thus relies on more plausible assumptions than standard methodology. Our analyses revealed positive effects on math scores for children who applied to the program from certain types of schools-those with average test scores below the citywide median. Among these children, the effects are stronger for children who applied in the first grade and for African-American children."
"10.1198/016214503000125","2003","Boosting with the {$L_2$} loss: regression and classification","7","This article investigates a computationally simple variant of boosting, L(2)Boost, which is constructed from a functional gradient descent algorithm with the L-2-loss function. Like other boosting algorithms, L(2)Boost uses many times in an iterative fashion a prechosen fitting method, called the learner. Based on the explicit expression of refitting of residuals of L(2)Boost, the case with (symmetric) linear learners is studied in detail in both regression and classification. In particular, with the boosting iteration m working as the smoothing or regularization parameter, a new exponential bias-variance trade-off is found with the variance (complexity) term increasing very slowly as m tends to infinity. When the learner is a smoothing spline, an optimal rate of convergence result holds for both regression and classification and the boosted smoothing spline even adapts to higher-order, unknown smoothness. Moreover, a simple expansion of a (smoothed) 0-1 loss function is derived to reveal the importance of the decision boundary, bias reduction, and impossibility of an additive bias-variance decomposition in classification. Finally, simulation and real dataset results are obtained to demonstrate the attractiveness of L(2)Boost. In particular, we demonstrate that L(2)Boosting with a novel component-wise cubic smoothing spline is both practical and effective in the presence of high-dimensional predictors."
"10.1198/016214503000134","2003","A model-free test for reduced rank in multivariate regression","6","We propose a test of dimension in multivariate regression. This test is in the spirit of tests on the rank of the coefficient matrix in a multivariate linear model, but it does not require a prespecified model. The test may be particularly useful at the outset of an analysis before a multivariate model is posited, because it can lead to low-dimensional summary plots that are inferred to contain all of the sample information on the multivariate mean function."
"10.1198/016214503000143","2003","Generalized nonlinear modeling with multivariate free-knot regression splines","1","A Bayesian method is presented for the nonparametric modeling of univariate and multivariate non-Gaussian response data. Data-adaptive multivariate regression splines are used where the number and location of the knot points are treated as random. The posterior model space is explored using a reversible-jump Markov chain Monte Carlo sampler. Computational difficulties are partly alleviated by introducing a random residual effect in the model that leaves many of the posterior conditional distributions of the model parameters in standard form. The use of the latent residual effect provides a convenient vehicle for modeling correlation in multivariate response data, and as such our method can be seen to generalize the seemingly unrelated regression model to non-Gaussian data. We illustrate the method on a number of examples, including two previously unpublished datasets relating to the spatial smoothing of multivariate accident data in Texas and the modeling of credit card use across multiple retail sectors."
"10.1198/016214503000152","2003","Identification of linear directions in multivariate adaptive spline models","0","Identifying linear directions in multivariate regression has been a statistical challenge and has attracted particular attention since projection pursuit was developed. To this end, we propose and investigate the use of linear discriminant analysis and projection Hessian directions in conjunction with multivariate adaptive regression splines. Simulation studies in a variety of settings demonstrate the usefulness of our approach in revealing both the functional forms and the linear substructures based on the observed data. Mathematical results are also provided to support our approach. Comparisons are made between our approach and existing approaches, and the improvements are evident. Depending on the circumstance, the extent of improvement can be substantial."
"10.1198/016214503000161","2003","Nonlinear state-space models with state-dependent variances","1","Nonlinear state-space models with state-dependent variances (SDVs) are commonly used in financial time series. Important examples include stochastic volatility (SV) and affine term structure models. We propose a methodology for state smoothing in this class of models. Our smoothing technique is simulation based and uses an auxiliary mixture model. Key features of the auxiliary mixture model are the use of state-dependent weights and efficient block sampling algorithms to jointly update all unobserved states given latent mixture indicators. Conditional on latent indicator variables, the auxiliary mixture model reduces to a normal dynamic linear model. We illustrate our methodology with two time series applications. First, we show how to construct the auxiliary model for a logarithmic SV model and compare the performance of our methodology with the current literature. Next, we implement a square-root SV model with jumps for short-term interest rates in Hong Kong."
"10.1198/016214503000170","2003","Spatial modeling with spatially varying coefficient processes","5","In many applications, the objective is to build regression models to explain a response variable over a region of interest under the assumption that the responses are spatially correlated. In nearly all of this work, the regression coefficients are assumed to be constant over the region. However, in some applications, coefficients are expected to vary at the local or subregional level. Here we focus on the local case. Although parametric modeling of the spatial surface for the coefficient is possible, here we argue that it is more natural and flexible to view the surface as a realization from a spatial process. We show how such modeling can be formalized in the context of Gaussian responses providing attractive interpretation in terms of both random effects and explaining residuals. We also offer extensions to generalized linear models and to spatio-temporal setting. We illustrate both static and dynamic modeling with a dataset that attempts to explain (log) selling price of single-family houses."
"10.1198/016214503000189","2003","Clustering for sparsely sampled functional data","15","We develop a flexible model-based procedure for clustering functional data. The technique can be applied to all types of curve data but is particularly useful when individuals are observed at a sparse set of time points. In addition to producing final cluster assignments, the procedure generates predictions and confidence intervals for missing portions of curves. Our approach also provides many useful tools for evaluating the resulting models. Clustering can be assessed visually via low-dimensional representations of the curves, and the regions of greatest separation between clusters can be determined using a discriminant function. Finally, we extend the model to handle multiple functional and finite-dimensional covariates and show how it can be applied to standard finite-dimensional clustering problems involving missing data."
"10.1198/016214503000198","2003","Semiparametric regression for the area under the receiver operating characteristic curve","1","Medical advances continue to provide new and potentially better means for detecting disease. Such is true in cancer, for example, where biomarkers are sought for early detection and where improvements in imaging methods may pick up the initial functional and molecular changes associated with cancer development. In other binary classification tasks, computational algorithms such as neural networks, support vector machines, and evolutionary algorithms have been applied to areas as diverse as credit scoring, object recognition, and peptide-binding prediction. Before a classifier becomes an accepted technology, it must undergo rigorous evaluation to determine its ability to discriminate between states. Characterization of factors influencing classifier performance is an important step in this process. Analysis of covariates may reveal subpopulations in which classifier performance is greatest or identify features of the classifier that improve accuracy. We develop regression methods for the nonparametric area under the receiver operating characteristic curve, a well-accepted summary measure of classifier accuracy. The estimating function generalizes standard approaches and, interestingly, is related to the two-sample Mann-Whitney U statistic. Implementation is straightforward, because it is an adaptation of binary regression methods. Asymptotic theory is nonstandard, because the regressor variables are cross-correlated. Nevertheless, simulation studies show that the method produces estimates with small bias and reasonable coverage probability. Application of the method to evaluate the covariate effects on a new device for diagnosing hearing impairment reveals that the device performs better in more severely impaired subjects and that certain test parameters, which are adjustable by the device operator, are key to test performance."
"10.1198/016214503000206","2003","Sample size reestimation for clinical trials with censored survival data","1","A flexible design with updating of sample size in clinical trials is proposed for censored survival data. Patients enter trials serially and are subject to random loss to follow-up. The statistical inference is based on a weighted average of the linear rank statistics, where the weight function at each look depends on prior observed data. A stopping rule is devised to allow early termination and acceptance of the null hypothesis when the experimental treatment offers no advantage or is inferior to the control. The null hypothesis that two survival distributions are equal may be, rejected only at the last step when the weight function is used up. The overall type I error rate is preserved. Independent increments for the sequential linear rank statistic is key to deriving asymptotic properties of the test statistic. Some extensive simulations have been carried out to compare the operating characteristics of the method under different scenarios, and for a comparison with the usual log-rank test with fixed sample design. The methodology is also illustrated with a colon cancer clinical trial."
"10.1198/016214503000215","2003","Inferring spatial phylogenetic variation along nucleotide sequences: a multiple changepoint model","0","We develop a Bayesian multiple changepoint model to infer spatial phylogenetic variation (SPV) along aligned molecular sequence data. SPV occurs in sequences from organisms that have undergone biological recombination or when evolutionary rates and selective pressures vary, along the sequences. This Bayesian approach permits estimation of uncertainty regarding recombination, the crossing-over locations, and all other model parameters. The model assumes that the sites along the data separate into an unknown number of contiguous segments, each with possibly different evolutionary relationships between organisms, evolutionary rates, and transition: transversion ratios. We develop a transition kernel, use reversible-jump Markov chain Monte Carlo to fit our model, and draw inference from both simulated and real data. Through simulation, we examine the minimal length recombinant segment that our model can detect for several levels of evolutionary divergence. We examine the entire genome of a reported human immunodeficiency virus (HIV)-1 isolate, related to a purported recombinant virus thought to be the causative agent of an epidemic outbreak of HIV-1 infection among intravenous drug users in Russia. We find that regions of the genome differ in their evolutionary history and selective pressures. There is strong evidence for multiple crossovers along the genome and frequent shifts in selective pressure changes throughout the vif through env genes."
"10.1198/016214503000224","2003","Detecting differentially expressed genes in microarrays using {B}ayesian model selection","9","DNA microarrays open up a broad new horizon for investigators interested in studying the genetic determinants of disease. The high throughput nature of these arrays, where differential expression for thousands of genes can be measured simultaneously, creates an enormous wealth of information, but also poses a challenge for data analysis because of the large multiple testing problem involved. The solution has generally been to focus on optimizing false-discovery rates while sacrificing power. The drawback of this approach is that more subtle expression differences will be missed that might give investigators more insight into the genetic environment necessary for a disease process to take hold. We introduce a new method for detecting differentially expressed genes based on a high-dimensional model selection technique, Bayesian ANOVA for microarrays (BAM), which strikes a balance between false rejections and false nonrejections. The basis of the new approach involves a weighted average of generalized ridge regression estimates that provides the benefits of using shrinkage estimation combined with model averaging. A simple graphical tool based on the amount of shrinkage is developed to visualize the trade-off between low false-discovery rates and finding more genes. Simulations are used to illustrate BAM's performance, and the method is applied to a large database of colon cancer gene expression data. Our working hypothesis in the colon cancer analysis is that large differential expressions may not be the only ones contributing to metastasis-in fact, moderate changes in expression of genes may be involved in modifying the genetic environment to a sufficient extent for metastasis to occur. A functional biological analysis of gene effects found by BAM, but not other false-discovery-based approaches, lends support to this hypothesis."
"10.1198/016214503000233","2003","Surveillance of a simple linear regression","1","This article considers an important aspect of the general sequential analysis problem where a process is in control up to some unknown point i = v - 1, after which the distribution from which the observations are generated changes. An extensive sequential analytic literature assumes that the change in distribution is abrupt, for example, from N(0, 1) to N(mu, 1). There is also an extensive literature that deals with a gradual change in the case where the decision (whether or not a change has occurred) is based on a fixed set of observations, rather than an ongoing process of decision making every time a new observation is obtained. However, there is virtually no literature on the practical case of sequentially detecting a gradual change in distribution (visualize a machine deteriorating gradually). This article considers solutions to this problem. As a first approximation, the gradual change problem can be modeled as a change from a fixed distribution to a-model of simple linear regression with respect to time (i.e., there is an abrupt change of slope, from a 0 to a nonzero slope). We study an extension of this case to a general context of sequential detection of a change in the slope of a simple linear regression. The residuals are assumed to be normally distributed. We consider both the case in which the baseline parameters are known and the case in which they are not. Finally, as an application, we monitor for an increase in the rate of global warming."
"10.1198/016214503000242","2003","From the statistics of data to the statistics of knowledge: symbolic data analysis","0","Increasingly, datasets are so large they must be summarized in some fashion so that the resulting summary dataset is of a more manageable size, while still retaining as much knowledge inherent to the entire dataset as possible. One consequence of this situation is that the data may no longer be formatted as single values such as is the case for classical data, but rather may be represented by lists, intervals, distributions, and the like. These summarized data are examples of symbolic data. This article looks at the concept of symbolic data in general, and then attempts to review the methods currently available to analyze such data. It quickly becomes clear that the range of methodologies available draws analogies with developments before 1900 that formed a foundation for the inferential statistics of the 1900s, methods largely limited to small (by comparison) datasets and classical data formats. The scarcity of available methodologies for symbolic data also becomes clear and so draws attention to an enormous need for the development of a vast catalog (so to speak) of new symbolic methodologies along with rigorous mathematical and statistical foundational work for these methods."
"10.1198/016214504000000683","2004","A model-based background adjustment for oligonucleotide expression arrays","0","High-density oligonucleotide expression arrays are widely used in many areas of biomedical research. Affymetrix GeneChip arrays are the most popular, In the Affymetrix system. a fair amount of further preprocessing and data reduction occurs after the image-processing step. Statistical procedures developed by academic groups have been successful in improving the default algorithms provided by the Affymetrix system. In this article we present a solution to one of the preprocessing steps-background adjustment-based on a formal statistical framework. Our Solution greatly improves the performance of the technology in various practical applications. These arrays use short oligonucleotides to probe for genes in an RNA sample. Typically, each gene is represented by 11-20 pairs of oligonucleotide probes. The first component of these pairs is referred to as a perfect match probe and is designed to hybridize only with transcripts from the intended gene (i.e.. specific hybridization). However, hybridization by other sequences (i.e., nonspecific hybridization) is unavoidable. Furthermore. hybridization strengths are measured by a scanner that introduces optical noise. Therefore, the observed intensities need to be adjusted to give accurate measurements of specific hybridization. We have found that the default ad hoc adjustment, provided as part of the Affymetrix system can be improved through the use of estimators derived from a statistical model that uses probe sequence information. A final step in preprocessing is to summarize the probe-level data for each gene to define a measure of expression that represents the amount of the corresponding mRNA species. In this article we illustrate the practical consequences of not adjusting appropriately for the presence of nonspecific hybridization and provide a solution based on our background adjustment procedure. Software that computes our adjustment is available as part of the Bioconductor Project (http://bioconductor.org)."
"10.1198/016214504000001268","2004","Structural mean effects of noncompliance: estimating interaction with baseline prognosis and selection effects","0","A randomized, placebo-controlled blood pressure (BP) reduction trial has recorded compliance measures by electronic monitoring. Causal questions regarding dose effects are of interest, but one often wants to avoid the assumption that compliance quantiles on both randomized arms are comparable. Structural mean models (SMMs) do avoid this assumption; however, they do not allow for interaction effects between variables observed on different randomized arms. Such an interaction between observed exposure on the treatment arm and latent treatment-free response (i.e., placebo response) was suggested by the data. Building on structural models, we propose an approach that makes this possible. To allow for a structural interaction effect with potential placebo response, we invoke a compliance selection model. It turns out that SMMs imply identifiable selection models; hence the secondary model is testable. Next, to assess the amount of variation explained by the structural model, we identify separate variance components for different error types. This yields a measure of the variation in treatment effects over individuals, In addition, it allows us to compare the fit of different plausible structural models. On the BP data, we find a significant interaction effect: Higher exposure levels are estimated to have more effect in subgroups with poorer placebo response. Finite-sarnple properties of the proposed estimators are verified through simulation."
"10.1198/016214504000000674","2004","A random pattern-mixture model for longitudinal data with dropouts","0","Pattern-mixture models are frequently used for longitudinal data analysis with dropouts because they do not require explicit specification Of the dropout mechanism. These models stratify the data according to time to dropout and formulate a model for each stratum. This usually results in underindentifiability, because we need to estimate many pattern-specific parameters even though the eventual interest is usually or, the marginal parameters. In this article we extend this framework to a random pattern-mixture model, where the pattern-specific parameters are treated as nuisance parameters and modeled as random instead of fixed. The pattern is defined according to a surrogate for the dropout process. A constraint is then put oil the pattern by linking it to the time to dropout using a random-effects survival model. We assume, conditional on the latent pattern effects. that the longitudinal outcome and the dropout process are independent. This model retains the robustness of the traditional pattern-mixture models. while avoiding the overparameterization problem. When we define each subject as a separate stratum. this model reduces to the shared parameter model. Maximum likelihood estimates are obtained using an EM Newton-Raphson algorithm. We apply the method to the depression data from the Prevention of Suicide in Primary Care Elderly Collaborative Trial (PROSPECT). We show when the dropout information is adjusted for under the proposed model, the treatment seems to reduce depression in the elderly."
"10.1198/016214504000000656","2004","Improved semiparametric time series models of air pollution and mortality","2","In 2002, methodological issues around time series analyses of air pollution and health attracted the attention of the scientific community, policy makers, the press, and the diverse stakeholders concerned with air pollution. As the U.S. Environmental Protection Agency (EPA) was finalizing its most recent review of epidemiologic evidence on particulate matter air pollution (PM), statisticians and epidemiologists found that the S-PLUS implementation of generalized additive models (GAMs) can overestimate effects of air pollution and understate statistical uncertainty in time series studies of air pollution and health. This discovery delayed completion of the PM Criteria Document prepared as part of the review of the U.S. National Ambient Air Quality Standard, because the time series findings represented a critical component of the evidence. In addition, it raised concerns about the adequacy of current model formulations and their software implementations. In this article we provide improvements in semiparametric regression directly relevant to risk estimation in time series studies of air pollution. First, we introduce a closed-form estimate of the asymptotically exact covariance matrix of the linear component of a GAM. To ease the implementation of these calculations, we develop the S package gam.exact, an extended version of gain. Use of gam.exact allows a more robust assessment of the statistical uncertainty of the estimated pollution coefficients. Second, we develop a bandwidth selection method to reduce confounding bias in the pollution-mortality relationship due to unmeasured time-varying factors, such as season and influenza epidemics. Third, we introduce a conceptual framework to fully explore the sensitivity of the air pollution risk estimates to model choice. We apply our methods to data of the National Mortality Morbidity Air Pollution Study, which includes time series data from the 90 largest U.S. cities for the period 1987-1994."
"10.1198/016214504000001312","2004","Location-scale depth","1","This article introduces a halfspace depth in the location-scale model that is along the lines of the general theory given by Mizera, based on the idea of Rousseeuw and Hubert, and is complemented by a new likelihood-based principle for designing criterial functions. The most tractable version of the proposed depth-the Student depth-turns Out to be nothing but the bivariate halfspace depth interpreted in the Poincare plane model of the Lobachevski geometry. This fact implies many fortuitous theoretical and computational properties, in particular equivariance with respect to the Mobius group and favorable time complexities of algorithms. It also opens a way to introduce some other depth notions in the location-scale context, for instance. location-scale simplicial depth. A maximum depth estimator of location and scale-the Student median-is introduced. Possible applications of the proposed concepts are investigated on data examples."
"10.1198/016214504000001646","2004","Optimal sample size for multiple testing: the case of gene expression microarrays","7","We consider the choice of an optimal sample size for multiple-comparison problems. The motivating application is the choice of the number of microarray experiments to be carried out when learning about differential gene expression. However, the approach is valid in any application that involves multiple comparisons in a large number of hypothesis tests. We discuss two decision problems in the context of this setup: the. sample size selection and the decision about the multiple comparisons. We adopt a decision-theoretic approach, using loss functions that combine the competing goals of discovering as many differentially expressed genes as possible, while keeping the number of false discoveries manageable. For consistency, we use the same loss function for both decisions. The decision rule that emerges for the multiple-comparison problem takes the exact form of the rules proposed in the recent literature to control the posterior expected false-discovery rate. For the sample size selection, we combine the expected utility argument with an additional sensitivity analysis, reporting the conditional expected utilities and conditioning on assumed levels of the true differential expression. We recognize the resulting diagnostic as a form of statistical power facilitating interpretation and communication. As a sampling model for observed gene expression densities across genes and arrays, we use a variation of a hierarchical gamma/gamma model. But the discussion of the decision problem is independent of the chosen probability model. The approach is valid for any model that includes positive prior probabilities for the null hypotheses in the multiple comparisons and that allows for efficient marginal and posterior simulation, possibly by dependent Markov chain Monte Carlo simulation."
"10.1198/0162145000001655","2004","False discovery control for random fields","10","This article extends false discovery rates to random fields. for which there are uncountably many hypothesis tests. We develop a method for finding regions in the field's domain where there is a significant signal while controlling either the proportion of area or the proportion of clusters in which false rejections occur. The method produces confidence envelopes for the proportion of false discoveries as a function of the rejection threshold. From the confidence envelopes, we derive threshold procedures to control either the mean or the specified tail probabilities of the false discovery proportion. An essential ingredient of this construnction is a new algorithm to compute a confidence superset for the set of all true-null locations. We demonstrate our method with applications to scan statistics and functional neuroimaging."
"10.1198/016214504000000548","2004","Cross-validation and the estimation of conditional probability densities","7","Many practical problems, especially some connected with forecasting, require nonparametric estimation of conditional densities from mixed data. For example, given an explanatory data vector X for a prospective customer, with components that could include the customer's salary, occupation, age, sex, marital status, and address, a company might wish to estimate the density of the expenditure, Y, that could be made by that person, basing the inference on observations of (X, Y) for previous clients. Choosing appropriate smoothing parameters for this problem can be tricky, not in the least because plug-in rules take particularly complex form in the case of mixed data. An obvious difficulty is that there exists no general formula for the optimal smoothing parameters. More insidiously, and more seriously, it can be difficult to determine which components of X are relevant to the problem of conditional inference. For example, if the jth component of X is independent of Y, then that component is irrelevant to estimating the density of Y given X, and ideally should be dropped before conducting inference. In this article we show that cross-validation overcomes these difficulties. It automatically determines which components are relevant and which are not, through assigning large smoothing parameters to the latter and consequently shrinking them toward the uniform distribution on the respective marginals. This effectively removes irrelevant components from contention, by suppressing their contribution to estimator variance: they already have very small bias, a consequence of their independence of Y. Cross-validation also yields important information about which components are relevant; the relevant components are precisely those that cross-validation has chosen to smooth in a traditional way, by assigning them smoothing parameters of conventional size. Indeed, cross-validation produces asymptotically optimal smoothing for relevant components, while eliminating irrelevant components by oversmoothing. In the problem of nonparametric estimation of a conditional density, cross-validation comes into its own as a method with no obvious peers."
"10.1198/016214504000001664","2004","On a likelihood approach for {M}onte {C}arlo integration","2","The use of estimating equations has been a common approach for constructing Monte Carlo estimators. Recently, Kong et al. proposed a formulation of Monte Carlo integration as a statistical model, making explicit what information is ignored and what is retained about the baseline measure. From simulated data, the baseline measure is estimated by maximum likelihood, and then integrals of interest are estimated by substituting the estimated measure. For two different situations in which independent observations are simulated from multiple distributions, we show that this likelihood approach achieves the lowest asymptotic variance possible by using estimating equations. In the first situation, the normalizing constants of the design distributions are estimated, and Meng and Wong's bridge sampling estimating equation is considered. In the second situation, the values of the normalizing constants are known, thereby imposing linear constraints on the baseline measure. Estimating equations including Hesterberg's stratified importance sampling estimator, Veach and Guibas's multiple importance sampling estimator, and Owen and Zhou's method of control variates are considered."
"10.1198/016214504000001673","2004","Fully exponential {L}aplace approximations using asymptotic modes","0","Posterior means of positive functions can be expressed as the ration integrals which is called fully exponential form. When approximating the posterior means analytically, we usually use Laplace's method. Tierney and Kadane presented a second-order approximation by using the Laplace approximations in each of the numerator and denominator of the fully exponential form. However, Laplace's method requires the exact mode of the integrand. In this article we introduce the concept of asymptotic modes and present the Laplace method via an asymptotic mode under regularity conditions. Furthermore, we propose second-order approximations to the posterior means of positive functions without evaluating the third derivatives of a log-likelihood function and the exact modes of integrands. We also give an Edgeworth-like expansion for the random variable according to a poster or distribution using the Laplace method via an asymptotic mode."
"10.1198/016214504000000557","2004","Bayesian estimation of the spectral density of a time series","5","This article describes a Bayesian approach to estimating the spectral density of a stationary time series. A nonparametric prior on the spectral density is described through Bernstein polynomials. Because the actual likelihood is very complicated, a pseudoposterior distribution is obtained by updating the prior using the Whittle likelihood. A Markov chain Monte Carlo algorithm for sampling front this posterior distribution is described that is used for computing the posterior mean, variance, and other statistics. A consistency result is established for this pseudoposterior distribution that holds for a short-memory Gaussian time series and under some conditions on the prior. To prove this asymptotic result, a general consistency theorem of Schwartz is extended for a triangular array of independent, nonidentically distributed observations. This extension is also of independent interest. A simulation study is conducted to compare the proposed method with some existing methods. The method is illustrated with the well-studied sunspot dataset."
"10.1198/016214504000001682","2004","Models and confidence intervals for true values in interlaboratory trials","1","We consider the one-way random-effects model with unequal sample sizes and heterogeneous variances. Using the method of generalized confidence intervals. we develop a new confidence interval procedure for the mean. Additionally, we investigate two alternative models based on different sets of assumptions regarding between-group variability and derive generalized confidence interval procedures for the mean. These procedures are applicable to small samples. Statistical simulation is used to demonstrate that the coverage probabilities of these procedures are close enough to the nominal value so that they am useful in practice. Although the methods are quite general, the procedures are explained with the backdrop of interlaboratory studies."
"10.1198/016214504000000782","2004","Smooth and accurate multivariate confidence regions","0","This article describes multivariate approximate conditional confidence regions for canonical exponential families. These confidence regions have actual coverage probabilities that are closer to their nominal levels than are the actual coverage probabilities of traditional normal-theory regions and have boundaries that are smoother than those obtained by inverting traditional exact tests. Our method is based on constructing one-dimensional conditional tests, combining p values, and inverting. More specifically, consider a statistical model with three parameters, of which two are of interest and one is not of interest. We generate a confidence region for the two parameters of interest by first generating a confidence interval for one of the parameters, conditional on sufficient statistics associated with the other interest parameter and the nuisance parameter. For values of this bounded parameter inside the confidence interval, we determine a confidence interval for the remaining interest parameter conditional on the sufficient statistic associated with the nuisance parameter. This procedure determines the boundaries of a confidence region. This method is illustrated through applications to logistic and positive Poisson regression examples, in which parameters of interest are alternative representations of a single underlying physical quantity; in our examples, they represent the effectiveness of a study drug relative to a standard drug in a crossover trial, measured under two different orderings, and the intensity of infection among a certain demographic group, measured in two different day care centers."
"10.1198/016214504000000520","2004","Testing for trend in the presence of autoregressive error","0","A popular model for assessing dependence on time is the time series model composed of a linear trend plus a zero mean autoregressive (AR) process. Although considerable effort has been devoted developing tests for linear trend in the presence of serial correlation, the testing procedures used in practice are less than satisfactory for portions of the parameter space for the AR coefficient. This is because the variance of the feasible generalized least squares (FGLS) estimator of the trend coefficient is heavily dependent on the parameters of the AR process. A test based on the Gauss-Newton procedure is shown to have more uniform behavior over the parameter space than tests based on FGLS. The test based on Gauss-Newton procedure also has good power."
"10.1198/016214504000001691","2004","Local global neural networks: a new approach for nonlinear time series modeling","0","We propose the local-global neural networks model within the context of time series models. This formulation encompasses some already existing nonlinear models and also admits the mixture of experts approach. We emphasize the linear expert case and extensively discuss the theoretical aspects of the model: stationarity conditions, existence, consistency and asymptotic normality of the parameter estimates, and model identifiability. The proposed model consists of a mixture of stationary and nonstationary linear models and is able to describe ""intermittent"" dynamics; the system spends a large fraction of time in a bounded region, but sporadically develops an instability that grows exponentially for some time and then suddenly collapses. Intermittency is a commonly observed behavior in ecology and epidemiology, fluid dynamics, and other natural systems. A model-building strategy is also considered, and the parameters are estimated by concentrated maximum likelihood. The procedure is illustrated with two real time series."
"10.1198/016214504000001709","2004","Predicting the conditional probability of discovering a new class","3","Consider a population comprising disjoint classes. An important problem arising from various fields is prediction of the random conditional probability of discovering a new class. The asymptotic normality of the discovery probability is established in a Poisson model, where the number of individuals from each class is a Poisson process with a class-specific rate. A new derivation is presented for the well-known Good-Toulmin predictor as a moment-based estimator for the asymptotic limit of the discovery probability. The Good-Toulmin predictor is also shown to be a nonparametric empirical Bayes estimator for the expectation of the discovery probability given the rates of the Poisson processes and an approximation to an unbiased estimator only for the identifiable part of the expectation of the discovery probability in a multinomial model. The properties of the moment based estimator are investigated so that confidence and prediction intervals, can be constructed, The Good-Toulmin predictor and the discovery probability are shown to have a nonnegative correlation. A conditional nonparametric maximum likelihood estimator is developed as an alternative to the moment-based estimator. As an application, the methods are used to predict the probability of discovering a new gene from expressed sequence tags in a genomic sequencing experiment."
"10.1198/016214504000001718","2004","Predicting random effects from finite population clustered samples with response error","0","In many situations there is interest in parameters (e.g., mean) associated with the response distribution of individual Clusters in a finite clustered population. We develop predictors of such parameters using a two-stage sampling probability model with response error. The probability model sterns directly from finite population sampling without additional assumptions, and thus is design-based. The predictors are closely related to best linear unbiased predictors (BLUP) that arise from common mixed-model methods, as well as to model-based predictors obtained via super population approaches for survey sampling, The context assumes Clusters of equal size and equal size sampling of units within clusters, Target parameters may correspond to clusters realized in the sample, as well as nonrealized clusters. In either case, the predictors are linear and unbiased, and minimize the expected mean squared error. They correspond to the sum of predictors of responses for realized and nonrealized units in the Cluster, accounting directly for the second-stage sampling fraction. In contrast, the BLUP commonly used in mixed models can be interpreted as predicting only the responses of second-stage units not observed for a cluster. not the cluster mean. The development reveals that two-stage sampling does riot give rise to it more general variance structure often assumed in superpopulation models, even when variances within clusters are heterogeneous. With response error present, we predict target random variables defined as an expected (or average) response over units in a cluster."
"10.1198/016214504000000601","2004","Combining independent regression estimators from multiple surveys","1","Efficient generalized regression (GREG) procedures are proposed for the combination of comparable information collected independently front multiple surveys of the same population. In particular. for combination through alignment of estimated totals of common target characteristics. an efficiency improvement of Zieschang's original composite GREG method is developed, involving a correction of the GREG estimation for different effective sample sizes in a multiple-sample setting. The proposed method is nearly as efficient as the alternative method of Renssen and Nieuwenbroek for general sampling designs, and considerably more practical. Under broad sampling design conditions. the proposed method produce,; composite estimators that are equally efficient as the estimators of Renssen and Nieuwenbrock for common characteristics and more efficient for noncommon characteristics. Under some of these sampling design conditions, an equivalence of extended GREG estimation and extended optimal regression estimation is established."
"10.1198/016214504000000566","2004","Analysis of time-to-event data with incomplete event adjudication","0","In many multicenter, randomized clinical trials, the primary outcome is the time to the first of a number of possible clinical events. An event classification committee may be convened to determine whether events that have been reported by investigators meet the predetermined criteria for primary endpoint events. When interim analyses are performed in such trials, the final classification for many reported events will not be known. Failure to account for the uncertain status of these events may result in incorrect interim analysis. The probability that an unadjudicated event will be confirmed as a primary event can typically be estimated from those events for which adjudication is complete. We show that if each unadjudicated event is weighted according to the probability that it will be the first primary event, then consistent estimates of survival probabilities and regression parameters can be obtained and unbiased log-rank tests of treatment differences performed. Moderate sample consistency of point estimates and variance estimates is verified by simulation. The procedure is illustrated using data from the Coumadin Aspiring Reinfarction Study (CARS) and the Weekly Intervention with Zithromax for Atherosclerosis and Related Diseases (WIZARD) study."
"10.1198/016214504000001033","2004","Joint modeling and estimation for recurrent event processes and failure time data","4","Recurrent event data are commonly encountered in longitudinal follow-up Studies related to biomedical science, econometrics. reliability, and demography. In many studies. recurrent events serve as important measurements for evaluating disease progression, health deterioration. or insurance risk. When analyzing recurrent event data, an independent censoring condition is typically required for the construction of statistical methods. In some situations. however, the terminating time for observing recurrent events could be correlated with the recurrent event process. thus violating the assumption of independent censoring. In this article, we consider joint modeling of it recurrent event process and a failure time in which a common subject-specific latent variable is used to model the association between the intensity of the recurrent event process and the hazard of the failure time. The proposed joint model is flexible in that no parametric assumptions on the distributions of censoring times and latent variables are made, and under the model, informative censoring is allowed for observing both the recurrent events and failure times. We propose a ""borrow-strength estimation procedure"" by first estimating, the value of the latent variable from recurrent event data, then using the estimated value in the failure time model. Some interesting implications and trajectories of the proposed model are presented. Properties of the regression parameter estimates and the estimated baseline cumulative hazard functions are also studied."
"10.1198/016214504000000755","2004","Smoothing spline nonlinear nonparametric regression models","1","Almost all of the current nonparametric regression methods, such as smoothing splines, generalized additive models, and varying-coefficients models, assume a linear relationship when nonparametric functions are regarded as parameters. In this article we propose a general class of smoothing spline nonlinear nonparametric models that allow nonparametric functions to act nonlinearly. They arise in many fields as either theoretical or empirical models. Our new estimation methods are based on an extension of the, Gauss-Newton method to infinite-dimensional spaces and the backfitting procedure. We extend the generalized cross-validation and generalized maximum likelihood methods to estimate smoothing parameters. We establish connections between some nonlinear nonparametric models and nonlinear mixed-effects models. We derive approximate Bayesian confidence intervals for inference, We illustrate the methods with an application to term structure of interest rates and conduct simulations to evaluate the finite-sample performance of our methods."
"10.1198/016214504000001727","2004","Nonparametric and semiparametric models for missing covariates in parametric regression","5","Robustness of covariate modeling for the missing-covariate problem in parametric regression is studied under the missing-at-random assumption. For a simple missing-covariate pattern, nonparametric covariate model is proposed and is shown to yield a consistent and semiparametrically efficient estimator for the regression parameter, Total robustness is achieved in this situation. For more general missing-covariate patterns, a novel semiparametric modeling approach is proposed for the covariates. In this approach, the covariate distribution is first decomposed into the product of a series of conditional distributions according to the overall missing-data patterns, and the conditional distributions are then represented in the general odds ratio form. The general odds ratios are modeled parametrically, and the other components of the covariate distribution are modeled nonparametrically. Maximum semiparametric likelihood is used to find the parameter estimates. The proposed method yields a consistent estimator for the regression parameter when the odds ratios are modeled correctly. In general, the semiparametric covariate modeling strategy increases the robustness against covariate, model misspecification when compared with the parametric modeling strategy proposed by Lipsitz and Ibrahim. The new covariate modeling approach can also be incorporated into the doubly robust procedure of Robins et al. to increase protection against misspecification of the missing-data mechanism. In addition, the proposed modeling strategy avoids the usually intractable integrations involved in the maximization of the incomplete-data likelihood with parametric covariate models. The proposed method can be applied to many regression models to handle incomplete covariates."
"10.1198/016214504000001736","2004","Optimal experimental designs when some independent variables are not subject to control","0","This article considers the problem of constructing optimal designs for regression models when the design space is a product space and some of the variables are not under the control of the practitioner. A variable that is not tinder control can have known values before the experiment is performed or else unknown values before the experiment is realized. The first case is briefly discussed in the literature. The aim of this work is to provide equivalence theorems for the second case and the mixture of both cases. Iterative algorithms for generating approximate optimal designs are given, and a real case of lung cancer is discussed."
"10.1198/016214504000000872","2004","Calibrated probabilistic mesoscale weather field forecasting: the geostatistical output perturbation method","8","Probabilistic weather forecasting consists of finding a joint probability distribution for future weather quantities or events. It is typically done by using a numerical weather prediction model, perturbing the inputs to the model in various ways, and running the model for each perturbed set of inputs. The result is then viewed as an ensemble of forecasts, taken to be a sample from the joint probability distribution of the future weather quantities of interest. This is typically not feasible for mesoscale weather prediction carried out locally by organizations without the vast data and computing resources of national weather centers. Instead, we propose a simpler method that breaks with much previous practice by perturbing the outputs, or deterministic forecasts, from the model. Forecast errors are modeled using a geostatistical model, and ensemble members are generated by simulating realizations of the geostatistical model. The method is applied to 48-hour mesoscale forecasts of temperature in the North American Pacific Northwest between 2000 and 2002. The resulting forecast intervals turn out to be empirically well calibrated for individual meteorological quantities, to be sharper than those obtained from approximate climatology, and to be consistent with aspects of the spatial correlation structure of the observations."
"10.1198/016214504000000953","2004","Rejoinder","0",""
"10.1198/016214504000000638","2004","Relative risk forests for exercise heart rate recovery as a predictor of mortality","0","Recent studies have confirmed heart rate fall after treadmill exercise testing, or heart rate recovery, as a powerful predictor of mortality from heart disease. Heart rate recovery depends on central reactivation of vagal tone and decreased vagal activity is a risk factor for death. If heart rate recovery is defined as the fall in heart rate after I minute following peak exercise, then a heart rate recovery value of 12 beats per minute (bpm) or lower has been shown to be a good prognostic threshold for identifying patients at high risk. Although this finding establishes a simple, useful relationship between heart recovery and mortality, a working understanding of how heart rate recovery interacts with other characteristics of a patient in determining risk of death is still largely unexplored. Such knowledge, addressed in this article, could improve the prognostic value of the exercise test. Our analysis is based on over 23,000 patients who underwent exercise testing. A rich assortment of data was collected on these patients, including clinical and physiological information, heart rate recovery, and other exercise test performance measures. Our approach was to grow relative risk forests, a novel method that combines random forest methodology with survival trees grown using Poisson likelihoods. Our analysis reveals a complex relationship between peak heart rate, age, level of fitness, heart rate recovery, and risk of death."
"10.1198/016214504000000665","2004","Hierarchical {B}ayesian neural networks: an application to a prostate cancer study","1","Prostate cancer is one of the most common cancers in American men. Management depends on the staging of prostate cancer. Only cancers that are confined to organs of origin are potentially curable. The article considers a hierarchical Bayesian neural network approach for posterior prediction probabilities of certain features indicative of non-organ-confined prostate cancer. The Bayesian procedure is implemented by an application of the Markov chain Monte Carlo numerical integration technique. For the problem at hand, the hierarchical Bayesian neural network approach is shown to be superior to the approach based on hierarchical Bayesian logistic regression model as well as the classical feedforward neural networks."
"10.1198/016214504000000647","2004","Full matching in an observational study of coaching for the {SAT}","1","Among matching techniques for observational studies, full matching is in principle the best, in the sense that its alignment of comparable treated and control subjects is as good as that of any alternate method, and potentially much better. This article evaluates the practical performance of full matching for the first time, modifying it in order to minimize variance as well as bias and then using it to compare coached and uncoached takers of the SAT. In this new version, with restrictions on the ratio of treated subjects to controls within matched sets, full matching makes use of many more observations than does pair matching, but achieves far closer matches than does matching with k greater than or equal to 2 controls. Prior to matching, the coached and uncoached groups are separated on the propensity score by 1.1 SDs. Full matching reduces this separation to 1% or 2% of an SD. In older literature comparing matching and regression, Cochran expressed doubts that ani method of adjustment could substantially reduce observed bias of this magnitude.To accommodate missing data, regression-based analyses by ETS researchers rejected a subset of the available sample that differed significantly from the subsample they analyzed. Full matching on the propensity score handles the same problem simply and without rejecting observations. In addition, it eases the detection and handling of nonconstancy of treatment effects, which the regression-based analyses had obscured, and it makes fuller use of covariate information. It estimates a somewhat larger effect of coaching on the math score than did ETS's methods."
"10.1198/016214504000000692","2004","The estimation of prediction error: covariance penalties and cross-validation","8","Having constructed a data-based estimation rule, perhaps a logistic regression or a classification tree, the statistician would like to know its performance as a predictor of future cases. There are two main theories concerning prediction error: (I) penalty methods such as C-p, Akaike's information criterion, and Stein's unbiased risk estimate that depend on the covariance between data points and their corresponding predictions; and (2) cross-validation and related nonparametric bootstrap techniques. This article concerns the connection between the two theories. A Rao-Blackwell type of relation is derived in which nonparametric methods such as cross-validation are seen to be randomized versions of their covariance penalty counterparts. The model-based penalty methods offer substantially better accuracy, assuming that the model is believable."
"10.1198/016214504000000971","2004","Smooth design-adapted wavelets for nonparametric stochastic regression","1","We treat nonparametric stochastic regression using smooth design-adapted wavelets built by means of the lifting scheme. The proposed method automatically adapts to the nature of the regression problem, that is, to the irregularity of the design, to data on the interval, and to arbitrary sample sizes (which do not need to be a power of 2). As such, this method provides a uniform solution to the usual criticisms of first-generation wavelet estimators. More precisely, starting from the unbalanced Haar basis orthogonal with respect to the empirical design measure, we use weighted average interpolation to construct biorthogonal wavelets with a higher number of vanishing analyzing moments. We include a lifting step that improves the conditioning through constrained local semiorthogonalization. We propose a wavelet thresholding algorithm and show its numerical performance both on real data and in simulations including white, correlated, and heteroscedastic noise."
"10.1198/016214504000000593","2004","Variable selection and model building via likelihood basis pursuit","5","This article presents a nonparametric penalized likelihood approach for variable selection and model building, called likelihood basis pursuit (LBP). In the setting of a tenser product reproducing kernel Hilbert space, we decompose the log-likelihood into the sum of different functional components such as main effects and interactions, with each component represented by appropriate basis functions. Basis functions are chosen to be compatible with variable selection and model building in the context of a smoothing spline ANOVA model. Basis pursuit is applied to obtain the optimal decomposition in terms of having the smallest L-1 norm on the coefficients. We use the functional l(1) norm to measure the importance of each component and determine the ""threshold"" value by a sequential Monte Carlo bootstrap test algorithm. As a generalized LASSO-type method, LBP produces shrinkage estimates for the coefficients, which greatly facilitates the variable selection process and provides highly interpretable multivariate functional estimate,,, at the same time. To choose the regularization parameters appearing in the LBP models, generalized approximate cross-validation (GACV) is derived as a tuning criterion. To make GACV widely applicable to large datasets, its randomized version is proposed as well. A technique ""slice modeling"" is used to solve the optimization problem and makes the computation more efficient. LBP has great potential for a wide range of research and application areas such as medical studies, and in this article we apply it to two large ongoing epidermologic studies, the Wisconsin Epidermologic Study of Diabetic Retinopathy (WESDR) and the Beaver Dam Eye Study (BDES)."
"10.1198/016214504000000980","2004","Stable and efficient multiple smoothing parameter estimation for generalized additive models","5","Representation of generalized additive models (GAM's) using penalized regression splines allows GAM's to be employed in a straightforward manner using penalized regression methods. Not only is inference facilitated by this approach, but it is also possible to integrate model selection in the form of smoothing parameter selection into model fitting in a computationally efficient manner using well founded criteria such as generalized cross-validation. The current fitting and smoothing parameter selection methods for such models are usually effective, but do not provide the level of numerical stability to which users of linear regression packages, for example, are accustomed. In particular the existing methods cannot deal adequately with numerical rank deficiency of the GAM fitting problem, and it is not straightforward to produce methods that can do so, given that the degree of rank deficiency can be smoothing parameter dependent. In addition, models with the potential flexibility of GAM's can also present practical fitting difficulties as a result of indeterminacy in the model likelihood: Data with many zeros fitted by a model with a log link are a good example. In this article it is proposed that GAM's with a ridge penalty provide a practical solution in such circumstances, and a multiple smoothing parameter selection method suitable for use in the presence of such a penalty is developed. The method is based on the pivoted QR decomposition and the singular value decomposition, so that with or without a ridge penalty it has good error propagation properties and is capable of detecting and coping elegantly with numerical rank deficiency. The method also allows mixtures of user specified and estimated smoothing parameters and the setting of lower bounds on smoothing parameters. In terms of computational efficiency, the method compares well with existing methods. A simulation study compares the method to existing methods, including treating GAM's as mixed models."
"10.1198/016214504000000999","2004","Functional convex averaging and synchronization for time-warped random curves","5","Data that can be best described as a sample of curves are now fairly common in science and engineering. When the dynamics of development, growth, or response over time are at issue, subjects or experimental units may experience events at different temporal paces. For functional data where trajectories may be individually time-transformed, it is usually inadequate to use commonly used sample statistics, such as the cross-sectional mean or median or the cross-sectional sample variance. If one observes time-warped curve data (i.e., random curves or random trajectories that exhibit random transformations of the time scale), then the usual L-2 norm and metric typically are inadequate. One may then consider subjecting each observed curve to a time transformation in an attempt to reverse the warping of the time scale before further statistical analysis. Dynamic time warping, alignment, curve registration, and landmark-based methods have been put forward with the goal of finding adequate empirical time transformations. Previous analyses of warping typically have not been based on a model in which individual observed curves are viewed as realizations of a stochastic process. We propose a functional convex synchronization model, under the premise that each observed curve is the realization of a stochastic process. Monotonicity constraints on time evolution provide the motivation for a functional convex calculus with the goal of obtaining sample statistics such as a functional mean. Observed random functions in warped time space are represented by a bivariate random function in synchronized time space, consisting of a stochastic monotone time transformation function and an unrestricted random amplitude function. Our theory assumes a monotone time warping transformation that maps synchronized time to warped (i.e., observed) time. This leads to the definition of a functional convex average or ""longitudinal average,"" in contrast to the conventional ""cross-sectional"" average. We discuss various implementations of functional convex averaging and derive a functional limit theorem and asymptotic confidence intervals for functional convex means. The results are illustrated with a novel time-warping transformation and extend to commonly used warping and registration methods, such as landmark registration. The methods are applied to simulated data and the Berkeley growth data."
"10.1198/016214504000001006","2004","Exact and approximate inferences for nonlinear mixed-effects models with missing covariates","0","Nonlinear mixed-effects (NLME) models are popular in many longitudinal studies, including human immunodeficiency virus (HIV) viral dynamics, pharmacokinetic analyses, and studies of growth and decay. In practice, covariates in these studies often contain missing data, and so standard complete-data methods are not directly applicable. In this article we propose Monte Carlo parameter-expanded (PX)-EM algorithms for exact and approximate likelihood inferences for NLME models with missing covariates when the missing-data mechanism is ignorable. We allow arbitrary missing-data patterns and allow the covariates to be categorical, continuous, and mixed. The PX-EM algorithm maintains the simplicity and stability of the standard EM algorithm and may converge much faster than EM. The approximate method is computationally more efficient and may be preferable to the exact method when the exact method exhibits convergence problems, such as slow convergence or nonconvergence. It becomes an exact method for linear mixed-effects models and certain NLME models with missing covariates. We also discuss several sampling methods and convergence of the Monte Carlo (PX) EM algorithms. We illustrate the methods using a real data example from the study of HIV viral dynamics and compare the methods via a simulation study."
"10.1198/016214504000001060","2004","New estimation and model selection procedures for semiparametric modeling in longitudinal data analysis","23","Semiparametric regression models are very useful for longitudinal data analysis. The complexity of semiparametric models and the structure of longitudinal data pose new challenges to parametric inferences and model selection that frequently arise from longitudinal data analysis. In this article, two new approaches are proposed for estimating the regression coefficients in a semiparametric model. The asymptotic normality of the resulting estimators is established. An innovative class of variable selection procedures is proposed to select significant variables in the semiparametric models. The proposed procedures are distinguished from others in that they simultaneously select significant variables and estimate unknown parameters. Rates of convergence of the resulting estimators are established. With a proper choice of regularization parameters and penalty functions, the proposed variable selection procedures are shown to perform as well as an oracle estimator. A robust standard error formula is derived using a sandwich formula and is empirically tested. Local polynomial regression techniques are used to estimate the baseline function in the semiparametric model."
"10.1198/016214504000001079","2004","Survival analysis with heterogeneous covariate measurement error","1","This article is motivated by a time-to-event analysis where the covariate of interest was measured at the wrong time. We show that the problem can be formulated as a special case of survival analysis with heterogeneous covariate measurement error and develop a general analytic framework. We study the asymptotic behavior of the naive partial likelihood estimates and analytically demonstrate that under the heterogeneous measurement error structure and the assumption that all components of the covariate vector and the measurement error vector combined are mutually independent, these naive estimates will shrink toward 0, and that the degree of attenuation increases as the measurement error increases. We also give counterexamples for reverse attenuation when the independence conditions are violated. We use our analytical results to derive a simple bias-correcting estimator that performs well in simulations for small and moderate amounts of measurement error. Our framework can be used to provide insight into the behavior of the commonly used partial likelihood score test for testing no association between a failure outcome and an exposure, for example, in the presence of measurement error or mistiming error. In particular, we derive the asymptotic distribution of the naive partial likelihood score test under a series of local alternatives and discuss the asymptotic relative efficiency. As a result, a simple sample size formula to account for the contamination of covariates is obtained."
"10.1198/016214504000001088","2004","Nonlinear and nonparametric regression and instrumental variables","4","We consider regression when the predictor is measured with error and an instrumental variable (TV) is available. The regression function., or nonparametrically. Our major new result shows that the regression function and all parameters in can be modeled linearly, nonlinearly the measurement error model are identified under relatively weak conditions, much weaker than previously known to imply identifiability. In addition, we exploit a characterization of the IV estimator as a classical ""correction for attenuation"" method based on a particular estimate of the variance of the measurement error. This estimate of the measurement error variance allows us to construct functional nonparametric regression estimators making no assumptions about the distribution of the unobserved predictor and structural estimators that use parametric assumptions about this distribution. The functional estimators uses, simulation extrapolation or deconvolution kernels and the structural method uses Bayesian Markov chain Monte Carlo. The Bayesian estimator is found to significantly outperform the functional approach."
"10.1198/016214504000001097","2004","Inference after model selection","6","Typical modeling strategies involve model selection, which has a significant effect on inference of estimated parameters. Common practice is to use a selected model ignoring uncertainty introduced by the process of model selection. This could yield overoptimistic inferences, resulting in false discovery. In this article we develop a general methodology via optimal approximation for estimating the mean and variance of complex statistics that involve the process of model selection. This allows us to make approximately unbiased inferences, taking into account the selection process. We examine the operating characteristics of the proposed methodology via asymptotic analyses and simulations. These results show that the proposed methodology yields correct inferences and outperforms common alternatives."
"10.1198/016214504000001105","2004","Discrimination and classification of nonstationary time series using the {SLEX} model","3","Statistical discrimination for nonstationary random processes is important in many applications. Our goal was to develop a discriminant scheme that can extract local features of the time series, is consistent, and is computationally efficient. Here, we propose a discriminant scheme based on the SLEX (smooth localized complex exponential) library. The SLEX library forms a collection of Fourier-type bases that are simultaneously orthogonal and localized in both time and frequency domains. Thus, the SLEX library has the ability to extract local spectral features of the time series. The first step in our procedure, which is the feature extraction step based on work by Saito, is to find a basis from the SLEX library that can best illuminate the difference between two or more classes of time series. In the next step, we construct a discriminant criterion that is related to the Kullback-Leibler divergence between the SLEX spectra of the different classes. The discrimination criterion is based on estimates of the SLEX spectra that are computed using the SLEX basis selected in the feature extraction step. We show that the discrimination method is consistent and demonstrate via finite sample simulation studies that our proposed method performs well. Finally, we apply our method to a seismic waves dataset with the primary purpose of classifying the origin of an unknown seismic recording as either an earthquake or an explosion."
"10.1198/016214504000001114","2004","Unit root quantile autoregression inference","6","We study statistical inference in quantile autoregression models when the largest autoregressive coefficient may be unity. The limiting distribution of a quantile autoregression estimator and its t-statistic is derived. The asymptotic distribution is not the conventional Dickey-Fuller distribution, but rather a linear combination of the Dickey-Fuller distribution and the standard normal, with the weight determined by the correlation coefficient of related time series. Inference methods based on the estimator are investigated asymptotically. Monte Carlo results indicate that the new inference procedures have power gains over the conventional least squares-based unit root tests in the presence of non-Gaussian disturbances. An empirical application of the model to U.S. macroeconomic time series data further illustrates the potential of the new approach."
"10.1198/016214504000001123","2004","Optimal {B}ayesian design by inhomogeneous {M}arkov chain simulation","0","We consider decision problems defined by a utility function and an underlying probability model for all unknowns. The utility function quantifies the decision maker's preferences over consequences. The optimal decision maximizes the expected utility function where the expectation is taken with respect to all unknowns, that is, future data and parameters. In many problems, the solution is not analytically tractable. For example, the utility function might involve moments that can be computed only by numerical integration or simulation. Also, the nature of the decision space (i.e., the set of all possible actions) might have a shape or dimension that complicates the maximization. The motivating application for this discussion is the choice of a monitoring network when the optimization is performed over the high-dimensional set of all possible locations of monitoring stations, possibly including choice of the number of locations. We propose an approach to optimal Bayesian design based on inhomogeneous Markov chain simulation. We define a chain such that the limiting distribution identifies the optimal solution. The approach is closely related to simulated annealing. Standard simulated annealing algorithms assume that the target function can be evaluated for any given choice of the variable with respect to which we wish to optimize. For optimal design problems the target function (i.e., expected utility) is in general not available for efficient evaluation and might require numerical integration. We overcome the problem by defining an inhomogeneous Markov chain on an appropriately augmented space. The proposed inhomogeneous Markov chain Monte Carlo method addresses within one simulation both problems, evaluation of the expected utility and maximization."
"10.1198/016214504000001132","2004","Getting it right: joint distribution tests of posterior simulators","0","Analytical or coding errors in posterior simulators can produce reasonable but incorrect approxii nations of posterior moments. This article develops simple tests of posterior simulators that detect both kinds of errors, and uses them to detect and correct errors in two previously published articles. The tests exploit the fact that a Bayesian model specifies the joint distribution of observables (data) and unobservables (parameters). There are two joint distribution simulators. The roarginal-conditional simulator draws unobservables from the prior and then observables conditional on unobservables. The successive-conditional simulator alternates between the posterior simulator and an observables simulator. Formal comparison of moment approximations of the two simulators reveals existing analytical or coding errors in the posterior simulator."
"10.1198/016214504000001141","2004","The invariance of some score tests in the linear model with classical measurement error","0","The linear model with classical measurement error is an alternative to the standard regression model, in which it is assumed that the independent variables are subject to error. This assumption can cause statistical inferences and parameter estimators to differ dramatically from those obtained by the standard regression model. However, in some cases, inferences remain unchanged even though the independent variables are assumed to be subject to error. This article investigates the invariance property of score tests for assessing heteroscedasticity, first-order autoregressive disturbance, and the need for a Box-Cox power transformation. Under specific constraints, we show that the score tests for measurement error models are identical to the corresponding well-established tests derived from standard regression models. Hence practitioners can assess assumptions of constant variance and independent errors, as well as the need for a Box-Cox transformation, irrespective of whether or not the variables are measured with error. We also discuss some possible generalizations."
"10.1198/016214504000001150","2004","A nonparametric test for spatial isotropy using subsampling","5","A common requirement for spatial modeling is the development of an appropriate correlation structure. Although the assumption of isotropy is often made for this structure, it is not always appropriate. A conventional practice when checking for isotropy is to informally assess plots of direction-specific sample (semi)variograms. Although a useful diagnostic, these graphical techniques are difficult to assess and open to interpretation. Formal alternatives to graphical diagnostics are valuable, but have been applied to a limited class of models. In this article we propose a formal approach to test for isotropy that is both objective and valid for a wide class of models. This approach, which is based on the asymptotic joint normality of the sample variogram, can be used to compare sample variograms in multiple directions. An L-2-consistent subsampling estimator for the asymptotic covariance matrix of the sample variogram is derived and used to construct a test statistic. A subsampling approach and a limiting chi-squared approach are developed to obtain p values of the test. Our testing approach is purely nonparametric in that no explicit knowledge of the marginal or joint distribution of the process is needed. In addition, the shape of the random field can be quite irregular. The results apply to regularly spaced data as well as to irregularly spaced data when the point locations are generated by a homogeneous Poisson process. A data example and simulation experiments demonstrate the efficacy of the approach."
"10.1198/016214504000001169","2004","The impact of dichotomization on the efficiency of testing for an interaction effect in exponential family models","0","In a number of special cases, it has been shown that categorization of a continuous explanatory variable for use in a regression model can lead to efficiency losses. Nevertheless, categorization remains a popular means of dealing with continuous variables, particularly in medical statistics. Two topics of current interest in medical statistics are subgroup effects in clinical trials and gene-environment interactions in genetic studies. In a regression model setting, these topics involve the examination of interaction effects, often between a binary explanatory variable and a continuous explanatory variable. In this article the efficiency losses associated with dichotomization of a continuous explanatory variable for the testing of such interaction effects are calculated and compared with those associated with the testing of main effects. It is shown that considerable additional efficiency loss can arise because of dichotomization in both a main effect and an interaction. The theoretical development is done in the context of exponential family models, thus also generalizing earlier results on efficiency loss for main effects. Some indication is also given of the losses in efficiency associated with more detailed categorization. The further impact of censoring in time-to-event models is investigated and shown to not alter the qualitative conclusions. The practical importance of the findings is illustrated through the analysis of data from a clinical trial in patients with prostate cancer."
"10.1198/016214504000000584","2004","Improving the efficiency of relative-risk estimation in case-cohort studies","4","The case-cohort design is a common means of reducing the cost of covariate measurements in large failure-time studies. Under this design, complete covariate data are collected only on the cases (i.e., the subjects whose failure times are uncensored) and on a subcohort randomly selected from the whole cohort. In many applications, certain covariates are readily measured on all cohort members, and surrogate measurements of the expensive covariates also may be available. The existing relative-risk estimators for the case-cohort design disregard the covariate data collected outside the case-cohort sample and thus incur loss of efficiency. To make better use of the available data, we develop a class of weighted estimators with general time-varying weights that are related to a class of estimators proposed by Robins, Romitzky, and Zhao. The estimators are shown to be consistent and asymptotically normal under appropriate conditions. We identify the estimator within this class that maximizes efficiency, numerical studies demonstrate that the efficiency gains of the proposed estimator over the existing ones can be substantial in realistic settings. We also study the estimation of the cumulative hazard function. An illustration with data taken from Wilms' tumor studies is provided."
"10.1198/016214504000001178","2004","Unbiased estimating equations from working correlation models for irregularly timed repeated measures","1","The method of generalized estimating equation-, (GEEs) has been criticized recently for a failure to protect against misspecification of working correlation models, which in some cases leads to loss of efficiency or infeasibility of solutions. However, the feasibility and efficiency of GEE methods can be enhanced considerably by using flexible families of working correlation models. We propose two ways of constructing unbiased estimating equations from general correlation models for irregularly timed repeated measures to supplement and enhance GEE. The supplementary estimating equations are obtained by differentiation of the Cholesky decomposition of the working correlation, or as score equations for decoupled Gaussian pseudolikelihood. The estimating equations are solved with computational effort equivalent to that required for a first-order GEE. Full details and analytic expressions are developed for a generalized Markovian model that was evaluated through simulation. Large-sample "".sandwich"" standard errors for working correlation parameter estimates are derived and shown to have good performance. The proposed estimating functions are further illustrated in an analysis of repeated measures of pulmonary function in children."
"10.1198/016214504000001187","2004","Causal inference with general treatment regimes: generalizing the propensity score","1","In this article we develop the theoretical properties of the propensity function, which is a generalization of the propensity score of Rosenbaum and Rubin. Methods based on the propensity score have long been used for causal inference in observational studies; they are easy to use and can effectively reduce the bias caused by nonrandom treatment assignment. Although treatment regimes need not be binary in practice, the propensity score methods are generally confined to binary treatment scenarios. Two possible exceptions have been suggested for ordinal and categorical treatments. In this article we develop theory and methods that encompass all of these techniques and widen their applicability by allowing for arbitrary treatment regimes. We illustrate our propensity function methods by applying them to two datasets; we estimate the effect of smoking on medical expenditure and the effect of schooling on wages. We also conduct simulation studies to investigate the performance of our methods."
"10.1198/016214504000001196","2004","Membership functions and probability measures of fuzzy sets","0","The notion of fuzzy sets has proven useful in the context of control theory, pattern recognition, and medical diagnosis. However, it has also spawned the view that classical probability theory is unable to deal with uncertainties in natural language and machine learning, so that alternatives to probability are needed. One such alternative is what is known as ""possibility theory."" Such alternatives have come into being because past attempts at making fuzzy set theory and probability theory work in concert have been unsuccessful. The purpose of this article is to develop a line of argument that demonstrates that probability theory has a sufficiently rich structure for incorporating fuzzy sets within its framework. Thus probabilities of fuzzy events can be logically induced. The philosophical underpinnings that make this happen are a subjectivistic interpretation of probability, an introduction of Laplace's famous genie, and the mathematics of encoding expert testimony. The benefit of making probability theory work in concert with fuzzy set theory is an ability to deal with different kinds of uncertainties that may arise within the same problem."
"10.1198/016214504000001259","2004","The price of {K}aplan-{M}eier","0","Miller has studied the asymptotic efficiency of the nonparametric, Kaplan-Meier survival estimator relative to parametric estimates based on the exponential and Weibull distributions. He concluded that in certain cases, the asymptotic efficiency is low and recommended that analysts give more consideration to parametric estimators, particularly for estimation of small tail probabilities. In this article we revisit this issue and examine the performance of the nonparametric procedure for estimation not only of a point on the survival curve, but also of the mean (or restricted mean) lifetime. In addition to the exponential and Weibull families, we consider the performance of the Kaplan-Meier procedure relative to a more flexible parametric model proposed by Efron. We find that the reduction in efficiency of the Kaplan-Meier survival estimate becomes negligible fairly quickly as the number of parameters in the parametric model increases. Moreover, for estimation of the mean or restricted mean, the loss in efficiency, even relative to the exponential distribution, is small or nil. We conclude that a parametric estimate of the survival curve may be necessary in certain extreme situations, such as when the sample size is very small. In these cases, careful attention must be given to considering the degree of fit, although with sparse data, this must be assessed from outside sources. For certain functionals of the survival curve, such as the mean or restricted mean, the nonparametric approach is unbiased and entails little or no loss in efficiency, and therefore would generally be preferred over a parametric-based estimate."
"10.1198/016214504000000287","2004","Variable selection in data mining: building a predictive model for bankruptcy","4","We predict the onset of personal bankruptcy using least squares regression. Although well publicized, only 2,244 bankruptcies occur in our dataset of 2.9 million months of credit-card activity. We use stepwise selection to find predictors of these from a mix of payment history. debt load. demographics, and their interactions. This combination of rare responses and over 67,000 possible predictors leads to a challenging modeling question: How does one separate coincidental from useful predictors? We show that three modifications turn stepwise regression into an effective methodology for predicting bankruptcy. Our version of stepwise regression (1) organizes calculations to accommodate interactions, (2) exploits modern decision theoretic criteria to choose predictors, and (3) conservatively estimates p-values to handle sparse data and a binary response. Omitting any one of these leads to poor performance. A final step in our procedure calibrates regression predictions. With these modifications, stepwise regression predicts bankruptcy as well as, if not better than, recently developed data-mining tools. When sorted. the largest 14,000 resulting predictions hold 1,000 of the 1,800 bankruptcies hidden in a validation sample of 2.3 million observations. If the cost of missing a bankruptcy is 200 times that of a false positive. our predictions incur less than 2/3 of the costs of classification errors produced by the tree-based classifier C4.5."
"10.1198/016214504000000296","2004","Bayesian factor analysis for spatially correlated data, with application to summarizing area-level material deprivation from census data","2","This article describes a Bayesian hierarchical model for factor analysis of spatially correlated multivariate data. The first level specifies, for each area on a map, the distribution of a vector of manifest variables conditional on an underlying latent factor; at the second level, the area-specific latent factors have a joint distribution that incorporates spatial correlation. The framework allows for both marginal and conditional (e.g., conditional autoregressive) specifications of spatial correlation. The model is used to quantify material deprivation at the census tract level using data from the 1990 U.S. Census in Rhode Island. An existing and widely used measure of material deprivation is the Townsend index, an unweighted sum of four standardized census variables (i.e., Z scores) corresponding to area-level proportions of unemployment, car ownership. crowding. and home ownership. The Townsend and many related indices are computed as linear combinations of measured census variables, which motivates the factor-analytic structure adopted here. The model-based index is the posterior expectation of the latent factor. given the census variables and model parameters. Index construction based on a model allows several improvements over Townsend's and similarly constructed indices: (1) The index can be represented as a weighted sum of (standardized) census variables, with data-driven weights; (2) by using posterior summaries, the indices can be reported with corresponding measures of uncertainty and (3) incorporating information from neighboring areas improves precision of the posterior parameter distributions. Using data from Rhode Island census tracts, we apply our model to summarize variations in material deprivation across the state. Our analysis entertains various spatial covariance structures. We summarize the relative contributions of each census variable to the latent index, suggest ways to report material deprivation at the area level, and compare our model-based summaries with those found by applying the standard Townsend index."
"10.1198/016214504000000304","2004","A classical study of catch-effort models for {H}ector's dolphins","1","Effective management is the key to the protection of many endangered species. Identification of the primary factors that affect their survival often lead to L introduction of strategies to improve survival rates. In this article, we consider a small population of Hector's dolphins located off the coast of New Zealand and the impact that the establishment of a seasonal sanctuary has on their survival and migration rates. Using Akaike's information criterion and an extension of the simulated annealing algorithm, we distinguish between a wide range of competing models that correspond to different assumptions as to the region and temporal dependence of the survival rates. We also examine the impact of the inclusion of catch-effort information and demonstrate the added value that these data provide in terms of both model discrimination and parameter estimation. In particular, we find a whole class of models that provide a far better fit to the data (and therefore better prediction and ultimately better management) than those adopted for previous analyses."
"10.1198/016214504000000449","2004","Semiparametric regression analysis with missing response at random","12","We develop inference tools in a semiparametric partially linear regression model with missing response data. A class of estimators is defined that includes as special cases a semiparametric regression imputation estimator, a marginal average estimator, and a (marginal) propensity score weighted estimator. We show that any of our class of estimators is asymptotically normal. The three special estimators have the same asymptotic variance. They achieve the serniparametric efficiency bound in the homoscedastic Gaussian case. We show that the jackknife method can be used to consistently estimate the asymptotic variance. Our model and estimators are defined with a view to avoid the curse of dimensionality, which severely limits the applicability of existing methods. The empirical likelihood method is developed. It is shown that when missing responses are imputed using the semiparametric regression method the empirical log-likelihood is asymptotically a scaled chi-squared variable. An adjusted empirical log-likelihood ratio, which is asymptotically standard chisquared, is obtained. Also, a bootstrap empirical log-likelihood ratio is derived and its distribution is used to approximate that of the imputed empirical log-likelihood ratio. A simulation study is conducted to compare the adjusted and bootstrap empirical likelihood with the normal approximation-based method in terms of coverage accuracies and average lengths of confidence intervals. Based on biases and standard errors, a comparison is also made by simulation between the proposed estimators and the related estimators."
"10.1198/016214504000000313","2004","Sieve maximum likelihood estimator for semiparametric regression models with current status data","5","In a randomized controlled clinical trial study where the response variable of interest is the time to occurrence of a certain event, it is often too expensive or even impossible to observe the exact time. However, the current status of the subject at a random time of inspection is much more natural, feasible, and practical in terms of cost-effectiveness. This article considers a semiparametric regression model that consists of parametric and nonparametric regression components. A sieve maximum likelihood estimator (MLE) is proposed to estimate the regression parameter, allowing exploration of the nonlinear relationship between a certain covariate and the response function. Asymptotic properties of the proposed sieve MLEs are discussed. Under some mild conditions, the estimators are shown to be strongly consistent. Moreover, the estimators of the unknown parameters are asymptotically efficient and normally distributed, and the estimator of the nonparametric function has an optimal convergence rate. Simulation Studies were carried out to investigate the performance of the proposed method. For illustration purposes, the method is applied to a dataset from a study of the calcification of the hydrogel intraocular lenses, a complication of cataract treatment."
"10.1198/016214504000000421","2004","Estimation in partially linear models with missing covariates","3","The partially linear model Y = X(T)beta + v(Z) + epsilon has been studied extensively when data are completely observed. In this article, we consider the case where the covariate X is sometimes missing, with missingness probability pi depending on (Y, Z). New methods are developed for estimating and v(.). Our methods are shown to outperform asymptotically methods based only on the complete data. Asymptotic efficiency is discussed. and the semiparametric efficient score function is derived. Justification of the use of the nonparametric bootstrap in this context is sketched. The proposed estimators are extended to a working independence analysis of longitudinal/clustered data and applied to analyze an AIDS clinical trial dataset. The results of a simulation experiment are also given to illustrate our approach."
"10.1198/016214504000000412","2004","Heteroscedastic one-way {ANOVA} and lack-of-fit tests","2","Recent articles have considered the asymptotic behavior of the one-way analysis of variance (ANOVA) F statistic when the number of levels or groups is large. In these articles, the results were obtained under the assumption of homoscedasticity and for the case when the sample or group sizes n(i) remain fixed as the number of groups, a, tends to infinity. In this article. we study both weighted and unweighted test statistics in the heteroscedastic case. The unweighted statistic is new and can be used even with small group sizes. We demonstrate that an asymptotic approximation to the distribution of the weighted statistic is possible only if the group sizes tend to infinity suitably fast in relation to a. Our investigation of local alternatives reveals a similarity between lack-of-fit tests for constant regression in the present case of replicated observations and the case of no replications, which uses smoothing techniques. The asymptotic theory uses a novel application of the projection principle to obtain the asymptotic distribution of quadratic forms."
"10.1198/016214504000000403","2004","Cholesky residuals for assessing normal errors in a linear model with correlated outcomes","2","Despite the widespread popularity of linear models for correlated outcomes (e.g., linear mixed models and time series models), distribution diagnostic methodology remains relatively underdeveloped in this context. In this article we present an easy-to-implement approach that lends itself to graphical displays of model fit. Our approach involves multiplying the estimated marginal residual vector by the Cholesky decomposition of the inverse of the estimated marginal variance matrix. The resulting ""rotated"" residuals are used to construct an empirical cumulative distribution function and pointwise standard errors. The theoretical framework, including conditions and asymptotic properties, involves technical details that are motivated by Lange and Ryan. Pierce, and Randles. Our method appears to work well in a variety of circumstances. including models having independent units of sampling (clustered data) and models for which all observations are correlated (e. g.. a single time series). Our methods can produce satisfactory results even for models that do not satisfy all of the technical conditions stated in our theory."
"10.1198/016214504000000395","2004","Multiple comparison of several linear regression models","3","Research on multiple comparison during the past 50 years or so has focused mainly on the comparison of several population means. Several years ago. Spurrier considered the multiple comparison of several simple linear regression lines. He constructed simultaneous confidence bands for all of the contrasts of the simple linear regression lines over the entire range (-infinity, infinity) when the models have the same design matrices. This article extends Spurrier's work in several directions. First. multiple linear regression models are considered and the design matrices are allowed to be different. Second. the predictor variables are either unconstrained or constrained to finite intervals. Third, the types of comparison allowed can be very flexible, including pairwise. many-one, and successive. Two simulation methods are proposed for the calculation of critical constants. The methodologies are illustrated with examples."
"10.1198/016214504000000386","2004","On priors with a {K}ullback-{L}eibler property","0","In this paper, we highlight properties of Bayesian models in which the prior puts positive mass on all Kullback-Leibler neighborhoods of all densities. These properties are concerned with model choice via the Bayes factor, density estimation and the maximization of expected utility for decision problems. In four illustrations we focus on the Bayes factor and show that whatever models are being compared, the [log(Bayes factor)]/[sample size] converges to a non-random number which has a nice interpretation. A parametric versus semiparametric model comparison provides a fifth illustration."
"10.1198/016214504000000377","2004","A {B}ayesian insertion/deletion algorithm for distant protein motif searching via entropy filtering","0","Bayesian models have been developed that find ungapped motifs in multiple protein sequences. In (his article. we extend the model to allow for deletions and insertions in motifs. Direct generalization of the ungapped algorithm, based on Gibbs sampling, proved unsuccessful because the configuration space became much larger. To alleviate the convergence difficulty, a two-stage procedure is introduced. At the first stage. we develop a method called entropy filtering, which quick]), searchs ""good"" starting points for the alignment approach without the concern of deletion/insertion patterns. At the second stage, we switch to an algorithm that generates both a random vector that represents insertion/deletion patterns and a random variable of motif locations. After the two steps, gapped-motif alignments are obtained for multiple sequences. When applied to datasets that consist of helix-loop-helix proteins and high mobility group proteins, respectively. our methods show great improvements over those that produce ungapped alignments."
"10.1198/016214504000000368","2004","Propriety of the posterior distribution and existence of the {MLE} for regression models with covariates missing at random","4","Characterizing model identifiability in the presence of missing covariate data is a very important issue in missing data problems. In this article. we characterize the propriety of the posterior distribution of the regression coefficients for some general classes of regression models, including the class of generalized linear models (GLM's) and parametric survival models with right-censored data. Toward this goal, we 14 derive some very general and easy-to-check conditions for the matrix of covariates. We also derive sufficient conditions for the existence of the maximum likelihood estimates and establish novel results for checking propriety of the posterior when the sample size is large. Several theorems are given to establish propriety of the posterior and the existence of the maximum likelihood estimator. The conditions reduce to solving a system of linear equations. which can be carried Out using software such as MAPLE, IMSL, or SAS. We assume that the missing covariates are missing at random and assume an improper uniform prior for the regression coefficients. In addition, we establish these results assuming a very general form for the covariate distribution, allowing for both missing categorical and/or continuous covariates. A small dataset is used to illustrate that the posterior can be improper based on complete cases while proper when all of the cases are used in the analysis. Two real datasets are presented to demonstrate verification of posterior propriety for GLM's and parametric survival models, and also to illustrate propriety for large datasets."
"10.1198/016214504000000359","2004","Inferential aspects of the skew exponential power distribution","1","When the analysis of real data indicates that normality assumptions are untenable, more flexible models that cope with the most prevalent deviations from normality can be adopted. In this context, the skew exponential power (SEP) distribution warrants special attention, because it encompasses distributions having both heavy tails and skewness, it allows likelihood inference, and it includes the normal model as a special case. This article concerns likelihood inference about the parameters of the SEP family. In particular, the information matrix of the maximum likelihood estimators (MLEs) is obtained and finite-sample properties of the estimators are investigated numerically. Special attention is given to the properties of the MLEs and likelihood ratio statistics when the data are drawn from a normal distribution, because this case is relevant for using the SEP distribution to test for normality. Application of the SEP distribution in robust estimation problems is considered for both independent and dependent data. Under moderate deviations from normality, estimators obtained under the SEP distribution are shown to outperform the normal-based estimators and to compete with robust estimators; furthermore, the SEP distribution offers the benefits of having a specified distribution, such as interpretable location and scale parameters under nonnormality."
"10.1198/016214504000000340","2004","Robust analysis of generalized linear mixed models","3","The method of maximum likelihood (ML) is widely used for analyzing generalized linear mixed models (GLMM's). A full maximum likelihood analysis requires numerical integration techniques for calculation of the log-likelihood, and to avoid the computational problems involving irreducibly high-dimensional integrals, several maximum likelihood algorithms have been proposed in the literature to estimate the model parameters by approximating the log-likelihood function. Although these likelihood algorithms are useful for fitting the GLMM's efficiently under strict model assumptions, they can be highly influenced by the presence of unusual data points. In this article, the author develops a technique for finding robust maximum likelihood (RML) estimates of the model parameters in GLMM's, which appears to be useful in downweighting the influential data points when estimating the parameters. The asymptotic properties of the robust estimators are investigated under some regularity conditions. Small simulations are carried out to study the behavior of the robust estimates in the presence of outliers. and these estimates are also compared to the ordinary classical estimates. To avoid the computational problems involving high-dimensional integrals, the author proposes a robust Monte Carlo Newton-Raphson (RMCNR) algorithm for fitting GLMM's. The proposed robust method is illustrated in an analysis of data from a clinical experiment described in a biometrical journal."
"10.1198/016214504000000331","2004","Universal optimality for selected crossover designs","3","Hedayat and Yang earlier proved that balanced uniform designs in the entire class of crossover designs based on t treatments, n subjects, and p = t periods are universally optimal when n less than or equal to t (t - 1)/2. Surprisingly, in the class of crossover designs with t treatments and p = t periods, a balanced uniform design may not be universally optimal if the number of subjects exceeds t (t -1)/2. This article, among other results, shows that (a) a balanced uniform design is universally optimal in the entire class of' crossover designs with p = t as long as n is not greater than t(t + 2)/2 and 3 less than or equal to t less than or equal to 12; (b) a balanced uniform design with n = 2t, t greater than or equal to 3, and p = t is universally optimal in the entire class of crossover designs with n = 2t and p = t; and (c) for the case where p less than or equal to t, the design suggested by Stufken is universally optimal, thus completing Kushner's result that a Stufken design is universally optimal if n is divisible by t (p - 1)."
"10.1198/016214504000000322","2004","Modified large-sample confidence intervals for linear combinations of variance components: extension, theory, and application","0","We consider the problem of setting a confidence interval or bound for a linear combination of variance components related to a multivariate normal distribution, which includes important applications such as comparing variance components and testing the bioequivalence between two drug products. The lack of an exact confidence interval for a general linear combination of variance components spurred the development of a modified large-sample (MLS) method that was shown to be superior to many other approximation methods. But existing MLS method requires the use of independent estimators of variance components. Using a chi-squared representation of a quadratic form of a multivariate normal vector. we extend the MLS method to situations in which estimators of variance components are dependent. Using Edgeworth and Cornish-Fisher expansions, we explicitly derive the second-order asymptotic coverage error of the MLS confidence bound. Our results show that the MLS confidence bound is not second-order accurate in general, but is much better than the confidence bound based on normal approximation and is nearly second-order accurate in some special cases. Our results also show how to construct an MLS confidence bound that is second-order accurate. As an application, we discuss the use of the MLS method in assessing population bioequivalence, with some 14 simulation results and an example."
"10.1198/016214504000000430","2004","A semiparametric basis for combining estimation problems under quadratic loss","0","When there is uncertainty concerning the appropriate statistical model-estimator to use in representing the data sampling process, we consider a basis for optimally combining estimation problems. The objective is to produce natural adaptive estimators that are free of subjective choices and tuning parameters. In the context of two competing multivariate linear statistical models-estimators, we demonstrate a semiparametric Stein-like (SPSL) estimator, (&beta;) over bar((&alpha;) over cap), that, under quadratic loss, has superior risk performance relative to the conventional least squares estimator. The relationship of the SPSL estimator to the family of Stein estimators is noted, and asymptotic and analytic finite-sample risk properties of the estimator are developed for some special cases. As an application we consider the problem of combining two polar linear models and demonstrate a corresponding SPSL estimator. An extensive sampling experiment is used to investigate the finite-sample performance of the SPSL estimator over a wide range of data sampling designs and symmetric and skewed distributions. Bootstrapping procedures are used to develop confidence sets and serve as a basis for inference."
"10.1198/016214504000000494","2004","Testing homogeneity in a mixture distribution via the {$L^2$} distance between competing models","4","Ascertaining the number of components in a mixture distribution is an interesting and challenging problem for statisticians. Chen, Chen. and Kalbfleisch recently proposed a modified likelihood ratio test (MLRT), which is distribution-free and locally most powerful, asymptotically. In this article we present a new method for testing whether a finite mixture distribution is homogeneous. Our method, the D test, is based on the L-2 distance between a fitted homogeneous model and a fitted heterogeneous model. For mixture components from standard parametric families, the D-test statistic has a closed-form expression in terms of parameter estimators, whereas likelihood ratio-type test statistics do not; the latter test statistics are nontrivial functions of both the parameter estimators and the full dataset. The convergence rates of the D-test statistic under a null hypothesis of homogeneity and an alternative hypothesis of heterogeneity are established. The D test is shown to be competitive with the MLRT when the Mixture components come from a normal location family. However, in the exponential scale and normal location/scale cases, the relative performances of the D test and the MLRT are mixed. In cases such as these two, we propose to use a weighted D test, in which the measure underlying the L-2 distance is changed to accentuate the disparities between the homogeneous and heterogeneous models. Changing the measure is equivalent to computing the D-test statistic using a weighting function or to transforming the data before conducting the D test. Appropriately weighted D tests are competitive in both the exponential scale and normal location/scale cases. After applying the D test to a dataset in which the observations are measurements of firms' financial performances, we conclude with discussion and remarks."
"10.1198/016214504000000485","2004","A conditionally distribution-free multivariate sign test for one-sided alternatives","0","We consider the problem of testing the hypothesis that a multivariate location vector is in the positive orthant. A conditionally distribution-free sign test is proposed for this problem. This test is related to the Hodges test and can be motivated by the union-intersection principle. Moreover, it is valid under very mild assumptions. A characterization of the conditional null distribution of the test statistic is given. We provide a step-by-step procedure that can be used to perform the test in practice. In the bivariate case, an explicit formula for the exact null conditional distribution of the test statistic is derived. This conditional distribution can be used to compute exact conditional P values. A simulation study compares the new test to some competitors, including the likelihood ratio test. The results show that the new test is very competitive for a wide variety of distributional models. A real data example illustrating the use of the test is also presented."
"10.1198/016214504000000511","2004","Equivalence between conditional and mixture approaches to the {R}asch model and matched case-control studies, with applications","1","Analyses of data using Rasch models. including the special case of matched case-control studies, are common applications of conditional likelihood in which the usual inferential procedures are applied only after conditioning on an approximately ancillary statistic. Another common approach to the analysis of Rasch models is to integrate the nuisance parameters over a mixing distribution, using the marginal likelihood obtained as the basis for inference. We show that the full conditional likelihood can always be obtained exactly via the marginal approach. given a particular choice of mixing distribution, and derive necessary and sufficient conditions for the two approaches to agree. Previous work has shown that with sufficient flexibility in the mixing distribution, the maxima of the marginal and conditional likelihoods will be equivalent under concordance criteria. Our argument requires no such criteria, and for any dataset guarantees the equivalence of the whole of the two likelihoods, not just their maxima. This substantially enhances the previous results and provides an alternative derivation for any existing conditional analysis, We give examples of mixing distributions that guarantee the agreement of the two approaches, and explore equivalence classes of such distributions, together with some of their attractive symmetry properties. Our argument also allows for the adaption and extension of analytic techniques already widely used with Rasch data. and in particular with matched case-control studies potential applications of these advances are illustrated with several examples. These include new numerical algorithms for evaluating the conditional likelihood without directly specifying its computationally awkward functional form, inferences about complex functions of the parameters of interest obtained using existing Markov chain Monte Carlo methods, powerful measures of goodness of fit derived from likelihood contributions that are ignored by the conditional approach, and the justifiable addition of prior knowledge to existing conditional analyses."
"10.1198/016214504000000476","2004","Monte {C}arlo state-space likelihoods by weighted posterior kernel density estimation","1","Maximum likelihood estimation and likelihood ratio tests for nonlinear. non-Gaussian state-space models require numerical integration for likelihood calculations. Several methods, including Monte Carlo (MC) expectation maximization, MC likelihood ratios, direct MC integration. and particle filter likelihoods. are inefficient for the motivating problem of stage-structured population dynamics models in experimental settings. An MC kernel likelihood (MCKL) method is presented that estimates classical likelihoods up to a constant by weighted kernel density estimates of Bayesian posteriors. MCKL is derived by using Bayesian posteriors as importance sampling densities for unnormalized kernel smoothing integrals. MC error and mode bias due to kernel smoothing are discussed and two methods for reducing mode bias are proposed: ""zooming in"" on the maximum likelihood parameters using a focused prior based on an initial estimate and using a posterior cumulant-based approximation of mode bias. A simulated example shows that MCKL can be much more efficient than previous approaches for the population dynamics problem. The zooming-in and cumulant-based corrections are illustrated with a multivariate variance estimation problem for which accurate results are obtained even in 20 parameter dimensions."
"10.1198/016214504000000458","2004","Parameterization and {B}ayesian modeling","3","Progress in statistical computation often leads to advances in statistical modeling. For example, it is surprisingly common that an existing model is reparameterized. solely, for computational purposes, but then this new configuration motivates a new family of models that is useful in applied statistics. One reason why this phenomenon may not have been noticed in statistics is that reparameterizations do not change the likelihood. In a Bayesian framework, however, a transformation of parameters typically suggests a new family of prior distributions. We discuss examples in censored and truncated data, mixture modeling. multivariate imputation, stochastic processes, and multilevel models."
"10.1198/016214504000000467","2004","To model or not to model? {C}ompeting modes of inference for finite population sampling","1","Finite population sampling is perhaps the only area of statistics in which the primary mode of analysis is based on the randomization distribution, rather than on statistical models for the measured variables. This article reviews the debate between design-based and model-based inference. The basic features of the two approaches are illustrated using the case of inference about the mean from stratified random samples. Strengths and weakness of design-based and model-based inference for surveys are discussed. It is suggested that models that take into account the sample design and make weak parametric assumptions can produce reliable and efficient inferences in surveys settings. These ideas are illustrated using the problem of inference from unequal probability samples. A model-based regression analysis that leads to a combination of design-based and model-based weighting is described."
"10.1198/016214504000000016","2004","Does the statistics profession have an identity crisis?","0","The field of statistics has numerous areas of application and many positive attributes. Various companies and organizations around the world have chosen not only to adopt statistical tools, but also to integrate them into their associated work and ultimately into their decision making processes. Consequently, the value of statistics as a quantitative tool of importance to society appears to be continually increasing. In contrast, statisticians are less visible to society, and their importance and necessity is not always as evident. This is a result of many factors, some tied to the statistics profession itself and others due to external causes that are not easily controlled. This address presents some basic ideas for improving the recognition of statistics as a profession and of statisticians as the profession's core component. Included is a discussion of ways to promote the recognition of the value of statisticians to society."
"10.1198/016214504000000025","2004","Randomization inference with imperfect compliance in the {ACE}-inhibitor after anthracycline randomized trial","5","Anthracyclines are quite effective at curing certain cancers of childhood, but they may damage the heart. The ACE-Inhibitor After Anthracycline (AAA) study compared enalapril to placebo in a randomized trial in an effort to determine whether treatment with enalapril would preserve or improve cardiac function among children previously treated with anthracylines. As is true in many clinical trials, patient compliance with the study protocol was imperfect: some children took less than the prescribed dose of enalapril or placebo. Most analytical procedures that acknowledge imperfect compliance do so at significant cost, abandoning the tight logic of random assignment. With noncompliance, assignment to enalapril or placebo is randomized, but the dose of enalapril actually received is not, and self-selection effects parallel to those in observational studies can exist and have been documented in some instances. Some researchers advocate adherence to the strict logic of randomization by reporting only, or else strongly emphasizing, the so-called ""intent-to-treat"" analysis, which makes no use of information about compliance. Other researchers report analyses that are not justified by random assignment and can be subject to substantial biases, such as ""per protocol"" analyses or ""treatment received"" analyses. Here we apply a recent proposal for randomization inference with an instrumental variable that uses randomization as the ""reasoned basis for inference"" in Fisher's phrase. We make no assumption that compliance is random indeed, compliance may be severely biased. Importantly, the proposed analysis will find a statistically significant effect of the treatment if and only if the intent-to-treat analysis finds a significant effect; yet, unlike intent-to-treat analysis, our analysis acknowledges that a patient assigned to a drug that he or she does not take will not receive the drug's pharmacological benefits."
"10.1198/016214504000000034","2004","Causal models for randomized physician encouragement trials in treating primary care depression","2","This article addresses unique causal issues in the context of a randomized study on improving adherence to best practice guidelines by primary care physicians (PCP's) in treating their depressed patients. The study assessed an encouragement strategy to improve PCP guideline adherence. In this context. we compare two causal approaches: the conditional-compliance (CC) Bayesian latent class and the conditional-observable (CO) structural mean model methods. The CC methods estimate contrasts between randomized encouragement and no-encouragement arms [intent-to-treat (ITT) estimand] given latent PCP guideline complier classes. The CO methods estimate contrasts between PCP guideline adherence and nonadherence conditions (as-treated estimand) given observed PCP adherence status. The CC ITT estimand for patients with PCP compliers equals the CO as-treated estimand depending on assumptions. One such assumption pertains to the absence of physician defiers, who do the opposite of what they are encouraged to do in treating patients for depression. We relate these two estimands to each other in our clinical context when the no-defier assumption is not plausible. In other contexts, previous statistical literature has appropriately assumed the absence of defiers. However, indications in the behavioral literature, anecdotal evidence in the study, and results from the data analysis and simulations suggest that defers do exist in the context of physician-based interventions in primary care. Both simulation and empirical results show that even with a small estimated proportion of defiers under Bayesian model assumptions, inference is sensitive to different assumptions about this class of PCP noncompliers."
"10.1198/016214504000000043","2004","Dose-finding based on multiple toxicities in a soft tissue sarcoma trial","0","The scientific goal of a phase I oncology trial of a new chemotherapeutic agent is to find a dose with an acceptable level of toxicity. For ethical reasons, dose-finding is done adaptively, with doses chosen for successive cohorts of patients based on the data obtained from previous cohorts. Typically. patients are at risk for several qualitatively different toxicities, each occurring at several possible severity levels. In this article, we describe how we addressed the dose-finding problem in a phase I trial of gemcitabine for treatment of soft tissue sarcoma. The oncologists planning the trial wanted to account for differences in importance among the toxicities that they had identified. They also requested that the dose-finding method utilize the fact that a low-grade toxicity observed at a given dose, although not dose-limiting. provides a warning that a higher grade of that toxicity is likely to occur at a higher dose. Conventional phase I methods reduce each type of toxicity to an indicator of its occurrence at or above a severity level considered dose-limiting, define ""toxicity"" as the maximum of these indicators, and base dose-finding on that single binary variable. Because conventional methods do not address the aforementioned concerns, we developed a Bayesian method for dose-finding in the sarcoma trial based on a vector of correlated, ordinal-valued toxicities with severity levels varying with dose. We also developed a method for jointly eliciting the prior, a vector of weights quantifying the clinical importance of each level of each type of toxicity, and a target total toxicity burden acceptable to the physicians. Our method assigns each cohort the dose with a current posterior mean total toxicity burden closest to the target. The elicitation process is iterative, with the oncologists repeatedly shown the algorithm's behavior and asked to adjust their weights to ensure that the statistical decisions reflect appropriate clinical behavior. We describe how this methodology has worked in the sarcoma trial, present simulations and sensitivity analyses of the trial under several clinical scenarios, and provide guidelines for general application."
"10.1198/016214504000000052","2004","Bayesian survival analysis with nonproportional hazards: metanalysis of combination pravastatin-aspirin","0","Both pravastatin and aspirin are approved by the U.S. Food and Drug Administration (FDA) for secondary prevention of cardiovascular events. This article describes statistical analyses used for a successful submission to the FDA that contends that copackaging pravastatin and aspirin provides a health benefit. From the efficacy perspective this is taken to mean that the combination is more effective than either agent considered alone. We present three Bayesian hierarchical survival models and apply them to the results of five randomized clinical trials. These trials evaluated the benefit of pravastatin in the secondary-prevention setting. Aspirin use was recorded for patients in these trials. but assignment to aspirin was not randomized. We compare the effects of pravastatin and aspirin considered in combination and when given alone. We focus on time to myocardial infarction, although it was just one of several endpoints considered in the presentation to the FDA. Two of the models assume proportional hazards and the third does not. In all three models we adjust for known covariates. Our principal focus is the probability that the combination of pravastatin and aspirin is at least as effective as the agents considered separately. We also find the probability that the combination is synergistic in the sense that the effect of the combination is better than the sum of the effects of the two agents taken alone."
"10.1198/016214504000000061","2004","Analysis of contaminant co-occurrence in community water systems","0","The current framework for U.S. Environmental Protection Agency regulation of water quality in community drinking water supplies produces sequential rules for either single contaminants or small groups of similar contaminants. For both substantive and pragmatic reasons, some water industry experts have advocated the development of a more holistic regulatory process in which rules are promulgated less frequently but for larger contaminant classes. Such a framework would require the expansion of existing regulatory evaluation technologies to account for joint occurrence distributions of multiple contaminants. This article presents an analysis, using two national contaminant databases, of the joint distributions of seven contaminants (arsenic, nitrate, uranium, manganese, magnesium, calcium, and sulfate) in community water system source waters. Inferences are based on a flexible Bayesian hierarchical modeling structure with numerous features desirable for empirical exploration of multicontaminant regulations, including the simultaneous estimation of spatial heterogeneity in contaminant levels and covariations among contaminants, applicability to sparse data collected over a large spatial scale, and coherent assimilation of information provided by censored observations. The model is used to estimate a family of joint distributions for the contaminants indexed by water system characteristics, with empirically appropriate complexity given the resolution of the available data. The resulting distributions provide insights about the nature of, and uncertainty about, contaminant co-occurrence patterns, quantify the impact on national assessments of jointly modeling the contaminants, and facilitate identification of critical classes of water systems where uncertainty is highest."
"10.1198/016214504000000070","2004","Estimating the interest rate term structure of corporate debt with a semiparametric penalized spline model","2","This article provides a new methodology for estimating the term structure of corporate debt using a semiparametric penalized spline model. The method is applied to a case study of AT&T bonds. Typically, very few data are available on individual corporate bond prices, too little to find a nonparametric estimate of term structure from these bonds alone. This problem is solved by ""borrowing strength"" from Treasury bond data. More specifically, we combine a nonparametric model for the term structure of Treasury bonds with a parametric component for the credit spread. Our methodology generalizes the work of Fisher, Nychka. and Zervos in several ways. First, their model was developed for Treasury bonds only and cannot be applied directly to corporate bonds. Second, we more fully investigate the problem of choosing the smoothing parameter, a problem that is complicated because the forward rate is the derivative - log{D(t)}, where the discount function D is the function fit to the data. In our case study, estimation of the derivative requires substantially more smoothing than selected by generalized cross-validation (GCV). Another problem for smoothing parameter selection is possible correlation of the errors. We compare three methods of choosing the penalty parameter: generalized cross-validation (GCV), the residual spatial autocorrelation (RSA) method of Ellner and Seifu, and an extension of Ruppert's empirical bias bandwidth selection (EBBS) to splines. Third, we provide approximate sampling distributions based on asymptotics for the Treasury forward rate and the bootstrap for corporate bonds. Confidence bands and tests of interesting hypotheses, for example, about the functional form of the credit spreads, are also discussed."
"10.1198/016214504000000098","2004","Multicategory support vector machines: theory and application to the classification of microarray data and satellite radiance data","5","Two-category support vector machines (SVM) have been very popular in the machine learning community for classification problems. Solving multicategory problems by a series of binary classifiers is quite common in the SVM paradigm; however, this approach may fail under various circumstances. We propose the multicategory support vector machine (MSVM), which extends the binary SVM to the multicategory case and has good theoretical properties. The proposed method provides a unifying framework when there are either equal or unequal misclassification costs. As a tuning criterion for the MSVM, an approximate leave-one-out cross-validation function, called Generalized Approximate Cross Validation, is derived. analogous to the binary case. The effectiveness of the MSVM is demonstrated through the applications to cancer classification using microarray data and cloud classification with satellite radiance profiles."
"10.1198/016214504000000106","2004","Subsampling methods to estimate the variance of sample means based on nonstationary spatial data with varying expected values","1","Subsampling and block resampling methods have been suggested in the literature to nonparametrically estimate the variance of statistics computed from spatial data. Usually stationary data are required. However, in empirical applications, the assumption of stationarity often must be rejected. This article proposes nonparametric methods to estimate the variance of (functions of) sample means based on nonstationary spatial data using subsampling. We assume that data are observed on a lattice in some region of R-2. In the data that we consider, the information in the different picture elements (pixels) of the lattice are allowed to come from different distributions, with smoothly varying expected values, or with expected values decomposed additively into directional components. Furthermore, pixels are assumed to be locally dependent, and the dependence structure is allowed to differ over the lattice. Consistent variance estimators for (functions of) sample means, together with convergence rates in mean square, are provided under these assumptions. An example with applications to forestry, using satellite data, is discussed."
"10.1198/016214504000000089","2004","Large-scale simultaneous hypothesis testing: the choice of a null hypothesis","30","Current scientific techniques in genomics and image processing routinely produce hypothesis testing problems with hundreds or thousands of cases to consider simultaneously. This poses new difficulties for the statistician, but also opens new opportunities. In particular, it allows empirical estimation of an appropriate null hypothesis. The empirical null may be considerably more dispersed than the usual theoretical null distribution that would be used for any one case considered separately. An empirical Bayes analysis plan for this situation is developed, using a local version of the false discovery rate to examine the inference issues. Two genomics problems are used as examples to show the importance of correctly choosing the null hypothesis."
"10.1198/016214504000000197","2004","Semiparametric failure time regression with replicates of mismeasured covariates","2","This article proposes a general strategy for the regression analysis of univariate and multivariate failure time data when a subset of covariates cannot be measured precisely but replicate measurements of their surrogates are available. Multivariate failure time data include recurrent events and clustered Survival data. The number of replicate measurements can vary from subject to subject and can even depend on the failure time. No parametric assumption is imposed on the error or on any other random variable. Several semiparametric regression models are considered, including the Cox proportional hazards model for univariate failure time data, multiplicative intensity/rate models for recurrent events data, and marginal Cox proportional hazards models for general multivariate failure time data. The existing estimating functions in the absence of measurement error are corrected to yield consistent and asymptotically normal estimators of the regression parameters. The estimation of the underlying failure time distribution is also studied. The operating characteristics of the proposed estimators are assessed through extensive simulation studies. An application to multiple tumor recurrences from a cancer prevention trial is provided."
"10.1198/016214504000000115","2004","Kernel estimators for univariate binary regression","2","We present a rather thorough investigation of the use of kernel-based nonparametric estimators of the binary regression function in the case of a single covariate. We consider various versions of Nadaraya-Watson and local linear estimators. some involving a single bandwidth and others involving two bandwidths. The locally linear logistic estimator proves to be a good single-bandwidth estimator, although the basic Nadaraya-Watson estimator also fares quite well. Two-bandwidth methods show great potential when bandwidths are selected with knowledge of the target function, but much of their potential vanishes when data-based bandwidths are used. Likelihood cross-validation and plug-in approaches are the data-based bandwidth selection methods tested; both prove quite useful, with a preference for the latter. Adaptive two-bandwidth methods retain particularly good performance only in certain special situations (and separate estimation of the two bandwidths as for optimal density estimation is never recommended). We therefore propose a hybrid estimation procedure in which the local linear logistic estimator is used unless the ratio of (robust) variances of the covariate in the success and failure groups is greater than 2, in which case we switch to a two-bandwidth Nadaraya-Watson-type estimator, each using plug-in bandwidth selection."
"10.1198/016214504000000124","2004","A two-stage regression model for epidemiological studies with multivariate disease classification data","1","Polytomous logistic regression is commonly used to analyze epidemiological data with disease subtype information. In this approach effects of exposures on different disease subtypes are studied through separate exposure odds ratios comparing different case groups to the common control group. This article considers the situation where disease subtypes can be defined Using multiple characteristics of a disease. For efficient analysis of such data, a two-stage modeling approach is proposed. At the first stage, a standard polytomous logistic regression model is considered for all possible distinct disease subtypes that can be defined by the cross-classification of the different disease characteristics. At the second stage, the exposure odds ratio parameters for the first-stage disease subtypes are further modeled in terms of the defining characteristics of the Subtypes. When the total number of first-stage disease subtypes is small, standard maximum likelihood methods can be used for inference in the proposed model. For dealing with a large number of disease Subtypes. a novel semiparametric pseudo-conditional-likelihood approach is proposed that does not require any model assumption about the baseline probabilities for the different disease subtypes. This article develops the asymptotic theory for the estimator and studies its small-sample properties using simulation experiments. The proposed method is applied to study the effect of fiber on the risk of various forms of colorectal adenoma using data available from a large screening study, the Prostate, Lung, Colorectal and Ovarian Cancer (PLCO) Screening Trial."
"10.1198/016214504000000133","2004","Likelihood-based local linear estimation of the conditional variance function","2","We consider estimation of mean and variance functions with kernel-weighted local polynomial fitting in a heteroscedastic nonparametric regression model. Our preferred estimators Lire based on a localized normal likelihood, using a standard local linear form for estimating the mean and a local log-linear form for estimating the variance. It is important to allow two bandwidths in this problem, separate ones for mean and variance estimation. We provide data-based methods for choosing the bandwidths. We also consider asymptotic results. and Study and use them. The methodology is compared with a popular competitor and is seen to perform better for small and moderate sample sizes in simulations. A brief example is provided."
"10.1198/016214504000000142","2004","Testing independence for bivariate current status data","1","This article develops a nonparametric procedure for testing marginal independence based on bivariate current status data. Asymptotic properties of the proposed tests are derived, and their finite-sample performance is studied via simulations. The method is applied to analyze data from a community-based study of cardiovascular epidemiology in Taiwan."
"10.1198/016214504000000151","2004","Monte {C}arlo smoothing for nonlinear times series","2","We develop methods for performing smoothing computations in general state-space models. The methods rely on a particle representation of the filtering distributions, and their evolution through time using sequential importance sampling and resampling ideas. In particular, novel techniques are presented for generation of sample realizations of historical state sequences. This is carried out in a forward-filtering backward-smoothing procedure that can be viewed as the nonlinear, non-Gaussian counterpart of standard Kalman filter-based simulation smoothers in the linear Gaussian case. Convergence in the mean squared error sense of the smoothed trajectories is proved, showing the validity of our proposed method. The methods are tested in a substantial application for the processing of speech signals represented by a time-varying autoregression and parameterized in terms of time-varying partial correlation coefficients, comparing the results of our algorithm with those from a simple smoother based on the filtered trajectories."
"10.1198/016214504000000160","2004","Testing for difference between conditional means in a time series context","0","In this article we study tests for equality of two regression curves when the inputs are driven by a time series. The basic process underlying the test statistics is the empirical process of the time series marked by the difference in the pertaining dependent variables. The main results hold under strict stationarity of the input variables, but no mixing condition or special modeling of the time series will be necessary. A simulation study is reported on, which illustrates the quality of the distributional approximation and the power of the tests for small to moderate sample sizes. An application to a real dataset is also included."
"10.1198/016214504000000179","2004","Computational methods for multiplicative intensity models using weighted gamma processes: proportional hazards, marked point processes, and panel count data","3","We develop computational procedures for a class of Bayesian nonparametric and semiparametric multiplicative intensity models incorporating kernel mixtures of spatial weighted gamma measures. A key feature of our approach is that explicit expressions for posterior distributions of these models share many common structural features with the posterior distributions of Bayesian hierarchical models using the Dirichlet process. Using this fact, along with an approximation for the weighted gamma process, we show that with some care, one can adapt efficient algorithms used for the Dirichlet process to this setting. We discuss blocked Gibbs sampling procedures and Polya urn Gibbs samplers. We illustrate our methods with applications to proportional hazard models, Poisson spatial regression models, recurrent events, and panel count data."
"10.1198/016214504000000188","2004","On the statistical analysis of smoothing by maximizing dirty {M}arkov random field posterior distributions","0","We consider Bayesian nonpararnetric function estimation using a Markov random field prior based on the Laplace distribution. We describe efficient methods for finding the exact maximum a posteriori estimate, which handle constraints naturally and avoid the problems posed by nondifferentiability of the posterior distribution; the methods also make links to spline and wavelet smoothers and to a dual posterior distribution. Three automatic smoothing parameter selection procedures are described: empirical Bayes, two-fold cross-validation, and a universal rule for the Laplace prior. Monte Carlo Simulation with Gaussian and Poisson responses demonstrates that the flew estimator can give better estimates of nonsmooth functions than can a similar prior based on the Gaussian distribution or wavelet-based competitors. Applications are given to spectral density estimation and to Poisson image denoising."
"10.1198/016214504000000205","2004","An {ANOVA} model for dependent random measures","13","We consider dependent nonparametric models for related random probability distributions. For example, the random distributions might be indexed by a categorical covariate indicating the treatment levels in a clinical trial and might represent random effects distributions under the respective treatment combinations. We propose a model that describes dependence across random distributions in an analysis of variance (ANOVA)-type fashion. We define a probability model in such a way that marginally each random measure follows a Dirichlet process (DP) and use the dependent Dirichlet process to define the desired dependence across the related random measures. The resulting probability model can alternatively be described as a mixture of ANOVA models with a DP prior on the unknown mixing measure. The main features of the proposed approach are ease of interpretation and computational simplicity. Because the model follows the standard ANOVA structure, interpretation and inference parallels conventions for ANOVA models. This includes the notion of main effects. interactions, contrasts, and the like. Of course, the analogies are limited to Structure and interpretation. The actual objects of the inference are random distributions instead of the unknown normal means in standard ANOVA models. Besides interpretation and model structure, another important feature of the proposed approach is ease of posterior simulation. Because the model can be rewritten as a DP mixture of ANOVA models, it inherits all computational advantages of standard DP mixture models. This includes availability of efficient Gibbs sampling schemes for posterior simulation and ease of implementation of even high-dimensional applications. Complexity of implementing posterior simulation is-at least conceptually-dimension independent."
"10.1198/016214504000000214","2004","The {IOS} test for model misspecification","1","A new test of model misspecification is proposed, based on the ratio of in-sample and out-of-sample likelihoods. The test is broadly applicable and, in simple problems, approximates well-known, intuitive methods. Using jackknife influence curve approximations, it is shown that the test statistic can be viewed asymptotically as a multiplicative contrast between two estimates of the information matrix, both of which are consistent under correct model specification. This approximation is used to show that the statistic is asymptotically normally distributed, although it is suggested that p values be computed using the parametric bootstrap. The resulting methodology is demonstrated with various examples and simulations involving both discrete and continuous data."
"10.1198/016214504000000223","2004","Geometric ergodicity of van {D}yk and {M}eng's algorithm for the multivariate {S}tudent's {$t$} model","3","Let pi denote the posterior distribution that results when a random sample of size n from a d-dimensional location-scale Student's t distribution (with v degrees of freedom) is combined with the standard noninformative prior. van Dyk and Meng developed an efficient Markov chain Monte Carlo (MCMC) algorithm for sampling from pi and provided considerable empirical evidence to suggest that their algorithm converges to stationarity much faster than the standard data augmentation algorithm. In addition to its practical importance, this algorithm is interesting from a theoretical standpoint because it is based upon a Markov chain that is not positive recurrent. In this article, we formally analyze the relevant sub-Markov chain underlying van Dyk and Meng's algorithm. In particular, we establish drift and minorization conditions that show that, for many (d, v, n) triples, the sub-Markov chain is geometrically ergodic. This is the first general, rigorous analysis of an MCMC algorithm based upon a nonpositive recurrent Markov chain. Moreover, our results are important from a practical standpoint because (1) geometric ergodicity guarantees the existence of central limit theorems that can be used to construct Monte Carlo standard errors and (2) the drift and minorization conditions themselves allow for the calculation of exact upper bounds on the total variation distance to stationarity. The results are illustrated using a simple numerical example."
"10.1198/016214504000000232","2004","Methodology for evaluating a partially controlled longitudinal treatment using principal stratification, with application to a needle exchange program","3","We consider studies for evaluating the short-term effect of a treatment of interest on a time-to-event outcome. The studies we consider are partially controlled in the following sense: (I) Subjects' exposure to the treatment of interest can vary over time, but this exposure is not directly controlled by the study; (2) subjects' follow-up time is not directly controlled by the study; and (3) the study directly controls another factor that can affect subjects' exposure to the treatment of interest as well as subjects' follow-up time. When factors (1) and (2) are both present in the study, evaluating the treatment of interest using standard methods, including instrumental variables, does not generally estimate treatment effects. We develop the methodology for estimating the effect of treatment in this setting of partially controlled studies under explicit assumptions using the framework for principal stratification for causal inference. We illustrate our methods by a study to evaluate the efficacy of the Baltimore Needle Exchange Program to reduce the risk of human immunodeficiency virus (HIV) transmission, using data on distance of the program's sites from the subjects."
"10.1198/016214504000000241","2004","Inconsistent estimation and asymptotically equal interpolations in model-based geostatistics","15","it is shown that in model-based geostatistics, not all parameters in the Matern class can be estimated consistently if data are observed in an increasing density in a fixed domain, regardless of the estimation methods used. Nevertheless, one quantity can be estimated consistently by the maximum likelihood method, and this quantity is more important to spatial interpolation. The results are established by using the properties of equivalence and orthogonality of probability measures. Some sufficient conditions are provided for both Gaussian and non-Gaussian equivalent measures, and necessary conditions are provided for Gaussian equivalent measures. Two simulation studies are presented that show that the fixed-domain asymptotic properties can explain some finite-sample behavior of both interpolation and estimation when the sample size is moderately large."
"10.1198/016214504000000250","2004","Spatially balanced sampling of natural resources","2","The spatial distribution of a natural resource is an important consideration in designing an efficient survey or monitoring program for the resource. Generally, sample sites that are spatially balanced, that is, more or less evenly dispersed over the extent of the resource, are more efficient than simple random sampling. We review a unified strategy for selecting spatially balanced probability samples of natural resources. The technique is based on creating a function that maps two-dimensional space into one-dimensional space, thereby defining an ordered spatial address. We use a restricted randomization to randomly order the addresses, so that systematic sampling along the randomly ordered linear structure results in a spatially well-balanced random sample. Variable inclusion probability, proportional to an arbitrary positive ancillary variable, is easily accommodated. The basic technique selects points in a two-dimensional continuum, but is also applicable to sampling finite populations or one-dimensional continua embedded in two-dimensional space. An extension of the basic technique gives a way to order the sample points so that any set of consecutively numbered points is in itself a spatially well-balanced sample. This latter property is extremely useful in adjusting the sample for the frame imperfections common in environmental sampling."
"10.1198/016214504000000269","2004","Methods and criteria for model selection","2","Model selection is an important part of any statistical analysis and, indeed, is central to the pursuit of science in general. Many authors have examined the question of model selection from both frequentist and Bayesian perspectives, and many tools for selecting the ""best model"" have been suggested in the literature. This paper considers the various proposals from a Bayesian decision-theoretic perspective."
"10.1198/016214505000000682","2005","A kernel-based spatio-temporal dynamical model for nowcasting weather radar reflectivities","2","A good short-period forecast of heavy rainfall is essential for many meteorological and hydrological applications. Traditional deterministic and stochastic nowcasting methodologies have been inadequate in their characterization of pixelwise rainfall reflectivity propagation, intensity, and uncertainty. The methodology presented herein uses an approach that efficiently parameterizes spatio-temporal dynamic models in terms of integro-difference equations within a hierarchical framework. The approach accounts for the uncertainty in the prediction and provides relevant distributional information concerning the nowcast. An application is presented that shows the effectiveness of the technique and its potential for nowcasting weather radar reflectivities."
"10.1198/016214505000000754","2005","Imputation of binary treatment variables with measurement error in administrative data","3","Administrative systems-specifically, cancer registries-can be a valuable data source for studies of health care; however, provision of adjuvant chemotherapy or radiation therapy is often underreported in such databases. In a study of colorectal cancer in California, a relatively small physician follow-back survey allowed us to model the probability of underreporting. We then wished to model the relationship of true treatment status to covariates in the full database. We developed hierarchical models for imputation of corrected data using data recorded with error in the administrative system and the ""validation sample"" from the survey. The model includes a model for the probability of receipt of chemotherapy and a model for the probability of reporting given that chemotherapy was received. This factorization of the joint distribution of the true status and reported data is designed to permit generalization from the validation sample to a larger population in which the reporting process is similar but the prevalence of treatment may differ. Hospital random effects are included to represent variation in both treatment and reporting patterns across hospitals. We used Markov chain Monte Carlo simulation techniques to estimate model parameters and impute true treatment status. Valid inferences are obtained by combining the results from multiply imputed datasets. In an analysis of predictors of survival using imputed data that corrected for bias due to underreporting, uncertainty due to underreporting of chemotherapy substantially inflated the variance of estimates of the chemotherapy effect but had little effect on the estimation of coefficients of other characteristics."
"10.1198/016214504000001835","2005","Forecasts from nonrandom samples: the election night case","1","The 1990s was not the best of decades for electoral polls, with striking errors occurring in, among others, the British, French, and Spanish elections, including election night, when errors are more evident. This article proposes a model for predicting final election outcomes based on the consistency that polling stations show between elections. Using both past and incoming polling station vote proportions, the model produces continuously revised predictions. The method is validated predicting the 1995 Corts Valencianes (Valencia regional parliament) elections and displaying the real-time experience of the 1999 Corts Valencianes election night. The case study is completed by demonstrating the technique's efficacy in three additional elections. The results confirm that the procedure generates quick, highly reliable, and accurate forecasts. In fact, only a few minutes after starting the scrutiny, the proposal permits one to approximate the final results with great precision, even with only a small percentage of votes polled. The great flexibility of the procedure makes it possible to use the method under a wide variety of circumstances and electoral systems. Furthermore, this procedure has additional advantages, including robustness and lower cost, over other methods which can also be implemented during election night with the objective of forecasting final outcomes, like exit polls or quick counts of a meaningful sample of polling stations."
"10.1198/016214505000000619","2005","Estimating risks of identification disclosure in microdata","4","When statistical agencies release microdata to the public, malicious users (intruders) may be able to link records in the released data to records in external databases. Releasing data in ways that fail to prevent such identifications may discredit the agency or, for some data, constitute a breach of law. To limit disclosures, agencies often release altered versions of the data; however, there usually remain risks of identification. This article applies and extends the framework developed by Duncan and Lambert for computing probabilities of identification for sampled units. It describes methods tailored specifically to data altered by recoding and topcoding variables, data swapping, or adding random noise (and combinations of these common data alteration techniques) that agencies can use to assess threats from intruders who possess information on relationships among variables and the methods of data alteration. Using data from the Current Population Survey, the article illustrates a step-by-step process for evaluating identification disclosure risks for competing releases under varying assumptions of intruders' knowledge. Risk measures are presented for individual units and for entire datasets."
"10.1198/016214505000001005","2005","Structural equation models: a review with applications to environmental epidemiology","0","Structural equation models (SEMs) have been discussed extensively in the psychometrics and quantitative behavioral sciences literature. However, many statisticians and researchers in other areas of application are relatively unfamiliar with their implementation. Here we review some of the SEM literature and describe basic methods, using examples from environmental epidemiology. We make connections to recent work on latent variable models for multivariate outcomes and to measurement error methods, and discuss advantages and disadvantages of SEMs compared with traditional regressions. We give a detailed example in which two models fit the same data well, yet one is physiologically implausible. This underscores the critical role of subject matter knowledge in the successful implementation of SEMs. A brief discussion on open research areas is included."
"10.1198/016214505000000141","2005","Nonparametric model calibration estimation in survey sampling","0","Calibration is commonly used in survey sampling to include auxiliary information at the estimation stage of a population parameter. Calibrating the observation weights on population means (totals) of a set of auxiliary variables implies building weights that when applied to the auxiliaries give exactly their population mean (total). Implicitly, calibration techniques rely on a linear relation between the survey variable and the auxiliary variables. However, when auxiliary information is available for all units in the population, more complex modeling can be handled by means of model calibration; auxiliary variables are used to obtain fitted values of the survey variable for all units in the population, and estimation weights are sought to satisfy calibration constraints on the fitted values population mean, rather than on the auxiliary variables one. In this work we extend model calibration considering more general superpopulation models and use nonparametric methods to obtain the fitted values on which to calibrate. More precisely, we adopt neural network learning and local polynomial smoothing to estimate the functional relationship between the survey variable and the auxiliary variables. Under suitable regularity conditions, the proposed estimators are proven to be design consistent. The moments of the asymptotic distribution are also derived, and a consistent estimator of the variance of each distribution is then proposed. The performance of the proposed estimators for finite-size samples is investigated by means of simulation studies. An application to the assessment of the ecological conditions of streams in the mid-Atlantic highlands in the United States is also carried out."
"10.1198/016214505000000286","2005","A family of symmetric distributions on the circle","1","We propose a new family of symmetric unimodal distributions on the circle that contains the uniform, von Mises, cardioid, and wrapped Cauchy distributions, among others, as special cases. The basic form of the densities of this family is very simple, although its normalization constant involves an associated Legendre function. The family of distributions can also be derived by conditioning and projecting certain bivariate spherically and elliptically symmetric distributions on to the circle. Trigonometric moments are available, and a measure of variation is discussed. Aspects of maximum likelihood estimation are considered, and likelihood is used to fit the family of distributions to an example set of data. Finally, extension to a family of rotationally symmetric distributions on the sphere is briefly made."
"10.1198/01621450500000034910.1198/016214505000000349","2005","Independent particle filters","1","Sequential Monte Carlo methods, especially the particle filter (PF) and its various modifications, have been used effectively in dealing with stochastic dynamic systems. The standard PF samples the current state through the underlying state dynamics, then uses the current observation to evaluate the sample's importance weight. However, there is a set of problems in which the current observation provides significant information about the current state but the state dynamics are weak, and thus sampling using the current observation often produces more efficient samples than sampling using the state dynamics. In this article we propose a new variant of the PF, the independent particle filter (IPF), to deal with these problems. The IPF generates exchangeable samples of the current state from a sampling distribution that is conditionally independent of the previous states, a special case of which uses only the current observation. Each sample can then be matched with multiple samples of the previous states in evaluating the importance weight. We present some theoretical results showing that this strategy improves efficiency of estimation as well as reduces resampling frequency. We also discuss some extensions of the IPF, and use several synthetic examples to demonstrate the effectiveness of the method."
"10.1198/016214505000000169","2005","A tale of two time scales: determining integrated volatility with noisy high-frequency data","13","It is a common practice in finance to estimate volatility from the sum of frequently sampled squared returns. However, market microstructure poses challenges to this estimation approach, as evidenced by recent empirical studies in finance. The present work attempts to lay out theoretical grounds that reconcile continuous-time modeling and discrete-time samples. We propose an estimation approach that takes advantage of the rich sources in tick-by-tick data while preserving the continuous-time assumption on the underlying returns. Under our framework, it becomes clear why and where the ""usual"" volatility estimator fails when the returns are sampled at the highest frequencies. If the noise is asymptotically small, our work provides a way of finding the optimal sampling frequency. A better approach, the ""two-scales estimator,"" works for any size of the noise."
"10.1198/016214505000000303","2005","Diverging moments and parameter estimation","0","Heavy-tailed distributions are enjoying increased popularity and are becoming more readily applicable as the arsenal of analytical and numerical tools grows. They play key roles in modeling approaches in networking, finance, and hydrology, to name but a few areas. The tail parameter alpha is of central importance, because it governs both the existence of moments of positive order and the thickness of the tails of the distribution. Some of the best-known tail estimators, such as those of Koutrouvelis and Hill, are either parametric or show a lack of robustness or accuracy. This article develops a shift- and scale-invariant nonparametric estimator for both, upper and lower bounds for orders with finite moments. The estimator builds on the equivalence between tail behavior and the regularity of the characteristic function at the origin and achieves its goal by deriving a simplified wavelet analysis that is particularly suited to characteristic functions."
"10.1198/016214505000000600","2005","Optimal design for goodness-of-fit of the {M}ichaelis-{M}enten enzyme kinetic function","1","We construct efficient designs for the Michaelis-Menten enzyme kinetic model capable of checking model assumptions. An extended model called EMAX is also considered for this purpose. This model is widely used in pharmacokinetics and reduces to the Michaelis-Menten model for a specific choice of parameter settings. Our strategy is to find efficient designs for estimating the parameters in the EMAX model and at the same time test the validity of the Michaelis-Menten model against the EMAX model by maximizing a minimum of the D or D, efficiencies taken over a range of values for the nonlinear parameters. In particular, we show that such designs are (a) efficient for estimating parameters in the EMAX model, (b) about 70% efficient for estimating parameters in the Michaelis-Menten model, (c) efficient for testing the Michaelis-Menten model against the EMAX model, and (d) robust with respect to misspecification of the unknown parameters in the nonlinear model."
"10.1198/016214505000000385","2005","Multiscale, multigranular statistical image segmentation","0","We consider a problem in image segmentation in which the goal is to determine and label a relatively small number of homogeneous subregions in an image scene, based on multivariate, pixelwise measurements. Motivated by current challenges in the field of remote sensing land cover characterization, we introduce a framework that allows for adaptive choice of both the spatial resolution of subregions and the categorical granularity of labels. Our framework is based on a class of models that we call mixlets, a blending of recursive dyadic partitions and finite mixture models. The first component of these models allows for the sparse representation of a spatial structure at multiple resolutions. The second component provides a natural mechanism for capturing the varying degrees of mixing of pure categories that accompany the use of different resolutions and for relating these to a user-specified hierarchy of labels at multiple granularities in a straightforward manner. A segmentation is produced in our framework by selecting an optimal mixlet model, through complexity-penalized maximum likelihood, and summarizing the information in that model with respect to the categorical hierarchy. Both theoretical and empirical evaluations of the proposed framework are presented."
"10.1198/016214505000000420","2005","A fast, optimal spatial-prediction method for massive datasets","2","This article considers a class of multiresolution tree-structured models that are spatially shifted versions of each other and proposes a new spatial-prediction method that averages over the optimal spatial predictors produced from members of this class of models. As a consequence, the resulting predicted surface is smooth, even when the predictors generated separately from individual multiresolution tree-structured models are not. We call the new predictor the multiresolution spatial (MURS) predictor and develop a computationally efficient algorithm for it. The algorithm can handle massive datasets even when some observations are missing. Moreover, the MURS predictor can be shown to be the minimum mean squared error predictor for a large class of covariance functions. A simulation example for massive datasets shows that the MURS method consistently outperforms two commonly used filtering methods. Total column ozone data remotely sensed from a satellite are analyzed using the new methodology."
"10.1198/016214505000000547","2005","Linear unmixing of multivariate observations: a structural model","0","In many fields of science there are multivariate observations that may be assumed to be generated by a (physical) linear mixing process of contributions from different sources. If the compositions of the sources are constant for different observations, then these observations are, up to a random error term, nonnegative linear combinations of a fixed set of so-called ""source profiles"" that characterize the sources. The goal of linear unmixing is to recover both the source profiles and the source activities (also called scores) from a multivariate dataset. We present a new parametric mixing model that assumes a multivariate lognormal distribution for the scores. This model is proved to be identifiable. Moreover, consistency and asymptotic normality of the maximum likelihood estimator (MLE) are established in special cases. To calculate the MLE, we propose the combination of two variants of the Monte Carlo EM algorithm. The proposed model is applied to simulated datasets and to a set of air pollution measurements. In addition to the basic model, several extensions are discussed."
"10.1198/016214505000000259","2005","A generalized {W}ang-{L}andau algorithm for {M}onte {C}arlo computation","5","Inference for a complex system with a rough energy landscape is a central topic in Monte Carlo computation. Motivated by the successes of the Wang-Landau algorithm in discrete systems, we generalize the algorithm to continuous systems. The generalized algorithm has some features that conventional Monte Carlo algorithms do not have. First, it provides a new method for Monte Carlo integration based on stochastic approximation; second, it is an excellent tool for Monte Carlo optimization. In an appropriate setting, the algorithm can lead to a random walk in the energy space, and thus it can sample relevant parts of the sample space, even in the presence of many local energy minima. The generalized algorithm can be conveniently used in many problems of Monte Carlo integration and optimization, for example, normalizing constant estimation, model selection, highest posterior density interval construction, and function optimization. Our numerical results show that the algorithm outperforms simulated annealing and parallel tempering in optimization for the system with a rough energy landscape. Some theoretical results on the convergence of the algorithm are provided."
"10.1198/016214505000000529","2005","Outlier robust model selection in linear regression","2","We propose a new approach to the selection of regression models based on combining a robust penalized criterion and a robust conditional expected prediction loss function that is estimated using a stratified bootstrap. Both components of the procedure use robust criteria (i.e., robust rho-functions) rather than squared error loss to reduce the effects of large residuals and poor bootstrap samples. A key idea is to separate estimation from model selection by choosing estimators separately from the rho-function. Using the stratified bootstrap further reduces the likelihood of obtaining poor bootstrap samples. We show that the model selection procedure is consistent under some conditions and works well in our simulations. In particular, we find that simultaneous minimization of prediction error and conditional expected prediction loss is better than separate minimization of the prediction error or the conditional expected prediction loss."
"10.1198/016214505000000358","2005","On consistency of nonparametric normal mixtures for {B}ayesian density estimation","2","The past decade has seen a remarkable development in the area of Bayesian nonparametric inference from both theoretical and applied perspectives. As for the latter, the celebrated Dirichlet process has been successfully exploited within Bayesian mixture models, leading to many interesting applications. As for the former, some new discrete nonparametric priors have been recently proposed in the literature that have natural use as alternatives to the Dirichlet process in a Bayesian hierarchical model for density estimation. When using such models for concrete applications, an investigation of their statistical properties is mandatory. Of these properties, a prominent role is to be assigned to consistency. Indeed, strong consistency of Bayesian nonparametric procedures for density estimation has been the focus of a considerable amount of research; in particular, much attention has been devoted to the normal mixture of Dirichlet process. In this article we improve on previous contributions by establishing strong consistency of the mixture of Dirichlet process under fairly general conditions. Besides the usual Kullback-Leibler support condition, consistency is achieved by finiteness of the mean of the base measure of the Dirichlet process and an exponential decay of the prior on the standard deviation. We show that the same conditions are also sufficient for mixtures based on priors more general than the Dirichlet process. This leads to the easy establishment of consistency for many recently proposed mixture models."
"10.1198/016214505000000132","2005","Hierarchical mixture modeling with normalized inverse-{G}aussian priors","5","In recent years the Dirichlet process prior has experienced a great success in the context of Bayesian mixture modeling. The idea of overcoming discreteness of its realizations by exploiting it in hierarchical models, combined with the development of suitable sampling techniques, represent one of the reasons of its popularity. In this article we propose the normalized inverse-Gaussian (N-IG) process as an alternative to the Dirichlet process to be used in Bayesian hierarchical models. The N-IG prior is constructed via its finite-dimensional distributions. This prior, although sharing the discreteness property of the Dirichlet prior, is characterized by a more elaborate and sensible clustering which makes use of all the information contained in the data. Whereas in the Dirichlet case the mass assigned to each observation depends solely on the number of times that it occurred, for the N-IG prior the weight of a single observation depends heavily on the whole number of ties in the sample. Moreover, expressions corresponding to relevant statistical quantities, such as a priori moments and the predictive distributions, are as tractable as those arising from the Dirichlet process. This implies that well-established sampling schemes can be easily extended to cover hierarchical models based on the N-IG process. The mixture of N-IG process and the mixture of Dirichlet process are compared using two examples involving mixtures of normals."
"10.1198/016214505000000538","2005","A pseudo-partial likelihood method for semiparametric survival regression with covariate errors","6","This article presents an estimator for the regression coefficient vector in the Cox proportional hazards model with covariate error. The estimator is obtained by maximizing a likelihood-type function similar to the Cox partial likelihood. The likelihood function involves the cumulative baseline hazard function, for which a simple estimator is substituted. The method is capable of handling general covariate error structures; it is not restricted to the independent additive error model. It can be applied to studies with either an external or internal validation sample, and also to studies with replicate measurements of the surrogate covariate. The estimator is shown to be consistent and asymptotically normal, and an estimate of the asymptotic covariance matrix is derived. Some extensions to general transformation survival models are indicated. Simulation studies are presented for a setup with a single error-prone binary covariate and a setup with a single error-prone normally distributed covariate. These simulation studies show that the method typically produces estimates with low bias and confidence intervals with accurate coverage rates. Efficiency results relative to fully parametric maximum likelihood are also presented. The method is applied to data from the Framingham Heart Study."
"10.1198/016214505000000295","2005","Weighted estimators for proportional hazards regression with missing covariates","6","Missing covariate data are common in epidemiologic studies and disease prevention trials. In this article regression parameter estimation in the Cox proportional hazards model is considered when certain covariates are observed for all study subjects and other covariate data are collected only for a subset. The article presents both simple weighted and kernel-assisted fully augmented weighted estimators that use the partially incomplete data nonparametrically. We use nonparametric methods to estimate selection probabilities in the simple weighted estimating functions. We also use nonparametric kernel smoothing techniques to estimate certain conditional expectations in fully augmented weighted estimating functions. The proposed methods are nonparametric in the sense that they require neither a model for the missing-data mechanism nor specification of the conditional distribution of missing covariates given observed covariates. These estimators allow the missing-data mechanism to depend on outcome variables and observed covariates, and they are applicable to various cohort Sampling procedures, including case-cohort and nested case-control designs. We show that the simple and the kernel-assisted fully augmented weighted estimators are typically consistent and asymptotically normal. Moreover, the proposed estimators are more efficient than the simple weighted estimator with the inverse of true selection probability as weight. They also correct the bias of estimates from analysis of the complete data alone when the missing-data mechanism depends on outcome variables. In addition, when covariates are time-independent, certain simple weighted estimators are shown to be asymptotically equivalent to the kernel-assisted fully augmented weighted estimators. Moderate sample size performance of the estimators is examined via simulation and by application to two real datasets."
"10.1198/016214505000000583","2005","Nonparametric estimation of an additive quantile regression model","4","This article is concerned with estimating the additive components of a nonparametric additive quantile regression model. We develop an estimator that is asymptotically normally distributed with a rate of convergence in probability of n(-r/(2r+1)) when the additive components are r-times continuously differentiable for some r >= 2. This result holds regardless of the dimension of the covariates, and thus the new estimator has no curse of dimensionality. In addition, the estimator has an oracle property and is easily extended to a generalized additive quantile regression model with a link function. The numerical performance and usefulness of the estimator are illustrated by Monte Carlo experiments and an empirical example."
"10.1198/016214505000000330","2005","Quantiles for counts","1","This article studies the estimation of conditional quantiles of counts. Given the discreteness of the data, some smoothness must be artificially imposed on the problem. We show that it is possible to smooth the data in a way that allows inference to be performed using standard quantile regression techniques. The performance and implementation of the estimators are illustrated by simulations and an application."
"10.1198/016214505000000367","2005","Efficient empirical {B}ayes variable selection and estimation in linear models","3","We propose an empirical Bayes method for variable selection and coefficient estimation in linear regression models. The method is based on a particular hierarchical Bayes formulation, and the empirical Bayes estimator is shown to be closely related to the LASSO estimator. Such a connection allows us to take advantage of the recently developed quick LASSO algorithm to compute the empirical Bayes estimate, and provides a new way to select the tuning parameter in the LASSO method. Unlike previous empirical Bayes variable selection methods, which in most practical situations can be implemented only through a greedy stepwise algorithm, our method gives a global solution efficiently. Simulations and real examples show that the proposed method is very competitive in terms of variable selection, estimation accuracy, and computation speed compared with other variable selection and estimation methods."
"10.1198/016214505000000088","2005","Combining linear regression models: when and how?","1","Model-combining (i.e., mixing) methods have been proposed in recent years to deal with uncertainty in model selection. Even though advantages of model combining over model selection have been demonstrated in simulations and data examples, it is still unclear to a large extent when model combining should be preferred. In this work, first we propose an instability measure to capture the uncertainty of model selection in estimation, called perturbation instability in estimation (PIE), based on perturbation of the sample. We demonstrate that estimators from model selection can have large PIE values and that model combining substantially reduces the instability for such cases. Second, we propose a model combining method, adaptive regression by mixing with model screening (ARMS), and derive a theoretical property. In ARMS, a screening step is taken to narrow down the list of candidate models before combining, which not only saves computing time, but also can improve estimation accuracy. Third, we compare ARMS with EBMA (an empirical Bayesian model averaging) and model selection methods in a number of simulations and real data examples. The comparison shows that model combining produces better estimators when the instability of model selection is high and that ARMS performs better than EBMA in most such cases in our simulations. With respect to the choice between model selection and model combining, we propose a rule of thumb in terms of PIE. The empirical results support that PIE is a sensible indicator of model selection instability in estimation and is useful for understanding whether model combining is a better choice over model selection for the data at hand."
"10.1198/016214505000000376","2005","Univariate nonparametric regression in the presence of auxiliary covariates","0","This article addresses the problem of finding a relationship between the univariate predictor and the response when regression errors, created in part by known auxiliary covariates, are too large for a reliable regression estimation. A typical example is a controlled random design experiment with a large number of covariates, where the statistician is interested in the effect of a particular covariate and this effect is blurred by a large regression noise created by other covariates. This article develops a theory of asymptotically optimal nonparametric univariate regression estimation in the presence of auxiliary covariates. Here optimality means mimicking the performance of an oracle that knows the effects of auxiliary covariates on the response. The asymptotic theory shows that such an optimal estimation is possible, and also explains how to evaluate the noise created by auxiliary covariates and how to develop an estimator for the interesting case of small sample sizes. The concept of modeling regression noise is well known in analysis of covariance (ANCOVA), and here it is applied in the optimal way to a nonparametric regression setting. A procedure for small sample sizes, denoised scattergram, is tested on simulated examples and a real dataset with 84 observations and 9 auxiliary covariates; the results justify the practical feasibility of the developed method. The method also allows a practitioner to visualize how a dataset would appear if the effects of auxiliary covariates were eliminated and to determine why an exhibited regression function has any given particular shape. Many practical recommendations (in particular, how to use known shape restrictions) are presented and discussed. The asymptotic theory, a numerical study, and analysis of a real dataset indicate that the proposed method of reducing the variance of regression errors created by auxiliary covariances is feasible, is easy to implement, and improves the likelihood of a meaningful regression analysis."
"10.1198/016214505000000277","2005","Robust estimation in generalized partial linear models for clustered data","6","In this article we consider robust generalized estimating equations for the analysis of semiparametric generalized partial linear models (GPLMs) for longitudinal data or clustered data in general. We approximate the nonparametric function in the GPLM by a regression spline, and use bounded scores and leverage-based weights in the estimating equation to achieve robustness against outliers. We show that the regression spline approach avoids some of the intricacies associated with the profile-kernel method, and that robust estimation and inference can be carried out operationally as if a generalized linear model were used."
"10.1198/016214505000000321","2005","Artificially augmented samples, shrinkage, and mean squared error reduction","0","An inequality is provided that determines when shrinkage reduces the mean squared error (MSE) of an unbiased estimate. Artificially augmented samples are then used to obtain, among others, shrinkage estimates of the population's variance and covariance, which improve the unbiased estimates for all parameter values and for all probability models with marginals having finite second moments, and alternative jackknife estimates that complement the usual jackknife estimates in reducing the MSE."
"10.1198/016214505000000204","2005","Maximization by parts in likelihood inference","1","This article presents and examines a new algorithm for solving a score equation for the maximum likelihood estimate in certain problems of practical interest. The method circumvents the need to compute second-order derivatives of the full likelihood function. It exploits the structure of certain models that yield a natural decomposition of a very complicated likelihood function. In this decomposition, the first part is a log-likelihood from a simply analyzed model, and the second part is used to update estimates from the first part. Convergence properties of this iterative (fixed-point) algorithm are examined, and asymptotics are derived for estimators obtained using only a finite number of iterations. Illustrative examples considered in the article include multivariate Gaussian copula models, nonnormal random-effects models, generalized linear mixed models, and state-space models. Properties of the algorithm and of estimators are evaluated in simulation studies on a bivariate copula model and a nonnormal linear random-effects model."
"10.1198/016214505000000565","2005","A hierarchical framework for modeling and forecasting web server workload","1","Proactive management of web server farms requires accurate prediction of workload. An exemplary measure of workload is the amount of service requests per unit time. As a time series, the workload exhibits not only short-term random fluctuations, but also prominent periodic (daily) patterns that evolve randomly from one period to another. A hierarchical framework with multiple time scales is proposed to model such time series. This framework leads to an adaptive procedure that provides both long-term (in days) and short-term (in minutes) predictions with simultaneous confidence bands that accommodate not only serial correlation, but also heavy tailedness, heteroscedasticity, and nonstationarity of the data."
"10.1198/016214505000000150","2005","The joint measurement of technical and allocative inefficiencies: an application of {B}ayesian inference in nonlinear random-effects models","0","This article estimates technical and allocative inefficiencies and increase in costs therefrom of individual firms using a translog cost system consisting of the cost function and the cost share equations. We call it a nonlinear random-effects system because technical and allocative inefficiencies are random (hence the term random effects) and are separated from the random noise terms appearing in each equation of the system, and because the inefficiency terms appear in the system in a highly nonlinear fashion, which helps in separating them from random errors. We use Bayesian inference procedures based on Markov chain Monte Carlo (MCMC) techniques to estimate the proposed system. Inferences on firm-specific technical inefficiency and both input-specific and firm-specific allocative inefficiencies are developed using MCMC techniques. We apply the new methods to a sample of 500 U.S. commercial banks. We focus on input allocation problem based on the assumption that banks minimize cost. Empirical results show that cost for the top (bottom) 10% of banks is increased by at least 3% (11%) due to technical inefficiency. In contrast, very few banks are found to be efficient in allocating all the inputs. Costs are increased by 13% on average due to input misallocation. Increase in costs due to both technical and allocative inefficiencies for the top (bottom) 10% of the banks is at least 11% (29%). When translated into dollar figures, this result indicates that elimination of technical and allocative inefficiencies would save the top (bottom) 10% of the banks more than $.20 ($3.56) million. We also find that none of the banks in our sample exceeded the efficient scale size, although most of them were operating near their optimum scale (unitary returns to scale)."
"10.1198/016214505000000123","2005","Frailty survival model analysis of the national deceased donor kidney transplant dataset using {P}oisson variance structures","0","In a recent study of transplant outcomes, donor age, cerebrovascular accident as the cause of death (CVA), renal insufficiency (serum creatinine > 1.5 mg/dL), and history of hypertension have been identified as donor factors associated with elevated risk of kidney transplant failure. It is of great interest to know whether there remain other unmeasured donor factors associated with elevated risk of graft failure. In this article we study a sample of 6,024 deceased donor kidney transplants performed in 194 centers from 1995 to 2000. In addition to variation among transplant recipients, there are two other random effects: unmeasured donor and unrecorded center factors (data not available at the physician level). These two random effects are crossed, because the two kidneys from the same donor can be transplanted in different centers. Multivariate frailty models are applied to analyze the data. The likelihood functions of both parametric (e.g., with piecewise constant baseline hazard) and semiparametric multivariate frailty models are shown to be proportional to the likelihood functions of a class of mixed Poisson regression models. The penalized quasi-likelihood method is used as the numerical procedure for these mixed Poisson regression models. Thus we are able to estimate and model crossed random-effects structures for survival analysis. Although about 30% of recipient graft survival rate variation due to donor factors is explained by the measured donor characteristics, the remaining variation among donors in graft survival rate is still statistically significant, suggesting that there may be other unmeasured donor factors associated with a reduced graft survival rate. We also find significant variation of graft failure rates among transplant centers due to unrecorded center factors. Therefore, this study suggests that practice patterns at transplant centers and identification of other donor factors may merit further investigation."
"10.1198/016214505000000114","2005","Hierarchical graphical models: an application to pulmonary function and cholesterol levels in the normative aging study","0","There is continued debate regarding the exact relation between lower cholesterol levels and increased respiratory disease mortality. One of the goals of this study is to reveal the relationship between subcomponents of cholesterol and pulmonary function. We consider the subcomponents of total cholesterol, namely high-density lipoprotein cholesterol and low-density lipoprotein cholesterol, to investigate the relationship of cholesterol levels with pulmonary function in a longitudinal study. To answer these questions, we propose new methodology for hierarchical reciprocal graphical models. We consider the identification and estimation of these models, and propose maximum likelihood estimation using a generalized EM algorithm. A simulation study of the algorithm and the corresponding estimates reveals excellent performance of the proposed procedures. Application of this methodology to the Normative Aging Study reveals complicated associations between pulmonary function and the subcomponents of total cholesterol."
"10.1198/016214505000000664","2005","Transdimensional {M}arkov chains: a decade of progress and future perspectives","3","The last 10 years have witnessed the development of sampling frameworks that permit the construction of Markov chains that simultaneously traverse both parameter and model space. Substantial methodological progress has been made during this period. In this article we present a survey of the current state of the art and evaluate some of the most recent advances in this field. We also discuss future research perspectives in the context of the drive to develop sampling mechanisms with high degrees of both efficiency and automation."
"10.1198/016214505000000501","2005","Residual diagnostics for growth mixture models: examining the impact of a preventive intervention on multiple trajectories of aggressive behavior","0","Growth mixture modeling has become a prominent tool for studying the heterogeneity of developmental trajectories within a population. In this article we develop graphical diagnostics to detect misspecification in growth mixture models regarding the number of growth classes, growth trajectory means, and covariance structures. For each model misspecification, we propose a different type of empirical Bayes residual to quantify the departure. Our procedure begins by imputing multiple independent sets of growth classes for the sample. Then, from these so-called ""pseudoclass"" draws, we form diagnostic plots to examine the averaged empirical distributions of residuals in each such class. Our proposals draw on the property that each single set of pseudoclass adjusted residuals is asymptotically normal with known mean and (co)variance when the underlying model is correct. These methods are justified in simulation studies involving two classes of linear growth curves that also differ by their covariance structures. These are then applied to longitudinal data from a randomized field trial that tests whether children's trajectories of aggressive behavior could be modified during elementary and middle school. Our diagnostics lead to a solution involving a mixture of three growth classes. When comparing the diagnostics obtained from multiple pseudoclasses with those from multiple imputations, we show the computational advantage of the former and obtain a criterion for determining the minimum number of pseudoclass draws."
"10.1198/016214505000000024","2005","Estimation in the mixture of {M}arkov chains moving with different speeds","0","This article considers a new mixture of time-homogeneous finite Markov chains where the mixing is on the rate of movement and develops the EM algorithm for maximum likelihood estimation of the parameters of the mixture. Continuous- and discrete-time versions of the mixture are defined, and their estimation is considered separately. The developed methods are illustrated with an application to modeling bond ratings migration. The class of mixture models proposed in this article provides a framework for modeling population heterogeneity with respect to the rate of movement. The proposed mixture subsumes the mover-stayer model, which has been widely used in applications."
"10.1198/016214504000002023","2005","Generalized radius processes for elliptically contoured distributions","2","The use of Mahalanobis distances has a long history in statistics. Given a sample of size n and general location and scatter estimators, m(n) and Sigma(n), we can define ""generalized"" radii as r(i)(n) = root(X-i-m(n))' Sigma(-1)(n) (X-i-m(n)). If we wish to trim observations based on the estimators m(n) and Sigma(n), then it is natural to first remove the most remote ones (i.e., those with the largest r(i)(n,)s). With this in mind, we define a process that maps the trimming proportion, alpha in [0, 1], to the generalized radius of the observation that has just been removed by this level of trimming. We analyze the asymptotic behavior of this process for elliptically contoured distributions. We show that the limit law depends only on the elliptical family considered and how Sigma(n) serves to estimate the underlying ""scale"" factor through its determinant. We carry out Monte Carlo simulations for finite sample sizes, and outline an application for assessing fit to a fixed elliptical family and also for the case where a proportion of outlying observations is discarded."
"10.1198/016214504000002078","2005","Bayesian nonparametric spatial modeling with {D}irichlet process mixing","13","Customary modeling for continuous point-referenced data assumes a Gaussian process that is often taken to be stationary. When such models are fitted within a Bayesian framework, the unknown parameters of the process are assumed to be random. so a random Gaussian process results. Here we propose a novel spatial Dirichlet process mixture model to produce a random spatial process that is neither Gaussian nor stationary. We first develop a spatial Dirichlet process model for spatial data and discuss its properties. Because of familiar limitations associated with direct use of Dirichlet process models, we introduce mixing by convolving this process with a pure error process. We then examine properties of models created through such Dirichlet process mixing. In the Bayesian framework, we implement posterior inference using Gibbs sampling. Spatial prediction raises interesting questions, but these can be handled. Finally, we illustrate the approach using simulated data, as well as a dataset involving precipitation measurements over the Languedoc-Roussillon region in southern France."
"10.1198/016214504000002069","2005","Limited- and full-information estimation and goodness-of-fit testing in {$2^n$} contingency tables: a unified framework","1","High-dimensional contingency tables tend to be sparse, and standard goodness-of-fit statistics such as X-2 cannot be used without pooling categories. As an improvement on arbitrary pooling, for goodness of fit of large 2(n) contingency tables, we propose classes of quadratic form statistics based on the residuals of margins or multivariate moments up to order r. These classes of test statistics are asymptotically chi-squared distributed under the null hypothesis. Further, the marginal residuals are useful for diagnosing lack of fit of parametric models. We show that when r is small (r = 2, 3), the proposed statistics have better small-sample properties and are asymptotically more powerful than X-2 for some useful multivariate binary models. Related to these test statistics is a class of limited-information estimators based on low-dimensional margins. We show that these estimators have high efficiency for one commonly used latent trait model for binary data."
"10.1198/016214504000001637","2005","Inference for a class of transformed hazards models","0","A new class of transformed hazard rate models is considered that contains both the multiplicative hazards model and the additive hazards model as special cases. The sieve maximum likelihood estimators are derived for the model parameters, and the estimators for the regression coefficients are shown to be consistent and asymptotically normal with variance achieving the semiparametric efficiency bound. Simulation studies are conducted to examine the small-sample properties of the proposed estimates, and a real dataset is used to illustrate our approach."
"10.1198/016214504000001574","2005","C{ATS}: clustering after transformation and smoothing","2","CATS-clustering after transformation and smoothing-is a technique for nonparametrically estimating and clustering a large number of curves. Our motivating example is a genetic microarray experiment, but the method is very general. The method includes transformation and smoothing multiple curves, multiple nonparametric testing for screening out flat curves, clustering curves with similar shape, and nonparametrically inferring the clustering estimation error rate."
"10.1198/016214505000000079","2005","Locally efficient semiparametric estimators for generalized skew-elliptical distributions","3","We consider a class of generalized skew-normal distributions that is useful for selection modeling and robustness analysis and derive a class of semiparametric estimators for the location and scale parameters of the central part of the model. We show that these estimators are consistent and asymptotically normal. We present the semiparametric efficiency bound and derive the locally efficient estimator that achieves this bound if the model for the skewing function is correctly specified. The estimators that we propose are consistent and asymptotically normal even if the model for the skewing function is misspecified, and we compute the loss of efficiency in such cases. We conduct a simulation study and provide an illustrative example. Our method is applicable to generalized skew-elliptical distributions."
"10.1198/016214504000002087","2005","Bootstrap standard error estimates for linear regression","1","Standard errors of parameter estimates are widely used in empirical work. The bootstrap often can provide a convenient means of estimating standard errors. The conditions under which bootstrap standard error estimates are theoretically justified have not received much attention, however. This article establishes conditions for the consistency of the moving blocks bootstrap estimators of the variance of the least squares estimator in linear dynamic models with dependent data. We discuss several applications of this result, in particular, the use of bootstrap standard error estimates for bootstrapping Studentized statistics. A simulation study shows that inference based on bootstrap standard error estimates may be considerably more accurate in small samples than inference based on closed-form asymptotic estimates."
"10.1198/016214504000001772","2005","The profile sampler","5","We consider frequentist inference for the parametric component 0 separately from the nuisance parameter eta in semiparametric models based on sampling from the posterior of the profile likelihood. We prove that this procedure gives a first-order-correct approximation to the maximum likelihood estimator 0,, and consistent estimation of the efficient Fisher information for 0, without computing derivatives or using complicated numerical approximations. An exact Bayesian interpretation is established under a certain data-dependent prior. The sampler is useful in particular when the nuisance parameter is not estimable at the root n rate, where neither bootstrap validity nor general automatic variance estimation has been theoretically justified. Even when the nuisance parameter is root n consistent and the bootstrap is known to be valid, the proposed Markov chain Monte Carlo procedure can yield computational savings, because maximization of the likelihood is not required. The theory is verified for three examples. The methods are shown to perform well in simulations, and their practical utility is illustrated in two data analyses."
"10.1198/016214504000002005","2005","A penalized nonparametric maximum likelihood approach to species richness estimation","1","We propose a class of penalized nonparametric maximum likelihood estimators (NPMLEs) for the species richness problem. We use a penalty term on the likelihood because likelihood estimators that lack it have an extreme instability problem. The estimators are constructed using a conditional likelihood that is simpler than the full likelihood. We show that the full-likelihood NPMLE solution given by Norris and Pollock can be found (with great accuracy) by using an appropriate penalty term on the conditional likelihood, so it is an element of our class of estimators. A simple and fast algorithm for the penalized NPMLE is developed; it can be used to greatly speed up computation of the unconditional NPMLE. It can also be used to find profile mixture likelihoods. Based on our goal of attaining high stability while retaining sensitivity, we propose an adaptive quadratic penalty function. A systematic simulation study, using a wide range of scenarios, establishes the success of this method relative to its competitors. Finally, we discuss an application in the gene number estimation using expressed sequence tag (EST) data from genomics."
"10.1198/016214505000000042","2005","A unified nonparametric approach for unbalanced factorial designs","0","Motivated by questions arising from the field of statistical genetics, we consider the problem of testing main, nested, and interaction effects in unbalanced factorial designs. Based on the concept of composite linear rank statistics, a new notion of weighted rank is proposed. Asymptotic normality of weighted linear rank statistics is established under mild conditions, and consistent estimators are developed for the corresponding limiting covariance structure. A unified framework to use weighted rank to construct test statistics for main, nested, and interaction effects in unbalanced factorial designs is established. The proposed tests statistics are applicable to unbalanced designs with arbitrary cell replicates greater than one per cell. The limiting distributions under both the null hypotheses and Pitman alternatives are derived. Monte Carlo simulations are conducted to confirm the validity and power of the proposed tests. Genetic datasets from a simulated backcross study are analyzed to demonstrate the application of the proposed tests in quantitative trait loci mapping."
"10.1198/016214505000000097","2005","Multivariate nonparametric tests of independence","1","New test statistics are proposed for testing whether two random vectors are independent. Gieser and Randles, as well as Taskinen, Kankainen, and Oja have introduced and discussed multivariate extensions of the quadrant test of Blomqvist. This article serves as a sequel to this work and presents new multivariate extensions of Kendall's tau and Spearman's rho statistics. Two different approaches are discussed. First, interdirection proportions are used to estimate the cosines of angles between centered observation vectors and between differences of observation vectors. Second, covariances between affine-equivariant multivariate signs and ranks are used. The test statistics arising from these two approaches appear to be asymptotically equivalent if each vector is elliptically symmetric. The spatial sign versions are easy to compute for data in common dimensions, and they provide practical, robust alternatives to normal-theory methods. Asymptotic theory is developed to approximate the finite-sample null distributions as well, as to calculate limiting Pitman efficiencies. Small-sample null permutation distributions are also described. A simple simulation study is used to compare the proposed tests with the classical Wilks test. Finally, the theory is illustrated by an example."
"10.1198/016214504000001583","2005","Rank-sum tests for clustered data","2","The Wilcoxon rank-sum test is widely used to test the equality of two populations, because it makes fewer distributional assumptions than parametric procedures such as the t-test. However, the Wilcoxon rank-sum test can be used only if data are independent. When data are clustered, tests based on generalized estimating equations (GEEs) that generalize the t-test have been proposed. Here we develop a rank-sum test that can be used when data are clustered. As an application, we use our rank-sum test to develop a nonparametric test of association between a genetic marker and a quantitative trait locus. We also give a rank-sum test for equivalence of three or more populations that generalizes the Kruskal-Wallis test to situations with clustered data. Unlike previous rank tests for clustered data, our proposal is valid when members of the same cluster belong to different groups, or when the correlation between cluster members differs across groups."
"10.1198/016214504000001439","2005","Nonparametric inferences for additive models","8","Additive models with backfitting algorithms are popular multivariate nonparametric fitting techniques. However, the inferences of the models have not been very well developed, due partially to the complexity of the backfitting estimators. There are few tools available to answer some important and frequently asked questions, such as whether a specific additive component is significant or admits a certain parametric form. In an attempt to address these issues, we extend the generalized likelihood ratio (GLR) tests to additive models, using the backfitting estimator. We demonstrate that under the null models, the newly proposed GLR statistics follow asymptotically rescaled chi-squared distributions. with the scaling constants and the degrees of freedom independent of the nuisance parameters. This demonstrates that the Wilks phenomenon continues to hold under a variety of smoothing techniques and more relaxed models with unspecified error distributions. We further prove that the GLR tests are asymptotically optimal in terms of rates of convergence for nonparametric hypothesis testing. In addition, for testing a parametric additive model, we propose a bias corrected method to improve the performance of the GLR. The bias-corrected test is shown to share the Wilks type of property. Simulations are conducted to demonstrate the Wilks phenomenon and the power of the proposed tests. A real example is used to illustrate the performance of the testing approach."
"10.1198/016214505000000060","2005","Semiparametric regression analysis of longitudinal data with informative observation times","1","Statistical analysis of longitudinal data has been discussed by many authors, and a number of methods have been proposed. Most of the research have focused on situations where observation times are independent of or carry no information about the response variable and therefore rely on conditional inference procedures given the observation times. This article considers a different situation, where the independence assumption may not hold; that is, the observation times may carry information about the response variable. For inference, estimating equation approaches are proposed, and both large-sample and final-sample properties of the proposed methods are established. The methodology is applied to a bladder cancer study that motivated this investigation."
"10.1198/016214504000001989","2005","Dynamical correlation for multivariate longitudinal data","1","Nonparametric methodology for longitudinal data analysis is becoming increasingly popular. The analysis of multivariate longitudinal data, where data on several time courses are recorded for each subject, has received considerably less attention, despite its importance for practical data analysis. In particular, there is a need for measures and estimates to capture dependency between the components of vector-valued longitudinal data. We propose and analyze a simple and effective nonparametric method to quantify the covariation of components of multivariate longitudinal observations, which are viewed as realizations of a random process. This includes the notion of a correlation between derivatives and time-shifted versions. The concept of dynamical correlation is based on a scalar product obtained from pairs of standardized smoothed curves. The proposed method can be used when observation times are irregular and not matching between subjects or between responses within a subject. For higher-dimensional data, one may construct a dynamical correlation matrix that then serves as a starting point for standard multivariate analysis techniques, such as principal components. We iliustrate our methods via simulations as well as with data on five acute-phase blood proteins measured longitudinally from a study of hemodialysis patients."
"10.1198/016214504000002096","2005","Estimation of long memory in the presence of a smooth nonparametric trend","0","We consider semiparametric estimation of the long-memory parameter of a stationary process in the presence of an additive nonparametric mean function. We use a semiparametric Whittle-type estimator, applied to the tapered, differenced series. Because the mean function is not necessarily a polynomial of finite order, no amount of differencing will completely remove the mean. We establish a central limit theorem for the estimator of the memory parameter, assuming that a slowly increasing number of low frequencies are trimmed from the estimator's objective function. We find in simulations that tapering and trimming, applied either separately or together, are essential for the good performance of the estimator in practice. In our simulation study, we also compare the proposed estimator of the long-memory parameter with a direct estimator obtained from the raw data without differencing or tapering, and finally we study the question of feasible inference for the regression function. We find that the proposed estimator of the long-memory parameter is potentially far less biased than the direct estimator, and consequently that the proposed estimator may lead to more accurate inference on the regression function."
"10.1198/016214504000001871","2005","Measurement error in linear autoregressive models","0","Time series data are often subject to measurement error, usually the result of needing to estimate the variable of interest. Although it is often reasonable to assume that the measurement error is additive (i.e., the estimator is conditionally unbiased for the missing true value), the measurement error variances often vary as a result of changes in the population/process over time and/or changes in sampling effort. In this article we address estimation of the parameters in linear autoregressive models in the presence of additive and uncorrelated measurement errors, allowing heteroscedasticity in the measurement error variances. We establish the asymptotic properties of naive estimators that ignore measurement error and propose an estimator based on correcting the Yule-Walker estimating equations. We also examine a pseudo-likelihood method based on normality assumptions and computed using the Kalman filter. We review other techniques that have been proposed, including two that require no information about the measurement error variances, and compare the various estimators both theoretically and via simulations. The estimator based on corrected estimating equations is easy to obtain and readily accommodates (and is robust to) unequal measurement error variances. Asymptotic calculations and finite-sample simulations show that it is often relatively efficient."
"10.1198/016214504000002050","2005","The generalized dynamic factor model: one-sided estimation and forecasting","4","This article proposes a new forecasting method that makes use of information from a large panel of time series. Like earlier methods, our method is based on a dynamic factor model. We argue that our method improves on a standard principal component predictor in that it fully exploits all the dynamic covariance structure of the panel and also weights the variables according to their estimated signal-to-noise ratio. We provide asymptotic results for our optimal forecast estimator and show that in finite samples, our forecast outperforms the standard principal components predictor."
"10.1198/016214504000002032","2005","A two-way semilinear model for normalization and analysis of c{DNA} microarray data","9","A basic question in analyzing cDNA microarray data is normalization, the purpose of which is to remove systematic bias in the observed expression values by establishing a normalization curve across the whole dynamic range. A proper normalization procedure ensures that the normalized intensity ratios provide meaningful measures of relative expression levels. We propose a two-way semilinear model (TW-SLM) for normalization and analysis of microarray data. This method does not make the usual assumptions underlying some of the existing methods. For example, it does not assume that the percentage of differentially expressed genes is small or that there is symmetry in the expression levels of up-regulated and down-regulated genes, as required in the lowess normalization method. The TW-SLM also naturally incorporates uncertainty due to normalization into significance analysis of microarrays. We use a semiparametric approach based on polynomial splines in the TW-SLM to estimate the normalization curves and the normalized expression values. We study the theoretical properties of the proposed estimator in the TW-SLM, including the finite-sample distributional properties of the estimated gene effects and the rate of convergence of the estimated normalization curves when the number of genes under study is large. We also conduct simulation studies to evaluate the TW-SLM method and illustrate the proposed method using a published microarray dataset."
"10.1198/016214504000001781","2005","Semilinear high-dimensional model for normalization of microarray data: a theoretical analysis and partial consistency","11","Normalization of microarray data is essential for removing experimental biases and revealing meaningful biological results. Motivated by a problem of normalizing microarray data, a semilinear in-slide model (SLIM) has been proposed. To aggregate information from other arrays. SLIM is generalized to account for across-array information, resulting in an even more dynamic semiparametric regression model. This model can be used to normalize microarray data even when there is no replication within an array. We demonstrate that this semiparametric model has a number of interesting features. The parametric component and the nonparametric component that are of primary interest can be consistently estimated, the former having a parametric rate and the latter having a nonparametric rate, whereas the nuisance parameters cannot be consistently estimated. This is an interesting extension of the partial consistent phenomena, which itself is of theoretical interest. The asymptotic normality for the parametric component and the rate of convergence for the nonparametric component are established. The results are augmented by simulation studies and illustrated by an application to the cDNA microarray analysis of neuroblastoma cells in response to the macrophage migration inhibitory factor."
"10.1198/016214505000000051","2005","Spike and slab gene selection for multigroup microarray data","6","DNA microarrays can provide insight into genetic changes that characterize different stages of a disease process. Accurate identification of these changes has significant therapeutic and diagnostic implications. Statistical analysis for multistage (multigroup) data is challenging, however. ANOVA-based extensions of two-sample Z-tests, a popular method for detecting differentially expressed genes in two groups, do not work well in multigroup settings. False detection rates are high because of variability of the ordinary least squares estimators and because of regression to the mean induced by correlated parameter estimates. We develop a Bayesian resealed spike and slab hierarchical model specifically designed for the multigroup gene detection problem. Data preprocessing steps are introduced to deal with unique features of microarray data and to enhance selection performance. We show theoretically that spike and slab models naturally encourage sparse solutions through a process called selective shrinkage. This translates into oracle-like gene selection risk performance compared with ordinary least squares estimates. The methodology is illustrated on a large microarray repository of samples from different clinical stages of metastatic colon cancer. Through a functional analysis of selected genes, we show that spike and slab models identify important biological signals while minimizing biologically implausible false detections."
"10.1198/016214505000000105","2005","Statistical method for eliciting probability distributions","3","Elicitation is a key task for subjectivist Bayesians. Although skeptics hold that elicitation cannot (or perhaps should not) be done, in practice it brings statisticians closer to their clients and subject-matter expert colleagues. This article reviews the state of the art, reflecting the experience of statisticians informed by the fruits of a long line of psychological research into how people represent uncertain information cognitively and how they respond to questions about that information. In a discussion of the elicitation process, the first issue to address is what it means for an elicitation to be successful; that is, what criteria should be used. Our answer is that a successful elicitation faithfully represents the opinion of the person being elicited. It is not necessarily ""true"" in some objectivistic sense, and cannot be judged in that way. We see that elicitation as simply part of the process of statistical modeling. Indeed, in a hierarchical model at which point the likelihood ends and the prior begins is ambiguous. Thus the same kinds of judgment that inform statistical modeling in general also inform elicitation of prior distributions. The psychological literature suggests that people are prone to certain heuristics and biases in how they respond to situations involving uncertainty. As a result, some of the ways of asking questions about uncertain quantities are preferable to others, and appear to be more reliable. However, data are lacking on exactly how well the various methods work, because it is unclear, other than by asking using an elicitation method, just what the person believes. Consequently, one is reduced to indirect means of assessing elicitation methods. The tool chest of methods is growing. Historically, the first methods involved choosing hyperparameters using conjugate prior families, at a time when these were the only families for which posterior distributions could be computed. Modern computational methods, such as Markov chain Monte Carlo, have freed elicitation from this constraint. As a result, now both parametric and nonparametric methods are available for low-dimensional problems. High-dimensional problems are probably best thought of as lacking another hierarchical level, which has the effect of reducing the as-yet-unelicited parameter space. Special considerations apply to the elicitation of group opinions. Informal methods, such as Delphi, encourage the participants to discuss the issue in the hope of reaching consensus. Formal methods, such as weighted averages or logarithmic opinion pools, each have mathematical characteristics that are uncomfortable. Finally, there is the question of what a group opinion even means, because it is not necessarily the opinion of any participant."
"10.1198/016214505000000196","2005","Population-calibrated gene characterization: estimating age at onset distributions associated with cancer genes","0","Phenotypic characterization of rare disease genes poses a significant statistical challenge, but the need to do so is clear. Clinical management of patients carrying a disease gene depends crucially on an accurate characterization of the genetically predisposed disease, including its likelihood of occurrence among mutation carriers, natural history, and response to treatment. We propose a formal yet practical method for controlling for bias due to ignoring ascertainment, defined as the sampling mechanism, when quantifying the association between genotype and disease using data on high-risk families. The approach is more statistically efficient than conditioning on the variables used in sampling. In it, the likelihood is adjusted by a factor that is a function of sampling weights in strata defined by those variables. It requires that these variables and the sampling probabilities in the strata they define either are known or can be estimated. The latter requires a second.. population-based dataset. As an example, we derive ascertainment-corrected estimates of penetrance for the breast cancer susceptibility genes BRCA1 and BRCA2. The Bayesian analysis that we use incorporates a modified segregation model and prior data on penetrance derived from the literature. Markov chain Monte Carlo methods are used for inference."
"10.1198/016214505000000015","2005","Estimating size and composition of biological communities by modeling the occurrence of species","0","We develop a model that uses repeated observations of a biological community to estimate the number and composition of species in the community. Estimators of community-level attributes are constructed from model-based estimators of occurrence of individual species that incorporate imperfect detection of individuals. Data from the North American Breeding Bird Survey are analyzed to illustrate the variety of ecologically important quantities that are easily constructed and estimated using our model-based estimators of species occurrence. In particular, we compute site-specific estimates of species richness that honor classical notions of species-area relationships. We suggest extensions of our model to estimate maps of occurrence of individual species and to compute inferences related to the temporal and spatial dynamics of biological communities."
"10.1198/016214504000001826","2005","A {B}ayesian approach to 2000 {C}ensus evaluation using {ACE} survey data and demographic analysis","1","Demographic analysis of data on births, deaths, and migration, together with coverage measurement surveys that use capture-recapture methods, have established that U.S. Census counts are flawed for certain subpopulations. Previous work using 1990 Census data in African-Americans age 30-49 proposed a hierarchical Bayesian model that assembled Census, follow-up survey, and demographic data, providing a principled solution to the problem of negative estimated counts in some subpopulations, smoothing highly variable estimates across subpopulations, and providing estimates of precision that incorporate uncertainty in the demographic analysis estimates. This article extends that effort by refining the hierarchical model design, expanding the set of models considered, considering the presence of bias in the Census or follow-up survey counts, obtaining Bayes factors for use in model selection, and applying the methods to the entire 2000 U.S. Census. Comparisons with the 1990 U.S. Census results are included as well."
"10.1198/016214504000001754","2005","Contact surface models for infectious diseases: estimation from serologic survey data","0","Controlling of infectious diseases requires information about the rates at which individuals make contact. We propose a novel approach to modeling contact rates via a continuous contact surface. This provides a more realistic and flexible representation of contact rates than currently used methods. Our approach allows modeling of sources of heterogeneity due to age, individual effects, and gender. The models are fitted to serologic survey data by maximum likelihood. This involves solving an integral equation linking the contact surface to the infection hazards. The method is illustrated with two datasets, on mumps and rubella and on Epstein-Barr virus and herpes simplex virus type I infection. The advantages and shortcomings of the method, particularly the identifiability of the contact surface, are discussed."
"10.1198/016214504000001592","2005","Hidden {M}arkov models for longitudinal comparisons","2","Medical researchers interested in temporal, multivariate measurements of complex diseases have recently begun developing health state models, which divide the space of patient characteristics into medically distinct clusters. The current state of the art in health services research uses k-means clustering to form the health states and a first-order Markov chain to describe transitions between the states. This fitting procedure ignores information from temporally adjacent observations and prevents uncertainty from parameter estimation and cluster assignments from being incorporated into the analysis. A natural way to address these issues is to combine clustering and longitudinal analyses using a hidden Markov model. We fit hidden Markov models to longitudinal data using Bayesian methods that account for all of the uncertainty in the parameters, conditional only on the underlying correctness of the model. Potential lack of time homogeneity in the Markov chain is accounted for by embedding transition probabilities into a hierarchical model that provides Bayesian shrinkage across time. We illustrate this approach by developing a hidden Markov health state model for comparing the effectiveness of clozapine and haloperidol, two antipsychotic medications for schizophrenia. We find that clozapine outperforms haloperidol and identify the types of patients in whom clozapine's advantage is greatest and weakest. Finally, we discuss the advantages and disadvantages of hidden Markov models in comparison with the current methodology."
"10.1198/016214504000001844","2005","Missing-data methods for generalized linear models: a comparative review","7","Missing data is a major issue in many applied problems, especially in the biomedical sciences. We review four common approaches for inference in generalized linear models (GLMs) with missing covariate data: maximum likelihood (ML), multiple imputation (MI), fully Bayesian (FB), and weighted estimating equations (WEEs). There is considerable interest in how these four methodologies are related, the properties of each approach, the advantages and disadvantages of each methodology, and computational implementation. We examine data that are missing at random and nonignorable missing. For ML we focus on techniques using the EM algorithm, and in particular, discuss the EM by the method of weights and related procedures as discussed by Ibrahim. For MI, we examine the techniques developed by Rubin. For FB, we review approaches considered by Ibrahim et al. For WEE, we focus on the techniques developed by Robins et al. We use a real dataset and a detailed simulation study to compare the four methods."
"10.1198/016214504000001880","2005","Causal inference using potential outcomes: design, modeling, decisions","6","Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism-a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials."
"10.1198/016214504000001899","2005","Modeling reporting delays and reporting corrections in cancer registry data","0","The Surveillance, Epidemiology, and End Results (SEER) program of the National Cancer Institute is an authoritative source of cancer incidence statistics in the United States. The SEER program is a consortium of population-based cancer registries from different areas of the country. Each registry is charged with collecting data on all cancers that occur within its geographic area. As with any disease registry, there is a delay between the time that the disease (cancer) is first diagnosed and the time that it is reported to the registry. The SEER program has allowed for reporting delays of up to 19-months before releasing data for public use. Nevertheless, additional cases are discovered after the 19-month delay, and these cases are added in subsequent releases of the data. Further, any errors discovered are corrected in subsequent releases. Such reporting delays and corrections typically lead to underestimation of the cancer incidence rates in recent diagnosis years, making it difficult to monitor trends. In this article we study models that account for reporting delays and corrections in predicting eventual cancer counts for a diagnosis year from the preliminary counts. Previous models of this type have been studied, especially as applied to AIDS registries. We offer several additions to existing models. First, we explicitly model the reporting corrections. Second, we model the delay distribution with very general models, combining aspects of previous nonparametric-like models (i.e., models that have a separate parameter for each delay time) with more parametric models. Third, we allow random reporting-year effects in the model. Practical issues of model selection and how the data are classified are also discussed, particularly how the definition of a reporting correction may change depending on how subpopulations are defined. An example with SEER melanoma data is studied in detail."
"10.1198/016214504000001817","2005","Classification of missense mutations of disease genes","0","Clinical management of individuals found to harbor a mutation at a known disease-susceptibility gene depends on accurate assessment of mutation-specific disease risk. For missense mutations (MMs)-mutations that lead to a single amino acid change in the protein coded by the gene-this poses a particularly challenging problem. Because it is not possible to predict the structural and functional changes to the protein product for a given amino acid substitution, and because functional assays are often not available, disease association must be inferred from data on individuals with the mutation. Inference is complicated by small sample sizes and by sampling mechanisms that bias toward individuals at high familial risk of disease. We propose a Bayesian hierarchical model to classify the disease association of MMs given pedigree data collected in the high-risk setting. The model's structure allows simultaneous characterization of multiple MMs. It uses a group of pedigrees identified through probands tested positive for known disease associated mutations and a group of test-negative pedigrees, both obtained from the same clinic, to calibrate classification and control for potential ascertainment bias. We apply this model to study MMs of breast-ovarian susceptibility genes BRCA1 and BRCA2, using data collected at the Duke University Medical Center in Durham, North Carolina."
"10.1198/016214504000001808","2005","Statistical analysis of a telephone call center: a queueing-science perspective","5","A call center is a service network in which agents provide telephone-based services. Customers who seek these services are delayed in tele-queues. This article summarizes an analysis of a unique record of call center operations. The data comprise a complete operational history of a small banking call center, call by call, over a full year. Taking the perspective of queueing theory, we decompose the service process into three fundamental components: arrivals, customer patience, and service durations. Each component involves different basic mathematical structures and requires a different style of statistical analysis. Some of the key empirical results are sketched, along with descriptions of the varied techniques required. Several statistical techniques are developed for analysis of the basic components. One of these techniques is a test that a point process is a Poisson process. Another involves estimation of the mean function in a nonparametric regression with lognormal errors. A new graphical technique is introduced for nonparametric hazard rate estimation with censored data. Models are developed and implemented for forecasting of Poisson arrival rates. Finally, the article surveys how the characteristics deduced from the statistical analyses form the building blocks for theoretically interesting and practically useful mathematical models for call center operations."
"10.1198/016214504000001763","2005","A space-time conditional intensity model for evaluating a wildfire hazard index","0","Numerical indices are commonly used as tools to aid wildfire management and hazard assessment. Although the use of such indices is widespread, assessment of these indices in their respective regions of application is rare. We evaluate the effectiveness of the burning index (BI) for predicting wildfire occurrences in Los Angeles County, California using space-time point-process models. These models are based on an additive decomposition of the conditional intensity, with separate terms used to describe spatial and seasonal variability as well as contributions from the BI. We fit the models to wildfire and BI data from the years 1976-2000 using a combination of nonparametric kernel-smoothing methods and parametric maximum likelihood. In addition to using the Akaike information criterion (AIC) to compare competing models, we use new multidimensional residual methods based on approximate random thinning and resealing to detect departures from the models and to ascertain the precise contribution of the BI to predicting wildfire occurrence. We find that although the BI appears to have a positive impact on wildfire prediction, the contribution is relatively small after taking into account natural seasonal and spatial variation. In particular, the BI does not appear to take into account increased activity during the years 1979-1981 and can overpredict during the early months of the year."
"10.1198/016214504000002104","2005","Are maintenance practices for railroad tracks effective?","0","The Association of American Railroads wished to determine the effect of a maintenance practice known as grinding on the occurrence of rail fatigue defects and on the subsequent total traffic usage before a track must be replaced. Because a designed experiment was not practical, an analysis of historical data from the Canadian Northern Railroad is presented. In the analysis, certain covariate data are available, specifically the amount of grinding and some physical characteristics of the rail; other important covariate data are not available, however. A model for the number of defects as a function of traffic usage is developed based on a modulated Poisson point process. The model incorporates the effect of the available covariates and a mixture of Dirichlet processes set-up for the scale parameters of the individual rail sections that allows an assessment of the overall effect of the unavailable covariates. The model is then used to determine an optimal replacement period for a whole rail track. The analysis demonstrates that grinding reduces the expected number of defects and increases the optimal replacement interval."
"10.1198/016214504000001051","2005","Weather forecasting for weather derivatives","3","We take a simple time series approach to modeling and forecasting daily average temperature in U.S. cities, and we inquire systematically weather derivatives market. The answer is, perhaps supris- ingly to whether it may prove useful from the vantage point of participants in the ingly, yes. Time series modeling reveals conditional mean dynamics and, crucially, strong conditional variance dynamics in daily average temperature, and it reveals sharp differences between the distribution of temperature and the distribution of temperature surprises. As we argue, it also holds promise for producing the long-horizon predictive densities crucial for pricing weather derivatives, so that additional inquiry into time series weather forecasting methods will likely prove useful in weather derivatives contexts."
"10.1198/016214505000000033","2005","Bayesians, frequentists, and scientists","1","Broadly speaking, nineteenth century statistics was Bayesian, while the twentieth century was frequentist, at least from the point of view of most scientific practitioners. Here in the twenty-first century scientists are bringing statisticians much bigger problems to solve, often comprising millions of data points and thousands of parameters. Which statistical philosophy will dominate practice? My guess, backed up with some recent examples, is that a combination of Bayesian and frequentist ideas will be needed to deal with our increasingly intense scientific environment. This will be a challenging period for statisticians, both applied and theoretical, but it also opens the opportunity for a new golden age, rivaling that of Fisher, Neyman, and the other giants of the early 1900s. What follows is the text of the 164th ASA presidential address, delivered at the awards ceremony in Toronto on August 10. 2004."
"10.1198/016214504000001547","2005","Estimation of {P}oisson intensity in the presence of dead time","0","Phase Doppler interferometry (PDI) is a nonintrusive technique frequently used to obtain information about spray characteristics. Understanding spray characteristics is of critical importance in many areas of science, including liquid fuel spray combustion, spray coatings, fire suppression, and pesticides. PDI measures the size and velocity of individual droplets in a spray. Due to the design of the instrument, recordings of the PDI contain gaps, called dead times. The presence of recurring dead times greatly complicates estimation of the diffusion rate of the droplets. Modeling the spray process as a homogeneous Poisson process, we construct consistent and asymptotic normal estimators of the diffusion rate (Poisson intensity) under various conditions. Simulation produced a good agreement between our estimators (in the presence of dead time) and the maximum likelihood estimates obtained without dead time. We use experimental data to illustrate the estimation method."
"10.1198/016214504000002014","2005","Analyzing nonstationary spatial data using piecewise {G}aussian processes","1","In many problems in geostatistics the response variable of interest is strongly related to the underlying geology of the spatial location. In these situations there is often little correlation in the responses found in different rock strata, so the underlying covariance structure shows sharp changes at the boundaries of the rock types. Conventional stationary and nonstationary spatial methods are inappropriate, because they typically assume that the covariance between points is a smooth function of distance. In this article we propose a generic method for the analysis of spatial data with sharp changes in the underlying covariance structure. Our method works by automatically decomposing the spatial domain into disjoint regions within which the process is assumed to be stationary, but the data are assumed independent across regions. Uncertainty in the number of disjoint regions, their shapes, and the model within regions is dealt with in a fully Bayesian fashion. We illustrate our approach on a previously unpublished dataset relating to soil permeability of the Schneider Buda oil field in Wood County, Texas."
"10.1198/016214504000001466","2005","Outlier labeling with boxplot procedures","0","In this article we focus on the detection of possible outliers based on the widely used boxplot procedures. The outliers in a set of data are defined to be a subset of observations that appear to be inconsistent with the remaining observations. We identify the outliers by constructing a boxplot with its lower fence (LF) and upper fence (UF) either (a) satisfying the requirement that if the given sample is outlier-free, then the probability that one or more of the sample data would fall outside the region (LF, UF) is equal to a prescribed small value alpha, or (b) taken to be the tolerance limits, derived from an outlier-free random sample, within which a specified large proportion of the sampled population would be asserted to fall with a given large probability gamma. Exact expressions that can be routinely used to evaluate the constants needed in the construction of the boxplot's outlier region for samples taken from the family of location-scale distributions are obtained for both procedures. This article shows that the commonly constructed boxplot is in general inappropriate for detecting outliers in the normal and especially the exponential samples. We recommend that the graphical boxplot be constructed based on the knowledge of the underlying distribution of the dataset and by controling the risk of labeling regular observations as outliers."
"10.1198/016214504000001402","2005","Optimal conditionally unbiased bounded-influence inference in dynamic location and scale models","2","This article studies the local robustness of estimators and tests for the conditional location and scale parameters in a strictly stationary time series model. We first derive optimal bounded-influence estimators for such settings under a conditionally Gaussian reference model. Based on these results, we obtain optimal bounded-influence versions of the classical likelihood-based tests for parametric hypotheses. We propose a feasible and efficient algorithm for the computation of our robust estimators, which uses analytical Laplace approximations to estimate the auxiliary recentering vectors, ensuring Fisher consistency in robust estimation. This strongly reduces the computation time by avoiding the simulation of multidimensional integrals, a task that typically must be addressed in the robust estimation of nonlinear models for time series. In some Monte Carlo simulations of an AR(1)-ARCH(1) process, we show that our robust procedures maintain a very high efficiency under ideal model conditions and at the same time perform very satisfactorily under several forms of departure from conditional normality. In contrast, classical pseudo-maximum likelihood inference procedures are found to be highly inefficient under such local model misspecifications. These patterns are confirmed by an application to robust testing for autoregressive conditional heteroscedasticity."
"10.1198/016214504000001457","2005","Bayesian semiparametric isotonic regression for count data","3","This article proposes a semiparametric Bayesian approach for inference on an unknown isotonic regression function, f(x), characterizing the relationship between a continuous predictor, X, and a count response variable, Y, adjusting for covariates, Z. A Dirichlet process mixture of Poisson distributions is used to avoid parametric assumptions on the conditional distribution of Y given X and Z. Then, to also avoid parametric assumptions on f(x), a novel prior formulation is proposed that enforces the nondecreasing constraint and assigns positive prior probability to the null hypothesis of no association. Through the use of carefully tailored hyperprior distributions, we allow for borrowing of information across different regions of X in estimating f(x) and in assessing hypotheses about local increases in the function. Due to conjugacy properties, posterior computation is straightforward using a Markov chain Monte Carlo algorithm. The methods are illustrated using data from an epidemiologic study of sleep problems and obesity."
"10.1198/016214504000001565","2005","Bayesian variable selection in clustering high-dimensional data","9","Over the last decade, technological advances have generated an explosion of data with substantially smaller sample size relative to the number of covariates (p >> n). A common goal in the analysis of such data involves uncovering the group structure of the observations and identifying the discriminating variables. In this article we propose a methodology for addressing these problems simultaneously. Given a set of variables, we formulate the clustering problem in terms of a multivariate normal mixture model with an unknown number of components and use the reversible-jump Markov chain Monte Carlo technique to define a sampler that moves between different dimensional spaces. We handle the problem of selecting a few predictors among the prohibitively vast number of variable subsets by introducing a binary exclusion/inclusion latent vector, which gets updated via stochastic search techniques. We specify conjugate priors and exploit the conjugacy by integrating out some of the parameters. We describe strategies for posterior inference and explore the performance of the methodology with simulated and real datasets."
"10.1198/016214504000001411","2005","Semiparametric {B}ayesian analysis of matched case-control studies with missing exposure","0","This article considers Bayesian analysis of matched case-control problems when one of the covariates is partially missing. Within the likelihood context, the standard approach to this problem is to posit a fully parametric model among the controls for the partially missing covariate as a function of the covariates in the model and the variables making up the strata. Sometimes the strata effects are ignored at this stage. Our approach differs not only in that it is Bayesian, but, far more importantly, in the manner in which it treats the strata effects. We assume a Dirichlet process prior with a normal base measure for the stratum effects and estimate all of the parameters in a Bayesian framework. Three matched case-controt examples and a simulation study are considered to illustrate our methods and the computing scheme."
"10.1198/016214504000001745","2005","Functional data analysis for sparse longitudinal data","39","We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the covariance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement. A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle."
"10.1198/016214504000001556","2005","Functional adaptive model estimation","7","In this article we are interested in modeling the relationship between a scalar, Y, and a functional predictor, X(t). We introduce a highly flexible approach called functional adaptive model estimation (FAME), which extends generalized linear models (GLMs), generalized additive models (GAMs), and projection pursuit regression (PPR) to handle functional predictors. The FAME approach can model any of the standard exponential family of response distributions that are assumed for GLM or GAM while maintaining the flexibility of PPR. For example, standard linear or logistic regression with functional predictors, as well as far more complicated models, can easily be applied using this approach. We use a functional principal components decomposition of the predictor functions to aid visualization of the relationship between X(t) and Y. We also show how the FAME procedure can be extended to deal with multiple functional and standard finite-dimensional predictors, possibly with missing data. We illustrate the FAME approach on simulated data, as well as on the prediction of arthritis based on bone shape. We end with a discussion of the relationships between standard regression approaches. their extensions to functional data, and FAME."
"10.1198/016214504000001970","2005","Constructing stationary time series models using auxiliary variables with applications","0","Here we present a novel method for modeling stationary time series. Our approach is to construct the model with a specified marginal family and build the dependence structure around it. We show that the resulting time series is linear with a simple autocorrelation structure. We construct models that parallel existing structures, namely state-space models, autoregressive conditional heteroscedasticity (ARCH) models, and generalized ARCH models. We use Bayesian techniques to estimate the resulting models. We also demonstrate that the models perform well compared with competing methods for the applications considered, count models and volatility models."
"10.1198/016214504000001998","2005","Bootstrapping unit root tests for autoregressive time series","1","The theory developed for bootstrapping unit root tests in an autoregressive (AR) context has been concerned mainly with the large-sample behavior of the methods proposed under the assumption that the null hypothesis is true. No results exist for the relative performance and the power behavior of the bootstrap methods under the alternative. This article studies the properties of different AR bootstrap schemes of the unit root hypothesis, including a new proposal based on unrestricted residuals. It shows that bootstrap procedures based on differencing the observed series suffer from power problems as compared with bootstrap procedures based on unrestricted residuals. Whereas for finite-order AR processes differencing leads to just a loss of power, for infinite-order autoregressions such a differencing makes the application of sieve AR bootstrap schemes inappropriate if the alternative is true. The superiority of the new bootstrap proposal is shown, and some numerical examples illustrate our theoretical findings."
"10.1198/016214504000001510","2005","Diagnostic checking in {ARMA} models with uncorrelated errors","4","We consider tests for lack of fit in ARMA models with nonindependent innovations. In this framework, the standard Box-Pierce and Ljung-Box portmanteau tests can perform poorly. Specifically, the usual text book formulas for asymptotic distributions are based on strong assumptions and should not be applied without careful consideration. In this article we derive the asymptotic covariance matrix Sigma(rho m) of a vector of autocorrelations for residuals of ARMA models under weak assumptions on the noise. The asymptotic distribution of the portmanteau statistics follows. A consistent estimator of Sigma(rho m), and a modification of the portmanteau tests are proposed. This allows us to construct valid asymptotic significance limits for the residual autocorrelations, and (asymptotically) valid goodness-of-fit tests, when the underlying noise process is assumed to be noncorrelated rather than independent or a martingale difference. A set of Monte Carlo experiments, and an application to the Standard & Poor 500 returns, illustrate the practical relevance of our theoretical results."
"10.1198/016214504000001448","2005","S{LEX} analysis of multivariate nonstationary time series","6","We develop a procedure for analyzing multivariate nonstationary time series using the SLEX library (smooth localized complex exponentials), which is a collection of bases, each basis consisting of waveforms that are orthogonal and time-localized versions of the Fourier complex exponentials. Under the SLEX framework, we build a family of multivariate models that can explicitly characterize the time-varying spectral and coherence properties. Every model has a spectral representation in terms of a unique SLEX basis. Before selecting a model, we first decompose the multivariate time series into nonstationary components with uncorrelated (nonredundant) spectral information. The best SLEX model is selected using the penalized log energy criterion, which we derive in this article to be the Kullback-Leibler distance between a model and the SLEX principal components of the multivariate time series. The model selection criterion takes into account all of the pairwise cross-correlation simultaneously in the multivariate time series. The proposed SLEX analysis gives results that are easy to interpret, because it is an automatic time-dependent generalization of the classical Fourier analysis of stationary time series. Moreover, the SLEX method uses computationally efficient algorithms and hence is able to provide a systematic framework for extracting spectral features from a massive dataset. We illustrate the SLEX analysis with an application to a multichannel brain wave dataset recorded during an epileptic seizure."
"10.1198/016214504000001628","2005","Exact, nonparametric inference when doses are measured with random errors","0","Studies that estimate the effects of exposure to a possibly harmful agent often compare exposed subjects who received varied doses with matched controls who received zero dose. If the doses are measured with error, then one may wish to use the fallible doses to estimate a linear relationship between the unobserved true dose and the observed response. If one is willing to assume that the dose errors for exposed subjects are symmetrically distributed about 0-that is, the dose errors are pure errors and not, say, systematic underreporting of exposure-then the presence of zero-dose controls is all that is needed to obtain exact, distribution-free confidence intervals and tests, and consistent point estimates. The method is simpler for matched pairs than for matched sets with two or more matched subjects. and it is illustrated using two studies, one of each kind. With matched pairs. as in this first example, the method uses Wilcoxon's signed rank test as the basis for inference. When there are several zero-dose controls matched to each exposed subject, the familiar null distribution of the signed rank statistic is no longer applicable because of dependence within matched sets, so the appropriate exact distribution and large-sample approximation are developed."
"10.1198/016214504000001493","2005","A note on nonparametric estimation of the effective dose in quantal bioassay","2","For the common binary response model, we propose a direct method for the nonparametric estimation of the effective dose level. The estimator is obtained by the composition of a nonparametric estimate of the quantile response curve and a classical density estimate. The new method yields a simple and reliable monotone estimate of the effective dose-level curve alpha -> EDalpha and is appealing to users of conventional smoothing methods as kernel estimators, local polynomials. series estimators, or smoothing splines. Moreover. it is computationally very efficient, because it does not require a numerical inversion of a monotonized estimate of the quantile dose-response curve. We prove asymptotic normality of the new estimate and compare it with an available alternative estimate (based on a monotonized nonparametric estimate of the dose-response curve and calculation of the inverse function) by means of a simulation study."
"10.1198/016214504000001790","2005","Phase 2 and 3 combination designs to accelerate drug development","1","For late-stage clinical development, we propose combining phase 2 and 3 trials via a two-stage adaptive design. In the first stage, short-term safety and efficacy are examined, after which low doses that lack efficacy and high doses that cause safety concerns are eliminated from further evaluation. The trial continues to the second stage with doses that are not eliminated. For the second stage, the required sample size is adjusted to maintain power. All patients, including those enrolled in the first stage, are evaluated using a clinical endpoint requiring a longer follow-up. For the second stage, trend statistics are adaptively chosen based on the estimated dose-response curve in the clinical endpoint of the first-stage patients. At the end of the trial, pairwise statistics for the first stage and adaptive trend statistics for the second stage of the clinical endpoint are combined to establish dose-response and to identify the lowest effective dose. A notable feature of our proposed approach is that the adaptation rule governing dose selection, sample size calculation, and derivation of test statistics for the second stage need not be specified in advance to maintain the validity of the trial. The phase 2/3 combination design is effective in achieving robust statistical power and is also efficient, because the number of patients and time needed are substantially reduced."
"10.1198/016214504000001538","2005","Testing quasi-independence of failure and truncation times via conditional {K}endall's tau","2","Truncated survival data arise when the failure time is observed only if it falls within a subject-specific truncating set. Most analysis methods rely on the key assumption of quasi-independence, that is, factorization of the joint density of failure and truncation times into a product proportional to the individual densities in the observable region. Unlike independence of failure time and censoring time, quasi-independence can be tested. Tests of quasi-independence are available for one-sided truncation and for truncation that depends on a measured covariate, but not for more complex truncation schemes. Here tests of quasi-independence based on a multivariate conditional Kendall's tau are proposed for doubly truncated data, bivariate left-truncated data, and other forms of truncated survival data that arise when initiating or terminating event times are interval-censored. Asymptotic properties under the null are derived. The tests are illustrated using several real datasets and evaluated via simulation."
"10.1198/016214504000001420","2005","Maximum likelihood estimation for the proportional odds model with random effects","4","In this article we study the semiparametric proportional odds model with random effects for correlated, right-censored failure time data. We establish that the maximum likelihood estimators for the parameters of this model are consistent and asymptotically Gaussian. Furthermore, the limiting variances achieve the semiparametric efficiency bounds and can be consistently estimated. Simulation studies show that the asymptotic approximations are accurate for practical sample sizes and that the efficiency gains of the proposed estimators over those of Cai, Cheng, and Wei can be substantial. A real example is provided to illustrate the proposed methods."
"10.1198/016214504000001853","2005","An estimated likelihood method for continuous outcome regression models with outcome-dependent sampling","4","Many biomedical observational studies attempt to relate a continuous outcome to an environmental exposure and other important covariates. If the outcome is easier or cheaper to measure relative to the exposure of interest, then the outcome may be observed for every member of a finite-study population, whereas exposure measurements may be obtained only for a relatively small subsample of this population. Rather than selecting a simple random subsample of individuals for exposure measurement, investigators may attempt to enhance study efficiency by allowing the selection probabilities to depend on the observed outcomes; we refer to such sampling schemes as outcome-dependent sampling (ODS). Standard estimation methods that ignore the ODS design will yield biased and inconsistent parameter estimates. Furthermore, it is generally desirable to use estimators that incorporate all available data as analyses restricted to subjects with complete information are inefficient. To this end, we extend an estimated likelihood method, originally developed for discrete outcome measurement error problems in which accurate exposure measurements are made only for a simple random ""validation"" sample, to allow for continuous outcomes and ODS designs. We derive the asymptotic properties of the proposed estimator and use simulated data to show that the asymptotic results closely approximate the finite-sample properties in samples of moderate size. We also use simulated data to compare the performance of our proposed estimator with that of existing methods applicable to the ODS problem."
"10.1198/016214504000001385","2005","An overview of asymptotic properties of {$L_p$} regression under general classes of error distributions","1","We survey the asymptotic properties of regression L-p estimators under general classes of error distributions. It is found that the asymptotic distributions of L, estimators depend crucially on p and the shape of the error distribution near the origin. A number of important features arise as a result, among which are (a) use of a small p may yield accelerated convergence rates for L-p estimators under certain classes of error distributions; (b) for p < 1, L-p regression should, under some circumstances, be undertaken by locally maximizing, rather than minimizing, the sum of the pth powers of the absolute deviations, and (c) consistent estimation of the sampling distributions of the L-p estimators can be achieved by the m out of n bootstrap in general. Numerical examples are provided to illustrate our theoretical findings, and a computational algorithm is suggested for local maximization as may sometimes be required by the L-p procedure."
"10.1198/016214504000001529","2005","Component identification and estimation in nonlinear high-dimensional regression models by structural adaptation","2","This article proposes a new method of analysis of a partially linear model whose nonlinear component is completely unknown. The target of analysis is identification of the set of regressors that enter in a nonlinear way in the model function, and complete estimation of the model, including slope coefficients of the linear component and the link function of the nonlinear component. The procedure also allows selection of the significant regression variables. We also develop a test of linear hypothesis against a partially linear alternative or, more generally, a test that the nonlinear component is M-dimensional for M = 0, 1, 2,.... The approach proposed in this article is fully adaptive to the unknown model structure and applies under mild conditions on the model. The only important assumption is that the dimensionality of nonlinear component is relatively small. The theoretical results indicate that the procedure provides a prescribed level of the identification error and estimates the linear component with accuracy of order n(-1/2). A numerical study demonstrates a very good performance of the method for even small or moderate sample sizes."
"10.1198/016214504000001501","2005","Sufficient dimension reduction via inverse regression: a minimum discrepancy approach","19","A family of dimension-reduction methods, the inverse regression (IR) family, is developed by minimizing a quadratic objective function. An optimal member of this family, the inverse regression estimator (IRE), is proposed, along with inference methods and a computational algorithm. The IRE has at least three desirable properties: (1) Its estimated basis of the central dimension reduction subspace is asymptotically efficient, (2) its test statistic for dimension has an asymptotic chi-squared distribution, and (3) it provides a chi-squared test of the conditional independence hypothesis that the response is independent of a selected subset of predictors given the remaining predictors. Current methods like sliced inverse regression belong to a suboptimal class of the IR family. Comparisons of these methods are reported through simulation studies. The approach developed here also allows a relatively straightforward derivation of the asymptotic null distribution of the test statistic for dimension used in sliced average variance estimation."
"10.1198/016214504000000854","2005","Space-time covariance functions","7","This work considers a number of properties of space-time covariance functions and how these relate to the spatial-temporal interactions of the process. First, it examines how the smoothness away from the origin of a space-time covariance function affects, for example, temporal correlations of spatial differences. Models that are not smoother away from the origin than they are at the origin, such as separable models, have a kind of discontinuity to certain correlations that one might wish to avoid in some circumstances. Smoothness away from the origin of a covariance function is shown to follow from the corresponding spectral density having derivatives with finite moments. These results are used to obtain a parametric class of spectral densities whose corresponding space-time covariance functions are infinitely differentiable away from the origin and that allows for essentially arbitrary and possibly different degrees of smoothness for the process in space and time. Second, this work considers models that are asymmetric in space-time; the covariance between site x at time t and site y at time s is different than the covariance between site x at time s and site y at time t. A general approach is described for generating asymmetric models from symmetric models by taking derivatives. Finally, the implications of a Markov assumption in time on space-time covariance functions for Gaussian processes are examined, and an explicit characterization of all such continuous covariance functions is given. Several of the new models described in this work are applied to wind data from Ireland."
"10.1198/016214504000000818","2005","Variance estimation in a model with {G}aussian submodels","2","This article considers the problem of estimating the dispersion parameter in a Gaussian model that is intermediate between a model where the mean parameter is fully known (fixed) and a model where the mean parameter is completely unknown. One of the goals is to understand the implications of the two-step process of first selecting a model among a finite number of submodels, then estimating a parameter of interest after the model selection, but using the same sample data. The estimators are classified into global, two-step, and weighted estimators. Whereas the global-type estimators ignore the model space structure, the two-step estimators explore the structure adaptively and can be related to pretest estimators, and the weighted estimators are motivated by the Bayesian approach. Their performances are compared theoretically and through simulations using their risk functions based on a scale-invariant quadratic loss function. It is shown that in the variance estimation problem, efficiency gains arise by exploiting the submodel structure through the use of two-step and weighted estimators, especially when the number of competing submodels is few, but that this advantage may deteriorate or be lost altogether for some two-step estimators as the number of submodels increases or the distance between them decreases. Furthermore, it is demonstrated that weighted estimators, arising from properly chosen priors, outperform two-step estimators when there are many competing submodels or when the submodels are close to each other, whereas two-step estimators are preferred when the submodels are highly distinguishable. The results have implications for model averaging and model selection issues."
"10.1198/016214504000001015","2005","Bilinear mixed-effects models for dyadic data","2","This article discusses the use of asymmetric multiplicative interaction effect to capture certain types of third-order dependence patterns often present in social networks and other dyadic datasets. Such an effect, along with standard linear fixed and random effects, is incorporated into a generalized linear model, and a Markov chain Monte Carlo algorithm is provided for Bayesian estimation and inference. In an example analysis of international relations data, accounting for such patterns improves model fit and predictive performance."
"10.1198/016214504000000575","2005","Approximate and asymptotic distributions of chi-squared-type mixtures with applications","1","In this article we study how to approximate a random variable T of general chi-squared-type mixtures by a random variable of the form alpha chi(d)(2) + beta via matching the first three cumulants. The approximation error bounds for the density functions of the chi-squared approximation and the normal approximation are established. Applications of the results to some nonparametric goodness-of-fit tests, including those tests based on orthogonal series, smoothing splines, and local polynomial smoothers, are investigated. Two simulation studies are conducted to compare the chi-squared approximation and the normal approximation numerically. The chi-squared approximation is illustrated using a real data example for polynomial goodness-of-fit tests."
"10.1198/016214504000000863","2005","Estimating load-sharing properties in a dynamic reliability system","0","An estimator for the load-share parameters in an equal load-share model is derived based on observing k-component parallel systems of identical components that have a continuous distribution function F((.)) and failure rate r((.)). In an equal load-share model. after the first of k components fails, failure rates for the remaining components change from r(t) to gamma(1)r(t), then to gamma(2)r(t) after the next failure, and so on. On the basis of observations on n independent and identical systems, a semiparametric estimator of the component baseline cumulative hazard function R = - log(1 - F) is presented, and its asymptotic limit process is established to be a Gaussian process. The effect of estimation of the load-share parameters is considered in the derivation of the limiting process. Potential applications can be found in diverse areas, including materials testing, software reliability, and power plant safety assessment."
"10.1198/016214504000000764","2005","Inferences under a stochastic ordering constraint: the {$k$}-sample case","1","If X-1 and X-2 are random variables with distribution functions F-1 and F-2, then X-1 is said to be stochastically larger than X-2 if F-1 <= F-2. Statistical inferences under stochastic ordering for the two-sample case has a long and rich history. In this article we consider the k-saniple case; that is, we have k populations with distribution functions F-1, F-2,.... F-k, k >= 2, and we assume that F-1 <= F-2 <= (...) <= F-k. For k = 2, the nonparametric maximum likelihood estimators of F, and F2 under this order restriction have been known for a long time; their asymptotic distributions have been derived only recently. These results have very complicated forms and are hard to deal with when making statistical inferences. We provide simple estimators when k >= 2. These are strongly uniformly consistent, and their asymptotic distributions have simple forms. If (F) over cap (i) and (F) over cap (i)* are the empirical and our restricted estimators of F-i, then we show that, asymptotically, P(vertical bar(F) over cap (i)*(x) - F-i(x)vertical bar <= u) >= P(vertical bar(F) over cap (i)(x) - F-i(x)vertical bar <= u) for all x and all u > 0, with strict inequality in some cases. This clearly shows a uniform improvement of the restricted estimator over the unrestricted one. We consider simultaneous confidence bands and a test of hypothesis of homogeneity against the stochastic ordering of the k distributions. The results have also been extended to the case of censored observations. Examples of application to real life data are provided."
"10.1198/016214504000001024","2005","A {B}ayesian semiparametric model for random-effects meta-analysis","4","In meta-analysis, there is an increasing trend to explicitly acknowledge the presence of study variability through random-effects models. That is, one assumes that for each study there is a study-specific effect and one is observing an estimate of this latent variable. In a random-effects model, one assumes that these study-specific effects come from some distribution, and one can estimate the parameters of this distribution, as well as the study-specific effects themselves. This distribution is most often modeled through a parametric family, usually a family of normal distributions. The advantage of using a normal distribution is that the mean parameter plays an important role, and much of the focus is on determining whether or not this mean is 0. For example, it may be easier to justify funding further studies if it is determined that this mean is not 0. Typically, this normality assumption is made for the sake of convenience, rather than from some theoretical justification, and may not actually hold. We present a Bayesian model in which the distribution of the study-specific effects is modeled through a certain class of nonparametric priors. These priors can be designed to concentrate most of their mass around the family of normal distributions but still allow for any other distribution. The priors involve a univariate parameter that plays the role of the mean parameter in the normal model, and they give rise to robust inference about this parameter. We present a Markov chain algorithm for estimating the posterior distributions under the model. Finally, we give two illustrations of the use of the model."
"10.1198/016214504000000827","2005","A statistical model for signature verification","0","A Bayesian model for off-line signature verification involving the representation of a signature through its curvature is developed. The prior model makes use of a spatial point process for specifying the knots in an approximation restricted to a buffer region close to a template curvature, along with an independent time-warping mechanism. In this way, prior shape information about the signature can be built into the analysis. The observation model is based on additive white noise superimposed on the underlying curvature. The approach is implemented using Markov chain Monte Carlo and applied to a collection of documented instances of William Shakespeare's signature."
"10.1198/016214504000001277","2005","Regression analysis with linked data","0","Record linkage, or exact matching, can be used to join together two files that contain information on the same individuals but lack unique personal identification codes. The possibility of errors in linkage causes problems for estimating the relationships between variables on the two files. The effect is analogous to the impact of measurement error. A model of a linear regression relationship between variables in linked files is proposed. Assuming the probabilities that pairs of records are links are known, an unbiased estimator of the regression coefficients is derived. Methods for estimating the linkage probabilities by using mixture models are discussed. A consistent estimator of the covariance matrix of the proposed estimator is proposed. A bootstrap estimator is used to reflect the impact of the uncertainty in record linkage model parameters on the estimators of the regression parameters. A simulation study compares the performance of the proposed estimator and alternatives."
"10.1198/016214504000000836","2005","Nonparametric identification and estimation of a censored location-scale regression model","0","In this article we consider identification and estimation of a censored nonparametric location scale-model. We first show that in the case where the location function is strictly less than the (fixed) censoring point for all values in the support of the explanatory variables, the location function is not identified anywhere. In contrast, when the location function is greater or equal to the censoring point with positive probability, the location function is identified on the entire support, including the region where the location function is below the censoring point. In the latter case we propose a simple estimation procedure based on combining conditional quantile estimators for various higher quantiles. The new estimator is shown to converge at the optimal nonparametric rate with a limiting normal distribution. A small-scale simulation study indicates that the proposed estimation procedure performs well in finite samples. We also present an empirical illustration on unemployment insurance duration using administrative-level data from New Jersey."
"10.1198/016214504000000773","2005","Constrained inverse regression for incorporating prior information","1","Inverse regression methods facilitate dimension-reduction analyses of high-dimensional data by extracting a small number of factors that are linear combinations of the original predictor variables. But the estimated factors may not lend themselves readily to interpretation consistent with prior information. Our approach to solving this problem is to first incorporate prior information via theory- or data-driven constraints on model parameters, and then apply the proposed method, constrained inverse regression (CIR), to extract factors that satisfy the constraints. We provide chi-squared and t tests to assess the significance of each factor and its estimated coefficients, and we also generalize CIR to other inverse regression methods in situations where both dimension reduction and factor interpretation are important. Finally, we investigate CIR's small-sample performance, test data-driven constraints, and present a marketing example to illustrate its use in discovering meaningful factors that influence the desirability of brand logos."
"10.1198/016214504000000809","2005","A calculus for design of two-stage adaptive procedures","0","We propose calculus for designing two-stage adaptive procedures. We describe the design components that specify a two-stage adaptive design, define their interrelationships, and demonstrate how changes in one design component effect changes in the other components. Our technique allows us to control many aspects of a two-stage adaptive clinical trial. including the type I and type II error rates and the maximum total sample size. We also conduct an ANOVA-type study to understand the effects of different components of the design specification on the performance characteristics of the design. Stage I components-namely sample size, type I error rate, and type II error rate-are found to be the most influential."
"10.1198/016214504000001286","2005","Functional association models for multivariate survival processes","1","We consider multivariate temporal processes that are continuously observed within overlapping time windows. The intended application is in censored multistate and multivariate survival settings, where point processes are continuously observed. These data differ from other functional data, like longitudinal data, which are discretely observed at irregular times. Existing functional approaches to survival processes use intensity models, which require smoothing and depend critically on the choice of smoothing parameters, similarly to discretely observed data. In this article we study functional mean and association regression models for the point processes, with unspecified time-varying coefficients. The continuous observation scheme is exploited; the coefficients may be estimated nonparametrically by extending generalized estimating equations to continuously observed data. The estimators automatically converge at the parametric rate without smoothing, unlike with discretely observed data. Uniform consistency and weak convergence is established with empirical process techniques. The nonparametric estimators yield new tests for covariate effects, parametric submodeling of these effects, and goodness-of-fit testing. Simulation studies and an analysis of familial aggregation of alcoholism illustrate the methodology's practical utility."
"10.1198/016214504000000845","2005","On the {C}ox model with time-varying regression coefficients","7","In the analysis of censored failure time observations, the standard Cox proportional hazards model assumes that the regression coefficients are time invariant. Often, these parameters vary over time, and the temporal covariate effects on the failure time are of great interest. In this article, following previous work of Cai and Sun, we propose a simple estimation procedure for the Cox model with time-varying coefficients based on a kernel-weighted partial likelihood approach. We construct pointwise and simultaneous confidence intervals for the regression parameters over a properly chosen time interval via a simple resampling technique. We derive a prediction method for future patients' survival with any specific set of covariates. Building on the estimates for the time-varying coefficients, we also consider the mixed case and present an estimation procedure for time-independent parameters in the model. Furthermore. we show how to use an integrated function of the estimate for a specific regression coefficient to examine the adequacy of proportional hazards assumption for the corresponding covariate graphically and numerically. All of the proposals are illustrated extensively with a well-known study from the Mayo Clinic."
"10.1198/016214504000001295","2005","A general statistical framework for unifying interval and linkage disequilibrium mapping: toward high-resolution mapping of quantitative traits","0","The nonrandom association between different genes, termed linkage disequilibrium (LD), provides a powerful tool for high-resolution mapping of quantitative trait loci (QTL) underlying complex traits. This LD-based mapping approach can be made more efficient when it is coupled with interval mapping characterizing the genetic distance between markers and QTL. This article describes a general statistical framework for simultaneously estimating the linkage and LD that are related in a two-stage hierarchical sampling scheme. This framework is constructed within a maximum likelihood context and can be expanded to fine-scale mapping of complex traits for different population structures and reproductive behaviors. We provide a closed-form solution for joint estimation of quantitative genetic parameters describing QTL effects, QTL position and residual variances, and population genetic parameters describing allele frequencies and QTL-marker LD. We perform simulation studies to investigate the statistical properties of our joint analysis model for interval and LD mapping. An example using body weights of dogs from a multifamily outcrossed pedigree illustrates the use of the model."
"10.1198/016214504000000629","2005","Efficient semiparametric marginal estimation for longitudinal/clustered data","16","We consider marginal generalized semiparametric partially linear models for clustered data. Lin and Carroll derived the semiparametric efficient score function for this problem in the multivariate Gaussian case, but they were unable to construct a semiparametric efficient estimator that actually achieved the semiparametric information bound. Here we propose such an estimator and generalize the work to marginal generalized partially linear models. We investigate asymptotic relative efficiencies of the estimators that ignore the within-cluster correlation structure either in nonparametric curve estimation or throughout. We evaluate the finite-sample performance of these estimators through simulations and illustrate it using a longitudinal CD4 cell count dataset. Both theoretical and numerical results indicate that properly taking into account the within-subject correlation among the responses can substantially improve efficiency."
"10.1198/016214504000000610","2005","Multifold predictive validation in {ARMAX} time series models","0","This article presents a new procedure for multifold predictive validation in time series. The procedure is based on the so-called ""filtered residuals,"" in-sample prediction errors evaluated in such a way that they are similar to out-of-sample ones. The filtered residuals are obtained from parameters estimated by eliminating from the estimation process the estimated innovations at the points to be predicted. Thus, instead of using the deletion of observations to validate the predictions, as in classical cross-validation, the procedure is based on deletion of the estimated innovations. It is proved that the filtered residuals are uncorrelated, up to terms of small order, with the in-sample innovations, a property shared with the out-of-sample residuals. The parameters needed for computing the filtered residuals can be obtained by estimating a model with innovational outliers at the points to be predicted. The proposed multifold predictive validation is asymptotically equivalent to an efficient model selection procedure. Some Monte Carlo evidence of the performance of the procedure is presented, and the application is illustrated in an example."
"10.1198/016214504000001042","2005","Homogeneous linear predictor models for contingency tables","2","Maximum likelihood (ML) inference for the class of homogeneous linear predictor (HLP) models for contingency tables is described. HLP models constrain the expected table counts m through L(m) = X beta, where the link L is allowed to be a many-to-one, nonlinear function. Generalized log-linear, association trend, marginal cumulative probit, and conditional marginal homogeneity models are given as specific examples. ML fit results, which include point estimates, goodness-of-fit statistics, and asymptotic-based approximate distributions, are described and compared for equivalent HLP models. The results are valid for a wide variety of sampling plans including combinations of product multinomial and Poisson sampling. An important practical implication of this article is that the implementation of ML fitting and theory is straightforward, and an attractive alternative to weighted least squares estimation, for HLP models."
"10.1198/016214504000001303","2005","Sequential {M}onte {C}arlo methods for statistical analysis of tables","1","We describe a sequential importance sampling (SIS) procedure for analyzing two-way zero-one or contingency tables with fixed marginal sums. An essential feature of the new method is that it samples the columns of the table progressively according to certain special distributions. Our method produces Monte Carlo samples that are remarkably close to the uniform distribution, enabling one to approximate closely the null distributions of various test statistics about these tables. Our method compares favorably with other existing Monte Carlo-based algorithms, and sometimes is a few orders of magnitude more efficient. In particular, compared with Markov chain Monte Carlo (MCMC)-based approaches, our importance sampling method not only is more efficient in terms of absolute running time and frees one from pondering over the mixing issue, but also provides an easy and accurate estimate of the total number of tables with fixed marginal sums, which is far more difficult for an MCMC method to achieve."
"10.1198/016214504000000539","2005","Exact and approximate stepdown methods for multiple hypothesis testing","8","Consider the problem of testing k hypotheses simultaneously. In this article we discuss finite- and large-sample theory of stepdown methods that provide control of the familywise error rate (FWE). To improve on the Bonferroni method or on Holm's stepdown method, Westfall and Young made effective use of resampling to construct stepdown methods that implicitly estimate the dependence structure of the test statistics. However, their methods depend on an assumption known as ""subset pivotality."" Our goal here is to construct general stepdown methods that do not require such an assumption. To accomplish this, we take a close look at what makes stepdown procedures work; a key component is a monotonicity requirement of critical values. By imposing monotonicity on estimated critical values (which is not an assumption on the model but rather is an assumption on the method), we show how to construct stepdown tests that can be applied in a stagewise fashion so that at most k tests need to be computed. Moreover, at each stage, an intersection test that controls the usual probability of a type 1 error is calculated, which allows us to draw on an enormous resampling literature as a general means of test construction. In addition, it is possible to carry out this method using the same set of resamples (or subsamples) for each of the intersection tests."
"10.1198/016214504000001907","2005","False discovery rate-adjusted multiple confidence intervals for selected parameters","7","Often in applied research, confidence intervals (CIs) are constructed or reported only for parameters selected after viewing the data. We show that such selected intervals fail to provide the assumed coverage probability. By generalizing the false discover), rate (FDR) approach from multiple testing to selected multiple CIs, we suggest the false coverage-statement rate (FCR) as a measure of interval coverage following selection. A general procedure is then introduced, offering FCR control at level q under any selection rule. The procedure constructs a marginal CI for each selected parameter, but instead of the confidence level 1 - q being used marginally, q is divided by the number of parameters considered and multiplied by the number selected. If we further use the FDR controlling testing procedure of Benjamini and Hochberg for selecting the parameters, the newly suggested procedure offers CIs that are dual to the testing procedure and are shown to be optimal in the independent case. Under the positive regression dependency condition of Benjamini and Yekutieli, the FCR is controlled for one-sided tests and CIs, as well as for a modification for two-sided testing. Results for general dependency are also given. Finally, using the equivalence of the CIs to testing, we prove that the procedure of Benjamini and Hochberg offers directional FDR control as conjectured."
"10.1198/016214506000000573","2006","Nonparametric modeling of the left censorship of analytical data in food risk assessment","0","Contaminants and natural toxicants such as mycotoxins may be present in various food items that may be considered dangerous to human health if the cumulative intake remains above the toxicologic safe references. This intake or exposure can be estimated using both consumption surveys and analytical data that record the contamination levels of food. Analytical data often present some left censorship, that is, data below some limit of detection or quantification. This article proposes the integration of a nonparametric modeling of the left censorship of analytical data in a model aiming at giving a quantitative evaluation of the risk due to the presence of some particular contaminants in food. We focus on the estimation of the ""risk,"" defined as the probability for exposure to exceed the so-called ""provisional tolerable weekly intake"" (PTWI), when both consumption data and contamination data are independently available. To account for the left censorship of the contamination data (due to the existence of detection/quantification limits), we propose using a Kaplan-Meier estimator instead of the empirical cumulative distribution function generally used in nonparametric procedures. We give the asymptotic behavior of our estimator and derive the asymptotic properties of the associated risk estimator. Several confidence intervals are obtained using a double-bootstrap procedure. A detailed algorithm is proposed. As an illustration, we present an evaluation of the risk exposure to ochratoxin A in France and use our risk estimator to show that children under age 10 years are a population at particular risk. Imposing some maximum limits on particular food items, namely cereals and wine, would not significantly reduce the risk."
"10.1198/016214506000001068","2006","The hazard potential: introduction and overview","0","This is an expository article directed at reliability theorists, survival analysts, and others interested in looking at life history and event data. Here we introduce the notion of a hazard potential as an unknown resource that art item is endowed with at inception. The item fails when this resource becomes depleted. The cumulative hazard is a proxy for the amount of resource consumed, and the hazard function is a proxy for the rate at which this resource is consumed. With this conceptualization of the failure process, we are able to characterize accelerated, decelerated, and normal tests and are also able to provide a perspective on the cause of interdependent lifetimes. Specifically, we show that dependent life lengths are the result of dependent hazard potentials. Consequently, we are able to generate new families of multivariate life distributions using dependent hazard potentials as a seed. For an item that operates in a dynamic environment, we argue that its lifetime is the killing time of a continuously increasing stochastic process by a random barrier, and this barrier is the item's hazard potential. The killing time perspective enables us to see competing risks from a process standpoint and to propose a framework for the joint modeling of degradation or cumulative damage and its markers. The notion of the hazard potential generalizes to the multivariate case. This generalization enables us to replace a collection of dependent random variables by a collection of independent exponentially distributed random variables, each having a different time scale."
"10.1198/016214506000000636","2006","What do randomized studies of housing mobility demonstrate?: {C}ausal inference in the face of interference","1","During the past 20 years, social scientists using observational studies have generated a large and inconclusive literature on neighborhood effects. Recent workers have argued that estimates of neighborhood effects based on randomized studies of housing mobility, such as the ""Moving to Opportunity"" (MTO) demonstration, are more credible. These estimates are based on the implicit assumption of no interference between units; that is, a subject's value on the response depends only on the treatment to which that subject is assigned, not on the treatment assignments of other subjects. For the MTO studies, this assumption is not reasonable. Although little work has been done on the definition and estimation of treatment effects when interference is present, interference is common in studies of neighborhood effects and in many other social settings (e.g., schools and networks), and when data from such studies are analyzed under the ""no-interference assumption,"" very misleading inferences can result. Furthermore, the consequences of interference (e.g., spillovers) should often be of great substantive interest, even though little attention has been paid to this. Using the MTO demonstration as a concrete context, this article develops a framework for causal inference when interference is present and defines a number of causal estimands of interest. The properties of the usual estimators of treatment effects, which are unbiased and/or consistent in randomized studies without interference, are also characterized. When interference is present, the difference between a treatment group mean and a control group mean (unadjusted or adjusted for covariates) estimates not an average treatment effect, but rather the difference between two effects defined on two distinct subpopulations. This result is of great importance, for a researcher who fails to recognize this could easily infer that a treatment is beneficial when in fact it is universally harmful."
"10.1198/016214506000000591","2006","Small-area estimation with state-space models subject to benchmark constraints","0","This article shows how to benchmark small-area estimators, produced by fitting separate state-space models within the areas, to aggregates of the survey direct estimators within a group of areas. State-space models are used by the U.S. Bureau of Labor Statistics (BLS) for the production of all of the monthly employment and unemployment estimates in census divisions and the states. Computation of the benchmarked estimators and their variances is accomplished by incorporating the benchmark constraints within a joint model for the direct estimators in the different areas, which requires the development of a new filtering algorithm for state-space models with correlated measurement errors. The new filter coincides with the familiar Kalman filter when the measurement errors are uncorrelated. The properties and implications of the use of the benchmarked estimators are discussed and illustrated using BLS unemployment series. The problem of small-area estimation is how to produce reliable estimates of area (domain) characteristics and compute their variances when the sample sizes within the areas are too small to warrant the use of traditional direct survey estimates. This problem is commonly handled by borrowing strength from either neighboring areas and/or from previous surveys, using appropriate cross-sectional/time series models. To protect against possible model breakdowns and for consistency in publication, the area model-dependent estimates often must be benchmarked to an estimate for a group of the areas, which it is sufficiently accurate. The latter estimate is a weighted sum of the direct survey estimates in the various areas, so that the benchmarking process defines another way of borrowing strength across the areas."
"10.1198/016214506000000627","2006","A nonstationary negative binomial time series with time-dependent covariates: {E}nterococcus counts in {B}oston {H}arbor","0","Boston Harbor has a history of poor water quality, including contamination by enteric pathogens. We conduct a statistical analysis of data collected by the Massachusetts Water Resources Authority (MWRA) between 1996 and 2002 to evaluate the effects of court-mandated improvements in sewage treatment. Motivated by the ineffectiveness of standard Poisson mixture models and their zero-inflated counterparts, we propose a new negative binomial model for time series of Enterococcus counts in Boston Harbor, where nonstationarity and autocorrelation are modeled using a nonparametric smooth function of time in the predictor. Without further restrictions, this function is not identifiable in the presence of time-dependent covariates; consequently, we use a basis orthogonal to the space spanned by the covariates and use penalized quasi-likelihood (PQL) for estimation. We conclude that Enterococcus counts were greatly reduced near the Nut Island Treatment Plant (NITP) outfalls following the transfer of wastewaters from NITP to the Deer Island Treatment Plant (DITP) and that the transfer of wastewaters from Boston Harbor to the offshore diffusers in Massachusetts Bay reduced the Enterococcus counts near the DITP outfalls."
"10.1198/016214506000000465","2006","Using wavelet-based functional mixed models to characterize population heterogeneity in accelerometer profiles: a case study","2","We present a case study illustrating the challenges of analyzing accelerometer data taken from a sample of children participating in an intervention study designed to increase physical activity. An accelerometer is a small device worn on the hip that records the minute-by-minute activity levels throughout the day for each day it is worn. The resulting data are irregular functions characterized by many peaks representing short bursts of intense activity. We model these data using the wavelet-based functional mixed model. This approach incorporates multiple fixed-effects and random-effects functions of arbitrary form, the estimates of which are adaptively regularized using wavelet shrinkage. The method yields posterior samples for all functional quantities of the model, which can be used to perform various types of Bayesian inference and prediction. In our case study, a high proportion of the daily activity profiles are incomplete (i.e., have some portion of the profile missing), and thus cannot be modeled directly using the previously described method. We present a new method for stochastically imputing the missing data that allows us to incorporate these incomplete profiles in our analysis. Our approach borrows strength from both the observed measurements within the incomplete profiles and from other profiles, from the same child as well as from other children with similar covariate levels, while appropriately propagating the uncertainty of the imputation throughout all subsequent inference. We apply this method to our case study, revealing some interesting insights into children's activity patterns. We point out some strengths and limitations of using this approach to analyze accelerometer data."
"10.1198/016214506000000609","2006","A {B}ayesian approach for incorporating variable rates of heterogeneity in linkage analysis","0","A widely used approach for dealing with locus heterogeneity in linkage analysis is based on mixture likelihood, in which a single mixing (heterogeneity) parameter represents the probability that each family is of linked type. However, in general, different types of families exhibit different heterogeneity levels. To incorporate this variability, we propose a new approach wherein each family has its own heterogeneity parameter representing the probability that it is of linked type. These parameters are nuisance parameters, whereas the main parameter of interest is the location of the disease gene, if there is any. We model the problem in the Bayesian framework and implement it using the Markov chain Monte Carlo (MCMC) methodology. In particular, we use the reversible-jump MCMC sampler to move between the two models: linkage and no linkage. We first estimate the posterior probability of linkage on a chromosome and the corresponding Bayes factor. If linkage is inferred, then the location of the disease gene along with its credible set is estimated. The asymptotic joint distribution of the estimators is derived. We show that this approach is more powerful than the currently used approach in detecting linkage, whereas the two approaches have comparable false-positive rates. The proposed method was applied to a lung cancer dataset of Genetic Epidemiology of Lung Cancer Consortium and an asthma dataset consisting of three samples from Genetic Analysis Workshop 12. Since both lung cancer and asthma are complex traits with heterogeneous genetic predisposition, they provide suitable applications for the proposed method."
"10.1198/016214506000000933","2006","Rejoinder","0",""
"10.1198/016214505000000394","2006","Hidden {M}arkov models for microarray time course data in multiple biological conditions","6","Among the first microarray experiments were those measuring expression over time, and time course experiments remain common. Most methods to analyze time course data attempt to group genes sharing similar temporal profiles within a single biological condition. However. with time course data in multiple conditions, a main goal is to identify differential expression patterns over time. An intuitive approach to this problem would be to apply at each time point any of the many methods for identifying differentially expressed genes across biological conditions and then somehow combine the results of the repeated marginal analyses. But considering each time point in isolation is inefficient, because it does not use the information contained in the dependence structure of the time course data. This problem is exacerbated in microarray studies, where low sensitivity is a problematic feature of many methods. Furthermore, a gene's expression pattern over time might not be identified by simply combining results from repeated marginal analyses. We propose a hidden Markov modeling approach developed to efficiently identify differentially expressed genes in time course microarray experiments and classify genes based on their temporal expression patterns. Simulation studies demonstrate a substantial increase in sensitivity, with little increase in the false discovery rate, compared with a marginal analysis at each time point. This increase is also observed in data from a case study of the effects of aging on stress response in heart tissue, where a significantly larger number of genes are identified using the proposed approach."
"10.1198/016214506000000618","2006","Tests of spatial randomness adjusted for an inhomogeneity: a general framework","0","In many applications, it is of interest to test whether a spatial point pattern is randomly generated after adjusting for an underlying spatial inhomogeneity. A great variety of different test statistics have been proposed for this purpose by scientists in different fields; these are reviewed in this article. Despite apparent dissimilarities in terms of their original formulations, most of these statistics can be placed into one general framework of which they are special cases. This makes it easier to see exactly how they relate to one another and also to determine which test to use for a particular application. The general framework can also be used for proposing new tests by combining properties of existing tests, for developing theoretical foundations for these types of test statistics, for doing structured comparative evaluations, and for software development."
"10.1198/016214505000001339","2006","Doubly robust estimation of the area under the receiver-operating characteristic curve in the presence of verification bias","0","The area under the receiver operating characteristic curve (AUC) is a popular summary measure of the efficacy of a medical diagnostic test to discriminate between healthy and diseased subjects. A frequently encountered problem in studies that evaluate a new diagnostic test is that not all patients undergo disease verification because the verification test is expensive, invasive, or both. Furthermore, the decision to send patients to verification often depends on the new test and on other predictors of true disease status. In such cases, usual estimators of the AUC based on verified patients only are biased. In this article we develop estimators of the AUC of markers measured on any scale that adjust for selection to verification. These estimators adjust for measured patient covariates and diagnostic test results and also for an assumed degree of residual selection bias. They can then be used in a sensitivity analysis to examine how the AUC estimates change when different plausible degrees of residual association are assumed. As with other missing-data problems, due to the curse of dimensionality, a model for disease or a model for selection is needed to obtain well-behaved estimators of the AUC when the marker and/or the measured covariates are continuous. We describe a doubly robust estimator that has the attractive feature of being consistent and asymptotically normal if either the disease or the selection model (but not necessarily both) is correct."
"10.1198/016214506000000375","2006","Robust {$M$} tests without consistent estimation of the asymptotic covariance matrix","0","We extend the KVB approach of Kiefer, Vogelsang, and Bunzel (2000) to constructing robust M tests without consistent estimation of the asymptotic covariance matrix. We demonstrate that, when model parameters have to be estimated, the normalizing matrix computed using a full-sample estimator is able to eliminate the nuisance parameters when there is no estimation effect but not otherwise. To circumvent the problem of estimation effect, we propose using recursive estimators to compute the normalizing matrix and show that the resulting M test is asymptotically pivotal. This M test is, thus, robust not only to heteroscedasticity and serial correlations of unknown form but also to the presence of an estimation effect. As examples, we consider robust tests for serial correlations and robust information matrix tests. The former tests extend that of Lobato (2001) and are applicable to model residuals. For testing higher-order moments, we find that the latter tests are also robust when a lower-order moment is misspecified. Our simulations confirm that the proposed M tests are properly sized and have power advantage when other tests are computed based on inappropriate user-chosen parameters."
"10.1198/016214505000001401","2006","Distribution of runs and longest runs: a new generating function approach","0","Exact distributions of run statistics are traditionally obtained using combinatorial methods, which, under certain situations, become very tedious. Run distributions of multiple object systems, although appearing frequently in applications from various fields, such as computational biology, are not commonly used, due in part to the lack of easy-to-use formulas. In this article, a method for evaluating partition functions of lattice models in the field of statistical mechanics is used to develop a systematic method to study various run statistics in multiple object systems. By using particular generating functions for the specified situation under study, many new distributions can be obtained in a unified and coherent way. The method makes it possible to manipulate formulas of run statistics by using binomial identities to obtain more general, yet simpler formulas. To illustrate the applications of the general method, the distributions of the total number of runs and the longest runs are investigated. Novel and general explicit formulas are derived for the distribution and moments of the total number of runs, and simple explicit formulas are derived for the distributions of the longest runs. In addition, some classical run statistics are recovered and generalized in the same unified way. As examples of applications to biological sequence analysis, the run statistics developed using the general method are applied to several protein sequences to examine their global and local features."
"10.1198/016214506000000050","2006","Modeling marked point processes via bivariate mixture transition distribution models","0","We propose new probability models for the analysis of marked point processes. These models deal with the type of data that arrive or are observed in possibly unequal time intervals, such as financial transactions and earthquakes, among others. The models treat both the time between event arrivals and the observed marks as stochastic processes. We adopt a class of bivariate distributions to form the bivariate mixture transition distribution. In these models the conditional bivariate distribution of the next observation given the past is a mixture of conditional distributions given each one of the last p observations or a selection of past p events. The identifiability of the model is investigated, and an EM algorithm is developed to obtain estimates of the model parameters. Simulation and real data examples are used to demonstrate the utility of these models."
"10.1198/016214506000000311","2006","Efficient estimation of semiparametric multivariate copula models","1","We propose a sieve maximum likelihood estimation procedure for a broad class of semiparametric multivariate distributions. A joint distribution in this class is characterized by a parametric copula function evaluated at nonparametric marginal distributions. This class of distributions has gained popularity in diverse fields due to its flexibility in separately modeling the dependence structure and the marginal behaviors of a multivariate random variable, and its circumvention of the ""curse of dimensionality"" associated with purely nonparametric multivariate distributions. We show that the plug-in sieve maximum likelihood estimators (MLEs) of all smooth functionals, including the finite-dimensional copula parameters and the unknown marginal distributions, are semiparametrically efficient, and that their asymptotic variances can be estimated consistently. Moreover, prior restrictions on the marginal distributions can be easily incorporated into the sieve maximum likelihood estimation procedure to achieve further efficiency gains. Two such cases are studied: (a) the marginal distributions are equal but otherwise unspecified, and (b) some but not all marginal distributions are parametric. Monte Carlo studies indicate that the sieve MLEs perform well in finite samples, especially when prior information on the marginal distributions is incorporated."
"10.1198/016214506000000429","2006","Estimation and testing for varying coefficients in additive models with marginal integration","1","We propose marginal integration estimation and testing methods for the coefficients of varying-coefficient multivariate regression models. Asymptotic distribution theory is developed for the estimation method, which enjoys the same rate of convergence as univariate function estimation. For the test statistic, asymptotic normal theory is established. These theoretical results are derived under the fairly general conditions of absolute regularity (beta-mixing). Application of the test procedure to West German real GNP (gross national product) data reveals that a partially linear varying coefficient model is best parsimonious in fitting the data dynamics, a fact that is also confirmed with residual diagnostics."
"10.1198/016214506000000096","2006","Principal components analysis based on multivariate {MM} estimators with fast and robust bootstrap","3","We consider robust principal components analysis (PCA) based on multivariate MM estimators. We first study the robustness and efficiency of these estimators, particularly in terms of eigenvalues and eigenvectors. We then focus on inference procedures based on a fast and robust bootstrap for MM estimators. This method is an alternative to the approach based on the asymptotic distribution of the estimators and can also be used to assess the stability of the principal components. A formal consistency proof for the bootstrap method is given, and its finite-sample performance is investigated through simulations. We illustrate the use of the robust PCA and the bootstrap inference on a real dataset."
"10.1198/016214506000000014","2006","On {$m$} out of {$n$} bootstrapping for nonstandard {M}-estimation with nuisance parameters","3","Nonstandard M-estimation, with nuisance parameters consistently estimated in the criterion function, often yields M-estimators converging weakly at rates different from n(1/2) with weak limits that are typically non-Gaussian. The complicated asymptotics involved makes distributional estimation of the M-estimators analytically prohibitive. We show that the problem is resolved by m out of n bootstrapping under very general conditions, which provides a universal and convenient approach to consistently estimating sampling distributions of M-estimators. We illustrate our findings with applications to least median of squares regression estimators, studentized location M-estimators, shorth estimators, and robust M-estimators derived from L(r)-type loss functions. We provide empirical evidence using a simulation study to construct confidence intervals and globally estimate sampling distributions."
"10.1198/016214506000000474","2006","Generalized poststratification and importance sampling for subsampled {M}arkov chain {M}onte {C}arlo estimation","0","Benchmark estimation is motivated by the goal of producing an approximation to a posterior distribution that is better than the empirical distribution function. This is accomplished by incorporating additional information into the construction of the approximation. We focus here on generalized poststratification, the most successful implementation of benchmark estimation in our experience. We develop generalized poststratification for settings where the source of the simulation differs from the posterior that is to be approximated. This allows us to use the techniques in settings where it is advantageous to draw from a distribution different than the posterior, whether for exploration of the data and/or model, for algorithmic simplicity, for improved convergence of the simulation, or for improved estimation of selected features of the posterior. We develop an asymptotic (in simulation size) theory for the estimators, providing conditions under which central limit theorems hold. The central limit theorems apply both to an importance sampling context and to direct sampling from the posterior distribution. The asymptotic results, coupled with large-sample (size of data) approximation results provide guidance on how to implement generalized poststratification. The theoretical results also explain the gains associated with generalized poststratification and the empirically observed robustness to cutpoints for the strata. We note that the results apply well beyond the setting of Markov chain Monte Carlo simulation. The technique is illustrated with an infinite-dimensional semiparametric Bayesian regression model and a low-dimensional, overdispersed hierarchical Bayesian model. In both cases, the technique shows substantial benefits."
"10.1198/016214505000001393","2006","Post-processing posterior predictive {$p$}-values","2","This article addresses issues of model criticism and model comparison in Bayesian contexts, and focuses on the use of the so-called posterior predictive p values (ppp). These involve a general discrepancy or conflict measure and depend on the prior, the model, and the data. They are used in statistical practice to quantify the degree of surprise or conflict in data and to compare different combinations of prior and model. The distribution of such ppp values is far from uniform however, as we demonstrate for different models, making their interpretation and comparison a difficult matter. We propose a natural calibration of the ppp values, where the resulting cppp values are uniform on the unit interval under model conditions. The cppp values, which in general rely on a double-simulation scheme for their computation, may then be used to assess and compare different priors and models. Our methods also make it possible to compare parametric and nonparametric model specifications, in that genuine ""measures of surprise"" are put on the same canonical uniform scale. We illustrate our techniques for some applications to real data. We also present supplementing theoretical results on various properties of the ppp and cppp."
"10.1198/016214506000000357","2006","Likelihood subgradient densities","0","We introduce likelihood subgradient densities and explore their basic properties. Using mixtures of likelihood subgradient densities, we propose an approach for constructing tight enveloping functions in the Bayesian context. In the case of normal priors with normal data, the area underneath the resulting enveloping function is bounded above by 2/root pi approximate to 1.128. The approach is extended to k-dimensional models where the corresponding bound is (2/root pi)(k). More generally, our approach should also yield tight enveloping functions for other models in which the data are close to normal. Such models include generalized linear models (e.g., Bayesian Poisson regression and the Bayesian logit model). Simulations based on the approach are performed for two separate models using accept-reject methods."
"10.1198/016214506000000203","2006","Bayes linear calibrated prediction for complex systems","0","A calibration-based approach is developed for predicting the behavior of a physical system that is modeled by a computer simulator. The approach is based on Bayes linear adjustment using both system observations and evaluations of the simulator at parameterizations that appear to give good matches to those observations. This approach can be applied to complex high-dimensional systems with expensive simulators, where a fully Bayesian approach would be impractical. It is illustrated with an example concerning the collapse of the thermohaline circulation (THC) in the Atlantic Ocean."
"10.1198/016214506000000221","2006","A statistical measure of regularity for the study of wind-generated wave field images","0","The study of water waves has generated a wealth of sophisticated modeling developments in applied mathematics. Empirical observation capabilities have created a need for novel data analysis tools. This article is motivated by consideration of wind-generated wave field image data from a wave tank facility. A quantitative measure of wave field regularity is developed. The methodology is based on decomposition of the wave field into simple plane waves (an adaptation of projection pursuit). The percent variance explained as a function of the number of terms in the plane wave representation is used to define a variogram, and regularity of the wave field is assessed in terms of the weighted difference between the observed variogrant and the expected variogram for a completely random field. The proposed regularity measure is illustrated by application to image data from wind-generated waves. The results suggest that the regularity measure is a function of parameters describing the generation of the wave field (wind speed and evolution). An analysis of the statistical behavior of the regularity measure as a function of sample size (image resolution) is carried out. Even though the regularity measure is based on the nonparametric estimation of functions, provided that this estimation is carried out in a consistent fashion, the error in estimation of regularity has a parametric dependence on image resolution."
"10.1198/016214506000000410","2006","Nonparametric two-sample methods for ranked-set sample data","1","Anew collection of procedures is developed for the analysis of two-sample, ranked-set samples, providing an alternative to the Bohn-Wolfe procedure. These procedures split the data based on the ranks in the ranked-set sample and lead to tests for the centers of distributions, confidence intervals, and point estimators. The advantages of the new tests are that they require essentially no assumptions about the mechanism by which rankings are imperfect, that they maintain their level whether rankings are perfect or imperfect, that they lead to generalizations of the Bohn-Wolfe procedure that can be used to increase power in the case of perfect rankings, and that they allow one to analyze both balanced and unbalanced ranked-set samples. A new class of imperfect ranking models is proposed, and the performance of the procedure is investigated under these models. When rankings are random, a theorem is presented which characterizes efficient data splits. Because random rankings are equivalent to iid samples, this theorem applies to a wide class of statistics and has implications for a variety of computationally intensive methods."
"10.1198/016214506000000032","2006","Estimating a unimodal distribution from interval-censored data","1","In this article we consider three nonparametric maximum likelihood estimators based on mixed-case interval-censored data. Apart from the unrestricted estimator, we consider estimators under the assumption that the underlying distribution function of event times is concave or unimodal. Characterizations of the estimates are derived, and algorithms are proposed for their computation. The estimators are shown to be asymptotically consistent, and the benefits of additional constraints are illustrated through simulations. Finally, the estimators are used as an ingredient in a nonparametric comparison of two samples."
"10.1198/016214506000000131","2006","Rank estimation of accelerated lifetime models with dependent censoring","0","Under independent censoring, estimation of the covariate effects in the accelerated lifetime model may be based on censored data rank tests. Similar rank methodology has been developed with bivariate accelerated lifetime models for dependent censoring but uses artificial censoring, which may lead to substantial information loss. We present a new artificial censoring technique using pairwise ranking and establish the asymptotic properties of a pairwise rank estimator. Simulations show that the pairwise approach achieves large reductions in artificial censoring and large efficiency gains over the existing rank estimator. The simulations evidence moderate efficiency gains under independent censoring over a rank estimator that is semiparametric efficient under independent censoring. An AIDS data analysis illustrates the practical utility of the inferential procedures."
"10.1198/016214506000000212","2006","Locally efficient estimation with bivariate right-censored data","0","Estimation of the survival curve for independently right-censored bivariate failure time data is a problem that has been studied extensively over the past 20 years. In this article we propose a new class of estimators for the bivariate survivor function based on locally efficient (LE) estimation. The LE estimator takes bivariate estimators F, and G,, of the distributions of the time variables (T-1, T-2) and the censoring variables (C-1, C-2), and maps them to the resulting estimator LE. If F, and G, are appropriate consistent estimators of F and G, then LE will be nonparametrically efficient (thus the term ""locally efficient""). However, if either F-n or G(n) (but not both) is not a consistent estimator of F or G, then S-LE will still be consistent and asymptotically normally distributed. We propose a locally efficient estimator that uses a consistent, nonparametric estimator for G and allows the user to supply lower-dimensional (semiparametric or parametric) working model for F. Because the estimator that we choose for G is consistent, the resulting LE estimator will always be consistent and asymptotically normal, and our simulation studies have indicated that using a lower-dimensional model for F gives excellent small-sample performance. In addition, our algorithm for calculation of the efficient influence curve at true distributions for F and G computes the efficiency bound for the model that can be used to calculate relative efficiencies for any bivariate estimator. In this article we introduce the LE estimator for bivariate right-censored data, present an asymptotic result, present the results of simulation studies, and perform a brief data analysis illustrating the use of the LE estimator."
"10.1198/016214506000000348","2006","Geoadditive survival models","3","Survival data often contain small-area geographical or spatial information, such as the residence of individuals. In many cases, the impact of such spatial effects on hazard rates is of considerable substantive interest. Therefore, extensions of known survival or hazard rate models to spatial models have been suggested. Mostly, a spatial component is added to the usual linear predictor of the Cox model. In this article flexible continuous-time geoaddifive models are proposed, extending the Cox model with respect to several aspects often needed in applications. The common linear predictor is generalized to an additive predictor, including nonparametric components for the log-baseline hazard, time-varying effects, and possibly nonlinear effects of continuous covariates or further time scales, and a spatial component for geographical effects. In addition, uncorrelated frailty effects or nonlinear two-way interactions can be incorporated. Inference is developed within a unified fully Bayesian framework. Penalized regression splines and Markov random fields are suggested as basic building blocks, and geostatistical (kriging) models are also considered. Posterior analysis uses computationally efficient Markov chain Monte Carlo sampling schemes. Smoothing parameters are an integral part of the model and are estimated automatically. Propriety of posteriors is shown under fairly general conditions, and practical performance is investigated through simulation studies. Our approach is applied to data from a case study in London and Essex that aims to estimate the effect of area of residence and further covariates on waiting times to coronary artery bypass grafting. Results provide clear evidence of nonlinear time-varying effects, and considerable spatial variability of waiting times to bypass grafting."
"10.1198/016214506000000393","2006","Estimation in linear models based on observations with unknown and possibly unequal scaling","0","We consider k groups of observations X-11...,X(1n)1..,X-kl...,X-knk and unknown scalars lambda l,...,lambda k, and we assume that the distribution of the scaled observations X-11/ lambda(1),...,X-ln1/lambda(1),...,X-k1/lambda X-k,...,(knk)/lambda(k) follows a normal linear model on Rnl+...+nk. This general setup includes several interesting models that have appeared in the literature in different contexts and fields of application. The simplest example is the model of equal coefficients of variation in k normal samples. We give, in the general setup, a necessary and sufficient condition in terms of ""degrees of freedom"" for when the maximum likelihood estimator (MLE) exists and is unique with probability 1, and furthermore, we give an algorithm for obtaining the MLE."
"10.1198/016214506000000258","2006","Statistical inference for the difference between the best treatment mean and a control mean","0","In many experiments, researchers are interested in comparing several treatment means with a control mean. When there are some treatments significantly better than the control, it is often of interest to evaluate the difference between the best treatment mean and the control mean and to identify the best treatment. In this article we derive lower confidence bounds for the aforementioned difference for the case that treatments are at least as effective as the control and for the case that no restriction is placed on the treatment means and the control mean. The evaluation of the lower confidence bound for the difference between the best treatment mean and the control mean is a concave programming problem subject to homogeneous linear inequality constraints. We propose two efficient computation algorithms and discuss the connection between our procedures and Gupta's subset selection procedure. We compare the expected lower confidence bounds of the two procedures with that of Dunnett's procedure. An application to a real-life data is included."
"10.1198/016214506000000401","2006","On lower tolerance limits with accurate coverage probabilities for the normal random effects model","0","A method for constructing accurate lower tolerance limits for the balanced one-way normal random-effects model is derived by conditioning on an estimator of the unknown expected mean square ratio. Simulation studies indicate that the present procedure is less conservative than that of several existing methods and gives more accurate coverage rates and smaller standard deviations. Numerical examples are given to illustrate the use of the new procedure. Tables needed to implement the procedure are also included."
"10.1198/016214505000001384","2006","Optimal and efficient crossover designs when subject effects are random","1","Most studies on optimal crossover designs are based on models that assume subject effects to be fixed effects. In this article we identify and study optimal and efficient designs for a model with random subject effects. With the number of periods not exceeding the number of treatments, we find that totally balanced designs are universally optimal for treatment effects in a large subclass of competing designs. However, in the entire class of designs, totally balanced designs are in general not optimal, and their efficiency depends on the ratio of the subject effects variance and the error variance. We develop tools to study the efficiency of totally balanced designs and to identify designs with higher efficiency."
"10.1198/016214506000000195","2006","Estimation in multiple-frame surveys","1","Multiple-frame surveys are commonly used to decrease costs of sampling or to reduce undercoverage that could occur if only one sampling frame were used. We describe potential uses and examples of multiple-frame surveys. We then derive optimal linear estimators and pseudomaximum likelihood estimators for the population total when samples are taken independently from each frame using probability sampling designs. We explore the properties of these estimators theoretically and through a simulation study. We also derive variance estimators and discuss some practical problems that may be encountered in multiple-frame surveys."
"10.1198/016214506000000186","2006","Functional variance processes","0","We introduce the notion of a functional variance process to quantify variation in functional data. The functional data are modeled as samples of smooth random trajectories observed under additive noise. The noise is assumed to be composed of white noise and a smooth random process-the functional variance process-which gives rise to smooth random trajectories of variance. The functional variance process is a tool for analyzing stochastic time trends in noise variance. As a smooth random process, it can be characterized by the eigenfunctions and eigenvalues of its autocovariance operator. We develop methods to estimate these characteristics from the data, applying concepts from functional data analysis to the residuals obtained after an initial smoothing step. Asymptotic justifications for the proposed estimates are provided. The proposed functional variance process extends the concept of a variance function, an established tool in nonparametric and semiparametric regression analysis, to the case of functional data. We demonstrate that functional variance processes offer a novel data analysis technique that leads to relevant findings in applications, ranging from a seismic discrimination problem to the analysis of noisy reproductive trajectories in evolutionary biology."
"10.1198/016214506000000726","2006","Rejoinder","0",""
"10.1198/016214506000000672","2006","Quantile autoregression","3","We consider quantile autoregression (QAR) models in which the autoregressive coefficients can be expressed as monotone functions of a single, scalar random variable. The models can capture systematic influences of conditioning variables on the location, scale, and shape of the conditional distribution of the response, and thus constitute a significant extension of classical constant coefficient linear time series models in which the effect of conditioning is confined to a location shift. The models may be interpreted as a special case of the general random-coefficient autoregression model with strongly dependent coefficients. Statistical properties of the proposed model and associated estimators are, studied. The limiting distributions of the autoregression quantile process are derived. QAR inference methods are also investigated. Empirical applications of the model to the U.S. unemployment rate, short-term interest rate, and gasoline prices highlight the model's potential."
"10.1198/016214506000000456","2006","Calibrated probabilistic forecasting at the stateline wind energy center: the regime-switching space-time method","4","With the global proliferation of wind power, the need for accurate short-term forecasts of wind resources at wind energy sites is becoming paramount. Regime-switching space-time (RST) models merge meteorological and statistical expertise to obtain accurate and calibrated, fully probabilistic forecasts of wind speed and wind power. The model formulation is parsimonious, yet takes into account all of the salient features of wind speed: alternating atmospheric regimes, temporal and spatial correlation, diurnal and seasonal nonstationarity, conditional heteroscedasticity, and non-Gaussianity. The RST method identifies forecast regimes at a wind energy site and fits a conditional predictive model for each regime. Geographically dispersed meteorological observations in the vicinity of the wind farm are used as off-site predictors. The RST technique was applied to 2-hour-ahead forecasts of hourly average wind speed near the Stateline wind energy center in the U.S. Pacific Northwest. The RST point forecasts and distributional forecasts were accurate, calibrated, and sharp, and they compared favorably with predictions based on state-of-the-art time series techniques. This suggests that quality meteorological data from sites upwind of wind farms can be efficiently used to improve short-term forecasts of wind resources."
"10.1198/016214506000000438","2006","Trees for correlated survival data by goodness of split, with applications to tooth prognosis","0","In this article the regression tree method is extended to correlated survival data and applied to the problem of developing objective prognostic classification rules in periodontal research. The robust logrank statistic is used as the splitting statistic to measure the between-node difference in survival, while adjusting for correlation among failure times from the same patient. The partition-based survival function estimator is shown to converge to the true conditional survival function. Tooth loss data from 100 periodontal patients (2,509 teeth) was analyzed using the proposed method. The goal is to assign each tooth to one of the five prognosis categories (good, fair, poor, questionable, or hopeless). After the best-sized tree was identified, an amalgamation procedure was used to form five prognostic groups. The prognostic rules established here may be used by periodontists, general dentists, and insurance companies in devising appropriate treatment plans for periodontal patients."
"10.1198/016214506000000230","2006","Investigating heterogeneity in pneumococcal transmission: a {B}ayesian {MCMC} approach applied to a follow-up of schools","0","The analysis of communicable agent transmission from field data is typically hampered by missing data, dependence between individual trajectories, and sometimes by heterogeneity among competing pathogens. Methods based on data augmentation and Markov chain Monte Carlo sampling have been used to analyze such data in small communities (typically households), with little diversity in pathogens. In this article the approach is extended to, analyze the transmission of 15 Streptococcus pneumoniae serotypes in schoolchildren, where hundreds of individual trajectories interact and a substantial portion of trajectories are unobserved. For each child, the data were augmented to describe the detailed time course of S. pneumoniae carriage. The Bayesian hierarchical model ensured consistency between observed and augmented data; described the latent dynamics of S. pneumoniae acquisition and clearance; and specified priors. To investigate heterogeneity among serotypes, a clustering step was introduced to select a parsimonious description of transmission characteristics. The joint posterior distribution of parameters, augmented data, and clusters of serotypes was explored by reversible-jump MCMC sampling. The approach made it possible to make inferences simultaneously on the number of clusters of serotypes and on the transmission characteristics of each cluster."
"10.1198/016214505000001429","2006","Bayesian inference for a two-part hierarchical model: an application to profiling providers in managed health care","0","Profiling is currently an important, and hotly debated, topic in health care and other industries looking for ways to control costs, increase profitability, and increase service quality. Managed care in particular has seen a proliferation in the use of statistical profiling methodology, particularly with regard to monitoring expenditure data. This article focuses on the specific problem of developing statistical methods appropriate for profiling physician contributions to patient pharmacy expenditures incurred in a managed care setting. The two-part hierarchical model with a correlated random-effects structure considered here accounts for both the skewed, zero-inflated nature of pharmacy expenditure data and the fact that patient pharmacy expenditures are correlated within physicians. The random-effects structure has an attractive interpretation in terms of a conceptual model for physician prescribing patterns. Using this model, we propose to rank physicians based on an appropriately constructed provider-level performance measure. This information is subsequently used to develop a novel financial incentive scheme. Inference is conducted in a Bayesian framework using Markov chain Monte Carlo."
"10.1198/016214505000001375","2006","Multiple imputation of missing income data in the {N}ational {H}ealth interview survey","1","The National Health Interview Survey (NHIS) provides a rich source of data for studying relationships between income and health and for monitoring health and health care for persons at different income levels. However, the nonresponse rates are high for two key items, total family income in the previous calendar year and personal earnings from employment in the previous calendar year. To handle the missing data on family income and personal earnings in the NHIS, multiple imputation of these items, along with employment status and ratio of family income to the federal poverty threshold (derived from the imputed values of family income), has been performed for the survey years 1997-2004. (There are plans to continue this work for years beyond 2004 as well.) Files of the imputed values, as well as documentation, are available at the NHIS website (http://www.cdc.gov/nchs/nhis.htm). This article describes the approach used in the multiple-imputation project and evaluates the methods through analyses of the multiply imputed data. The analyses suggest that imputation corrects for biases that occur in estimates based on the data without imputation, and that multiple imputation results in gains in efficiency as well."
"10.1198/016214506000000104","2006","A hierarchical multivariate two-part model for profiling providers' effects on health care charges","0","Procedures for analyzing and comparing health care providers' effects on health services delivery and outcomes have been referred to as provider profiling. In a typical profiling procedure, patient-level responses are measured for clusters of patients treated by providers that in turn can be considered statistically exchangeable. Thus a hierarchical model naturally represents the structure of the data. When provider effects on multiple responses are profiled, a multivariate model rather than a series of univariate models can capture associations among responses at both the provider and patient levels. When responses are in the form of charges for health care services and sampled patients include nonusers of services, charge variables are a mix of 0's and highly skewed positive values that present a modeling challenge. For analysis of covariate effects on charges for a single service, a frequently used approach is a two-part model that combines logistic or probit regression on any use of the service and linear regression on log-positive charges given use of the service. Here we extend the two-part model to the case of charges for multiple services, using a log-linear model and a general multivariate lognormal model, and use the resultant multivariate two-part model as the within-provider component of a hierarchical model. The log-linear likelihood is reparameterized as proposed by Fitzmaurice and Laird, so that covariate effects on any use of each service are marginal with respect to any use of other services. The general multivariate lognormal likelihood is structured in such a way that the variance of log-positive charges for each service is provider-specific but correlations among logs of positive charges for different services are uniform across providers. A data augmentation step is included in the Gibbs sampler used to fit the hierarchical model to accommodate the fact that values of log-positive charges are undefined for unused services. We apply this hierarchical, multivariate, two-part model to analyze the effects of primary care physicians on their patients' annual charges for two services, primary care and specialty care. We also demonstrate an approach for incorporating prior information about the effects of patient morbidity on response variables, to improve the accuracy of provider profiles based on patient samples of limited size."
"10.1198/016214506000000447","2006","Evaluating kindergarten retention policy: a case study of causal inference for multilevel observational data","0","This article considers the policy of retaining low-achieving children in kindergarten rather than promoting them to first grade. Under the stable unit treatment value assumption (SUTVA) as articulated by Rubin, each child at risk of retention has two potential outcomes: Y(1) if retained and Y(0) if promoted. But SUTVA is questionable, because a child's potential outcomes will plausibly depend on which school that child attends and also on treatment assignments of other children. We develop a causal model that allows school assignment and peer treatments to affect potential outcomes. We impose an identifying assumption that peer effects can be summarized through a scalar function of the vector of treatment assignments in a school. Using a large, nationally representative sample, we then estimate (1) the effect of being retained in kindergarten rather than being promoted to the first grade in schools having a low retention rate, (2) the retention effect in schools having a high retention rate, and (3) the effect of being promoted in a low-retention school as compared to being promoted in a high-retention school. This third effect is not definable under SUTVA. We use multilevel propensity score stratification to approximate a two-stage experiment. At the first stage, intact schools are blocked on covariates and then, within blocks, randomly assigned to a policy of retaining comparatively more or fewer children in kindergarten. At the second stage, ""at-risk"" students within schools are blocked on covariates and then assigned at random to be retained. We find evidence that retainees learned less on average than did similar children who were promoted, a result found in both high-retention and low-retention schools. We do not detect a peer treatment effect on low-risk students."
"10.1198/016214505000001258","2006","Randomization inference with natural experiments: an analysis of ballot effects in the 2003 {C}alifornia recall election","0","Since the 2000 U.S. Presidential election, social scientists have rediscovered a long tradition of research examining the effects of ballot format on voting. Using a new dataset collected by The New York Times, we investigate the causal effect of being listed on the first ballot page in the 2003 California gubernatorial recall election. California law mandates a unique randomization procedure of ballot order that, when appropriately modeled, can be used to approximate a classical randomized experiment in a real world setting. We apply randomization inference based on Fisher's exact test, which directly incorporates the exact randomization procedure and yields accurate nonparametric confidence intervals. Our results suggest that being listed on the first ballot page causes a statistically significant increase in vote shares for more than 40% of the minor candidates, whereas there is no significant effect for the top two candidates. We also investigate how randomization inference differs from conventional estimators that do not fully incorporate California's complex treatment assignment mechanism. The results indicate appreciable differences between the two approaches."
"10.1198/016214506000000168","2006","Testing for racial profiling in traffic stops from behind a veil of darkness","1","The key problem in testing for racial profiling in traffic stops is estimating the risk set, or ""benchmark,"" against which to compare the race distribution of stopped drivers. To date, the two most common approaches have been to use residential population data or to conduct traffic surveys in which observers tally the race distribution of drivers at a certain location. It is widely recognized that residential population data provide poor estimates of the population at risk of a traffic stop; at the same time, traffic surveys have limitations and are more costly to carry out than the alternative that we propose herein. In this article we propose a test for racial profiling that does not require explicit, external estimates of the risk set. Rather, our approach makes use of what we call the ""veil of darkness"" hypothesis, which asserts that police are less likely to know the race of a motorist before making a stop after dark than they are during daylight. If we assume that racial differences in traffic patterns, driving behavior, and exposure to law enforcement do not vary between daylight and darkness, then we can test for racial profiling by comparing the race distribution of stops made during daylight to the race distribution of stops made after dark. We propose a means of weakening this assumption by restricting the sample to stops made during the evening hours and controlling for clock time while estimating daylight/darkness contrasts in the race distribution of stopped drivers. We provide conditions under which our estimates are robust to a substantial nonreporting problem present in our data and in many other studies of racial profiling. We propose an approach to assess the sensitivity of our results to departures from our maintained assumptions. Finally, we apply our method to data from Oakland, California and find that in this example the,data yield little evidence of racial profiling in traffic stops."
"10.1198/016214506000000177","2006","Recidivism and social interactions","0","Using a national sample, this article identifies the risk factors for recidivism among female, male, black, white, and Hispanic felony probationers. The individual hazard function is assumed to depend on individual and neighborhood characteristics, as well as on social interactions among probationers. In selecting the covariates from a set of potential candidates, Bayesian model averaging is used to account for both model uncertainty and the subsequent inference. The results point to social interactions as one of the most significant factors affecting recidivism among all gender, ethnicity, and race groups. When a frailty parameter is added to account for the possibility of unobserved risk factors shared by probationers within neighborhoods, the empirical results remain robust indicating negligible unobserved neighborhood-level heterogeneity."
"10.1198/016214505000001311","2006","Large-sample joint posterior approximations when full conditionals are approximately normal: application to generalized linear mixed models","0","Modern Bayesian statistical methods, such as Gibbs and Metropolis-Hastings sampling, were developed to liberate statisticians from the necessity of making large-sample assumptions and to facilitate the numerical approximation of problems that had previously been analytically intractable. Counter to this trend, we develop a method for constructing asymptotic joint posterior approximations based on models with k blocks of parameters and where the corresponding properly normalized full conditionals are themselves asymptotically normal. We illustrate these techniques by applying them to particular linear and generalized linear mixed models (GLMMs). We also consider the relevance of different parameterizations with regard to our asymptotics. Recent work has indicated that Gibbs samplers based on so-called ""centering parameterizations"" result in better convergence properties for the resulting Markov chains. Our results for the one-way random-effects model shed some light on this issue. For this example, we also consider the distinction between letting the within-group sample size, n, tend to infinity versus letting the number of groups K (as defined by the random-effects part of the model) tend to infinity. Letting n grow results in a proper limiting normal distribution only when the weight on the prior for the variance component grows at a rate comparable to n. With large K, on the other hand, proper limits are obtained without this assumption, and thus it is seen that the information in the data will ultimately swamp standard prior information. We compare results based on simulated data when n and K are large. A dataset involving the effect of smoking on hormone function is analyzed using our asymptotics and compared with results based on Gibbs sampling."
"10.1198/016214505000001267","2006","On convergence and bias correction of a joint estimation algorithm for multiple sinusoidal frequencies","0","Twenty years ago, Kay proposed an iterative filtering algorithm (IFA) for jointly estimating the frequencies of multiple complex sinusoids from noisy observations. IFA is based on the fact that the noiseless signal is an autoregressive (AR) process, so the frequency estimation problem can be reformulated as the problem of estimating the AR coefficients. By iterating the cycle of AR coefficient estimation and AR filtering, IFA provides a computationally simple procedure yet capable of accurate frequency estimation especially at low signal-to-noise ratio (SNR). However, the convergence of IFA has not been established beyond simulation and a very special case of a single frequency and infinite sample size. This article provides a statistical analysis of the algorithm and makes several important contributions. It shows that the poles of the AR filter must be reduced by an extra shrinkage parameter to accommodate poor initial values and avoid being trapped into false solutions. It also shows that the AR estimates in each iteration must be bias-corrected to produce a more accurate frequency estimator; a closed-form expression is provided for bias correction. Finally, it shows that for a sufficiently large sample size, the resulting algorithm, called new IFA (NIFA), converges to the desired fixed point, which constitutes a consistent frequency estimator. Numerical examples, including a real data example in radar applications, are provided to demonstrate the findings. It is shown in particular that the shrinkage parameter not only controls the estimation accuracy, but also determines the requirements of initial values. It is also shown that the proposed bias-correction method considerably improves the estimation accuracy, especially for high SNR."
"10.1198/016214505000001212","2006","A constructive representation of univariate skewed distributions","0","We introduce a general perspective on the introduction of skewness into symmetric distributions. Through inverse probability integral transformations we provide a constructive representation of skewed distributions, where the skewing mechanism and the original symmetric distributions are specified separately. We study the effects of the skewing mechanism on, e.g., modality, tail behavior and the amount of skewness generated. The representation is used to introduce novel classes of skewed distributions, where we induce certain prespecified characteristics through particular choices of the skewing mechanism. Finally, we use a Bayesian linear regression framework to compare the new classes with some existing distributions in the context of two empirical examples."
"10.1198/073500105000000243","2006","A class of latent marginal models for capture-recapture data with continuous covariates","0","We introduce a new family of latent class models for the analysis of capture-recapture data where continuous covariates are available. The present approach exploits recent advances in marginal parameterizations to model simultaneously, and conditionally on individual covariates, the size of the latent classes, the marginal probabilities of being captured by each list given the latent, and possible higher-order marginal interactions among lists conditionally on the latent. An EM algorithm for maximum likelihood estimation is described, and an expression for the expected information matrix is derived. In addition, a new method for computing confidence intervals for the size of the population having given covariate configurations is proposed and its asymptotic properties are derived. Applications to data on patients with human immunodeficiency virus, in the region of Veneto, Italy, and to new cases of cancer in Tuscany are discussed."
"10.1198/016214505000001023","2006","Bayesian sample size determination for case-control studies","0","Case-control studies are among the most commonly used means of assessing association between exposure and outcome. Sample size determination and the optimal control-to-case ratio are vital to the design of such studies. In this article we investigate Bayesian sample size determination and the control-to-case ratio for case-control studies, when interval estimation is the goal of the eventual statistical analysis. In certain cases we are able to derive approximate closed-form sample size formulas. We also describe two Monte Carlo methods, each of which provides a unified approach to the sample size problem, because they may be applied to a wide range of interval-based criteria. We compare the accuracy of the different methods. We also extend our methods to include cross-sectional designs and designs for gene-environment interaction studies."
"10.1198/016214505000001087","2006","Optimal designs for dose-response models with restricted design spaces","4","In close-response studies, the dose range is often restricted because of concerns over drug toxicity and/or efficacy. We derive optimal designs for estimating the underlying dose-response curve for a restricted or unrestricted dose range with respect to a broad class of optimality criteria. The underlying curve belongs to a diversified set of link functions suitable for the dose-response studies and having a common canonical form. These include the fundamental binary response models-the logit and the probit, as well as the skewed versions of these models. Our methodology is based on a new geometric interpretation of optimal designs with respect to Kiefer's Phi(p) criteria in regression models with two parameters, which is of independent interest. It provides an intuitive illustration of the number and locations of the support points of Phi(p)-optimal designs. Moreover, the geometric results generalize the classical characterization of D-optimal designs by the minimum covering ellipsoid to the class of Kiefer's Phi(p) criteria. The results are illustrated through the redesign of a dose ranging trial."
"10.1198/016214505000001014","2006","On {$\delta$}-equivalence with the best in {$k$}-sample models","0","In recent work we introduced a general, a weak, and a strong partitioning principles for the construction of multiple decision procedures as multiple testing or selection procedures. Partitioning principles can be viewed as natural extensions of the closure principle and sometimes yield more powerful decision procedures. In this article we consider the problem of establishing equivalence with the best with respect to k treatment means, where equivalence is defined in terms of a threshold value delta > 0. We reformulate the original selection problem as a multiple testing problem and develop various step-down and step-up procedures by applying the closure principle and partitioning principles. The new step-down procedure is shown to provide a uniform improvement over procedures currently in use. Moreover, we propose some projection methods that yield confidence intervals being compatible with stepwise tests and selection procedures."
"10.1198/016214505000001276","2006","Testing for covariate effects in the fully nonparametric analysis of covariance model","0","Traditional inference questions in the analysis of covariance mainly focus on comparing different factor levels by adjusting for the continuous covariates, which are believed to also exert a significant effect on the outcome variable. Testing hypotheses about the covariate effects, although of substantial interest in many applications, has received relatively limited study in the semiparametric/nonparametric setting. In the context of the fully nonparametric analysis of covariance model of Akritas et al., we propose methods to test for covariate main effects and covariate-factor interaction effects. The idea underlying the proposed procedures is that covariates can be thought of as factors with many levels. The test statistics are closely related to some recent developments in the asymptotic theory for analysis of variance when the number of factor levels is large. The limiting normal distributions are established under the null hypotheses and local alternatives by asymptotically approximating a new class of quadratic forms. The test statistics bear similar forms to the classical F-test statistics and thus are convenient for computation. We demonstrate the methods and their properties on simulated and real data."
"10.1198/016214505000001122","2006","Semiparametric transformation models for survival data with a cure fraction","5","We propose a class of transformation models for survival data with a cure fraction. The class of transformation models is motivated by biological considerations and includes both the proportional hazards and the proportional odds cure models as two special cases. An efficient recursive algorithm is proposed to calculate the maximum likelihood estimators (MLEs). Furthermore, the MLEs for the regression coefficients are shown to be consistent and asymptotically normal, and their asymptotic variances attain the semiparametric efficiency bound. Simulation studies are conducted to examine the finite-sample properties of the proposed estimators. The method is illustrated on data from a clinical trial involving the treatment of melanoma."
"10.1198/016214505000000871","2006","Inference in semiparametric dynamic models for binary longitudinal data","0","This article deals with the analysis of a hierarchical sermparametric model for dynamic binary longitudinal responses. The main complicating components of the model are an unknown covariate function and serial correlation in the errors. Existing estimation methods for models with these features are of O(N-3), where N is the total number of observations in the sample. Therefore, nonparametric estimation is largely infeasible when the sample size is large, as in typical in the longitudinal setting. Here we propose a new O(N) Markov chain Monte Carlo based algorithm for estimation of the nonparametric function when the errors are correlated, thus contributing to the growing literature on semiparametric and nonparametric mixed-effects models for binary data. In addition, we address the problem of model choice to enable the formal comparison of our semiparametric model with competing parametric and semiparametric specifications. The performance of the methods is illustrated with detailed studies involving simulated and real data."
"10.1198/016214505000001131","2006","Outlier detection in multivariate time series by projection pursuit","0","In this article we use projection pursuit methods to develop a procedure for detecting outliers in a multivariate time series. We show that testing for outliers in some projection directions can be more powerful than testing the multivariate series directly. The optimal directions for detecting outliers are found by numerical optimization of the kurtosis coefficient of the projected series. We propose an iterative procedure to detect and handle multiple outliers based on a univariate search in these optimal directions. In contrast with the existing methods, the proposed procedure can identify outliers without prespecifying a vector ARMA model for the data. The good performance of the proposed method is illustrated in a Monte Carlo study and in a real data analysis."
"10.1198/016214505000001320","2006","Bounded-influence robust estimation in generalized linear latent variable models","0","Latent variable models are used for analyzing multivariate data. Recently, generalized linear latent variable models for categorical, metric, and mixed-type responses estimated via maximum likelihood (ML) have been proposed. Model deviations, such as data contamination, are shown analytically, using the influence function and through a simulation study, to seriously affect ML estimation. This article proposes a robust estimator that is made consistent using the basic principle of indirect inference and can be easily numerically implemented. The performance of the robust estimator is significantly better than that of the ML estimators in terms of both bias and variance. A real example from a consumption survey is used to highlight the consequences in practice of the choice of the estimator."
"10.1198/016214505000001140","2006","On the large-sample minimal coverage probability of confidence intervals after model selection","2","We give a large-sample analysis of the minimal coverage probability of the usual confidence intervals for regression parameters when the underlying model is chosen by a ""conservative"" (or ""overconsistent"") model selection procedure. We derive an upper bound for the large-sample limit minimal coverage probability of such intervals that applies to a large class of model selection procedures including the Akaike information criterion as well as various pretesting procedures. This upper bound can be used as a safeguard to identify situations where the actual coverage probability can be far below the nominal level. We illustrate that the (asymptotic) upper bound can be statistically meaningful even in rather small samples."
"10.1198/016214505000001186","2006","Semiparametric normal transformation models for spatially correlated survival data","4","There is an emerging interest in modeling spatially correlated survival data in biomedical and epidemiologic studies. In this article we propose a new class of semiparametric normal transformation models for right-censored spatially correlated survival data. This class of models assumes that survival outcomes marginally follow a Cox proportional hazard model with unspecified baseline hazard, and their joint distribution is obtained by transforming survival outcomes to normal random variables, whose joint distribution is assumed to be multivariate normal with a spatial correlation structure. A key feature of the class of semiparametric normal transformation models is that it provides a rich class of spatial survival models where regression coefficients have population average interpretation and the spatial dependence of survival times is conveniently modeled using the transformed variables by flexible normal random fields. We study the relationship of the spatial correlation structure of the transformed normal variables and the dependence measures of the original survival times. Direct nonparametric maximum likelihood estimation in such models is practically prohibited due to the high-dimensional intractable integration of the likelihood function and the infinite-dimensional nuisance baseline hazard parameter. We hence develop a class of spatial semiparametric estimating equations, which conveniently estimate the population-level regression coefficients and the dependence parameters simultaneously. We study the asymptotic properties of the proposed estimators and show that they are consistent and asymptotically normal. The proposed method is illustrated with an analysis of data from the East Boston Asthma Study, and its performance is evaluated using simulations."
"10.1198/016214505000001230","2006","Random forests and adaptive nearest neighbors","1","In this article we study random forests through their connection with a new framework of adaptive nearest-neighbor methods. We introduce a concept of potential nearest neighbors (k-PNNs) and show that random forests can be viewed as adaptively weighted k-PNN methods. Various aspects of random forests can be studied from this perspective. We study the effect of terminal node sizes on the prediction accuracy of random forests. We further show that random forests with adaptive splitting schemes assign weights to k-PNNs in a desirable way: for the estimation at a given target point, these random forests assign voting weights to the k-PNNs of the target point according to the local importance of different input variables. We propose a new simple splitting scheme that achieves desirable adaptivity in a straightforward fashion. This simple scheme can be combined with existing algorithms. The resulting algorithm is computationally faster and gives comparable results. Other possible aspects of random forests, such as using linear combinations in splitting, are also discussed. Simulations and real datasets are used to illustrate the results."
"10.1198/016214505000001041","2006","Incorporating additional information to normal linear discriminant rules","0","The most useful and broadly known rule in the classical two-group linear normal discriminant analysis is Anderson's rule. In this article we propose some alternative procedures that prove useful when prior constraints on the mean vectors are known. These rules are based on new estimators of the difference of means. We prove under mild conditions that the new rules perform better when the common covariance matrix is known. Simulated experiments show that the misclassification errors are lower for the restricted rules defined here in the general case of an unknown covariance matrix. The prior constraints on the mean vector restrict the parameter space to a cone. A family of estimators indexed by a parameter gamma, with 0 <= gamma <= 1, is defined using an iterative procedure in such a way that the estimator with a higher value for gamma takes values closer to the center of the cone with a greater probability. When gamma = 0, the restricted maximum likelihood estimator is given, although the most interesting rule from a theoretical and practical standpoint is obtained when the estimator chosen is given by gamma = 1. The usefulness of the proposed rules with real data is demonstrated by their application to two medical examples, the first dealing with heart attack patients and the second dealing with a diabetes dataset. In the former case, restrictions among surviving and nonsurviving patients are used; in the latter, the restrictions arise from differences between the healthy and diabetic populations."
"10.1198/016214505000001177","2006","Bent-cable regression theory and applications","0","We use the so-called ""bent-cable"" model to describe natural phenomena that exhibit a potentially sharp change in slope. The model comprises two linear segments, joined smoothly by a quadratic bend. The class of bent cables includes, as a limiting case, the popular piecewise-linear model (with a sharp kink), otherwise known as the broken stick. Associated with bent-cable regression is the estimation of the bend-width parameter, through which the abruptness of the underlying transition may be assessed. We present worked examples and simulations to demonstrate the regularity and irregularity of bent-cable regression encountered in finite-sample settings. We also extend existing bent-cable asymptotics that previously were limited to the basic model with known linear slopes of 0 and 1. Practical conditions on the design are given to ensure regularity of the full bent-cable estimation problem if the underlying bend segment has nonzero width. Under such conditions, the least-squares estimators are shown to be consistent and to asymptotically follow a multivariate normal distribution. Furthermore, the deviance statistic (or the likelihood ratio statistic, if the random errors are normally distributed) is shown to have an asymptotic chi-squared distribution."
"10.1198/016214505000001050","2006","Goodness-of-fit tests for linear and nonlinear time series models","0","In this article we study a general class of goodness-of-fit tests for a parametric conditional mean of a linear or nonlinear time series model. Among the properties of the proposed tests are that they are suitable when the conditioning set is infinite-dimensional; that they are consistent against a broad class of alternatives, including Pitman's local alternatives converging at the parametric rate n(-1/2), with n the sample size; and that they do not need to choose a lag order depending on the sample size or to smooth the data. It turns out that the asymptotic null distributions of the tests depend on the data generating process, so a new bootstrap procedure is proposed and theoretically justified. The proposed bootstrap tests are robust to higher-order dependence, particularly to conditional heteroscedasticity of unknown form. A simulation study compares the finite-sample performance of the proposed and competing tests and shows that our tests can play a valuable role in time series modeling. Finally, an application to an economic price series highlights the merits of our approach."
"10.1198/016214505000001032","2006","Bootstrap approximations in model checks for binary data","0","Consider a binary regression model in which the conditional expectation of a binary variable given an explanatory variable belongs to a parametric family. To check whether a sequence of independent and identically distributed observations belongs to such a parametric family, we use Kolmogorov-Smirnov and Cramer-von Mises type tests based on a marked empirical process introduced by Stute. We propose and study a new resampling scheme for a bootstrap in this setup to approximate critical values for these tests. We also apply this approach to simulated and real data. In the latter case we check some parametric models that are used to analyze right-censored lifetime data under a semiparametric random censorship model."
"10.1198/016214505000000880","2006","Analysis of failure time data arising from studies with alternating treatment schedules","0","We develop statistical methods for designing and analyzing studies in which treatments are deliberately interrupted and reinitiated, but where interest lies in making inferences about continuous treatment use. We refer to such designs as alternating designs, because subjects alternate between periods in which they are taking the treatment of interest and periods when they are not. Our goals are to deter-mine how to estimate the distribution of time to an event if the treatment were given continuously, to compare the distributions of two such continuously given treatments, and to assess the effects of covariates on the distribution of a continuously given treatment. We examine a nonparametric estimator of the cumulative hazard function for continuous treatment using data from an alternating design and show it to be uniformly consistent and asymptotically normal under certain conditions relating to the effects of interrupting the treatment. We then introduce nonparametric tests for comparing the distributions corresponding to two such continuously given treatments and derive their asymptotic properties under general alternatives to the null and under various conditions related to the interruption of treatment. We compare the properties of the alternating treatment design and the classical parallel group design and present results from a simulation study that assesses the size and power of the test procedures introduced. Finally, we examine partial likelihood methods for assessing the effects of covariates and continuous treatment on time until an event under a proportional hazards model. We illustrate the proposed methods using the results from a recent study in which subjects alternate between taking an active drug and placebo on an annual basis."
"10.1198/016214505000000781","2006","Multicategory {$\psi$}-learning","5","In binary classification, margin-based techniques usually deliver high performance. As a result, a multicategory problem is often treated as a sequence of binary classifications. In the absence of a dominating class, this treatment may be suboptimal and may yield poor performance, such as for support vector machines (SVMs). We propose a novel multicategory generalization of psi-learning that treats all classes simultaneously. The new generalization eliminates this potential problem while at the same time retaining the desirable properties of its binary counterpart. We develop a statistical learning theory for the proposed methodology and obtain fast convergence rates for both linear and nonlinear learning examples. We demonstrate the operational characteristics of this method through a simulation. Our results indicate that the proposed methodology can deliver accurate class prediction and is more robust against extreme observations than its SVM counterpart."
"10.1198/016214505000001294","2006","Advanced distribution theory for {S}i{Z}er","2","SiZer is a powerful method for exploratory data analysis. In this article approximations to the distributions underlying the simultaneous statistical inference are investigated, and large improvements are made in the approximation using extreme value theory. This results in improved size, and also in an improved global inference version of SiZer. The main points are illustrated with real data and simulated examples."
"10.1198/016214505000001249","2006","Forecasting cause-age specific mortality using two random processes","1","Mortality forecasts are critical information for assessing the health of a population and are necessary for making informed decisions about how best to direct health-related resources and activities. Timeliness in making health statistics available is crucial to identify and address current health problems. Being motivated to meet these needs, we propose a method to forecast the number of cause-age specific deaths through a two random processes model. Unlike the previous methods, the new method incorporates both cross-sectional and longitudinal correlations into our model without a high-dimensional problem. A bootstrap confidence interval is presented to measure the validity of our model and to detect an unusual occurrence of deaths. Our data analysis demonstrates that our method gives promising results compared with the true final counts."
"10.1198/016214505000000952","2006","Specifying and implementing nonparametric and semiparametric survival estimators in two-stage (nested) cohort studies with missing case data","0","Since 1986, we have been studying a cohort of individuals from a region in China with epidemic rates of gastric cardia cancer and have conducted numerous two-stage studies to assess the association of various exposures with this cancer. Two-stage studies are a commonly used statistical design. Stage one involves observing the outcomes and accessible baseline covariate information on all cohort members, and stage two involves using the stage one observations to select a subset of the cohort for measurements of exposures that are difficult to obtain. When the outcomes are censored failure times, such as in our studies, the most common designs used are the case-cohort and nested case-control designs. One limitation of both these designs is that the estimators of the cumulative hazards, and hence survivals and absolute risks, are biased when some cases are missing the stage two measurements. In our experience, such missingness is present in virtually all two-stage studies that (like ours) use biological specimens to obtain exposure measurements. In earlier work we derived and characterized the efficiency of a class of nonparametric and a class of semiparametric cumulative hazard estimators that are unbiased regardless of whether or not all cases are measured. In this article we limit the presentation of the mathematical derivation of these two classes to aspects important to study design and analysis. We analyze data from a two-stage study that we conducted on the association of Helicobacter pylori infection with incident gastric cardia cancers. We discuss the substantive reasons why we deliberately sampled only 25% of the available cancer cases. Through simulations, we demonstrate that substantial variation in precision exists between unbiased estimators within each class, and express the origin of these differences in terms of parameters familiar to investigators. We describe how preexistent knowledge about these parameters can be used to increase estimator precision, and detail specific strategies for constructing such estimators. Computer code in R that implements these estimators is available from the authors on request."
"10.1198/016214505000001302","2006","Assessing evidence inconsistency in mixed treatment comparisons","0","Randomized comparisons among several treatments give rise to an incomplete-blocks structure known as mixed treatment comparisons (MTCs). To analyze such data structures, it is crucial to assess whether the disparate evidence sources provide consistent information about the treatment contrasts. In this article we propose a general method for assessing evidence inconsistency in the framework of Bayesian hierarchical models. We begin with the distinction between basic parameters, which have prior distributions, and functional parameters, which are defined in terms of basic parameters. Based on a graphical analysis of MTC structures, evidence inconsistency is defined as a relation between a functional parameter and at least two basic parameters, supported by at least three evidence sources. The inconsistency degrees of freedom (ICDF) is the number of such inconsistencies. We represent evidence consistency as a set of linear relations between effect parameters on the log odds ratio scale, then relax these relations to allow for inconsistency by adding to the model random inconsistency factors (ICFs). The number of ICFs is determined by the ICDF. The overall consistency between evidence sources can be assessed by comparing models with and without ICFs, whereas their posterior distribution reflects the extent of inconsistency in particular evidence cycles. The methods are elucidated using two published datasets, implemented with standard Markov chain Monte Carlo software."
"10.1198/016214505000001221","2006","A {B}ayesian approach for clustered longitudinal ordinal outcome with nonignorable missing data: evaluation of an asthma education program","0","Asthma, a chronic inflammatory disease of the airways, affects an estimated 6.3 million children under age 18 in the United States. A key to successful asthma management, and hence improved quality of life (QOL), calls for an active partnership between asthma patients and their health care providers. To foster this partnership, an intervention program was designed and evaluated using a randomized longitudinal study. The study focused on several outcomes where typically missing data remained a pervasive problem. We suspected that the underlying missing-data mechanism may not be ignorable. Thus here we present a method for analyzing clustered longitudinal data with missing values resulting from a nonignorable missing-data mechanism. Them transition Markov model with random effects was used to investigate changes in ordinal outcomes over time. A Bayesian pattern-mixture model with the flexibility to incorporate models for missing data in both outcome and time-varying covariates was used to model the nonignorable missing-data mechanism. The pattern-mixture model uses easy-to-understand parameters-namely, ratios of the cumulative odds across patterns with the complete-data pattern-as the reference pattern. Sensitivity analysis was performed using different prior distributions for the parameters. A fully Bayesian approach was derived by integrating over a class of prior distributions. The data from the Asthma Intervention Study were analyzed to explore the effect of the intervention program on improving QOL."
"10.1198/016214505000001203","2006","Conditional inference methods for incomplete {P}oisson data with endogenous time-varying covariates: emergency department use among {HIV}-infected women","1","We investigate the effect of protease inhibitors (PIs) on the rate of emergency room (ER) visits among HIV-infected women from a longitudinal cohort study. One strategy for accounting for serial correlation in longitudinal studies is to assume that observations are independent, conditional on unit-specific nuisance parameters. It is possible to estimate these models using unconditional maximum likelihood, where the nuisance parameters are assigned a parametric distribution and integrated out of the likelihood. Alternatively, we can proceed using conditional inference, where we eliminate the nuisance parameters from the likelihood by conditioning on a sufficient statistic for these parameters. An advantage of conditional inference methods over parametric random-effects models is that all patient-level time-invariant factors (both measured and unmeasured) are accounted for in the analysis. A limitation is that standard conditional inference methods assume that missing data are missing completely at random and do not allow endogenous time-varying covariates (i.e., past ER visits cannot predict future PI use). Both assumptions are unlikely to be met for these data, because one would expect ""sicker"" patients would be more likely to receive treatment and/or drop out of the study. We develop new estimation strategies that allow endogenous time-varying covariates and missing-at-random dropouts. The analysis shows that PI use reduces the rate of ER visits in patients whose CD4 cell count was < 200 cells/ml at baseline. The size of the effect is substantially smaller than that estimated using a random-effects approach."
"10.1198/016214505000001168","2006","How many people do you know in prison?: using overdispersion in count data to estimate social structure in networks","1","Networks-sets of objects connected by relationships-are important in a number of fields. The study of networks has long been central to sociology, where researchers have attempted to understand the causes and consequences of the structure of relationships in large groups of people. Using insight from previous network research, Killworth et al. and McCarty et al. have developed and evaluated a method for estimating the sizes of hard-to-count populations using network data collected from a simple random sample of Americans. In this article we show how, using a multilevel overdispersed Poisson regression model, these data also can be used to estimate aspects of social structure in the population. Our work goes beyond most previous research on networks by using variation, as well as average responses, as a source of information. We apply our method to the data of McCarty et al. and find that Americans vary greatly in their number of acquaintances. Further, Americans show great variation in propensity to form ties to people in some groups (e.g., males in prison, the homeless, and American Indians), but little variation for other groups (e.g., twins, people named Michael or Nicole). We also explore other features of these data and consider ways in which survey data can be used to estimate network structure."
"10.1198/016214505000001069","2006","On the correlation matrix of the discrete {F}ourier transform and the fast solution of large {T}oeplitz systems for long-memory time series","0","We show that for long-memory time series, the Toeplitz system Sigma(n)(f)x = b can be solved in O(n log(5/2) n) operations using a well-known version of the preconditioned conjugate gradient method, where Sigma(n)(f) is the n x n covariance matrix, f is the spectral density, and b is a known vector. Solutions of such systems are needed for optimal linear prediction and interpolation. We establish connections between this preconditioning method and the frequency domain analysis of time series. Indeed, the running time of the algorithm is determined by the rate of increase in the condition number of the correlation matrix of the discrete Fourier transform (DFT) vector, as the sample size tends to infinity. We derive an upper bound for this condition number. The bound is of interest in its own right, because it sheds some light on the widely used but heuristic approximation that the standardized DFT coefficients are uncorrelated with equal variances. We present applications of the preconditioning methodology to the forecasting of volatility in a long-memory stochastic volatility model, and to the evaluation of the Gaussian likelihood function of a long-memory model."
"10.1198/016214505000001159","2006","Bayesian-optimal design via interacting particle systems","0","We propose a new stochastic algorithm for Bayesian-optimal design in nonlinear and high-dimensional contexts. Following Peter Muller, we solve an optimization problem by exploring the expected utility surface through Markov chain Monte Carlo simulations. The optimal design is the mode of this surface considered a probability distribution. Our algorithm relies on a ""particle"" method to efficiently explore high-dimensional multimodal surfaces, with simulated annealing to concentrate the samples near the modes. We first test the method on an optimal allocation problem for which the explicit solution is available, to compare its efficiency with a simpler algorithm. We then apply our method to a challenging medical case study in which an optimal protocol treatment needs to be determined. For this case, we propose a formalization of the problem in the framework of Bayesian decision theory, taking into account physicians' knowledge and motivations. We also briefly review further improvements and alternatives."
"10.1198/016214505000001410","2006","Estimating mean dimensionality of analysis of variance decompositions","0","Analysis of variance (ANOVA) is now often applied to functions defined on the unit cube, where it serves as a tool for the exploratory analysis of functions. The mean dimension of a function, defined as a natural weighted combination of its ANOVA mean squares, provides one measure of how hard or easy it is to integrate the function by quasi-Monte Carlo sampling. This article presents some new identities relating the mean dimension, and some analogously defined higher moments, to the variance importance measures of I. M. Sobol. As a result, we are able to measure the mean dimension of certain functions arising in computational finance. We produce an unbiased and nonnegative estimate of the variance contribution of the highest-order interaction that avoids the cancellation problems of previous estimates. In an application to extreme value theory, we find that, among other things, the minimum of d independent U[0, 1] random variables has a mean dimension of 2(d + 1)/(d + 3)."
"10.1198/016214505000000916","2006","Nonparametric density estimation from covariate information","0","An increasing number of statistical problems arise in connection with functional calibration. In each case, inexpensive indirect data in a particular context are combined with direct expensive-to-acquire data from different but related settings to estimate quantities in the former case. Sometimes (e.g., in chemometrics problems where spectroscopic calibration is used) the indirect data are functional. But more commonly, they are scalar or vector-valued, and the functional component is the quantity that we wish to estimate. The problem treated here is of the latter type. We observe data that give us access to the distribution of U given V, and from these and data on U, we wish to estimate the density of V. The motivating real datasets are of age and covariate information in fish populations. We suggest two methodologies, each of which is based on transforming the problem to one involving inversion of a symmetric linear operator. Our techniques have connections to methods for functional data analysis and for a variety of mixture and deconvolution problems, as well as to calibration techniques."
"10.1198/016214505000001285","2006","On sliced inverse regression with high-dimensional covariates","8","Sliced inverse regression is a promising method for the estimation of the central dimension-reduction subspace (CDR space) in semiparametric regression models. It is particularly useful in tackling cases with high-dimensional covariates. In this article we study the asymptotic behavior of the estimate of the CDR space with high-dimensional covariates, that is, when the dimension of the covariates goes to infinity as the sample size goes to infinity. Strong and weak convergence are obtained. We also suggest an estimation procedure of the Bayes information criterion type to ascertain the dimension of the CDR space and derive the consistency. A simulation study is conducted."
"10.1198/016214505000001195","2006","Non-{G}aussian {B}ayesian geostatistical modeling","2","Sampling models for geostatistical data are usually based on Gaussian processes. However, real data often display non-Gaussian features, such as heavy tails. In this article we propose a more flexible class of sampling models. We start from the spatial linear model that has a spatial trend plus a stationary Gaussian error process. We extend the sampling model to non-Gaussianity by including a scale parameter at each location. We make sure that we obtain a valid stochastic process. The scale parameters are spatially correlated to ensure that the process is mean square continuous. We derive expressions for the moments and the kurtosis of the process. This more general stochastic process allows us to accommodate and identify observations that would be outliers under a Gaussian sampling process. For the spatial correlation structure, we adopt the flexible Matern class with unknown smoothness parameter. Furthermore, a nugget effect is included in the model. Bayesian inference (posterior and predictive) is performed using a Markov chain Monte Carlo algorithm. The choice of the prior distribution is discussed and its importance assessed in a sensitivity analysis. We also examine identifiability of the parameters. Our methods are illustrated with two datasets."
"10.1198/016214505000001078","2006","Optimal model assessment, selection, and combination","7","Central to statistical theory and application is statistical modeling, which typically involves choosing a single model or combining a number of models of different sizes and from different sources. Whereas model selection seeks a single best modeling procedure, model combination combines the strength of different modeling procedures. In this article we look at several key issues and argue that model assessment is the key to model selection and combination. Most important, we introduce a general technique of optimal model assessment based on data perturbation, thus yielding optimal selection, in particular model selection and combination. From a frequentist perspective, we advocate model combination over a selected subset of modeling procedures, because it controls bias while reducing variability, hence yielding better performance in terms of the accuracy of estimation and prediction. To realize the potential of model combination, we develop methodologies for determining the optimal tuning parameter, such as weights and subsets for combining via optimal model assessment. We present simulated and real data examples to illustrate main aspects."
"10.1198/016214506000000564","2006","Concomitants of multivariate order statistics with application to judgment poststratification","0","We generalize the definition of a concomitant of an order statistic in the multivariate case, develop general expressions for its density, and establish related properties. We study the concomitant of a normal random vector in detail and discuss methods for calculating its moments. Furthermore, we apply the theory to develop new estimators of the mean from a judgment poststratified sample, where poststrata are formed by rank classes of auxiliary variables. Our estimators are shown to be more efficient than existing ones and robust against violations of the normality assumption. They are also well suited to applications requiring cost efficiency."
"10.1198/016214506000000267","2006","Algorithms for constructing combined strata variance estimators","0","A jackknife or balanced repeated replication variance estimator in a large survey typically requires a large number of replicates and replicate weights. Reducing the number of replicates may have important advantages for computations and for limiting the risk of data disclosure from public use data files. This article proposes algorithms adapted from scheduling theory to combine variance strata and, thus, reduce the number of replicates. The algorithms are simple and efficient and can be adapted to easily account for vector characteristics and analytic domains. An important concern with combining strata is that the resulting variance estimators may be inconsistent. We establish conditions for the consistency of the combined variance estimator and show that the proposed algorithms ensure they are met. We also derive bounds on the degrees of freedom that the algorithms will assure. The algorithms are applied both to a real sample survey and to samples from simulated populations, and the algorithms perform very well, attaining variance estimators with precision levels close to the upper bounds."
"10.1198/016214506000000320","2006","Optimizing the expected overlap of survey samples via the northwest corner rule","0","In survey sampling there is often a need to coordinate the selection of pairs of samples drawn from two overlapping populations so as to maximize or minimize their expected overlap, subject to constraints on the marginal probabilities determined by the respective designs. For instance, maximizing the expected overlap between repeated samples can stabilize the resulting estimates of change and reduce the costs of first contacts; minimizing the expected overlap can avoid overburdening respondents with multiple surveys. We focus on the important special case in which both samples are selected by simple random sampling without replacement (SRSWOR) conducted independently within each stratum. Optimizing the expected sample overlap can be formulated as a linear programming problem known as a transportation problem (TP). We show that by appropriately grouping and ordering the possible samples in each survey, one can reduce the initial TP to a much smaller TP amenable to solution by an algorithm known as the Northwest Corner Rule (NWCR). The proposed NWCR method proceeds in two easily implemented steps: first selecting the numbers of births (new units) and deaths (deleted units) by a random selection from a hypergeometric distribution, and then selecting the births and deaths by SRSWOR. We formally prove properties of the NWCR solutions, including a minimal variance property of the minimal overlap solution. In a simulation study, the NWCR method compares favorably with a popular method based on assignment of permanent random numbers to each sampling unit."
"10.1198/016214506000000528","2006","Inference on the number of species through geometric lower bounds","1","Estimating the number of species in a population from a sample of individuals is investigated in a nonparametric Poisson mixture model. A sequence of lower bounds to the odds that a species is unseen in the sample are proposed from a geometric perspective. A lower bound and its representing mixing distribution can be computed by linear programming with guaranteed convergence. These lower bounds can be estimated by the maximum likelihood method and used to construct lower confidence limits for the number of species by the bootstrap method. Computing the nonparametric maximum likelihood estimator is discussed. Simulation is used to assess the performance of estimated lower bounds and compare them with several existing estimators. A genornic application is investigated."
"10.1198/016214506000000159","2006","Families of multivariate distributions involving the {R}osenblatt construction","0","Recently, Jones pointed out a useful device for enriching families of univariate distributions. Typically, one may construct a random variable with distribution F by considering F-1 (U), where U is a uniform(0, 1) random variable. Jones suggested replacing the uniform random variable by a beta-distributed random variable. In this article an analogous enriching process is applied to the Rosenblatt construction of multivariate distributions. Several parametric families are introduced using this construction, together with discussion of appropriate parameter estimation strategies. It turns out that the resulting families of distributions are very flexible and easy to estimate. The method is illustrated with simulated and real data."
"10.1198/016214506000000140","2006","Fourier methods for estimating the central subspace and the central mean subspace in regression","8","In regression with a high-dimensional predictor vector, it is important to estimate the central and central mean subspaces that preserve sufficient information about the response and the mean response. Using the Fourier transform, we have derived the candidate matrices whose column spaces recover the central and central mean subspaces exhaustively. Under the normality assumption of the predictors, explicit estimates of the central and central mean subspaces are derived. Bootstrap procedures are used for determining dimensionality and choosing tuning parameters. Simulation results and an application to a real data are reported. Our methods demonstrate competitive performance compared with SIR, SAVE, and other existing methods. The approach proposed in the article provides a novel view on sufficient dimension reduction and may lead to more powerful tools in the future."
"10.1198/016214506000000023","2006","A distributional approach for causal inference using propensity scores","13","Drawing inferences about the effects of treatments and actions is a common challenge in economics, epidemiology, and other fields. We adopt Rubin's potential outcomes framework for causal inference and propose two methods serving complementary purposes. One can be used to estimate average causal effects, assuming no confounding given measured covariates. The other can be used to assess how the estimates might change under various departures from no confounding. Both methods are developed from a nonparametric likelihood perspective. The propensity score plays a central role and is estimated through a parametric model. Under the assumption of no confounding, the joint distribution of covariates and each potential outcome is estimated as a weighted empirical distribution. Expectations from the joint distribution are estimated as weighted averages or, equivalently to first order, regression estimates. The likelihood estimator is at least as efficient and the regression estimator is at least as efficient and robust as existing estimators. Regardless of the no-confounding assumption, the marginal distribution of covariates times the conditional distribution of observed outcome given each treatment assignment and covariates is estimated. For a fixed bound on unmeasured confounding, the marginal distribution of covariates times the conditional distribution of counterfactual outcome given each treatment assignment and covariates is explored to the extreme and then compared with the composite distribution corresponding to observed outcome given the same treatment. assignment and covariates. We illustrate the methods by analyzing the data from an observational study on right heart catheterization."
"10.1198/016214505000001366","2006","Regression and weighting methods for causal inference using instrumental variables","2","Recent researches in econometrics and statistics have gained considerable insights into the use of instrumental variables (lVs) for causal inference. A basic idea is that IVs serve as an experimental handle, the turning of which may change each individual's treatment status and, through and only through this effect, also change observed outcome. The average difference in observed outcome relative to that in treatment status gives the average treatment effect for those whose treatment status is changed in this hypothetical experiment. We build on the modern IV framework and develop two estimation methods in parallel to regression adjustment and propensity score weighting in the case of treatment selection based on covariates. The IV assumptions are made explicitly conditional on covariates to allow for the fact that instruments can be related to these background variables. The regression method focuses on the relationship between responses (observed outcome and treatment status jointly) and instruments adjusted for covariates. The weighting method focuses on the relationship between instruments and covariates to balance different instrument groups with respect to covariates. For both methods, modeling assumptions are made directly on observed data and separated from the IV assumptions, whereas causal effects are inferred by combining observeddata models with the IV assumptions through identification results. This approach is straightforward and flexible enough to host various parametric and serniparametric techniques that attempt to learn associational relationships from observed data. We illustrate the methods by an application to estimating returns to education."
"10.1198/016214506000000483","2006","Generalized exponential predictors for time series forecasting","0","We consider the problem of prediction for stationary and nonstationary univariate time series using a modification suggested by the usual exponentially weighted moving average method. The modification leads to a class of general exponential predictors that can improve on the usual finite approximations to an infinite autoregressive process. We provide the theoretical justifications and suggest a class of predictors that covers modified and finite autoregressive fits as special cases. Two examples involving sample data show how the method is competitive with autoregressive integrated moving average (ARIMA) when applied to a U.S. energy use series and improves on ARIMA when applied to a global temperature series. A simulation indicates that considerable improvements are possible for infinite autoregressive (ARIMA) processes exhibiting certain special patterns of long-range dependence."
"10.1198/016214506000000276","2006","A {M}onte {C}arlo approach to filtering for a class of marked doubly stochastic {P}oisson processes","0","Marked doubly stochastic Poisson processes are a particular type of marked point processes that are characterized by the number of events in any time interval as being conditionally Poisson distributed, given another positive stochastic process called intensity. Here we consider a subclass of these processes in which the intensity is assumed to be a deterministic function of another nonexplosive marked point process. In particular, we will investigate an intensity jump process with an exponential decay having an analytic form for the distribution of the times and sizes of the jumps, which can be seen as a generalization of the classical shot noise process. Assuming that the intensity is unobservable, interest here is in its filtering, that is, in the computation of its conditional distribution, over a whole time interval, given an observed trajectory of realized events. Because, in general, this computation cannot be performed analytically, we propose a simulation method that provides an approximate solution, which relies on the reversible-jump Markov chain Monte Carlo algorithm. Interestingly, the proposed filtering algorithm also allows the setup of a likelihood-based procedure for the estimation of the parameters of the model based on stochastic versions of the expectation-maximization (EM) algorithm. The potential of the filtering and estimation methods proposed are illustrated through some simulation experiments as well as on a financial ultra-high-frequency dataset of intraday S&P500 futures prices."
"10.1198/016214506000000302","2006","Hierarchical {D}irichlet processes","14","We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the ""Chinese restaurant franchise."" We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling."
"10.1198/016214506000000384","2006","Inference for mixtures of finite {P}olya tree models","5","Mixtures of Polya tree models provide a flexible alternative when a parametric model may only hold approximately. I provide computational strategies for obtaining full serniparametric inference for mixtures of finite Polya tree models given a standard parameterization, including models that would be troublesome to fit using Dirichlet process mixtures. Recommendations are put forth on choosing the level of a finite Polya tree, and model comparison is discussed. Several examples demonstrate the utility of finite Polya tree modeling, including data fit to generalized linear mixed models and several survival models."
"10.1198/016214506000000492","2006","Fixed-width output analysis for {M}arkov chain {M}onte {C}arlo","5","Markov chain Monte Carlo is a method of producing a correlated sample to estimate features of a target distribution through ergodic averages. A fundamental question is when sampling should stop; that is, at what point the ergodic averages are good estimates of the desired quantities. We consider a method that stops the simulation when the width of a confidence interval based on an ergodic average is less than a user-specified value. Hence calculating a Monte Carlo standard error is a critical step in assessing the simulation output. We consider the regenerative simulation and batch means methods of estimating the variance of the asymptotic normal distribution. We give sufficient conditions for the strong consistency of both methods and investigate their finite-sample properties in various examples."
"10.1198/016214506000000898","2006","Empirical {B}ayesian analysis for computer experiments involving finite-difference codes","0","Computer experiments are increasingly used in scientific investigations as substitutes for physical experiments in cases where the latter are difficult or impossible to perform. A Computer experiment consists of several runs of a computer model or ""code"" for the purpose of better understanding the input output relationship. One practical difficulty in the use of these models is that a single run may require a prohibitive amount of computational resources in some situations. A recent approach uses statistical approximations as less expensive surrogates for such computer codes; these provide both point predictors and uncertainty characterization of the outputs. A widely used class ofcomputer codes is the finite-difference solvers of differential equations, which produce multivariate output (e.g., time series). The finite-difference relationship underpins the statistical model proposed here, and we show that this strategy has clear computational and accuracy advantages over a direct multivariate extension of the popular scalar modeling methodology."
"10.1198/016214506000000537","2006","Nonparametric analysis of factorial designs with random missingness: bivariate data","0","We propose a nonparametric approach to the analysis of factorial designs where each subject is observed at two time points and both observations are subject to missingness. The procedures are fully nonparametric in that they do not require continuity, and do not impose models to describe the relation of the response distribution in different factor-level combinations. The approach for estimating and testing treatment and time effects is based on a method, which we introduce, for estimating a distribution function. The method requires a patternmixture-type assumption on the missingness mechanism, which is weaker than the missing-completely-at-random assumption but neither weaker nor stronger than the missing-at-random assumption. This missingness assumption is the minimal requirement for nonparametric analysis. Comparisons with normal-based likelihood ratio tests indicate that the proposed tests fare well when the data are normal and homoscedastic, and outperform them in many other cases. Simulations also confirm that the proposed method has higher power than common nonparametric complete-pairs tests for observations missing completely at random. Finally, a dataset on the delinquent values of boys released from correctional institutions is analyzed and discussed."
"10.1198/016214506000000500","2006","A composite likelihood approach in fitting spatial point process models","2","We propose a new likelihood-based approach in fitting spatial point process models. A composite likelihood is first formed by adding some pairwise composite likelihood functions that are defined in terms of the second-order intensity function of the underlying process, and then used for estimating the unknown parameters. The estimation procedure is computationally simple and yields consistent and asymptotically normal estimators under some mild conditions. We demonstrate through a simulation study and applications to two real data examples that the proposed approach may lead to improved estimations compared with the commonly used ""minimum contrast estimation"" approach."
"10.1198/016214506000000041","2006","Bayesian wombling: curvilinear gradient assessment under spatial process models","1","Large-scale inference for random spatial surfaces over a region using spatial process models has been well studied. Under such models, local analysis of the surface (e.g., gradients at given points) has received recent attention. A more ambitious objective is to move from points to curves, to attempt to assign a meaningful gradient to a curve. For a point, if the gradient in a particular direction is large (positive or negative), then the surface is rapidly increasing or decreasing in that direction. For a curve, if the gradients in the direction orthogonal to the curve tend to be large, then the curve tracks a path through the region where the surface is rapidly changing. In the literature. learning about where the surface exhibits rapid change is called wombling, and a curve such as we have described is called a wombling boundary. Existing wombling methods have focused mostly on identifying points and then connecting these points using an ad hoc algorithm to create curvilinear wombling boundaries. Such methods are not easily incorporated into a statistical modeling setting. The contribution of this article is to formalize the notion of a curvilinear wombling boundary in a vector analytic framework using parametric curves and to develop a comprehensive statistical framework for curvilinear boundary analysis based on spatial process models for point-referenced data. For a given curve that may represent a natural feature (e.g., a mountain, a river, or a political boundary), we address the issue of testing or assessing whether it is a wombling boundary. Our approach is applicable to both spatial response surfaces and, often more appropriately, spatial residual surfaces. We illustrate our methodology with a simulation study, a weather dataset for the state of Colorado, and a species presence/absence dataset from Connecticut."
"10.1198/016214506000000555","2006","Robust estimation of mixture complexity","2","In many applications, it is important to find the mixture with fewest number of components, known as the mixture complexity, that provides a satisfactory fit to the data. This article focuses on developing an estimator of mixture complexity that is consistent when the form of component densities are unknown but are postulated to be members of some parametric family and is simultaneously robust against model misspecification. We treat the estimation of mixture complexity as a model selection problem and construct an estimator of mixture complexity as a byproduct of minimizing a Hettinger information criterion. This estimator is shown to be consistent for any parametric family of mixtures. When the model is correctly specified, Monte Carlo simulations for a wide variety of normal mixtures show that our estimator is very competitive with several others in the literature in correctly identifying the true mixture complexity. The basic construction, being firmly rooted in the minimum Hettinger distance approach, enables our estimator to naturally inherit the property of robustness, which is examined, through simulations, under symmetric departures from postulated component normality. In terms of correctly identifying the mixture complexity under model misspecification, our estimator performs much better than an estimator based on the Kullback-Leibler distance due to James, Priebe, and Marchette. An example concerning hypertension is revisited to further illustrate the performance of our estimator."
"10.1198/016214506000000519","2006","Locally efficient estimators for semiparametric models with measurement error","3","We derive constructive locally efficient estimators in semiparametric measurement error models. The setting is one in which the likelihood function depends on variables measured with and without error, where the variables measured without error can be modeled nonparametrically. The algorithm is based on backfitting. We show that if one adopts a parametric model for the latent variable measured with error and if this model is correct, then the estimator is semiparametric efficient; if the latent variable model is misspecified, then our methods lead to a consistent and asymptotically normal estimator. Our method further produces an estimator of the nonparametric function that achieves the standard bias and variance property. We extend the methodology to allow estimation of parameters in the measurement error model by additional data in the form of replicates or instrumental variables. The methods are illustrated through a simulation study and a data example, where the putative latent variable distribution is a shifted lognormal, but concerns about the effects of misspecification of this assumption and the linear assumption of another covariate demand a more model-robust approach. A special case of wide interest is the partial linear measurement error model. If one assumes that the model error and the measurement error are both normally distributed, then our estimator has a closed form. When a normal model for the unobservable variable is also posited, our estimator becomes consistent and asymptotically normally distributed for the general partially linear measurement error model, even without any of the normality assumptions under which the estimator is originally derived. We show that the method in fact reduces to a same estimator as that of Liang et al., thus demonstrating a previously unknown optimality property of their method."
"10.1198/016214506000000069","2006","Focused information criteria and model averaging for the {C}ox hazard regression model","2","This article is concerned with variable selection methods for the Cox proportional hazards regression model. Including excessive covariates causes extra variability and inflated confidence intervals for regression parameters; thus regimes for discarding the less informative ones are needed. Our framework has p covariates designated as ""protected,"" while variables from a further set of q covariates are examined for possible inclusion or exclusion. We develop a focused information criterion (FIC) that for given interest parameter finds the best subset of covariates. Thus the FIC might find that the best model for predicting median survival time is different than the best model for estimating probabilities, and the best overall model for analyzing men's survival might riot be the same as the best overall model for analyzing women's survival. Methodology is also developed for model averaging, wherein the final estimate of a quantity is a weighted average of estimates computed for a range of submodels. Our methods are illustrated in simulations and for a survival study of Danish skin cancer patients."
"10.1198/016214506000000366","2006","Estimating network loss rates using active tomography","0","Active network tomography refers to an interesting class of large-scale inverse problems that arise in estimating the quality of service parameters of computer and communications networks. This article focuses on estimation of loss rates of the internal links of a network using end-to-end measurements of nodes located on the periphery. A class of flexible experiments for actively probing the network is introduced, and conditions under which all of the link-level information is estimable are obtained. Maximum likelihood estimation using the EM algorithm, the structure of the algorithm, and the properties of the maximum likelihood estimators are investigated. This includes simulation studies using the ns (network simulator) to obtain realistic network traffic. The optimal design of probing experiments is also studied. Finally, application of the results to network monitoring is briefly illustrated."
"10.1198/016214506000000735","2006","The adaptive lasso and its oracle properties","75","The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the l(1) penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection."
"10.1198/016214506000000339","2006","Exceedance control of the false discovery proportion","0","Multiple testing methods to control the false discovery rate, the expected proportion of falsely rejected null hypotheses among all rejections, have received much attention. It can be valuable to control not the mean of this false discovery proportion (FDP), but rather the probability that the FDP exceeds a specified bound. In this article we construct a general class of methods for exceedance control of FDP based on inverting tests of uniformity. The method also produces a confidence envelope for the FDP as a function of rejection threshold. We discuss how to select a procedure with good power."
"10.1198/016214505000000628","2006","Prediction by supervised principal components","6","In regression problems where the number of predictors greatly exceeds the number of observations, conventional regression techniques may produce unsatisfactory results. We describe a technique called supervised principal components that call be applied to this type of problem. Supervised principal components is similar to conventional principal components analysis except that it uses a subset of the predictors selected based on their association with the outcome. Supervised principal components can be applied to regression and generalized regression problems, such as survival analysis. It compares favorably to other techniques for this type of problem, and can also account for the effects of other covariates and help identify which predictor variables are most important. We also provide asymptotic consistency results to help support our empirical findings. These methods could become important tools for DNA microarray data. where they may be used to more accurately diagnose and treat cancer."
"10.1198/016214505000000943","2006","Adaptive thresholds: monitoring streams of network counts","0","This article describes a fast, statistically principled method for monitoring streams of network counts, which have long-term trends, rough cyclical patterns, outliers, and missing data. The key step is to build a reference (predictive) model for the counts that captures their complex, salient features but has just a few parameters that can be kept up-to-date as the counts flow by, without requiring access to past data. This article justifies using a negative binomial reference distribution with parameters that capture trends and patterns and method of moment estimators that can be computed quickly enough to keep up with the data flow. The reference distribution may be of interest in its own right for traffic engineering and load balancing, but a more challenging task is to use it to identify degraded network performance as quickly as possible. Here we detect changes in network performance not by monitoring quantiles of the predictive distribution directly but by applying control chart methodology to normal scores of the p values of the counts. Using p values adjusts for the lack of stationarity from one count to the next. Compared with thresholding isolated counts, control charting reduces the false-alarm rate, increases the chance of detecting ongoing low-level events and reduces the time to detection of long events. This adaptive count thresholding procedure is shown to perform well on both real and simulated data."
"10.1198/016214505000000934","2006","Piecewise constant cross-ratio estimation for association of age at a marker event and age at menopause","2","A question of significant interest in female reproductive aging is to identify bleeding criteria for menopausal transition. Although various bleeding criteria, or markers, have been proposed for menopausal transition, their validity has not been adequately examined. The Tremin Trust data were collected from a long-term cohort study that followed a group of women throughout their whole reproductive life. Such data provide a unique opportunity for evaluating the utility of a bleeding criterion-based marker event by assessing the association between age at onset of the bleeding marker and age at onset of menopause. Formal statistical analysis of this dependence is challenged by the facts that both the marker event and menopause are subject to right-censoring and that their association depends on age at the marker event. We propose using the cross-ratio to measure their dependence by assuming the cross-ratio to be a piecewise constant function of age at onset of the marker event. We propose two estimation procedures, the direct two-stage method and the sequential two-stage method, extending the latter to allow for covariates in marginal survival functions. We apply the proposed methods to the analysis of the Tremin Trust data and evaluate their performance using simulations."
"10.1198/016214505000000970","2006","Causal vaccine effects on binary postinfection outcomes","3","The effects of vaccine on postinfection outcomes, such as disease, death, and secondary transmission to others. are important scientific and public health aspects of prophylactic vaccination. As a result, evaluation of many vaccine effects condition on being infected. Conditioning on an event that occurs posttreatment (in our case, infection subsequent to assignment to vaccine or control) can result in selection bias. Moreover, because the set of individuals who would become infected if vaccinated is likely not identical to the set of those who would become infected if given control, comparisons that condition on infection do not have a causal interpretation. In this article we consider identifiability and estimation of causal vaccine effects on binary postinfection outcomes. Using the principal stratification framework, we define a postinfection causal vaccine efficacy estimand in individuals who would be infected regardless of treatment assignment. The estimand is shown to be not identifiable under the standard assumptions of the stable unit treatment value. monotonicity, and independence of treatment assignment. Thus selection models are proposed that identify the causal estimand. Closed-form maximum likelihood estimators (MLEs) are then derived under these models, including those assuming maximum possible levels of positive and negative selection bias. These results show the relations between the MLE of the causal estimand and two commonly used estimators for vaccine effects on postinfection Outcomes. For example, the usual intent-to-treat estimator is shown to be an upper bound on the postinfection causal vaccine effect provided that the magnitude of protection against infection is not too large. The methods are used to evaluate postinfection vaccine effects in a clinical trial of a rotavirus vaccine candidate and in a field study of a pertussis vaccine. Our results show that pertussis vaccination has a significant causal effect in reducing disease severity."
"10.1198/016214505000000989","2006","Estimation of expression indexes for oligonucleotide arrays using the singular value decomposition","1","Multiprobe oligonucleotide arrays are a widely used type of expression microarray with the attractive feature that numerous probes are used to represent each transcript. An ""expression index"" is a statistic used to represent expression level for a particular gene that is estimated from the probe hybridization intensities. We show that a popular model-based expression index proposed by Li and Wong has an interpretation as a component of the singular value decomposition (SVD) of the probe intensity matrix. Following this observation, we propose a new SVD model that accounts for differing variance structure across probes. We also propose that nonlinearity in intensity response to expression can be corrected to some extent through a data transformation, which is guided by an SVD entropy measure. The methods are demonstrated with simulation and applications to two real datasets."
"10.1198/016214505000001096","2006","Quality control and robust estimation for c{DNA} microarrays with replicates","0","We consider robust estimation of gene intensities from cDNA microarray data with replicates. Several statistical methods for estimating gene intensities from microarrays have been proposed, but little work has been done on robust estimation. This is particularly relevant for experiments with replicates, because even one outlying replicate can have a disastrous effect on the estimated intensity for the gene concerned. Because of the many steps involved in the experimental process from hybridization to image analysis, cDNA microarray data often contain outliers. For example, an outlying data value could occur because of scratches or dust on the surface, imperfections in the glass, or imperfections in the array production. We develop a Bayesian hierarchical model for robust estimation of cDNA microarray intensities. Outliers are modeled explicitly using a t-distribution, and our model also addresses such classical issues as design effects, normalization, transformation, and nonconstant variance. Parameter estimation is carried out using Markov chain Monte Carlo. By identifying potential outliers, the method provides automatic quality control of replicate, array, and gene measurements. The method is applied to three publicly available gene expression datasets and compared with three other methods: ANOVA-normalized log ratios, the median log ratio, and estimation after the removal of outliers based on Dixon's test. We find that the between-replicate variability of the intensity estimates is lower for our method than for any of the others. We also address the issue of whether the background should be subtracted when estimating intensities. It has been argued that this should not be done because it increases variability, whereas the arguments for doing so are that there is a physical basis for the image background, and that not doing so will bias downward the estimated log ratios of differentially expressed genes. We show that the arguments on both sides of this debate are correct for our data, but that by using our model one can have the best of both worlds: One can subtract the background without increasing variability by much."
"10.1198/016214505000000187","2006","A quantitative study of gene regulation involved in the immune response of anopheline mosquitoes: an application of {B}ayesian hierarchical clustering of curves","8","Malaria represents one of the major worldwide challenges to public health. A recent breakthrough in the study of the disease follows the annotation of the genome of the malaria parasite Plasmodium falciparum and the mosquito vector (an organism that spreads an infectious disease) Anopheles. Of particular interest is the molecular biology underlying the immune response system of Anopheles, which actively fights against Plasmodium infection. This article reports a statistical analysis of gene expression time profiles from mosquitoes that have been infected with a bacterial agent. Specifically, we introduce a Bayesian model-based hierarchical clustering algorithm for curve data to investigate mechanisms of regulation in the genes concerned; that is, we aim to cluster genes having similar expression profiles. Genes displaying similar, interesting profiles can then be highlighted for further investigation by the experimenter. We show how our approach reveals structure within the data not captured by other approaches. One of the most pertinent features of the data is the sample size, which records the expression levels of 2,771 genes at 6 time points. Additionally, the time points are unequally spaced, and there is expected nonstationary behavior in the gene profiles. We demonstrate our approach to be readily implementable under these conditions, and highlight some crucial computational savings that can be made in the context of a fully Bayesian analysis."
"10.1198/016214505000000961","2006","Bayesian model averaging with applications to benchmark dose estimation for arsenic in drinking water","1","An important component of quantitative risk assessment involves characterizing the dose-response relationship between an environmental exposure and adverse health outcome and then computing a benchmark dose, or the exposure level that yields a suitably low risk. This task is often complicated by model choice considerations, because risk estimates depend on the model parameters. We pro pose using Bayesian methods to address the problem of model selection and derive a model-averaged version of the benchmark dose. We illustrate the methods through application to data on arsenic-induced lung cancer from Taiwan."
"10.1198/016214505000000998","2006","Inverse decision theory: characterizing losses for a decision rule with applications in cervical cancer screening","0","Identifying an optimal decision rule using Bayesian decision theory requires priors, likelihoods, and losses. In many medical settings, we can develop priors and likelihoods, but specifying losses can be difficulty especially when considering both patient outcomes and economic costs. If there is a widely accepted treatment strategy, then we can consider the inverse problem and find a region in the space of losses where the procedure is optimal. We call this approach inverse decision theory (IDT). We apply IDT to the standard of care for diagnosis and sic treatment of precancerous lesions of the cervix, and consider an alternative procedure that has been proposed. We use a Bayesian approach to estimate the probabilities associated with the diagnostic tests and make inferences about the region in loss space where these medical procedures are optimal. In particular, we find evidence supporting the current standard of care."
"10.1198/016214505000000556","2006","A reference-free {C}uscore chart for dynamic mean change detection and a unified framework for charting performance comparison","3","To detect and estimate nonconstant, time-varying mean shifts, statistical process control (SPC) tools, such as the cumulative score (Cuscore) and generalized likelihood ratio test (GLRT) charts, have recently been proposed. However, their efficiency is based on previous and exact knowledge of a reference pattern. In this article a reference-free Cuscore (RFCuscore) chart is proposed that can trace and detect dynamic mean changes quickly without knowing the reference pattern. In addition, a unified framework that contains most of the control charts is presented and applied for a theoretical comparison of the RFCuscore, Cuscore, GLRT, and CUSUM charts in detecting dynamic mean changes. Moreover, numerical simulations and a real example are used to illustrate and verify the results. Both theoretical analysis and numerical results show that the RFCuscore chart performs not only robustly, but also quickly in detecting both small and large dynamic mean changes."
"10.1198/016214505000000312","2006","Clustering categorical data based on distance vectors","0","We introduce a novel statistical procedure for clustering categorical data based on Hamming distance (HD) vectors. The proposed method is conceptually simple and computationally straightforward, because it does not require any specific statistical models or any convergence criteria. Moreover, unlike most currently existing algorithms that compute the class membership or membership probability for every data point at each iteration, our algorithm sequentially extracts clusters from the given dataset. That is, at each iteration our algorithm strives to identify only one cluster, which will then be deleted from the dataset at the next iteration; this procedure repeats until there are no more significant clusters in the remaining data. Consequently, the number of clusters can be determined automatically by the algorithm. As for the identification and extraction of a cluster, we first locate the cluster center by using a Pearson chi-squared-type statistic on the basis of HD vectors. The partition of the dataset produced by our algorithm is unique and insensitive to the input order of data points. The performance of the proposed algorithm is examined using both simulated and real world datasets. Comparisons with two well-known clustering algorithms, K-modes and AutoClass, show that the proposed algorithm substantially outperforms these competitors, with the classification rate or the information gain typically improved by several orders of magnitude. Computational complexity and run time comparisons are also provided."
"10.1198/016214505000000637","2006","Global validation of linear model assumptions","0","An easy-to-implement global procedure for testing the four assumptions of the linear model is proposed. The test can be viewed as a Neyman smooth test and relies only on the standardized residual vector. If the global procedure indicates a violation of at least one of the assumptions, then the components of the global test statistic can be used to gain insight into which assumptions have been violated. The procedure can also be used in conjunction with associated deletion statistics to detect unusual observations. Simulation results are presented indicating the sensitivity of the procedure in detecting model violations under a variety of situations, and its performance is compared with three potential competitors, including a procedure based on the Box-Cox power transformation. The procedure is demonstrated by applying it to a new car mileage dataset and a water salinity dataset that has been used earlier to illustrate model diagnostics."
"10.1198/016214505000000718","2006","Count data distributions: some characterizations with applications","0","In this article we characterize all two-parameter count distributions (satisfying very general conditions) that are partially closed under addition. We also find those for which the maximum likelihood estimator of the population mean is the sample mean. Mixed Poisson models satisfying these properties are completely determined. Among these models are the negative binomial, Poisson-inverse Gaussian, and other known distributions. New count distributions can also be constructed using these characterizations. Three examples of application are given."
"10.1198/016214505000000574","2006","Distribution of the length of the longest significance run on a {B}ernoulli net and its applications","0","We consider the length of the longest significance run in a (two-dimensional) Bernoulli net and derive its asymptotic limit distribution. Our theoretical results: (1) reliabilityresults can be considered as generalizations of known theorems in significance runs. We give three types of t style lower and upper bounds, (2) Erdos-Renyi law, and (3) the asymptotic limit distribution. To understand the rate of convergence to the asymptotic distributions, we carry out numerical simulations. The convergence rates in a variety of situations are presented. To understand the relation between the length of the longest significance run(s) and the success probability p. we propose a dynamic programming algorithm to implement simultaneous simulations. Insights from numerical studies are important for choosing the values of design parameters in a particular application, which motivates this article. The distribution of the length of the longest significance run in a Bernoulli net is critical in applying a multiscale methodology in image detection and computational vision. Approximation strategies to some critical quantities are discussed."
"10.1198/016214505000000763","2006","Replication variance estimation for two-phase stratified sampling","2","In two-phase sampling, the second-phase sample is often a stratified sample based on the information observed in the first-phase sample. For the total of a population characteristic, either the double-expansion estimator or the reweighted expansion estimator can be used. Given a consistent first-phase replication variance estimator, we propose a consistent variance estimator that is applicable to both the double-expansion estimator and the reweighted expansion estimator. The proposed method can be extended to multiphase sampling."
"10.1198/016214505000000790","2006","Estimation of finite population domain means: a model-assisted empirical best prediction approach","2","In this article we introduce a general methodology for producing a model-assisted empirical best predictor (EBP) of a finite population domain mean using data from a complex survey. Our method improves on the commonly used design-consistent survey estimator by using a suitable mixed model. Such a model combines information from related sources, such as census and administrative data. Unlike a purely model-based EBP, the proposed model-assisted EBP converges in probability to the customary design-consistent estimator as the domain and sample sizes increase. The convergence in probability is shown to hold with respect to the sampling design, irrespective of the assumed mixed model, a property commonly known as design consistency. This property ensures robustness of the proposed predictor against possible model failures. In addition, the convergence in probability is shown to be valid with respect to the assumed mixed model (model consistency). A new mean squared prediction error (MSPE) estimator is proposed. Unlike earlier MSPE estimators, our MSPE estimator is second-order unbiased. Our simulation results demonstrate the robustness properties of our proposed model-assisted predictor and the usefulness of the second-order unbiased MSPE estimator."
"10.1198/016214505000000772","2006","High-breakdown inference for mixed linear models","2","Mixed linear models are used to analyze data in many settings. These models have a multivariate normal formulation in most cases. The maximum likelihood estimator (MLE) or the residual MLE (REML) is usually chosen to estimate the parameters. However, the latter are based on the strong assumption of exact multivariate normality. Welsh and Richardson have shown that these estimators are not robust to small deviations from multivariate normality. This means that in practice a small proportion of data (even only one) can drive the value of the estimates on their own. Because the model is multivariate, we propose a high-breakdown robust estimator for very general mixed linear models that include, for example, covariates. This robust estimator belongs to the class of S-estimators, from which we can derive asymptotic properties for inference. We also use it as a diagnostic tool to detect outlying subjects. We discuss the advantages of this estimator compared with other robust estimators proposed previously and illustrate its performance with simulation studies and analysis of three datasets. We also consider robust inference for multivariate hypotheses as an alternative to the classical F-test by using a robust score-type test statistic proposed by Heritier and Ronchetti, and study its properties through simulations and analysis of real data."
"10.1198/016214505000000510","2006","Sample size determination for robust {B}ayesian analysis","1","This article considers a robust Bayesian approach to the sample size determination problem. We focus on global Bayesian robustness that studies lower bound (L-n), upper bound (U-n), and range (R-n) of posterior quantities of interest. obtained as the prior varies in a class of distributions. Specifically, we are interested in the selection of an appropriate sample size that gives guarantees to the researcher of observing a small value of the range and, depending on the problems, either a sufficiently large lower bound or a sufficiently small upper bound. Toward this end, we approach the problem as a design issue and provide new sample size determination criteria based on summaries of the predictive distributions of L-n, U-n, and R-n, such as expectations and tail probabilities. Relationships and comparison to standard classical and (nonrobust) Bayesian methods are discussed. The proposed methods are studied for the normal model with conjugate priors and used for choosing the size of a clinical trial."
"10.1198/016214505000000411","2006","Parameter estimation for the truncated {P}areto distribution","0","The Pareto distribution is a simple model for nonnegative data with a power law probability tail. In many practical applications, there is a natural upper bound that truncates the probability tail. This article derives estimators for the truncated Pareto distribution, investigates their properties, and illustrates a way to check for fit. These methods are illustrated with applications from finance, hydrology, and atmospheric science."
"10.1198/016214505000000736","2006","Fiducial generalized confidence intervals","3","Generalized pivotal quantities (GPQs) and generalized confidence intervals (GCIs) have proven to be useful tools for making inferences in many practical problems. Although GCIs are not guaranteed to have exact frequentist coverage, a number of published and unpublished simulation studies suggest that the coverage probabilities of such intervals are sufficiently close to their nominal value so as to be useful in practice. In this article we single out a subclass of generalized pivotal quantities, which we call fiducial generalized pivotal quantities (FGPQs), and show that under some mild conditions, GCIs constructed using FGPQs have correct frequentist coverage, at least asymptotically. We describe three general approaches for constructing FGPQs-a recipe based on invertible pivotal relationships, and two extensions of it-and demonstrate their usefulness by deriving some previously unknown GPQs and GCls. It is fair to say that nearly every published GCI can be obtained using one of these recipes. As an interesting byproduct of our investigations, we note that the subfamily of fiducial generalized pivots has a close connection with fiducial inference proposed by R. A. Fisher. This is why we refer to the proposed generalized pivots as fiducial generalized pivotal quantities. We demonstrate these concepts using several examples."
"10.1198/016214505000000899","2006","Discrimination of locally stationary time series based on the excess mass functional","3","Discrimination of time series is an important practical problem with applications in various scientific fields. We propose and study a novel approach to this problem. Our approach is applicable to cases where time series in different categories have a different ""shape."" Although based on the idea of feature extraction, our method is not distance-based, and as such does not require aligning the time series. Instead, features are measured for each time series, and discrimination is based on these individual measures. An AR process with a time-varying variance is used as an underlying model. Our method then uses shape measures or, better, measures of concentration of the variance function, as a criterion for discrimination. It is this concentration aspect or shape aspect that makes the approach intuitively appealing. We provide some mathematical justification for our proposed methodology, as well as a simulation study and an application to the problem of discriminating earthquakes and explosions."
"10.1198/016214505000000745","2006","Structural break estimation for nonstationary time series models","5","This article considers the problem of modeling a class of nonstationary time series using piecewise autoregressive (AR) processes. The number and locations of the piecewise AR segments, as well as the orders of the respective AR processes. are assumed unknown. The minimum description length principle is applied to compare various segmented AR fits to the data. The goal is to find the ""best"" combination of the number of segments, the lengths of the segments, and the orders of the piecewise AR processes. Such a ""best"" combination is implicitly defined as the optimizer of an objective function, and a genetic algorithm is implemented to solve this difficult optimization problem. Numerical results from simulation experiments and real data analyses show that the procedure has excellent empirical properties. The segmentation of multivariate time series is also considered. Assuming that the true underlying model is a segmented autoregression, this procedure is shown to be consistent for estimating the location of the breaks."
"10.1198/016214505000000673","2006","Improved estimation of dissimilarities by presmoothing functional data","0","We examine the effect of presmoothing functional data oil estimating the dissimilarities among objects in a dataset, with applications to cluster analysis and other distance methods, such as multidimensional scaling and statistical matching. We prove that a shrinkage method of smoothing results in a better estimator of the dissimilarities among a set of noisy curves. For a model with independent noise structure, the smoothed-data dissimilarity estimator dominates the observed-data estimator. For a dependent-error model-often applicable when the functional data are measured nearly continuously over some domain-an asymptotic domination result is given for the smoothed-data estimator. A simulation study indicates the magnitude of improvement provided by the shrinkage estimator and examines its behavior for heavy-tailed noise structure. The shrinkage estimator presented here combines Stein estimation and basis function-based linear smoothers; in a novel manner. Statisticians increasingly analyze sizable sets of functional data. and the results in this article are a useful contribution to the theory of the effect of presmoothing oil functional data analysis."
"10.1198/016214505000000709","2006","A note on lack-of-fit tests for linear models without replication","1","A class of three tests-overall lack-of-fit test, between-cluster lack-of-fit test. and within-cluster lack-of-fit test-are proposed for testing the lack of fit of a linear regression model applied to experiments without replicates. The power of the proposed tests is significantly higher than those of the known tests under the situations considered here. The proposed tests are capable of detecting which type of lack of fit is dominant when both between-cluster and within-cluster lack of fit are present."
"10.1198/016214505000000592","2006","Partial linear regression models for clustered data","2","This article considers the analysis of clustered data via partial linear regression models. Adopting the idea of modeling the within-cluster correlation from the method of generalized estimating equations, a least squares type estimate of the slope parameter is obtained through piecewise local polynomial approximation of the nonparametric component. This slope estimate has several advantages: (a) It attains n(1/2)-consistency without undersmoothing; (b) it is efficient when correct within-cluster correlation is used, assuming multivariate normality of the error; (c) the preceding properties hold regardless of whether or not the nonparametric component is of cluster level; and (d) this estimation method naturally extends to deal with generalized partial linear models. Simulation studies and a real example are presented in support of the theory."
"10.1198/016214505000000727","2006","Order-based dependent {D}irichlet processes","18","In this article we propose a new framework for Bayesian nonparametric modeling with continuous covariates. In particular. we allow the nonparametric distribution to depend on covariates through ordering the random variables building the weights in the stick-breaking representation. We focus mostly on the class of random distributions that induces a Dirichlet process at each covariate value. We derive the correlation between distributions at different covariate values and use a point process to implement a practically useful type of ordering, Two main constructions with analytically known correlation structures are proposed. Practical and efficient computational methods are introduced. We apply our framework, through mixtures of these processes, to regression modeling, the modeling of stochastic volatility in time series data, and spatial geostatistical modeling."
"10.1198/016214506000000113","2006","Variable selection for model-based clustering","5","We consider the problem of variable or feature selection for model-based clustering. The problem of comparing two nested subsets of variables is recast as a model comparison problem and addressed using approximate Bayes factors. A greedy search algorithm is proposed for finding a local optimum in model space. The resulting method selects variables (or features), the number of clusters, and the clustering model simultaneously. We applied the method to several simulated and real examples and found that removing irrelevant variables often improved performance. Compared with methods based on all of the variables, our variable selection method consistently yielded more accurate estimates of the number of groups and lower classification error rates, as well as more parsimonious clustering models and easier visualization of results."
"10.1198/016214505000000646","2006","Objective {B}ayesian variable selection","4","A novel fully automatic Bayesian procedure for variable selection in normal regression models is proposed. The procedure uses the posterior probabilities of the models to drive a stochastic search. The posterior probabilities are computed using intrinsic priors, which can be considered default priors for model selection problems; that is, they are derived from the model structure and are free from tuning parameters. Thus they can be seen as objective priors for variable selection. The stochastic search is based on a Metropolis-Hastings algorithm with a stationary distribution proportional to the model posterior probabilities. The procedure is illustrated on both simulated and real examples."
"10.1198/016214505000000907","2006","Convexity, classification, and risk bounds","15","Many of the classification algorithms developed in the machine learning literature, including the support vector machine and boosting, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. The convexity makes these algorithms computationally efficient. The use of a surrogate, however, has statistical consequences that must be balanced against the computational virtues of convexity. To study these issues, we provide a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function-that it satisfies a pointwise form of Fisher consistency for classification. The relationship is based on a simple variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise. and show that in this case, strictly convex loss functions lead to faster rates of convergence of the risk than would be implied by standard uniform convergence arguments. Finally, we present applications of our results to the estimation of convergence rates in function classes that are scaled convex hulls of a finite-dimensional base class, with a variety of commonly used loss functions."
"10.1198/016214505000000808","2006","Likelihood-based inference on haplotype effects in genetic association studies","3","A haplotype is a specific sequence of nucleotides on a single chromosome. The population associations between haplotypes and disease phenotypes provide critical information about the genetic basis of complex human diseases. Standard genotyping techniques cannot distinguish the two homologous chromosomes of an individual, so only the unphased genotype (i.e., the combination of the two homologous haplotypes) is directly observable. Statistical inference about haplotype-phenotype associations based on unphased genotype data presents an intriguing missing-data problem, especially when the sampling depends on the disease status. The objective of this article is to provide a systematic and rigorous treatment of this problem. All commonly used study designs. including cross-sectional. case-control, and cohort studies, are considered. The phenotype can be a disease indicator, a quantitative trait. or a potentially censored time-to-disease variable. The effects of haplotypes on the phenotype are formulated through flexible regression models. which can accommodate various genetic mechanisms and gene-environment interactions. Appropriate likelihoods are constructed that may involve high-dimensional parameters. The identifiability of the parameters and the consistency, asymptotic normality, and efficiency of the maximum likelihood estimators are established. Efficient and reliable numerical algorithms are developed. Simulation studies show that the likelihood-based procedures perform well in practical settings. An application to the Finland-United States Investigation of NIDDM Genetics Study is provided. Areas in need of further development are discussed."
"10.1198/016214507000000699","2007","Sequential implementation of stepwise procedures for identifying the maximum tolerated dose","1","This article considers the problem of finding the maximum tolerated dose (MTD) of a drug in human trials. The MTD is defined as the maximum test dose with toxicity probability less than or equal to a target toxicity rate. We adopt the multiple test framework, with step-down tests used in an escalation stage and step-up tests used in a deescalation stage, to allow sequential dose assignments for ethical purposes. By formulating the estimation problem as a testing problem, the proposed procedures formally control the error probability of selecting an unsafe dose. In addition, we can control the probability of correctly selecting the MTD under a parameter subspace where no toxicity probability lies in an interval bracketed by the target toxicity rate and an unacceptably high toxicity rate, the so-called ""indifference zone."" This frequentist property, which is currently lacking in the conduct of dose-finding trials in humans, is appealing from a regulatory standpoint. We give the general expressions of the selection probabilities and apply some common statistical tests to the stepwise procedure. The design parameters are calibrated so that the average number of patients receiving an overdose is kept low. From a practical viewpoint, stepwise tests are simple and easy to understand, and the sequential implementation operates in a manner similar to the traditional algorithm familiar to clinicians. Extensive simulations illustrate that our methods yield good, competitive operating characteristics under a wide range of scenarios with realistic sample size and performs well even in situations in which other existing methods may fail, namely when the dose-toxicity curve is flat up to the targeted MTD."
"10.1198/016214507000000914","2007","A few remarks on ``{F}ixed-width output analysis for {M}arkov chain {M}onte {C}arlo'' by {J}ones et al. [MR2279478]","1",""
"10.1198/016214506000001464","2007","Bayesian analysis of frequency of allelic loss data","0","One objective of allelic-loss studies is to identify chromosomal locations that may harbor tumor-suppressor genes. An instability-selection model has been developed for allelic-loss data in which the loss events are available for each tumor and each marker (allelotypes). In performing pooled analyses of published allelic-loss experiments, however, only summaries of the frequency of allelic loss (EAL) may be available. The instability-selection model can be applied to these summary data, but naive computational approaches are prohibitive. A hidden Markov model (HMM) maximum likelihood approach has recently been proposed for EAL data, but the computation remains challenging. Moreover, precise methods for hypothesis testing and location inference are not available. We propose an alternative Bayesian treatment of the instability-selection model for FAL data. Advantages of the Bayesian approach include the availability of (1) natural imputation approaches to handle missing data, (2) hypothesis testing using Bayes factors, and (3) interpretable posterior intervals for tumor-suppressor locations. We apply our Bayesian approach to four previously reported allelic-loss studies."
"10.1198/016214507000001102","2007","A robust-likelihood cumulative sum chart","0","In practice, the cumulative sum (CUSUM) control chart is often used to detect small shifts in the mean of a normally distributed process, but it performs poorly for thick-tailed processes and for large shifts. This article provides a robust-likelihood cumulative sum (RLCUSUM) chart that discounts outliers and yet has the ability to detect large shifts quickly. The new chart is motivated by the likelihood underpinnings of the CUSUM. It is based on the likelihood of a variate constructed to ensure robust performance of the resulting chart. The new chart is compared with the conventional CUSUM in terms of average run length, with the finding that the RLCUSUM is much better than the conventional CUSUM, especially for large shifts. For small shifts, the two charts are essentially equivalent. We study the properties of the conditional limiting distribution. A final application of the RLCUSUM in assessing livestock disease shows that the method is applicable in process control."
"10.1198/016214507000001094","2007","Small-area estimation under informative probability sampling of areas and within the selected areas","1","In this article we show how to predict small-area means and obtain valid mean squared error estimators and confidence intervals when the areas represented in the sample are sampled with unequal probabilities possibly related to the true (unknown) area means and the sampling of units within the selected areas is with probabilities possibly related to the outcome values. Ignoring the effects of the sampling process on the distribution of the observed outcomes in such cases may bias the inference very severely. Classical design-based inference that uses the randomization distribution of probability-weighted estimators cannot be applied for predicting the means of nonsampled areas. We propose simple test statistics for testing the informativeness of the selection of areas and sampling of units within the selected areas. We illustrate the proposed procedures by a simulation study and a real application of estimating mean body mass index in U.S. counties, using data from the Third National Health and Nutrition Examination Survey."
"10.1198/016214507000001157","2007","Nonparametric association analysis of bivariate competing-risks data","3","Although nonparametric analyses of bivariate failure times under independent censoring have been widely studied, nonparametric analyses of bivariate competing-risks data have not yet been investigated. Such analyses are important in familial association studies, where multiple interacting failure types may violate the independent censoring assumption. We develop nonparametric estimators for the bivariate cause-specific hazards function and the bivariate cumulative incidence function that are natural analogs of their univariate counterparts and make no assumptions about the dependence of the risks. The estimators are shown to be uniformly consistent and to converge weakly to Gaussian processes. They provide the basis for novel time-dependent association measures, with the associated inferences yielding tests of cause-specific independence in pairs. The methodology performs well in simulations with realistic sample sizes. Its practical utility is illustrated in an analysis of dementia in the Cache County Study, where the nonparametric methods indicate that mother-child disease associations are strongly time-varying."
"10.1198/016214507000001085","2007","Efficient estimation for the accelerated failure time model","1","The accelerated failure time model provides a natural formulation of the effects of covariates on potentially censored response variable. The existing semiparametric estimators are computationally intractable and statistically inefficient. In this article we propose an approximate nonparametric maximum likelihood method for the accelerated failure time model with possibly time-dependent covariates. We estimate the regression parameters by maximizing a kernel-smoothed profile likelihood function. The maximization can be achieved through conventional gradient-based search algorithms. The resulting estimators are consistent and asymptotically normal. The limiting covariance matrix attains the semiparametric efficiency bound and can be consistently estimated. We also provide a consistent estimator for the error distribution. Extensive simulation studies demonstrate that the asymptotic approximations are accurate in practical situations and the new estimators are considerably more efficient than the existing ones. Illustrations with clinical and epidemiologic studies are provided."
"10.1198/016214507000000879","2007","A thinned block bootstrap variance estimation procedure for inhomogeneous spatial point patterns","6","When modeling inhomogeneous spatial point patterns, it is of interest to fit a parametric model for the first-order intensity function (FOIF) of the process in terms of some measured covariates. Estimates for the regression coefficients, say, can be obtained by maximizing a Poisson maximum likelihood criterion. Little work has been done on the asymptotic distribution of except in some special cases. In this article we show that is asymptotically normal for a general class of mixing processes. To estimate the variance of, we propose a novel thinned block bootstrap procedure that assumes that the point process is second-order reweighted stationary. To apply this procedure, only the FOIF, and not any high-order terms of the process, needs to be estimated. We establish the consistency of the resulting variance estimator, and demonstrate its efficacy through simulations and an application to a real data example."
"10.1198/016214507000001111","2007","Portmanteau test of independence for functional observations","2","In various fields, observations are curves over some natural time interval. These curves often arise from finely spaced measurements (e.g., in physical sciences and finance) or from smoothing unequally spaced sparse measurements. Recent years have seen the development of tools for analyzing such data in the growing field of functional data analysis. To validate the assumptions underlying these tools, it is important to verify that the functional observations form a simple random sample. If the curves form a (functional) time series, then model validation typically relies on checking whether model residuals are independent and identically distributed. We propose a test for independence and identical distribution of functional observations. To reduce dimension, curves are projected on the most important functional principal components, then a test statistic based on lagged cross-covariances of the resulting vectors is constructed. We show that this dimension-reduction step introduces asymptotically negligible terms; that is, the projections behave asymptotically as iid vector-valued observations. A complete asymptotic theory based on correlations of random matrices, functional principal component expansions, and Hilbert space techniques is developed. The test statistic has a chi-squared asymptotic null distribution and can be readily computed using the R package fda. The test has good empirical size and power, which in our simulations and examples is not affected by the choice of the functional basis. Its application is illustrated on two data sets: credit card sales activity and geomagnetic records."
"10.1198/016214507000000978","2007","A note on penalized spline smoothing with correlated errors","0","We investigate the behavior of data-driven smoothing parameters for penalized spline regression in the presence of correlated data. It has been shown for other smoothing methods that mean squared error minimizers, such as (generalized) cross-validation or the Akaike information criterion, are extremely sensitive to misspecifications of the correlation structure resulting in over- or (under-)fitting the data. In contrast to this, we show that a maximum likelihood-based choice of the smoothing parameter is more robust and that for a moderately misspecified correlation structure over- or (under-)fitting does not occur. This is demonstrated in simulations and data examples and is supported by theoretical investigations."
"10.1198/016214507000000860","2007","Unbalanced {H}aar technique for nonparametric function estimation","1","The discrete unbalanced Haar (UH) transform is a decomposition of one-dimensional data with respect to an orthonormal Haar-like basis where jumps in the basis vectors do not necessarily occur in the middle of their support. We introduce a multiscale procedure for estimation in Gaussian noise that consists of three steps: a UH transform, thresholding of the decomposition coefficients, and the inverse UH transform. We show that our estimator is mean squared consistent with near-optimal rates for a wide range of functions, uniformly over UH bases that are not ""too unbalanced."" A vital ingredient of our approach is basis selection. We choose each basis vector so that it best matches the data at a specific scale and location, where the latter parameters are determined by the ""parent"" basis vector. Our estimator is computable in O(n log n) operations. A simulation study demonstrates the good performance of our estimator compared with state-of-the-art competitors. We apply our method to the estimation of the mean intensity of the time series of earthquake counts occurring in northern California. We discuss extensions to image data and to smoother wavelets."
"10.1198/016214507000001166","2007","Weighted repeated median smoothing and filtering","0","We propose weighted repeated median filters and smoothers for robust nonparametric regression in general and for robust online signal extraction from time series in particular. The new methods allow us to remove outlying sequences and to preserve discontinuities (shifts) in the underlying regression function (the signal) in the presence of local linear trends. Suitable weighting of the observations according to their distances in the design space reduces the bias arising from nonlinearities and improves the efficiency using larger bandwidths, while still distinguishing long-term shifts from outlier sequences. Other localized robust regression techniques like S, M, and MM estimators as well as weighted L I regression, are examined for comparison."
"10.1198/016214507000000950","2007","Robust linear model selection based on least angle regression","1","In this article we consider the problem of building a linear prediction model when the number of candidate predictors is large and the data possibly contain anomalies that are difficult to visualize and clean. We want to predict the nonoutlying cases; therefore, we need a method that is simultaneously robust and scalable. We consider the stepwise least angle regression (LARS) algorithm which is computationally very efficient but sensitive to outliers. We introduce two different approaches to robustify LARS. The plug-in approach replaces the classical correlations in LARS by robust correlation estimates. The cleaning approach first transforms the data set by shrinking the outliers toward the bulk of the data (which we call multivariate Winsorization) and then applies LARS to the transformed data. We show that the plug in approach is time-efficient and scalable and that the bootstrap can be used to stabilize its results. We recommend using bootstrapped robustified LARS to sequence a number of candidate predictors to form a reduced set from which a more refined model can be selected."
"10.1198/016214507000000941","2007","False discovery rates for spatial signals","3","The problem of multiple testing for the presence of signal in spatial data can involve numerous locations. Traditionally, each location is tested separately for signal presence, but then the findings are reported in terms of clusters of nearby locations. This is an indication that the units of interest for testing are clusters rather than individual locations. The investigator may know a priori these more natural units or an approximation to them. We suggest testing these cluster units rather than individual locations, thus increasing the signal-to-noise ratio within the unit tested as well as reducing the number of hypothesis tests conducted. Because the signal may be absent from part of each cluster, we define a cluster as containing a signal if the signal is present somewhere within the cluster. We suggest controlling the false discovery rate (FDR) on clusters (i.e., the expected proportion of clusters rejected erroneously out of all clusters rejected) or its extension to general weights (WFDR). We introduce a powerful two-stage testing procedure and show that it controls the WFDR. Once the cluster discoveries have been made, we suggest ""cleaning"" locations in which the signal is absent. For this purpose, we develop a hierarchical testing procedure that first tests clusters, then tests locations within rejected clusters. We show formally that this procedure controls the desired location error rate asymptotically, and conjecture that this is also so for realistic settings by extensive simulations. We discuss an application to functional neuroimaging that motivated this research and demonstrate the advantages of the proposed methodology on an example."
"10.1198/016214507000001120","2007","Distance-weighted discrimination","5","High-dimension low-sample size statistical analysis is becoming increasingly important in a wide range of applied contexts. In such situations, the popular support vector machine suffers from ""data piling"" at the margin, which can diminish generalizability. This leads naturally to the development of distance-weighted discrimination, which is based on second-order cone programming, a modern computationally intensive optimization method."
"10.1198/016214507000000059","2007","Estimating time to event from longitudinal categorical data: an analysis of multiple sclerosis progression","0","The expanded disability status scale (EDSS) is an ordinal score that measures progression in multiple sclerosis (MS). Progression is defined as reaching EDSS of a certain level (absolute progression) or increasing EDSS by one point (relative progression). Survival methods for time to progression are not adequate for such data because they do not exploit the EDSS level at the end of follow-up. Instead, we suggest a Markov transitional model applicable for repeated categorical or ordinal data. This approach enables derivation of covariate-specific survival curves, obtained after estimation of the regression coefficients and manipulations of the resulting transition matrix. Large-sample theory and resampling methods are employed to derive pointwise confidence intervals, which perform well in simulation. Methods for generating survival curves for time to EDSS of a certain level, time to increase EDSS by at least one point, and time to two consecutive visits with EDSS greater than 3 are described explicitly. The regression models described are easily implemented using standard software packages. Survival curves are obtained from the regression results using packages that support simple matrix calculation. We present and demonstrate our method on data collected at the Partners Multiple Sclerosis Center in Boston. We apply our approach to progression defined by time to two consecutive visits with EDSS greater than 3 and calculate crude (without covariates) and covariate-specific curves."
"10.1198/016214506000001446","2007","Inference of tamoxifen's effects on prevention of breast cancer from a randomized controlled trial","1","The largest randomized, double-blind, placebo-controlled chemoprevention trial, the National Surgical Adjuvant Breast and Bowel Project's Breast Cancer Prevention Trial (NSABP-BCPT), evaluated the efficacy of tamoxifen in preventing breast cancer among women at high risk of developing the disease. The trial has reported a reduction of breast cancer incidence for the tamoxifen group; however, the effect of tamoxifen on the time to diagnosis of the disease over the 6-year follow-up of the trial has not been fully explored in the literature. We propose a flexible semiparametric model to assess the effects of tamoxifen on the incidence of breast cancer as well as time to diagnosis of the disease separately in the framework of a cure rate model. We used an estimating equation approach to estimating the unknown parameters and assessed the semiparametric model assumption with a test based on the area between two survival curves. In the NSABP-BCPT data, we found that tamoxifen has a substantial effect in reducing invasive breast cancer events in estrogen receptor (ER)-positive tumors, but has no effect on ER-negative tumors. Among women diagnosed with ER-positive breast cancer during study follow-up, there was little difference in terms of time to diagnosis between the two arms. However, tamoxifen may advance the time to breast cancer diagnosis for ER-negative breast cancer, whereas the incidence of ER-negative tumors is similar in the two treatment arms."
"10.1198/016214507000000031","2007","High-resolution space-time ozone modeling for assessing trends","2","This article proposes a space-time model for daily 8-hour maximum ozone levels to provide input for regulatory activities: detection, evaluation, and analysis of spatial patterns and temporal trend in ozone summaries. The model is applied to the analysis of data from the state of Ohio that contains a mix of urban, suburban, and rural ozone monitoring sites. The proposed space-time model is autoregressive and incorporates the most important meteorological variables observed at a collection of ozone monitoring sites as well as at several weather stations where ozone levels have not been observed. This misalignment is handled through spatial modeling. In so doing we adopt a computationally convenient approach based on the successive daily increments in meteorological variables. The resulting hierarchical model is specified within a Bayesian framework and is fitted using Markov chain Monte Carlo techniques. Full inference with regard to model unknowns as well as for predictions in time and space, evaluation of annual summaries, and assessment of trends are presented."
"10.1198/016214506000001356","2007","Incorporating historical control data when comparing tumor incidence rates","1","Animal carcinogenicity studies, such as those conducted by the U.S. National Toxicology Program (NTP), focus on detecting trends in tumor rates across dose groups. Over time, the NTP has compiled vast amounts of data on tumors in control animals. Currently, this information is used informally, without the benefit of statistical tests for carcinogenicity that directly incorporate historical data on control animals. This article proposes a survival-adjusted test for detecting dose-related trends in tumor incidence rates, which incorporates data on historical control rates and formally accounts for variation in these rates among studies. An extensive simulation, based on a wide range of realistic situations, demonstrates that the proposed test performs well compared with the current NTP test. which does not incorporate historical control data. In particular, our test can aid in interpreting the occurrence of a few tumors in treated animals that are rarely seen in controls. One such example, which motivates our work, concerns the analysis of histiocytic sarcoma in the NTP's 2-year cancer bioassay of benzophenone. Whereas the occurrence of three histiocytic sarcomas in female rats was not significant according to the current NTP testing procedure (p =.074), it was highly significant (p =.004) when control data from six recent historical studies were included and our test was applied to the combined data."
"10.1198/016214507000000040","2007","On the estimating of disability-free life expectancy: {S}ullivan's method and its extension","0","A rapidly aging population, such as the United States today, is characterized by the increased prevalence of chronic impairment. Robust estimation of disability-free life expectancy (DFLE), or healthy life expectancy, is essential for examining whether additional years of life are spent in good health and whether life expectancy is increasing faster than the decline of disability rates. Over 30 years since its publication, Sullivan's method remains the most widely used method to estimate DFLE. Therefore, it is surprising to note that Sullivan did not provide any formal justification of his method. Debates in the literature have centered around the properties of Sullivan's method and have yielded conflicting results regarding the assumptions required for Sullivan's method. In this article we establish a statistical foundation of Sullivan's method. We prove that, under stationarity assumptions, Sullivan's estimator is unbiased and consistent. This resolves the debate in the literature, which has generally concluded that additional assumptions are necessary. We also show that the standard variance estimator is consistent and approximately unbiased. Finally, we demonstrate that Sullivan's method can be extended to estimate DFLE without stationarity assumptions. Such an extension is possible whenever a cohort life table and either consecutive cross-sectional disability surveys or a longitudinal survey are available. Our empirical analysis of the 1907 and 1912 U.S. birth cohorts suggests that while mortality rates remain approximately stationary, disability rates decline during this time period."
"10.1198/016214506000001455","2007","Bayesian forecasting of an inhomogeneous {P}oisson process with applications to call center data","2","A call center is a centralized hub where customer and other telephone calls are handled by an organization. In today's economy, call centers have become the primary points of contact between customers and businesses. Thus accurate predictions of call arrival rates are indispensable to help call center practitioners staff their call centers efficiently and cost-effectively. This article proposes a multiplicative model for modeling and forecasting within-day arrival rates to a U.S. commercial bank's call center. Markov chain Monte Carlo sampling methods are used to estimate both latent states and model parameters. One-day-ahead density forecasts for the rates and counts are provided. The calibration of these predictive distributions is evaluated through probability integral transforms. Furthermore, 1-day-ahead forecasts comparisons with classical statistical models are given. Our predictions show significant improvements of up to 25% over these standards. A sequential Monte Carlo algorithm is also proposed for sequential estimation and forecasts of the model parameters and rates."
"10.1198/016214506000001176","2007","Testing forecast optimality under unknown loss","1","Empirical tests of forecast optimality have traditionally been conducted under the assumption of mean squared error loss or some other known loss function. In this article we establish new testable properties that hold when the forecaster's loss function is unknown but testable restrictions can be imposed on the data-generating process, trading off conditions on the data-generating process against conditions on the loss function. We propose flexible estimation of the forecaster's loss function in situations where the loss depends not only on the forecast error, but also on other state variables, such as the level of the target variable. We apply our results to the problem of evaluating the Federal Reserve's forecasts of output growth. Forecast optimality is rejected if the Fed's loss depends only on the forecast error. However, the empirical findings are consistent with forecast optimality provided that overpredictions of output growth are costlier to the Fed than underpredictions, particularly during periods of low economic growth."
"10.1198/016214506000000942","2007","Bayesian multivariate isotonic regression splines: applications to carcinogenicity studies","1","In many applications, interest focuses on assessing the relationship between a predictor and a multivariate outcome variable, and there may be prior knowledge about the shape of the regression curves. For example, regression functions that relate dose of a possible risk factor to different adverse outcomes can often be assumed to be nondecreasing. In such cases, interest focuses on (1) assessing evidence of an overall adverse effect, (2) determining which outcomes are most affected, and (3) estimating outcome-specific regression curves. This article proposes a Bayesian approach for addressing this problem, motivated by multi site tumor data from carcinogenicity experiments. A multivariate smoothing spline model is specified, that accommodates dependency in the multiple curves through a hierarchical Markov random field prior for the basis coefficients, while also allowing for residual correlation. A Gibbs sampler is proposed for posterior computation, and the approach is applied to data on body weight and tumor occurrence."
"10.1198/016214506000000951","2007","A multiresolution hazard model for multicenter survival studies: application to tamoxifen treatment in early stage breast cancer","0","In multicenter studies, one often needs to make inference about a population survival curve based on multiple, possibly heterogeneous survival data from individual centers. We investigate a flexible Bayesian method for estimating a population survival curve based on a semiparametric multiresolution hazard model that can incorporate covariates and account for center heterogeneity. The method yields a smooth estimate of the survival curve for ""multiple resolutions"" or time scales of interest. The Bayesian model used has the capability to accommodate general forms of censoring and a priori smoothness assumptions. We develop a model checking and diagnostic technique based on the posterior predictive distribution and use it to identify departures from the model assumptions. The hazard estimator is used to analyze data from 110 centers that participated in a multicenter randomized clinical trial to evaluate tamoxifen in the treatment of early stage breast cancer. Of particular interest are the estimates of center heterogeneity in the baseline hazard curves and in the treatment effects, after adjustment for a few key clinical covariates. Our analysis suggests that the treatment effect estimates are rather robust, even for a collection of small trial centers, despite variations in center characteristics."
"10.1198/016214506000001158","2007","Local smoothing image segmentation for spotted microarray images","0","Gene microarray data are used in a wide variety of applications, including pharmaceutical and clinical research. By comparing gene expression in normal and abnormal cells, microarrays can be used to identify genes involved in particular diseases, and these genes then can be targeted by therapeutic drugs. Most gene expression data are produced from spotted microarray images. A spotted microarray image consists of thousands of spots, with individual DNA sequences first printed at each spot and then equal amounts of probes (e.g., cDNA samples) from treatment and control cells mixed and hybridized with the printed DNA sequences. To obtain gene expression data, the image first must be segmented to separate foregrounds from backgrounds for individual spots, after which averages of foreground pixels are used to compute the gene expression data. Thus image segmentation of microarray images is directly related to the reliability of gene expression data. Several image segmentation procedures have been suggested in the literature and included in software packages handling gene microarray data. This article proposes a new image segmentation methodology based on local smoothing. Theoretical arguments and numerical studies show that it has good statistical properties and should perform well in applications."
"10.1198/016214507000001238","2007","Rejoinder [MR2412534]","0",""
"10.1198/016214507000000464","2007","Gait-based human recognition by classification of cyclostationary processes on nonlinear shape manifolds","0","We study the problem of analyzing and classifying human gait by modeling it as a stochastic process on a shape space. We consider gait as a evolution of human silhouettes as seen in video sequences, and focus on their shapes. More specifically, we define a shape space of planar, closed curves and model a human gait as a stochastic process on this space. Due to the periodic nature of human walk, this process is naturally constrained to be cyclostationary, that is, its mean path is assumed to be cyclic. We compare two subjects using a metric that quantifies differences between average gait cycles of each subject. This computation uses several tools from differential geometry of the shape space, including computation of geodesics, estimation of means of observed shapes, interpolation between observed shapes, and temporal registration of two gait cycles. Finally, we apply a nearest-neighbor classifier, using the gait metric, to perform human recognition, and present results from an experiment involving 26 subjects."
"10.1198/016214507000001201","2007","Rejoinder [MR2412530]","0",""
"10.1198/016214507000000581","2007","Statistical analysis of diffusion tensors in diffusion-weighted magnetic resonance imaging data","5","Diffusion tensor imaging has been widely used to reconstruct the structure and orientation of fibers in biological tissues, particularly in the white matter of the brain, because it can track the effective diffusion of water along those fibers. The raw diffusion-weighted images from which diffusion tensors are estimated, however, inherently contain noise. Noise in the images produces uncertainty in the estimation of the tensors (which are 3 x 3 positive-definite matrices) and of their derived quantities, including eigenvalues, eigenvectors, and the fiber pathways that are reconstructed based on those tensor elements. The aim of this article is to provide a comprehensive theoretical framework of statistical inference for quantifying the effects of noise on diffusion tensors, on their eigenvalues and eigenvectors, and on their morphological classification. We propose a semiparametric model to account for noise in diffusion-weighted images. We then develop a one-step, weighted least squares estimate of the tensors and justify use of the one-step estimates based on our theoretical framework and computational results. We also quantify the effects of noise on the eigenvalues and eigenvectors of the estimated tensors by establishing their limiting distributions. We construct pseudo-likelihood ratio statistics to classify tensor morphologies. Simulation studies show that our theoretical results can accurately predict the stochastic behavior of the estimated eigenvalues and eigenvectors, as well as the bias that is introduced by sorting the eigenvalues by their magnitudes. Implementation of these methods is illustrated in a diffusion-weighted dataset from seven healthy human subjects."
"10.1198/016214507000000518","2007","Robust modeling for inference from generalized linear model classes","1","Generalized linear models (GLMs) are widely used for data analysis; however, their maximum likelihood estimators can be sensitive to outliers. We propose new statistical models that allow robust inferences from the GLM class of models, including Poisson and binomial GLMs, and their extension to generalized linear mixed models. The likelihood score equations from the new models give estimators with bounded influence, so that the resulting estimators are robust against outliers while maintaining high efficiency in the absence of outliers."
"10.1198/016214507000000608","2007","Sensitivity analysis for instrumental variables regression with overidentifying restrictions","3","Instrumental variables regression (IV regression) is a method for making causal inferences about the effect of a treatment based on an observational study in which there are unmeasured confounding variables. The method requires one or more valid instrumental variables (IVs); a valid IV is a variable that is associated with the treatment, is independent of unmeasured confounding variables, and has no direct effect on the outcome. Often there is uncertainty about the validity of the proposed IVs. When a researcher proposes more than one IV, the validity of these IVs can be tested through the ""overidentifying restrictions test."" Although the overidentifying restrictions test does provide some information, the test has no power versus certain alternatives and can have low power versus many alternatives due to its omnibus nature. To fully address uncertainty about the validity of the proposed IVs, we argue that a sensitivity analysis is needed. A sensitivity analysis examines the impact of plausible amounts of invalidity of the proposed IVs on inferences for the parameters of interest. We develop a method of sensitivity analysis for IV regression with overidentifying restrictions that makes full use of the information provided by the overidentifying restrictions test but provides more information than the test by exploring sensitivity to violations of the validity of the proposed IVs in directions for which the test has low power. Our sensitivity analysis uses interpretable parameters that can be discussed with subject matter experts. We illustrate our method using a study of food demand among rural households in the Philippines."
"10.1198/016214507000000509","2007","Unified {LASSO} estimation by least squares approximation","7","We propose a method of least squares approximation (LSA) for unified yet simple LASSO estimation. Our general theoretical framework includes ordinary least squares, generalized linear models, quantile regression, and many others as special cases. Specifically, LSA can transfer many different types of LASSO objective functions into their asymptotically equivalent least squares problems. Thereafter, the standard asymptotic theory can be established and the LARS algorithm can be applied. In particular, if the adaptive LASSO penalty and a Bayes information criterion-type tuning parameter selector are used, the resulting LSA estimator can be as efficient as the oracle. Extensive numerical studies confirm our theory."
"10.1198/016214507000000590","2007","Variable selection in finite mixture of regression models","0","In the applications of finite mixture of regression (FMR) models, often many covariates are used, and their contributions to the response variable vary from one component to another of the mixture model. This creates a complex variable selection problem. Existing methods, such as the Akaike information criterion and the Bayes information criterion, are computationally expensive as the number of covariates and components in the mixture model increases. In this article we introduce a penalized likelihood approach for variable selection in FMR models. The new method introduces penalties that depend on the size of the regression coefficients and the mixture structure. The new method is shown to be consistent for variable selection. A data-adaptive method for selecting tuning parameters and an EM algorithm for efficient numerical computations are developed. Simulations show that the method performs very well and requires much less computing power than existing methods. The new method is illustrated by analyzing two real data sets."
"10.1198/016214507000000491","2007","Optimal geostatistical model selection","2","In many fields of science, predicting variables of interest over a study region based on noisy data observed at some locations is an important problem. Two popular methods for the problem are kriging and smoothing splines. The former assumes that the underlying process is stochastic, whereas the latter assumes it is purely deterministic. Kriging performs better than smoothing splines in some situations, but is outperformed by smoothing splines in others. However, little is known regarding selecting between kriging and smoothing splines. In addition, how to perform variable selection in a geostatistical model has not been well studied. In this article we propose a general methodology for selecting among arbitrary spatial prediction methods based on (approximately) unbiased estimation of mean squared prediction errors using a data perturbation technique. The proposed method accounts for estimation uncertainty in both kriging and smoothing spline predictors, and is shown to be optimal in terms of two mean squared prediction error criteria. A simulation experiment is performed to demonstrate the effectiveness of the proposed methodology. The proposed method is also applied to a water acidity data set by selecting important variables responsible for water acidity based on a spatial regression model. Moreover, a new method is proposed for estimating the noise variance that is robust and performs better than some well-known methods."
"10.1198/016214507000000527","2007","Functional principal component regression and functional partial least squares","6","Regression of a scalar response on signal predictors, such as near-infrared (NIR) spectra of chemical samples, presents a major challenge when, as is typically the case, the dimension of the signals far exceeds their number. Most solutions to this problem reduce the dimension of the predictors either by regressing on components [e.g., principal component regression (PCR) and partial least squares (PLS)l or by smoothing methods, which restrict the coefficient function to the span of a spline basis. This article introduces functional versions of PCR and PLS, which combine both of the foregoing dimension-reduction approaches. Two versions of functional PCR are developed, both using B-splines and roughness penalties. The regularized-components version applies such a penalty to the construction of the principal components (i.e., it uses functional principal components), whereas the regularized-regression version incorporates a penalty in the regression. For the latter form of functional PCR, the penalty parameter may be selected by generalized cross-validation, restricted maximum likelihood (REML), or a minimum mean integrated squared error criterion. Proceeding similarly, we develop two versions of functional PLS. Asymptotic convergence properties of regularized-regression functional PCR are demonstrated. A simulation study and split-sample validation with several NIR spectroscopy data sets indicate that functional PCR and functional PLS, especially the regularized-regression versions with REML, offer advantages over existing methods in terms of both estimation of the coefficient function and prediction of future observations."
"10.1198/016214507000000617","2007","Robust truncated hinge loss support vector machines","2","The support vector machine (SVM) has been widely applied for classification problems in both machine learning and statistics. Despite its popularity, however, SVM has some drawbacks in certain situations. In particular, the SVM classifier can be very sensitive to outliers in the training sample. Moreover, the number of support vectors (SVs) can be very large in many applications. To circumvent these drawbacks, we propose the robust truncated hinge loss SVM (RSVM), which uses a truncated hinge loss. The RSVM is shown to be more robust to outliers and to deliver more accurate classifiers using a smaller set of SVs than the standard SVM. Our theoretical results show that the RSVM is Fisher-consistent, even when there is no dominating class, a scenario that is particularly challenging for multicategory classification. Similar results are obtained for a class of margin-based classifiers."
"10.1198/016214507000000248","2007","Longitudinal studies with outcome-dependent follow-up: models and {B}ayesian regression","0","We propose Bayesian parametric and serniparametric partially linear regression methods to analyze the outcome-dependent follow-up data when the random time of a follow-up measurement of an individual depends on the history of both observed longitudinal outcomes and previous measurement times. We begin with the investigation of the simplifying assumptions of Lipsitz, Fitzmaurice, Ibrahim, Gelber, and Lipshultz, and present a new model for analyzing such data by allowing subject-specific correlations for the longitudinal response and by introducing a subject-specific latent variable to accommodate the association between the longitudinal measurements and the follow-up times. An extensive simulation study shows that our Bayesian partially linear regression method facilitates accurate estimation of the true regression line and the regression parameters. We illustrate our new methodology using data from a longitudinal observational study."
"10.1198/016214507000000545","2007","Oracle and adaptive compound decision rules for false discovery rate control","11","We develop a compound decision theory framework for multiple-testing problems and derive an oracle rule based on the z values that minimizes the false nondiscovery rate (FNR) subject to a constraint on the false discovery rate (FDR). We show that many commonly used multiple-testing procedures, which are p value-based, are inefficient, and propose an adaptive procedure based on the z values. The z value-based adaptive procedure asymptotically attains the performance of the z value oracle procedure and is more efficient than the conventional p value-based methods. We investigate the numerical performance of the adaptive procedure using both simulated and real data. In particular, we demonstrate our method in an analysis of the microarray data from a human immunodeficiency virus study that involves testing a large number of hypotheses simultaneously."
"10.1198/016214507000000725","2007","Rejoinder [MR2411651]","0",""
"10.1198/016214506000000122","2007","Implementation of estimating function-based inference procedures with {M}arkov chain {M}onte {C}arlo samplers","2","Under a serniparametric or nonparametric setting, inferences about the unknown parameter are often made based on a nonsmooth estimating function. Resampling methods are quite handy for obtaining good approximations to the distribution of the consistent estimator when the estimating equation and its resampled counterparts are not difficult to solve numerically. In this article we propose a simple, flexible procedure that provides such approximations through the standard Markov chain Monte Carlo sampler without so, solving any equations. More generally, the procedure may locate all possible roots of the estimating equation and provides an approximation to the distribution of each root. We illustrate our proposed procedure extensively with three examples and evaluate its performance comprehensively through a simulation study."
"10.1198/016214507000000068","2007","Variable selection in regression mixture modeling for the discovery of gene regulatory networks","0","The profusion of genomic data through genome sequencing and gene expression microarray technology has facilitated statistical research in determining gene interactions regulating a biological process. Current methods generally consist of a two-stage procedure: clustering gene expression measurements and searching for regulatory ""switches,"" typically short, conserved sequence patterns (motifs) in the DNA sequence adjacent to the genes. This process often leads to misleading conclusions as incorrect cluster selection may lead to missing important regulatory motifs or making many false discoveries. Treating cluster memberships as known, rather than estimated, introduces bias into analyses, preventing uncertainty about cluster parameters. Further, there is underutilization of the available data, as the sequence information is ignored for purposes of expression clustering and vice versa. We propose a way to address these issues by combining gene clustering and motif discovery in a unified framework, a mixture of hierarchical regression models, with unknown components representing the latent gene clusters, and genomic sequence features linked to the resultant gene expression through a multivariate hierarchical regression. We demonstrate a Monte Carlo method for simultaneous variable selection (for motifs) and clustering (for genes). The selection of the number of components in the mixture is addressed by computing the analytically intractable Bayes factor through a novel multistage mixture importance sampling approach. This methodology is used to analyze a yeast cell cycle dataset to determine an optimal set of motifs that discriminates between groups of genes and simultaneously finds the most significant gene clusters."
"10.1198/016214506000000771","2007","Structured measurement error in nutritional epidemiology: applications in the pregnancy, infection, and nutrition ({PIN}) study","1","Preterm birth, defined as delivery before 37 completed weeks' gestation, is a leading cause of infant morbidity and mortality. Identifying factors related to preterm delivery is an important goal of public health professionals who wish to identify etiologic pathways to target for prevention. Validation studies are often conducted in nutritional epidemiology in order to study measurement error in instruments that are generally less invasive or less expensive than ""gold standard"" instruments. Data from such studies are then used in adjusting estimates based on the full study sample. However, measurement error in nutritional epidemiology has recently been shown to be complicated by correlated error structures in the study-wide and validation instruments. investigators of a study of preterm birth and dietary intake designed a validation study to assess measurement error in a food frequency questionnaire (FFQ) administered during pregnancy and with the secondary goal of assessing whether a single administration of the FFQ could be used to describe intake over the relatively short pregnancy period, in which energy intake typically increases. Here, we describe a likelihood-based method via Markov chain Monte Carlo to estimate the regression coefficients in a generalized linear model relating preterm birth to covariates, where one of the covariates is measured with error and the multivariate measurement error model has correlated errors among contemporaneous instruments (i.e., FFQs, 24-hour recalls, and biomarkers). Because of constraints on the covariance parameters in our likelihood, identifiability for all the variance and covariance parameters is not guaranteed, and, therefore, we derive the necessary and sufficient conditions to identify the variance and covariance parameters under our measurement error model and assumptions. We investigate the sensitivity of our likelihood-based model to distributional assumptions placed on the true folate intake by employing serniparametric Bayesian methods through the mixture of Dirichlet process priors framework. We exemplify our methods in a recent prospective cohort study of risk factors for preterm birth. We use long-term folate as our error-prone predictor of interest, the FFQ and 24-hour recall as two biased instruments, and the serum folate biomarker as the unbiased instrument. We found that folate intake, as measured by the FFQ, led to a conservative estimate of the estimated odds ratio of preterm birth (.76) when compared to the odds ratio estimate from our likelihood-based approach, which adjusts for the measurement error (.63). We found that our parametric model led to similar conclusions to the serniparametric Bayesian model."
"10.1198/016214506000000744","2007","A statistical framework of optimal workload consolidation with application to capacity planning for on-demand computing","0","In on-demand computing services, the customer pays based on actual usage, and the service provider is free to allocate unused capacity to other customers. Capacity planning becomes an important issue in such a shared environment. It is desirable that a portfolio effect occurs so that less total capacity is required in a shared system than in a dedicated system. In this article a quantile-based statistical approach is proposed to quantify the portfolio effect and the associated risks. It is shown that the portfolio effect may or may not exist for a given set of workloads, depending crucially on their joint distribution and the assumed risk levels. For Gaussian workloads, the portfolio effect almost always exists regardless of the statistical correlation and risk level. For non-Gaussian workloads, the portfolio effect is not guaranteed and may even be negative, even if the workloads are negatively correlated or statistically independent. However, when simultaneously recorded workload history is available, the portfolio effect can be estimated directly from the data. Based on the data-driven approach, an optimization problem is formulated for capacity planning with the aim of maximizing the portfolio effect for a given set of workloads. This problem calls for consolidation of the workloads into one or more portfolios so that each portfolio can be served satisfactorily by a dedicated system and the total capacity requirement is minimized. Iterative algorithms are proposed to solve the optimization problem numerically. The method is applied to a server consolidation problem with real workload data."
"10.1198/016214506000000780","2007","Bayesian spatial modeling of extreme precipitation return levels","2","Quantification of precipitation extremes is important for flood planning purposes, and a common measure of extreme events is the r-year return level. We present a method for producing maps of precipitation return levels and uncertainty measures and apply it to a region in Colorado. Separate hierarchical models are constructed for the intensity and the frequency of extreme precipitation events. For intensity, we model daily precipitation above a high threshold at 56 weather stations with the generalized Pareto distribution. For frequency, we model the number of exceedances at the stations as binomial random variables. Both models assume that the regional extreme precipitation is driven by a latent spatial process characterized by geographical and climatological covariates. Effects not fully described by the covariates are captured by spatial structure in the hierarchies. Spatial methods were improved by working in a space with climatological coordinates. Inference is provided by a Markov chain Monte Carlo algorithm and spatial interpolation method, which provide a natural method for estimating uncertainty."
"10.1198/016214506000001040","2007","An analysis of the {N}ew {Y}ork {C}ity {P}olice {D}epartment's ``stop-and-frisk'' policy in the context of claims of racial bias","1","Recent studies by police departments and researchers confirm that police stop persons of racial and ethnic minority groups more often than whites relative to their proportions in the population. However, it has been argued that stop rates more accurately reflect rates of crimes committed by each ethnic group, or that stop rates reflect elevated rates in specific social areas, such as neighborhoods or precincts. Most of the research on stop rates and police-citizen interactions has focused on traffic stops, and analyses of pedestrian stops are rare. In this article we analyze data from 125,000 pedestrian stops by the New York Police Department over a 15-month period. We disaggregate stops by police precinct and compare stop rates by racial and ethnic group, controlling for previous race-specific arrest rates. We use hierarchical multilevel models to adjust for precinct-level variability, thus directly addressing the question of geographic heterogeneity that arises in the analysis of pedestrian stops. We find that persons of African and Hispanic descent were stopped more frequently than whites, even after controlling for precinct variability and race-specific estimates of crime participation."
"10.1198/016214506000001167","2007","Semiparametric mixed models for increment-averaged data with application to carbon sequestration in agricultural soils","0","Adoption of conservation tillage practice in agriculture offers the potential to mitigate greenhouse gas emissions. Studies comparing conservation tillage methods to traditional tillage pair fields under the two management systems and obtain soil core samples from each treatment. Cores are divided into multiple increments, and matching increments from one or more cores are aggregated and analyzed for carbon stock. These data represent not the actual value at a specific depth, but rather the total or average over a depth increment. A semiparametric mixed model is developed for such increment-averaged data. The model uses parametric fixed effects to represent covariate effects, random effects to capture correlation within studies, and an integrated smooth function to describe effects of depth. The depth function is specified as an additive model, estimated with penalized splines using standard mixed model software. Smoothing parameters are automatically selected using restricted maximum likelihood. The methodology is applied to the problem of estimating a change in carbon stock due to a change in tillage practice."
"10.1198/016214506000000825","2007","Oscillations and time trends in stratospheric ozone levels: a functional data analysis approach","0","A functional data analysis approach is presented to study altitude-dependent patterns of ozone variation in data from a sequence of ozonesonde flights. Ozonesondes are balloon-based instruments that measure ozone as the balloon ascends through the troposphere and lower stratosphere. This article concentrates on variation in the altitude range of 15.5-30.5 kin, in January-July in 1967-1998. Ozonesonde flights originating at a mid-latitude site, Hohenpeissenberg in Germany, are studied. Estimates are obtained of altitude-dependent nonlinear time trends in ozone partial pressures, together with ozone variation associated with the quasi-biennial oscillation (QBO), an atmospheric process thought to influence global ozone transport. Both methodological and scientific contributions are made. The data analysis approach combines dimension-reduction basis function approximations with low-dimensional spline-based models on the basis function coefficients. This provides an efficient and flexible approach for studying complex time/altitude variation in the ozone partial pressure profiles, including nonlinear time trends. In contrast, the standard approach uses multiple linear regression models to estimate linear or piecewise-linear ozone time trends separately for each altitude level. Scientific results include identifying clear bimodalities (in altitude) in the estimated QBO components of variation in certain months. These may relate to planetary wave transport of ozone from the tropics to mid-latitudes. Additional empirical evidence of an 11-year cycle in ozone levels also is provided, possibly linked with a solar cycle. However, ozone peaks at Hohenpeissenberg do not always coincide with the timing of the maxima of the I I-year solar cycle. This may indicate a transport-related lag in ozone maxima at some altitudes. The estimated QBO features are robust to the presence or absence of the data quality ""correction factors"" commonly used in ozonesonde studies. However, the nonlinear time trend components show greater sensitivity to these."
"10.1198/016214507000000824","2007","Rejoinder [MR2410107]","0",""
"10.1198/016214507000000761","2007","Subjective likelihood for the assessment of trends in the ocean's mixed-layer depth","0","This article describes a Bayesian statistical analysis of long-term changes in the depth of the ocean's mixed layer. The data are thermal profiles recorded by ships. For these data, there is no good sampling model and thus no obvious likelihood function. Our approach is to elicit posterior distributions for training data directly from the expert. We then infer the likelihood function and use it on large datasets."
"10.1198/016214507000000220","2007","Semiparametric estimation of spectral density with irregular observations","2","We propose a semiparametric method for estimating spectral densities of isotropic Gaussian processes with scattered data. The spectral density function (Fourier transform of the covariance function) is modeled as a linear combination of B-splines up to a cutoff frequency and, from this point, a truncated algebraic tail. We calculate an analytic expression for the covariance function and tackle several numerical issues that arise when calculating the likelihood. The parameters are estimated by maximizing the likelihood using the simulated annealing method. Our method directly estimates the tail behavior of the spectral density, which has the greatest impact on interpolation properties. The use of the likelihood in parameter estimation takes the correlations between observations fully into account. We compare our method with a kernel method proposed by Hall et al. and a parametric method using the Matern model. Simulation results show that our method outperforms the other two by several criteria. Application to rainfall data shows that our method outperforms the kernel method."
"10.1198/016214507000000301","2007","Jump surface estimation, edge detection, and image restoration","1","Surface estimation is important in many applications. When conventional smoothing procedures (e.g., running averages, local polynomial kernel smoothing procedures, smoothing spline procedures) are used for estimating jump surfaces from noisy data, jumps are blurred at the same time when noise is removed. In recent years, new smoothing methodologies have been proposed in the statistical literature for detecting jumps in surfaces and for estimating jump surfaces with jumps preserved. We provide a review of these methodologies. Because a monochrome image can be considered a jump surface of the image intensity function, with jumps at the outlines of objects, edge detection and image restoration problems in image processing are closely related to the jump surface estimation problem in statistics. We also review major methodologies on edge detection and image restoration, and discuss connections and differences among these methods and related methods in the statistical literature."
"10.1198/016214507000000202","2007","A nonparametric assessment of properties of space-time covariance functions","2","We propose a unified framework for testing various assumptions commonly made for covariance functions of stationary spatio-temporal random fields. The methodology is based on the asymptotic normality of space-time covariance estimators. We focus on tests for full symmetry and separability in this article, but our framework naturally covers testing for isotropy and Taylor's hypothesis. Our test successfully detects the asymmetric and nonseparable features in two sets of wind speed data. We per-form simulation experiments to evaluate our test and conclude that our method is reliable and powerful for assessing common assumptions on space-time covariance functions."
"10.1198/016214507000000086","2007","Smooth location-dependent bandwidth selection for local polynomial regression","0","A bandwidth function for local polynomial models is commonly obtained by optimizing a pointwise penalty criterion, such as an estimated mean squared error (MSE), over a grid of predictor locations. A resultant regression estimate may suffer from irregularities, such as discontinuities, and contextual information over nearby predictor locations is not used. To mediate these difficulties, ad hoe postprocessing is sometimes carried out in the form of smoothing of the penalty criterion and/or the bandwidth estimates. In this work a technique is developed for choosing a smooth bandwidth function that uses a smoothing spline selected based on new ""fit""and ""roughness"" penalties. The fit penalty pushes the bandwidth estimate to adhere to the chosen pointwise criterion, whereas the roughness penalty is imposed on the fitted regression estimate as opposed to the bandwidth estimate, which usually is not of direct interest. The technique can be used in conjunction with various adaptive bandwidth selection methods and provides a systematic way of incorporating contextual information into bandwidth estimation. To justify a spline bandwidth function, we show that under mild regularity conditions, there exists a smooth, asymptotically optimal bandwidth function. We also demonstrate empirically that the technique outperforms the ernpirical-bias bandwidth selector (EBBS) of Ruppert when using an EBBS MSE pointwise penalty estimate."
"10.1198/016214506000001400","2007","Pivotal bootstrap methods for {$k$}-sample problems in directional statistics and shape analysis","5","We propose a novel bootstrap hypothesis testing approach for the problem of testing a null hypothesis of a common mean direction, mean polar axis, or mean shape across several populations of real unit vectors (the directional case) or complex unit vectors (the two-dimensional shape case). Multisample testing problems of this type arise frequently in directional statistics and shape analysis (as in other areas of statistics), but to date there has been relatively little discussion of nonparametric bootstrap approaches to this problem. The bootstrap approach described here is based on a statistic that can be expressed as the smallest eigenvalue of a certain positive definite matrix. We prove that this statistic has a limiting chi-squared distribution under the null hypothesis of equality of means across populations. Although we focus mainly on the version of the statistic in which neither isotropy within populations nor constant dispersion structure across populations is assumed, we explain how to modify the statistic so that either or both of these assumptions can be incorporated. Our numerical results indicate that the bootstrap approach proposed here may be expected to perform well in practice."
"10.1198/016214507000000077","2007","High-dimension, low-sample size perspectives in constrained statistical inference: the {SARSC}o{V} {RNA} genome in illustration","0","High-dimensional categorical data models, often with inadequately large sample sizes, crop up in many fields of application. The SARS epidemic, originating in southern China in 2002, had an identified single-stranded and positive-sense RNA virus with large genome size and moderate mutation rate. The present genomic study is used as a prime illustration for motivating appropriate statistical methodology for comprehending the genomic variation in such high-dimensional categorical data models. Because of underlying restraints, a pseudomarginal approach based on Hamming distance is considered in a constrained statistical inference setup. The union-intersection principle and jackknifing methods are incorporated in exploring appropriate statistical procedures."
"10.1198/016214507000000158","2007","Robust model-based and model-assisted predictors of the finite population total","0","The prediction approach to finite population inference has received considerable attention in recent years. Under this approach, the finite population is assumed to be a realization from a superpopulation described by a known probability model, usually a linear model. The prediction approach is often criticized for its lack of robustness against model misspecification. In this article we revisit this important issue and introduce a new robust prediction approach in which the superpopulation model is chosen adaptively from the well-known BoxCox class of probability distributions. The richness of the Box-Cox class ensures robustness in our model-based prediction approach. We explain how our robust model-based predictor can be adjusted to handle zero observations for the study variable and to achieve the design-unbiasedness and benchmarking properties. We demonstrate the robustness of our proposed predictors using a Monte Carlo simulation study and a real life example."
"10.1198/016214507000000293","2007","Empirical likelihood for a varying coefficient model with longitudinal data","2","In this article local empirical likelihood-based inference for a varying coefficient model with longitudinal data is investigated. First, we show that the naive empirical likelihood ratio is asymptotically standard chi-squared when undersmoothing is employed. The ratio is self-scale invariant and the plug-in estimate of the limiting variance is not needed. Second, to enhance the performance of the ratio, mean-corrected and residual-adjusted empirical likelihood ratios are recommended. The merit of these two bias corrections is that without undersmoothing, both also have standard chi-squared limits. Third, a maximum empirical likelihood estimator (MELE) of the time-varying coefficient is defined, the asymptotic equivalence to the weighted least-squares estimator (WLSE) is provided, and the asymptotic normality is shown. By the empirical likelihood ratios and the normal approximation of the MELE/WLSE, the confidence regions of the time-varying coefficients are constructed. Fourth, when some components are of particular interest, we suggest using mean-corrected and residual-adjusted partial empirical likelihood ratios to construct the confidence regions/intervals. In addition, we also consider the construction of the simultaneous and bootstrap confidence bands. A simulation study is undertaken to compare the empirical likelihood, the normal approximation, and the bootstrap methods in terms of coverage accuracies and average areas/widths of confidence regions/bands. An example in epidemiology is used for illustration."
"10.1198/016214507000000095","2007","Analysis of longitudinal data with semiparametric estimation of convariance function","8","Improving efficiency for regression coefficients and predicting trajectories of individuals are two important aspects in the analysis of longitudinal data. Both involve estimation of the covariance function. Yet challenges arise in estimating the covariance function of longitudinal data collected at irregular time points. A class of semiparametric models for the covariance function by that imposes a parametric correlation structure while allowing a nonparametric variance function is proposed. A kernel estimator for estimating the nonparametric variance function is developed. Two methods for estimating parameters in the correlation structure - a quasi-likelihood approach and a minimum generalized variance method-are proposed. A semiparametric varying coefficient partially linear model for longitudinal data is introduced, and an estimation procedure for model coefficients using a profile weighted least squares approach is proposed. Sampling properties of the proposed estimation procedures are studied, and asymptotic normality of the resulting estimators is established. Finite-sample performance of the proposed procedures is assessed by Monte Carlo simulation studies. The proposed methodology is illustrated with an analysis of a real data example."
"10.1198/016214506000001392","2007","Scan statistics with weighted observations","0","We examine scan statistics for one-dimensional marked Poisson processes. Such statistics tabulate the maximum weighted count of event occurrences within a window of predetermined width over all windows within an observed interval. We derive analytical formulas and also give an importance sampling method for approximating the tail probabilities of scan statistics. Because high-throughput genomic sequencing has led to the availability of massive amounts of biomolecular sequence data, it is often of interest to search long DNA or protein sequences for local regions that are enriched for a certain characteristic. Thus scan statistics have become it useful tool in modern computational biology. We illustrate the application of our p value approximations with such examples."
"10.1198/016214506000001383","2007","On {$L_1$}-norm multiclass support vector machines: methodology and theory","2","Binary support vector machines (SVMs) have been proven to deliver high performance. In multiclass classification, however, issues remain with respect to variable selection. One challenging issue is classification and variable selection in the presence of variables in the magnitude of thousands, greatly exceeding the size of training sample. This often occurs in genomics classification. To meet the challenge, this article proposes a novel multiclass support vector machine, which performs classification and variable selection simultaneously through an L-1-norm penalized sparse representation. The proposed methodology, together with the developed regularization solution path, permits variable selection in such a situation. For the proposed methodology, a statistical learning theory is developed to quantify the generalization error in an attempt to gain insight into the basic structure of sparse learning, permitting the number of variables to greatly exceed the sample size. The operating characteristics of the methodology are examined through both simulated and benchmark data and are compared against some competitors in terms of accuracy of prediction. The numerical results suggest that the proposed methodology is highly competitive."
"10.1198/016214507000000130","2007","Sensitivity analyses comparing time-to-event outcomes existing only in a subset selected postrandomization","1","In some randomized studies, researchers are interested in determining the effect of treatment assignment on outcomes that may exist only in a subset chosen after randomization. For example, in preventative human immunodeficiency virus (HIV) vaccine efficacy trials, it is of interest to determine whether randomization to vaccine affects postinfection outcomes that may be right-censored. Such outcomes in these trials include time from infection diagnosis to initiation of antiretroviral therapy and time from infection diagnosis to acquired immune deficiency syndrome. Here we present sensitivity analysis methods for making causal comparisons on these postinfection outcomes. We focus on estimating the survival causal effect, defined as the difference between probabilities of not yet experiencing the event in the vaccine and placebo arms, conditional on being infected regardless of treatment assignment. This group is referred to as the always-infected principal stratum. Our key assumption is monotonicity-that subjects randomized to the vaccine arm who become infected would have been infected had they been randomized to placebo. We propose nonparametric, semiparametric, and parametric methods for estimating the survival causal effect. We apply these methods to the first Phase III preventative HIV vaccine trial, VaxGen's trial of AIDSVAX B/B."
"10.1198/016214507000000112","2007","Flexible cure rate modeling under latent activation schemes","0","With rapid improvements in medical treatment and health care, many datasets dealing with time to relapse or death now reveal a substantial portion of patients who are cured (i.e., who never experience the event). Extended survival models called cure rate models account for the probability of a subject being cured and can be broadly classified into the classical mixture models of Berkson and Gage (BG type) or the stochastic tumor models pioneered by Yakovlev and extended to a hierarchical framework by Chen, Ibrahim, and Sinha (YCIS type). Recent developments in Bayesian hierarchical Cure models have evoked significant interest regarding relationships and preferences between these two classes of models. Our present work proposes a unifying class of cure rate models that facilitates flexible hierarchical model-building while including both existing cure model classes as special cases. This unifying class enables robust modeling by accounting for uncertainty in underlying mechanisms leading to cure. Issues such as regressing on the cure fraction and propriety of the associated posterior distributions under different modeling assumptions are also discussed. Finally, we offer a simulation study and also illustrate with two datasets (on melanoma and breast cancer) that reveal our framework's ability to distinguish among underlying mechanisms that lead to relapse and cure."
"10.1198/016214506000001257","2007","Smoothed rank regression with censored data","1","A weighted rank estimating function is proposed to estimate the regression parameter vector in an accelerated failure time model with right censored data. In general, rank estimating functions are discontinuous in the regression parameter, creating difficulties in determining the asymptotic distribution of the estimator. A local distribution function is used to create a rank based estimating function that is continuous and monotone in the regression parameter vector. A weight is included in the estimating function to produce a bounded influence estimate. The asymptotic distribution of the regression estimator is developed and simulations are performed to examine its finite sample properties. A lung cancer dataset is used to illustrate the methodology."
"10.1198/016214506000001374","2007","Partially linear hazard regression for multivariate survival data","5","This article studies estimation of partially linear hazard regression models for multivariate survival data. A profile pseudo-partial likelihood estimation method is proposed under the marginal hazard model framework. The estimation on the parameters for the linear part is accomplished by maximization of a pseudo-partial likelihood profiled over the nonparametric part. This enables us to obtain root n-consistent estimators of the parametric component. Asymptotic normality is obtained for the estimates of both the linear and nonlinear parts. The new technical challenge is that the nonparametric component is indirectly estimated through its integrated derivative function from a local polynomial fit. An algorithm of fast implementation of our proposed method is presented. Consistent standard error estimates using sandwich-type ideas are also developed, which facilitates inferences for the model. It is shown that the nonparametric component can be estimated as well as if the parametric components were known and the failure times within each subject were independent. Simulations are conducted to demonstrate the performance of the proposed method. A real dataset is analyzed to illustrate the proposed methodology."
"10.1198/016214507000000149","2007","Evaluating prediction rules for {$t$}-year survivors with censored regression models","2","Suppose that we are interested in establishing simple but reliable rules for predicting future t-year survivors through censored regression models. In this article we present inference procedures for evaluating such binary classification rules based on various prediction precision measures quantified by the overall misclassification rate, sensitivity and specificity, and positive and negative predictive values. Specifically, under various working models, we derive consistent estimators for the above measures through substitution and cross-validation estimation procedures. Furthermore, we provide large-sample approximations to the distributions of these nonsmooth estimators without assuming that the working model is correctly specified. Confidence intervals, for example, for the difference of the precision measures between two competing rules can then be constructed. All of the proposals are illustrated with real examples, and their finite-sample properties are evaluated through a simulation study."
"10.1198/016214507000000121","2007","Shotgun stochastic search for ``large {$p$}'' regression","2","Model search in regression with very large numbers of candidate predictors raises challenges for both model specification and computation, for which standard approaches such as Markov chain Monte Carlo (MCMC) methods are often infeasible or ineffective. We describe a novel shotgun stochastic search (SSS) approach that explores ""interesting"" regions of the resulting high-dimensional model spaces and quickly identifies regions of high posterior probability over models. We describe algorithmic and modeling aspects, priors over the model space that induce sparsity and parsimony over and above the traditional dimension penalization implicit in Bayesian and likelihood analyses, and parallel computation using cluster computers. We discuss an example from gene expression cancer genomics, comparisons with MCMC and other methods, and theoretical and simulation-based aspects of performance characteristics in large-scale regression model searches. We also provide software implementing the methods."
"10.1198/016214506000001293","2007","Combining information from two surveys to estimate county-level prevalence rates of cancer risk factors and screening","1","Cancer surveillance research requires estimates of the prevalence of cancer risk factors and screening for small areas such as counties. Two popular data sources are the Behavioral Risk Factor Surveillance System (BRFSS), a telephone survey conducted by state agencies, and the National Health Interview Survey (NHIS), an area probability sample survey conducted through face-to-face interviews. Both data sources have advantages and disadvantages. The BRFSS is a larger survey and almost every county is included in the survey, but it has lower response rates as is typical with telephone surveys and it does not include subjects who live in households with no telephones. On the other hand, the NHIS is a smaller survey, with the majority of counties not included; but it includes both telephone and nontelephone households, and has higher response rates. A preliminary analysis shows that the distributions of cancer screening and risk factors are different for telephone and nontelephone households. Thus, information from the two surveys may be combined to address both nonresponse and noncoverage errors. A hierarchical Bayesian approach that combines information from both surveys is used to construct county-level estimates. The proposed model incorporates potential noncoverage and nonresponse biases in the BRFSS as well as complex sample design features of both surveys. A Markov chain Monte Carlo method is used to simulate draws from the joint posterior distribution of unknown quantities in the model that uses design-based direct estimates and county-level covariates. Yearly prevalence estimates at the county level for 49 states. as well as for the entire state of Alaska and the District of Columbia, are developed for six outcomes using BRFSS and NHIS data from the years 1997-2000. The outcomes include smoking and use of common cancer screening procedures. The NHIS/BRFSS combined county-level estimates are substantially different from those based on the BRFSS alone."
"10.1198/016214506000001284","2007","Accounting for spatial dependence in the analysis of {SPECT} brain imaging data","1","The size and complexity of brain imaging databases confront statistical analysts with a variety of issues when assessing brain activation differences between groups of subjects. Detecting small group differences in activation is compounded by the need to analyze hundreds of thousands of spatially correlated measurements per image. These analyses are especially problematic when. as is typical. the number of subjects in each group is small. In this article a comprehensive analysis of single-photon emission computed tomography (SPECT) brain images demonstrates that spatial modeling can increase the sensitivity of group comparisons. The key statistical approach for increasing the sensitivity of group comparisons is the spatial modeling of intervoxel correlations. Correlations among normalized SPECT counts in 2 x 2 x 2 mm(3) voxels are shown to be very large in neighboring voxels and to decrease in magnitude until they become negligible among those approximately 5-7 voxels (10-14 mm) apart. Exploiting this correlation structure, blocks of contiguous voxels are defined within each of several structures within the deep brain so that the geometric centers of the blocks are no closer than the range at which voxel counts can be considered uncorrelated. Using kriging methods, block averages and their prediction variances are calculated. For each structure of interest, the block averages within the structure are weighted by their prediction variances, producing a structure average for each subject. The subject averages and their prediction variances are used in a linear model to compare group effects. This analysis is shown to be more sensitive to group mean differences than the voxel-by-voxel analysis commonly used by medical researchers. The procedures are applied to comparisons of SPECT brain imaging data from four groups of subjects. three of which have variants of the 1991 Gulf War syndrome and one of which is a control group. Commonly used voxel-by-voxel group comparisons do not identify any brain structures that are significantly different for the syndrome and control groups in the analysis of cholinergic response to a physostigmine drug challenge. Spatial modeling and analyses of these data do identify regions of the deep brain that exhibit statistically significant group differences. These results are consistent with medical evidence that these structures might have been affected by Gulf War chemical exposures."
"10.1198/016214506000000960","2007","An ``unfolding'' latent variable model for {L}ikert attitude data: drawing inferences adjusted for response style","0","Likert attitude data consist of responses to favorable and unfavorable statements about an entity. where responses fall into ordered categories ranging from disagreement to agreement. Social science and rnarketing researchers frequently use data of this type to measure attitudes toward an entity such as a policy or product. We focus oil data on American and British attitudes toward their respective nations (""national pride""). We introduce a multidimensional Unfolding model (MUM) to describe the relationship between the data and the attitudes underlying I Just attitudes. but also response style, which is defined as a them. Unlike most existing, models, the MUM allows the data to reflect not consistent and content-independent pattern of response category selection such as a tendency to agree with all statements. The MUM can be used to model multiple attitudes, which allows researchers to expand their analysis of the data of interest to include all available Likert data so as to increase information on response style. For example. we include additional data on immigration attitudes to help distinguish the effects of response style and national pride on our data. The MUM can be used to fit linear models for the effects of background variables oil attitudes. Resulting inferences about attitudes are adjusted for response style arid should be less biased. Simulation results strongly suggest that, unlike Likert's popular scoring model. the MUM yields unbiased inferences even when there are Unequal proportions of favorable and unfavorable statements."
"10.1198/016214506000001347","2007","Spatiotemporal models for region of interest analyses of functional neuroimaging data","2","In vivo functional neuroimaging technology enables the evaluation of behavior-related changes in measured brain activity within specific cortical regions of interest (ROIs). When sufficient neurophysiologic evidence exists to restrict attention to a defined cortical region, an ROI analysis can provide powerful insights regarding neural representations of cognition. emotions. behaviors, and the neuropathology of psychiatric disorders. Given the complexity and abundance of data from neuroimaging experiments, anatomically focused research questions allow statisticians to explore models that more accurately reflect neurophysiologic characteristics of the data than global activation studies. Neural processing characteristics of particular interest in this article are spatial correlations stemming from the interplay between spatially distinct brain locations and temporal correlations between serial measures of brain activity. Despite the simplified data structure of ROI Studies, challenges remain in modeling spatial correlations due to, for example, the fact that the correlations do not necessarily decrease as a function of increasing separation between the measurement locations. This article presents a spatiotemporal model that incorporates a functionally defined distance metric into a parametric structure for spatial correlations and includes temporal correlations between repeated scans. We demonstrate the use of the spatiotemporal model using experimental data from a study of the effects of ethanol administration on brain activity in the cerebellum, which largely controls balance and posture. We further illustrate our model using data simulated from a study evaluating neural processing alterations in the right prefrontal cortex associated with mental arithmetic."
"10.1198/016214506000000997","2007","Disability and employment: reevaluating the evidence in light of reporting errors","0","Measurement error in health and disability status has been widely accepted as a central problem in social science research. Long-standing debates about the prevalence of disability, the role of health in labor market outcomes, and the influence of federal disability policy on declining employment rates have all emphasized issues regarding the reliability of self-reported disability. In addition to random error, inaccuracy in survey datasets may be produced by a host of economic, social, and psychological factors that can lead respondents to misreport work capacity. We develop a nonparametric foundation for assessing how assumptions on the reporting error process affect inferences on the employment gap between the disabled and nondisabled. Rather than imposing the strong assumptions required to obtain point identification, we derive sets of bounds that formalize the identifying power of primitive nonparametric assumptions that appear to share broad consensus in the literature. Within this framework, we introduce a finite-sample correction for the analog estimator of the monotone instrumental variable (MIV) bound. Our empirical results suggest that conclusions derived from conventional latent variable reporting error models may be driven largely by ad hoc distributional and functional form restrictions. We also find that under relatively weak nonparametric assumptions, nonworkers appear to systematically overreport disability."
"10.1198/016214506000001031","2007","Spatial {B}ayesian variable selection with application to functional magnetic resonance imaging","1","We propose a procedure to undertake Bayesian variable selection and model averaging for a series of regressions located on a lattice. For those regressors that are in common in the regressions, we consider using an Ising prior to smooth spatially the indicator variables representing whether or not the variable is zero or nonzero in each regression. This smooths spatially the probabilities that each independent variable is nonzero in each regression and indirectly smooths spatially the regression coefficients. We discuss how single-site sampling schemes can be used to evaluate the joint posterior distribution. The approach is applied to the problem of functional magnetic resonance imaging in medical statistics, where massive datasets arise that require prompt processing. Here the Ising prior with a three-dimensional neighborhood structure is used to smooth spatially activation maps from regression models of blood oxygenation. The Ising prior also has the advantage of allowing incorporation of anatomic prior information through the external field. Using a visual experiment, we show how a single-site sampling scheme can provide rapid evaluation of the posterior activation maps and activation amplitudes. The approach is shown to result in maps that are superior to those produced by a recent Bayesian approach using a continuous Markov random field for the activation amplitude."
"10.1198/016214506000001536","2007","Rejoinder [MR2370838]","0",""
"10.1198/016214506000001491","2007","Model-assisted estimation of forest resources with generalized additive models","0","Multiphase surveys are often conducted in forest inventories, with the goal of estimating forested area and tree characteristics over large regions. This article describes how design-based estimation of such quantities, based on information gathered during ground visits of sampled plots, can be made more precise by incorporating auxiliary information available from remote sensing. The relationship between the ground visit measurements and the remote sensing variables is modeled using generalized additive models. Nonparametric estimators for these models are discussed and applied to forest data collected in the mountains of northern Utah. Model-assisted estimators that use the nonparametric regression fits are proposed for these data. The design context of this study is two-phase systematic sampling from a spatial continuum, under which properties of model-assisted estimators are derived. Difficulties with the standard variance estimation approach, which assumes simple random sampling in each phase, are described. An alternative assessment of estimator performance based on a synthetic population is implemented and shows that using the model predictions in a model-assisted survey estimation procedure results in substantial efficiency improvements over current estimation approaches."
"10.1198/016214507000000275","2007","From data to policy: scientific excellence is our future","0","Science, engineering, technology, and people-these are the ingredients that must come together to support the growing complexity of today's global challenges, ranging from international security to space exploration. As scientists and engineers, it is essential that we develop the means to put our work into a decision context for policy makers; otherwise, our efforts will only inform the writers of textbooks and not the leaders who shape the world within which we live. Statisticians must step up to that challenge! Scientific and technical progress requires interdisciplinary teams, because it is impossible for a single individual to have enough knowledge to solve many of today's problems, for example, mapping the genome, modeling the spread of a pandemic, and developing diagnostic and treatment devices for developing countries. A principal role of the statistician is to bring the cutting edge of statistical sciences to these problems. By the nature of our training, statisticians are well poised to assume the role of science and technology integrator. To be successful, this must place statisticians closer to policy pressures and politics. This address will focus on the growing expectations facing statistical sciences and how we, as statisticians, must take responsibility for separating the scientific method from the politics of the scientific process to guarantee that scientific excellence and impact is communicated to decision makers."
"10.1198/016214507000000932","2007","The multiple adaptations of multiple imputation","3","Multiple imputation was first conceived as a tool that statistical agencies could use to handle nonresponse in large-sample public use surveys. In the last two decades, the multiple-imputation framework has been adapted for other statistical contexts. For example, individual researchers use multiple imputation to handle missing data in small samples, statistical agencies disseminate multiply-imputed data sets for purposes of protecting data confidentiality, and survey methodologists and epidemiologists use multiple imputation to correct for measurement errors. In some of these settings, Rubin's original rules for combining the point and variance estimates from the multiply-imputed data sets are not appropriate, because what is known-and thus the conditional expectations and variances used to derive inferential methods-differs from that in the missing-data context. These applications require new combining rules and methods of inference. In fact, more than 10 combining rules exist in the published literature. This article describes some of the main adaptations of the multiple-imputation framework, including missing data in large and small samples, data confidentiality, and measurement error. It reviews the combining rules for each setting and explains why they differ. Finally, it highlights research topics in extending the multiple-imputation framework."
"10.1198/016214507000000987","2007","Nonparametric regression estimation in the heteroscedastic errors-in-variables problem","4","In the classical errors-in-variables problem, the goal is to estimate a regression curve from data in which the explanatory variable is measured with error. In this context, nonparametric methods have been proposed that rely on the assumption that the measurement errors are identically distributed. Although there are many situations in which this assumption is too restrictive, nonparametric estimators in the more realistic setting of heteroscedastic errors have not been studied in the literature. We propose an estimator of the regression function in such a setting and show that it is optimal. We give estimators in cases in which the error distributions are unknown and replicated observations are available. Practical methods, including an adaptive bandwidth selector for the errors-in-variables regression problem, are suggested, and their finite-sample performance is illustrated through simulated and real data examples."
"10.1198/016214507000000851","2007","Regression analysis of longitudinal data in the presence of informative observation and censoring times","0","` Longitudinal data frequently occur in many studies, such as longitudinal follow-up studies. To develop statistical methods and theory for the analysis of these data, independent or noninformative observation and censoring times are typically assumed, which naturally leads to inference procedures conditional on observation and censoring times. But in many situations this may not be true or realistic; that is, longitudinal responses may be correlated with observation times as well as censoring times. This article considers the analysis of longitudinal data where these correlations may exist and proposes a joint modeling approach that uses some latent variables to characterize the correlations. For inference about regression parameters, estimating equation approaches are developed and both large-sample and final-sample properties of the proposed estimators are established. In addition, some graphical and numerical procedures are presented for model checking. The methodology is applied to a bladder cancer study that motivated this investigation."
"10.1198/016214507000001076","2007","Rank-based extensions of the {B}rock, {D}echert, and {S}cheinkman test","0","This article proposes new tests of randomness for innovations in a large class of time series models. These tests are based on functionals of empirical processes constructed from either the model residuals or their associated ranks. The asymptotic behavior of these processes is determined under the null hypothesis of randomness. The limiting distributions are seen to be independent of estimation errors under appropriate regularity conditions. Several test statistics are derived from these processes; the classical Brock, Dechert, and Scheinkman statistic and a rank-based analog are included as special cases. Because the limiting distributions of the rank-based test statistics are margin-free, their finite-sample p values can be easily calculated by simulation. Monte Carlo experiments show that these statistics are quite powerful against several classes of alternatives."
"10.1198/016214507000001067","2007","Multi-scale jump and volatility analysis for high-frequency financial data","7","The wide availability of high-frequency data for many financial instruments stimulates an upsurge interest in statistical research on the estimation of volatility. Jump-diffusion processes observed with market microstructure noise are frequently used to model high-frequency financial data. Yet existing methods are developed for either noisy data from a continuous-diffusion price model or data from a jump-diffusion price model without noise. We propose methods to cope with both jumps in the price and market microstructure noise in the observed data. These methods allow us to estimate both integrated volatility and jump variation from the data sampled from jump-diffusion price processes, contaminated with the market microstructure noise. Our approach is to first remove jumps from the data and then apply noise-resistant methods to estimate the integrated volatility. The asymptotic analysis and the simulation study reveal that the proposed wavelet methods can successfully remove the jumps in the price processes and the integrated volatility can be estimated as accurately as in the case with no presence of jumps in the price processes. In addition, they have outstanding statistical efficiency. The methods are illustrated by applications to two high-frequency exchange rate data sets."
"10.1198/016214507000001058","2007","Sieve maximum likelihood estimation for regression models with covariates missing at random","0","Missing covariates are common in regression problems. We propose a new semiparametric method based on a fully nonparametric distribution for the missing covariates that are assumed to be missing at random. The method of sieve maximum likelihood estimation is used to obtain the estimators of the regression coefficients. These estimators are shown to be consistent and asymptotically normal with their asymptotic covariance matrix that achieves the semiparametric efficiency bound. A bootstrap approach is used to estimate the asymptotic covariance matrix. Some practical modeling approaches for high-dimensional covariates are proposed. Extensive simulation studies are conducted to examine the finite-sample properties of the estimates, and a real data set from a liver cancer clinical trial is analyzed using the proposed method."
"10.1198/016214507000000969","2007","To how many simultaneous hypothesis tests can normal, {S}tudent's {$t$} or bootstrap calibration be applied?","6","In the analysis of microarray data, and in some other contemporary statistical problems, it is not uncommon to apply hypothesis tests in a highly simultaneous way. The number, N say, of tests used can be much larger than the sample sizes, n, to which the tests are applied, yet we wish to calibrate the tests so that the overall level of the simultaneous test is accurate. Often the sampling distribution is quite different for each test, so there may not be an opportunity to combine data across samples. In this setting, how large can N be, as a function of n, before level accuracy becomes poor? Here we answer this question in cases where the statistic under test is of Student's t type. We show that if either the normal or Student t distribution is used for calibration, then the level of the simultaneous test is accurate provided that log N increases at a strictly slower rate than n(1/3) as n diverges. On the other hand, if bootstrap methods are used for calibration, then we may choose log N almost as large as n(1/2) and still achieve asymptotic-level accuracy. The implications of these results are explored both theoretically and numerically."
"10.1198/016214506000001239","2007","Semiparametric transformation models with random effects for recurrent events","0","In this article we study a class of semiparametric transformation models with random effects for the intensity function of the counting process. These models provide considerable flexibility in formulating the effects of possibly time-dependent covariates on the developments of recurrent events while accounting for the dependence of the recurrent event times within the same subject. We show that the nonparametric maximum likelihood estimators (NPMLEs) for the parameters of these models are consistent and asymptotically normal. The limiting covariance matrices for the estimators of the regression parameters achieve the semiparametric efficiency bounds and can be consistently estimated. The limiting covariance function for the estimator of any smooth functional of the cumulative intensity function also can be consistently estimated. We develop a simple and stable EM algorithm to compute the NPMLEs as well as the variance and covariance estimators. Simulation studies demonstrate that the proposed methods perform well in practical situations. Two medical studies are provided for illustrations."
"10.1198/016214506000000988","2007","Inference for stereological extremes","4","In the production of clean steels, the occurrence of imperfections-so-called ""inclusions""-is unavoidable. The strength of a clean steel block is largely dependent on the size of the largest imperfection that it contains, so inference on extreme inclusion size forms an important part of quality control. Sampling is generally done by measuring imperfections on planar slices, leading to an extreme value version of a standard stereological problem: how to make inference on large inclusions using only the sliced observations. Under the assumption that inclusions are spherical, this problem has been tackled previously using a combination of extreme value models, stereological calculations, a Bayesian hierarchical model, and standard Markov chain Monte Carlo (MCMC) techniques. Our objectives in this article are twofold: (1) to assess the robustness of such inferences with respect to the assumption of spherical inclusions, and (2) to develop an inference procedure that is valid for nonspherical inclusions. We investigate both of these aspects by extending the spherical family for inclusion shapes to a family of ellipsoids. We then address the issue of robustness by assessing the performance of the spherical model when fitted to measurements obtained from a simulation of ellipsoidal inclusions. The issue of inference is more difficult, because likelihood calculation is not feasible for the ellipsoidal model. To handle this aspect, we propose a modification to a recently developed likelihood-free MCMC algorithm. After verifying the viability and accuracy of the proposed algorithm through a simulation study, we analyze a real inclusion dataset, comparing the inference obtained under the ellipsoidal inclusion model with that previously obtained assuming spherical inclusions."
"10.1198/016214506000001437","2007","Strictly proper scoring rules, prediction, and estimation","10","Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper. if the forecaster maximizes the expected score for an observation drawn from the distribution F if he or she issues the probabilistic forecast F, rather than G 4 F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage."
"10.1198/016214506000001130","2007","Robust tests in regression models with omnibus alternatives and bounded influence","1","A robust approach for testing the parametric form of a regression function versus an omnibus alternative is introduced. This generalizes existing robust methods for testing subhypotheses in a regression model. The new test is motivated by developments in modern smoothing-based testing procedures and can be viewed as a robustification of a smoothing-based conditional moment test. It is asymptotically normal under both the null hypothesis and local alternatives. The robustified test retains the ""omnibus"" property of the corresponding smoothing test; that is, it is consistent for any fixed smooth alternative in an infinite-dimensional space. It is shown that the bias of the asymptotic level under shrinking local contamination is bounded only if the second-order Hampel's influence function is bounded. The test's performance is demonstrated through both Monte Carlo simulations and application to an agricultural dataset."
"10.1198/016214506000000816","2007","Empirical likelihood inference in nonlinear errors-in-covariables models with validation data","3","In this article we study inference in parametric-nonparametric errors-in-covariables regression models using an empirical likelihood approach based on validation data. It is shown that the asymptotic behavior of the proposed estimator depends on the ratio of the sizes of the primary sample and the validation sample, respectively. Unlike cases without measurement errors, the limit distribution of the estimator is no longer tractable and cannot be used for constructing confidence regions. Monte Carlo approximations are employed to simulate the limit distribution. To increase the coverage accuracy of confidence regions, two adjusted empirical likelihood estimators are recommended, which in the limit have a standard chi-squared distribution. A simulation study is carried out to compare the proposed methods with other existing methods. The new methods outperform the least squares method, and one of them works better than simulation-extrapolation (SIMEX) estimation, even when the restrictive model assumptions needed for SIMEX are satisfied. An application to a real dataset illustrates our new approach."
"10.1198/016214506000000852","2007","Approximate likelihood for large irregularly spaced spatial data","10","Likelihood approaches for large, irregularly spaced spatial datasets are often very difficult, if not infeasible, to implement due to computational limitations. Even when we can assume normality, exact calculations of the likelihood for a Gaussian spatial process observed at n locations requires O(n(3)) operations. We present a version of Whittle's approximation to the Gaussian log-likelihood for spatial regular lattices with missing values and for irregularly spaced datasets. This method requires O(n log(2) n) operations and does not involve calculating determinants. We present simulations and theoretical results to show the benefits and the performance of the spatial likelihood approximation method presented here for spatial irregularly spaced datasets and lattices with missing values. We apply these methods to estimate the spatial structure of sea surface temperatures using satellite data with missing values."
"10.1198/016214506000001202","2007","Stochastic approximation in {M}onte {C}arlo computation","3","The Wang-Landau (WL) algorithm is an adaptive Markov chain Monte Carlo algorithm used to calculate the spectral density for a physical system. A remarkable feature of the WL algorithm is that it is not trapped by local energy minima, which is very important for systems with rugged energy landscapes. This feature has led to many successful applications of the algorithm in statistical physics and biophysics; however, there does not exist rigorous theory to support its convergence, and the estimates produced by the algorithm can reach only a limited statistical accuracy. In this article we propose the stochastic approximation Monte Carlo (SAMC) algorithm, which overcomes the shortcomings of the WL algorithm. We establish a theorem concerning its convergence. The estimates produced by SAMC can be improved continuously as the simulation proceeds. SAMC also extends applications of the WL algorithm to continuum systems. The potential uses of SAMC in statistics are discussed through two classes of applications, importance sampling and model selection. The results show that SAMC can work as a general importance sampling algorithm and a model selection sampler when the model space is complex."
"10.1198/016214506000000799","2007","A sturdy reduced-bias extreme quantile ({V}a{R}) estimator","0","The main objective of statistics of extremes lies in the estimation of quantities related to extreme events. In many areas of application, such as statistical quality control, insurance, and finance, a typical requirement is to estimate a high quantile, that is, the value at risk at a level p (VaR(p)), high enough so that the chance of an exceedance of that value is equal to p, small. In this article we deal with the semiparametric estimation of VaRp for heavy tails. The classical semiparametric estimators of parameters characterizing the tail behavior of the underlying model F usually exhibit a high bias for low thresholds, that is, for large values of k, the number of top order statistics used for the estimation. We shall here deal with bias reduction techniques for heavy tails; trying to improve the performance of the classical high quantile estimators through the use of an adequate bias-corrected tail index estimator. The new high quantile estimators have a mean squared error smaller than that of the classical estimators, even for small values of k. They are, thus, alternatives to the classical estimators not only around optimal levels but also for other levels. The asymptotic distributional properties of the proposed classes of estimators are derived. The estimators are compared with alternative ones, not only asymptotically but also for finite samples, through Monte Carlo techniques. An application to the analysis of different datasets in the field of finance is also provided."
"10.1198/016214506000000861","2007","Extending the {A}kaike information criterion to mixture regression models","0","We examine the problem of jointly selecting the number of components and variables in finite mixture regression models. We find that the Akaike information criterion is unsatisfactory for this purpose because it overestimates the number of components, which in turn results in incorrect variables being retained in the model. Therefore, we derive a new information criterion, the mixture regression criterion (MRC), that yields marked improvement in model selection due to what we call the ""clustering penalty function."" Moreover, we prove the asymptotic efficiency of the MRC. We show that it performs well in Monte Carlo studies for the same or different covariates across components with equal or unequal sample sizes. We also present an empirical example on sales territory management to illustrate the application and efficacy of the MRC. Finally, we generalize the MRC to mixture quasi-likelihood and mixture autoregressive models, thus extending its applicability to non-Gaussian models, discrete responses, and dependent data."
"10.1198/016214506000000843","2007","Controlling variable selection by the addition of pseudovariables","4","We propose a new approach to variable selection designed to control the false selection rate (FSR), defined as the proportion of uninformative variables included in selected models. The method works by adding a known number of pseudovariables to the real dataset, running a variable selection procedure, and monitoring the proportion of pseudovariables falsely selected. Information obtained from bootstrap-like replications of this process is used to estimate the proportion of falsely selected real variables and to tune the selection procedure to control the FSR."
"10.1198/016214506000000906","2007","Implementing optimal allocation in sequential binary response experiments","5","For sequential experiments with K treatments, we establish two formal optimization criteria to find optimal allocation strategies. Both criteria involve the sample sizes on each treatment and a concave noncentrality parameter from a multivariate test. We show that these two criteria are equivalent. We apply this result to specific questions: (1) How do we maximize power of a multivariate test of homogeneity with binary response?, and (2) for fixed power, how do we minimize expected treatment failures? Because the solutions depend on unknown parameters, we describe a response-adaptive randomization procedure that ""targets"" the optimal allocation and provides increases in power along the lines of 2-4% over complete randomization for equal allocation. The increase in power contradicts the conclusions of other authors who have explored other randomization procedures for K = 2 and have found that the variability induced by randomization negates any benefit of targeting an optimal allocation."
"10.1198/016214506000000889","2007","Transition models for multivariate longitudinal binary data","0","In many settings with longitudinal binary data, interest lies in modeling covariate effects on transition probabilities of an underlying stochastic process. When data from two or more processes are available, the scientific focus may be on the degree to which changes in one process are associated with changes in another process. Analysis based on independent Markov models permits separate examination of covariate effects on the transition probabilities for each process, but no insight into between-process associations is obtained. We propose a method of estimation and inference based on joint transitional models for multivariate longitudinal binary data using GEE2 or alternating logistic regression that allows modeling of covariate effects on marginal transition probabilities as well as the association parameters. Consistent estimates of regression coefficients and association parameters are obtained, and efficiency gains for the parameters governing the marginal transition probabilities are realized when the association between processes is strong. Extensions to deal with multivariate longitudinal categorical data are indicated."
"10.1198/016214506000001086","2007","Mixed hidden {M}arkov models: an extension of the hidden {M}arkov model to the longitudinal data setting","0","Hidden Markov models (HMMs) are a useful tool for capturing the behavior of overdispersed, autocorrelated data. These models have been applied to many different problems, including speech recognition, precipitation modeling, and gene finding and profiling. Typically, HMMs are applied to individual stochastic processes; HMMs for simultaneously modeling multiple processes-as in the longitudinal data setting-have not been widely studied. In this article I present a new class of models, mixed HMMs (MHMMs), where I use both covariates and random effects to capture differences among processes. I define the models using the framework of generalized linear mixed models and discuss their interpretation. I then provide algorithms for parameter estimation and illustrate the properties of the estimators via a simulation study. Finally, to demonstrate the practical uses of MHMMs, I provide an application to data on lesion counts in multiple sclerosis patients. I show that my model, while parsimonious, can describe the heterogeneity among such patients."
"10.1198/016214506000001112","2007","Interference between units in randomized experiments","3","In a randomized experiment comparing two treatments, there is interference between units if applying the treatment to one unit may affect other units. Interference implies that treatment effects are not comparisons of two potential responses that a unit may exhibit, one under treatment and the other under control, but instead are inherently more complex. Interference is common in social settings where people communicate, compete, or spread disease; in studies that treat one part of an organism using a symmetrical part as control; in studies that apply different treatments to the same organism at different times; and in many other situations. Available statistical tools are limited. For instance, Fisher's sharp null hypothesis of no treatment effect implicitly entails no interference, and so his randomization test may be used to test no effect, but conventional ways of inverting the test to obtain confidence intervals, say for an additive effect, are not applicable with interference. Another commonly used approach assumes that interference is of a simple parametric form confined to units that are near one another in time or space; this is useful when applicable but is of little use when interference may be widespread and of uncertain form. Exact, nonparametric methods are developed for inverting randomization tests to obtain confidence intervals for magnitudes of effect assuming nothing at all about the structure of the interference between units. The limitations of these methods are discussed. To illustrate the general approach, two simple methods and two simple empirical examples are discussed. Extension to randomization based covariance adjustment is briefly described."
"10.1198/016214506000000870","2007","Additive expectancy regression","2","Regression is a useful tool for studying association between an outcome variable and its covariates. In a classical linear regression model, the outcome's expectation is usually a function of the covariates and some regression parameters, and the underlying distribution is often normal. In this article we propose a new class of additive expectancy regression models in which the outcomes tend to be positively skewed. In the new regression models, regression parameters are practically meaningful and also useful with inferences on the means of skewed outcomes. Parametric and semiparametric methods are developed for model estimation and inferences. Model-based prediction and model adequacy assessment are discussed. The proposed methodologies are demonstrated by two real data analyses."
"10.1198/016214506000001121","2007","A simple risk-adjusted exponentially weighted moving average","0","In such contexts as medical monitoring there is a need for a simple procedure to estimate the level of a smoothly changing dynamic process while adjusting for risk factors associated with heterogeneous nonnormal observations. Standard methods exist for when the data are normal, and simple methods have been developed for nonnormal data without covariates. The dynamic generalized linear model (DGLM) may be considered a standard for nonnormal data with covariates, but perhaps one that is overcomplicated for many contexts. We propose adapting the standard exponentially weighted moving average (EWMA) to take into account the effect of risk factors. By approximating the correct exponential family likelihood, we derive a risk-adjusted EWMA (RA-EWMA) that is essentially a standard EWMA applied to ""pseudo-observations,"" which are the original observations adjusted for differential risk. The RA-EWMA can be expressed as a type of filter, in which current estimate = previous estimate + discounted predictive error. We review Bayesian state-space models related to the EWMA and examine the properties of each. The RA-EWMA and DGLM are compared algebraically, through simulation and an example. In the example, the expected mortality after surgery by a particular surgeon is allowed to vary over time and is estimated through the RA-EWMA. Each binary observation is adjusted for the effect of patient-specific risk factors to standardize information fed into the estimator for the expected mortality for a ""baseline"" patient. The tool used for adjustment can be used to provide future patient-specific preoperative risk assessments. We conclude that the RA-EWMA performs similarly to the DGLM, is conceptually and computationally simpler by virtue of having fewer and simpler stages, and has an intuitive appeal to a wide variety of stakeholders."
"10.1198/016214506000001059","2007","Minimum distance matched sampling with fine balance in an observational study of treatment for ovarian cancer","2","In observational studies of treatment effects, matched samples have traditionally been constructed using two tools, namely close matches on one or two key covariates and close matches on the propensity score to stochastically balance large numbers of covariates. Here we propose a third tool, fine balance, obtained using the assignment algorithm in a new way. We use all three tools to construct a matched sample for an ongoing study of provider specialty in the treatment of ovarian cancer. Fine balance refers to exact balance of a nominal covariate, often one with many categories, but it does not require individually matched treated and control subjects for this variable. In the example the nominal variable has 72 = 9 x 8 categories formed from 9 possible years of diagnosis and 8 geographic locations or SEER sites. We obtain exact balance on the 72 categories and close individual matches on clinical stage, grade, year of diagnosis, and other variables using a distance, and stochastically balance a total of 61 covariates using a propensity score. Our approach finds an optimal match that minimizes a suitable distance subject to the constraint that fine balance is achieved. This is done by defining a special patterned distance matrix and passing it to a subroutine that solves the optimal assignment problem, which optimally pairs the rows and columns of a matrix using a polynomial time algorithm. In the example we used the function Proc Assign in SAS. A new theorem shows that with our patterned distance matrix, the assignment algorithm returns an optimal, finely balanced matched sample whenever one exists, and otherwise returns an infinite distance, indicating that no such matched sample exists."
"10.1198/016214506000001004","2007","Episodic nonlinear event detection in the {C}anadian exchange rate","0","This article uses daily observations for the Canadian dollar-U.S. dollar nominal exchange rate over the recent flexible exchange rate period and a new statistical technique, recently developed by Hinich, to detect major political and economic events that have affected the exchange rate."
"10.1198/016214506000000834","2007","A unified semiparametric framework for quantitative trait loci analyses, with application to spike phenotypes","0","This article proposes a general semiparametric model for multiple quantitative trait loci (QTL) analyses of complex phenotypes in backcross and intercross designs. The model provides tests about genetic hypotheses, such as additivity, dominance, and epistasis, that do not require specifying the form of the phenotypic distribution. This contrasts with previous approaches based on transformations to normality and generalized linear models, which require careful consideration of the phenotypic distribution. Inferences involve extensions of partial and conditional likelihoods developed for single-QTL backcross models. We demonstrate that conditional likelihood is robust to unobserved selective genotyping, whereas partial likelihood and other standard methods are not. To facilitate genome screens, a novel resampling method is proposed that is similar in spirit to the popular permutation tests. Its main advantages are that it is broadly applicable to multiple QTLs with nonnormal phenotypes and achieves a substantial reduction in computational burden. A thorough case study of spike data on the genetic influences to recovery from Listeria infection in a mouse intercross experiment is presented. The application reveals that the proposed methods may give substantively different conclusions than those obtained with existing interval mapping methods from parametric models in the presence of unobserved selection."
"10.1198/016214506000000753","2007","Spatial analyses of periodontal data using conditionally autoregressive priors having two classes of neighbor relations","2","Attachment loss, the extent of a tooth's root (in millimeters) that is no longer attached to surrounding bone by periodontal ligament, is often used to measure the current state of a patient's periodontal disease and monitor disease progression. Attachment loss data can be analyzed using a conditionally autoregressive (CAR) prior distribution that smooths fitted values toward neighboring values. However, it may be desirable to have more than one class of neighbor relation in the spatial structure, so the different classes of neighbor relations can induce different degrees of smoothing. For example, we may wish to allow smoothing of neighbor pairs bridging the gap between teeth to differ from smoothing of pairs that do not bridge such gaps. Adequately modeling the spatial structure may improve the monitoring of periodontal disease progression. This article develops a two-neighbor-relation CAR model to handle this situation and presents associated theory to help explain the sometimes unusual posterior distributions of the parameters controlling the different types of smoothing. The posterior of these smoothing parameters often has long upper tails, and its shape can change dramatically depending on the spatial structure. Like previous authors, we show that the prior distribution on these parameters has little effect on the posterior of the fixed effects but has a marked influence on the posterior of both the random effects and the smoothing parameters. Our analysis of attachment loss data also suggests that the spatial structure itself varies between individuals."
"10.1198/016214506000001149","2007","Cost (or price) forecasting in the face of technological advance","0","The problem considered involves forecasting the future costs of hard drives of various capacities and speeds of revolution or, more generally, forecasting the future costs of various quantifiably different versions of a commodity that is subject to technological advance. In the primary development, it is supposed that the data consist of past and present costs. A model is proposed in which the past, present, and future costs of each version are related with each other and also with the costs of the other versions. The model encompasses a stochastic version of an empirical relationship known as Moore's law. A forecasting methodology was developed by adopting a Bayesian approach and taking the prior distribution to be of a relatively tractable form. An implementation of the Gibbs sampler was devised for making draws from the posterior distribution of the future costs; the forecasts are based on those draws. The proposed methodology was used to obtain forecasts retrospectively from data accumulated (over a 5-year period) on the quarterly costs of hard drives. The accuracy of the longer-term forecasts compared favorably with those of certain benchmark forecasts, whereas the accuracy of the shorter-term forecasts compared less favorably. Greater accuracy can be achieved through enhancements to the proposed methodology that provide for the use of supplementary information (i.e., information that is relevant but not fully reflected in the past and present costs)."
"10.1198/016214506000001022","2007","Periodic seasonal {R}eg-{ARFIMA}-{GARCH} models for daily electricity spot prices","0","Novel periodic extensions of dynamic long-memory regression models with autoregressive conditional heteroscedastic errors are considered for the analysis of daily electricity spot prices. The parameters of the model with mean and variance specifications are estimated simultaneously by the method of approximate maximum likelihood. The methods are implemented for time series of 1,200-4,400 daily price observations in four European power markets. Apart from persistence, heteroscedasticity, and extreme observations in prices, a novel empirical finding is the importance of day-of-the-week periodicity in the autocovariance function of electricity spot prices. In particular, the very persistent daily log prices from the Nord Pool power exchange of Norway are effectively modeled by our framework, which is also extended with explanatory variables to capture supply-and-demand effects. The daily log prices of the other three electricity markets-EEX in Germany, Powernext in France, and APX in The Netherlands-are less persistent, but periodicity is also highly significant. The dynamic behavior differs from market to market and depends primarily on the method of power generation: hydro power, power generated from fossil fuels, or nuclear power. The article improves on existing models in capturing the memory characteristics, which are important in derivative pricing and real option analysis."
"10.1198/016214506000000762","2007","Prediction of {U}.{S}. cancer mortality counts using semiparametric {B}ayesian techniques","2","We present two models for the short-term prediction of the number of deaths arising from common cancers in the United States. The first is a local linear model, in which the slope of the segment joining the number of deaths for any two consecutive time periods is assumed to be random with a nonparametric distribution, which has a Dirichlet process prior. For slightly longer prediction periods, we present a local quadratic model. This extension of the local linear model includes an additional ""acceleration"" term that allows it to quickly adjust to sudden changes in the time series. The proposed models can be used to obtain the predictive distributions of the future number of deaths, as well their means and variances through Markov chain Monte Carlo techniques. We illustrate our methods by runs on data from selected cancer sites."
"10.1198/016214507000000013","2007","The {\it pro bono} statistician","0",""
"10.1198/016214507000000536","2007","On directional regression for dimension reduction","13","We introduce directional regression (DR) as a method for dimension reduction. Like contour regression, DR is derived from empirical directions, but achieves higher accuracy and requires substantially less computation. DR naturally synthesizes the dimension reduction estimators based on conditional moments, such as sliced inverse regression and sliced average variance estimation, and in doing so combines the advantages of these methods. Under mild conditions, it provides exhaustive and root n-consistent estimate of the dimension reduction space. We develop the asymptotic distribution of the DR estimator, and from that a sequential test procedure to determine the dimension of the central space. We compare the performance of DR with that of existing methods by simulation and find strong evidence of its advantage over a wide range of models. Finally, we apply DR to analyze a data set concerning the identification of hand-written digits."
"10.1198/016214507000000455","2007","Bayesian curve classification using wavelets","2","We propose classification models for binary and multicategory data where the predictor is a random function. We use Bayesian modeling with wavelet basis functions that have nice approximation properties over a large class of functional spaces and can accommodate a wide variety of functional forms observed in real life applications. We develop an unified hierarchical model to encompass both the adaptive wavelet-based function estimation model and the logistic classification model. We couple together these two models are to borrow strengths from each other in a unified hierarchical framework. The use of Gibbs sampling with conjugate priors for posterior inference makes the method computationally feasible. We compare the performance of the proposed model with other classification methods, such as the existing naive plug-in methods, by analyzing simulated and real data sets."
"10.1198/016214507000000239","2007","Optimal tests of noncorrelation between multivariate time series","0","The problem of testing noncorrelation between two multivariate time series is considered. Assuming that the global process admits a joint vector autoregressive (VAR) representation, noncorrelation between the two component series is equivalent to the hypothesis that all off-diagonal blocks in the matrix coefficients and the innovation covariance of the joint VAR representation are zero. We establish an adequate local asymptotic normality (LAN) property for this VAR model in the vicinity of noncorrelation. This LAN structure allows construction of optimal pseudo-Gaussian tests-that is, tests that are locally and asymptotically optimal under Gaussian innovations, but remain valid under non-Gaussian ones-for the null hypothesis of noncorrelation and for comparing their local asymptotic powers with those of the heuristic tests (Haugh-El Himdi-Roy and Koch-Yang-Hallin-Saidi) proposed in the literature."
"10.1198/016214507000000680","2007","Importance sampling for estimating {$p$} values in linkage analysis","0","Importance sampling methods are proposed for estimating the probability that the maximum of a random process exceeds a high threshold, with particular attention to assessing genome-wide significance levels in linkage analysis. The proposed algorithm is applied to computing the conditional significance level, given observed phenotypes, of scan statistics to map quantitative traits in experimental populations and in extended pedigrees of moderate size with partially informative markers. For detecting an additive effect in an intercross, the importance sampling algorithm is roughly 100 times as efficient as direct Monte Carlo simulation when the true significance level is about.05. For pedigrees with partially informative markers, the efficiency varies greatly with intermarker distance, marker polymorphism, sample size, and missing data. In this case the importance sampling algorithm is used to efficiently explore how p values vary with these parameters. Extensions of the algorithm are discussed for the case of multidimensional statistics that arise when considering dominance effects or searching for multiple loci that may involve interactions."
"10.1198/016214507000000815","2007","Detecting sparse signals in random fields, with an application to brain mapping","1","Brain mapping data have been modeled as Gaussian random fields, and local increases in mean are detected by local maxima of a random field of test statistics derived from these data. Accurate p values for local maxima are readily available for isotropic data based on the expected Euler characteristic of the excursion set of the test statistic random field. In this article we give a simple method for dealing with nonisotropic data. Our approach has connections to the model of Sampson and Guttorp for nonisotropy in which there exists an unknown mapping of the support of the data to a space in which the random fields are isotropic. Heuristic justification for our approach comes from the Nash embedding theorem. Formally, we show that our method gives consistent unbiased estimators for the true p values based on new results of Taylor and Adler for random fields on manifolds, which replace the Euclidean metric by the variogram. The results are used to detect gender differences in the cortical thickness of the brain and to detect regions of the brain involved in sentence comprehension measured by functional magnetic resonance imaging."
"10.1198/016214506000001248","2007","Nonparametric tests for perfect judgment rankings","0","The ranked-set sampling literature includes both inference procedures that rely on the assumption of perfect rankings and inference procedures that are robust to violations of this assumption. Procedures that assume perfect rankings tend to be more efficient when rankings are in fact perfect, but they may be invalid when perfect rankings fail. As a result, users of ranked-set sampling must decide between efficiency and robustness, and there is at present little to guide their decision. In this article we introduce three rank-based goodness-of-fit tests that may be consulted in making these decisions. Our strategy in producing these tests is to think of the judgment order statistic classes as separate samples, compute the ranks of the units from each sample within the combined sample, and use these ranks to test whether the judgment rankings are perfect. Consideration of both power and ease of use leads us to recommend use of a test that rejects when the concordance between the vector of mean ranks and its null expectation is small. Tables of critical values and appropriate asymptotic theory for applying this test are provided, and we illustrate the use of the tests by applying them to a biological dataset."
"10.1198/016214506000001310","2007","Model averaging and dimension selection for the singular value decomposition","0","Many multivariate data-analysis techniques for an m x n matrix Y are related to the model Y = M + E, where Y is an m x 17 matrix of full rank and M is an unobserved mean matrix of rank K < (m boolean AND n). Typically the rank of M is estimated in a heuristic way and then the least-squares estimate of M is obtained via the singular value decomposition of Y, yielding an estimate that can have a very high variance. In this article we suggest a model-based alternative to the preceding approach by providing prior distributions and posterior estimation for the rank of M and the components of its singular value decomposition. In addition to providing more accurate inference, such an approach has the advantage of being extendable to more general data-analysis situations, such as inference in the presence of missing data and estimation in a generalized linear modeling framework."
"10.1198/016214506000001329","2007","Optimal incomplete block designs","2","Optimal incomplete block designs are pursued through the E criterion of minimizing maximal variance. Methodology is developed for design choice based on graphs and an extensive catalog of downloadable designs is compiled. Along with the general methodology, a near complete solution for E-optimal block designs is provided for up to 15 treatments. E optimality is revealed to be a flexible criterion that, depending on the application, can offer many choices for good designs."
"10.1198/016214507000000176","2007","Dynamic integration of time- and state-domain methods for volatility estimation","1","Time- and state-domain methods are two common approaches to nonparametric prediction. Whereas the former uses data predominantly from recent history, the latter relies mainly on historical information. Combining these two pieces of valuable information is an interesting challenge in statistics. We surmount this problem by dynamically integrating information from both the time and state domains. The estimators from these two domains are optimally combined based on a data-driven weighting strategy, which provides a more efficient estimator of volatility. Asymptotic normality is separately established for the time domain, the state domain, and the integrated estimators. By comparing the efficiency of the estimators, we demonstrate that the proposed integrated estimator uniformly dominates the other two estimators. The proposed dynamic integration approach is also applicable to other estimation problems in time series. Extensive simulations are conducted to demonstrate that the newly proposed procedure outperforms some popular ones, such as the RiskMetrics and historical simulation approaches, among others. In addition, empirical studies convincingly endorse our integration method."
"10.1198/016214506000001275","2007","Determining the number of factors in the general dynamic factor model","3","This article develops an information criterion for determining the number q of common shocks in the general dynamic factor model developed by Form et al., as opposed to the restricted dynamic model considered by Bai and Ng and by Amengual and Watson. Our criterion is based on the fact that this number q is also the number of diverging eigenvalues of the spectral density matrix of the observations as the number n of series goes to infinity. We provide sufficient conditions for consistency of the criterion for large n and T (where T is the series length). We show how the method can be implemented and provide simulations and empirics illustrating its very good finite-sample performance. Application to real data adds a new empirical facet to an ongoing debate on the number of factors driving the U.S. economy."
"10.1198/016214507000000211","2007","Multiple hypothesis testing by clustering treatment effects","2","Multiple hypothesis testing and clustering have been the subject of extensive research in high-dimensional inference, yet these problems usually have been treated separately. By defining true clusters in terms of shared parameter values, we could improve the sensitivity of individual tests, because more data bearing on the same parameter values are available. We develop and evaluate a hybrid methodology that uses clustering information to increase testing sensitivity and accommodates uncertainty in the true clustering. To investigate the potential efficacy of the hybrid approach, we first study a stylized example in which each object is evaluated with a standard z score but different objects are connected by shared parameter values. We show that there is increased testing power when the clustering is estimated sufficiently well. We next develop a model-based analysis using a conjugate Dirichlet process mixture model. The method is' general, but for specificity we focus attention on microarray gene expression data, to which both clustering and multiple testing methods are actively applied. Clusters provide the means for sharing information among genes, and the hybrid methodology averages over uncertainty in these clusters through Markov chain sampling. Simulations show that the hybrid method performs substantially better than other methods when clustering is heavy or moderate and performs well even under weak clustering. The proposed method is illustrated on microarray data from a study of the effects of aging on gene expression in heart tissue."
"10.1198/016214507000000167","2007","Estimating the null and the proportional of nonnull effects in large-scale multiple comparisons","10","An important issue raised by Efron in the context of large-scale multiple comparisons is that in many applications, the usual assumption that the null distribution is known is incorrect, and seemingly negligible differences in the null may result in large differences in subsequent studies. This suggests that a careful study of estimation of the null is indispensable. In this article we consider the problem of estimating a null normal distribution, and a closely related problem, estimation of the proportion of nonnull effects. We develop an approach based on the empirical characteristic function and Fourier analysis. The estimators are shown to be uniformly consistent over a wide class of parameters. We investigate the numerical performance of the estimators using both simulated and real data. In particular, we apply our procedure to the analysis of breast cancer and human immunodeficiency virus microarray datasets. The estimators perform favorably compared with existing methods."
"10.1198/016214506000001338","2007","Multiple testing of general contrasts: truncated closure and the extended {S}haffer-{R}oyen method","1","Powerful improvements are possible for multiple testing procedures when the hypotheses are logically related. Closed testing with alpha-exhaustive tests provides a unifying framework for developing such procedures, but can be computationally difficult and can be ""non-monotonic in p values."" Royen introduced a ""truncated"" closed testing method for the case of all pairwise comparisons in the analysis of variance that is monotonic in p values. Shaffer developed a similar truncated procedure for more general comparisons, but using Bonferroni tests rather than a-exhaustive tests, and Westfall extended Shaffer's method to allow a-exhaustive tests. This article extends Royen's method to general contrasts and proves that it is equivalent to the extended Shaffer procedure. For k contrasts, the method generally requires evaluation of O(2(k)) critical values that correspond to subset intersection hypotheses and is computationally infeasible for large k. The set of intersections is represented using a tree structure, and a branch-and-bound algorithm is used to search the tree and reduce the O(2(k)) complexity by obtaining conservative ""covering sets"" that retain control of the familywise type I error rate. The procedure becomes less conservative as the tree search deepens, but computation time increases. In some cases where hypotheses are logically restricted, even the more conservative covering sets provide much more power than standard methods.The methods described herein are general, computable, and often much more powerful than commonly used methods for multiple testing of general contrasts, as shown by applications to pairwise comparisons and response surfaces. In particular, with response surface tests, the method is computable with complete tree search, even when k is large."
"10.1198/016214506000000807","2007","Variance reduction in multiparameter likelihood models","1","Local likelihood modeling is a unified and effective approach to establishing the dependence of a response variable, which can be of various types, on independent variables. Therefore, these models have become popular in a wide range of applications. There is an increasing interest in employing multiparameter local likelihood models to investigate trends of sample extremes in environmental statistics. When sample maxima are modeled by a generalized extreme value distribution, the sample size is small in general and local likelihood estimation exhibits a large variation. In this article variance reduction techniques are employed to improve the efficiency of the inference. A simulation study and an application to annual maximum temperatures show that our methods are very effective in finite samples."
"10.1198/016214506000001095","2007","Power transformation toward a linear regression quantile","5","In this article we consider the linear quantile regression model with a power transformation on the dependent variable. Like the classical Box-Cox transformation approach, it extends the applicability of linear models without resorting to nonparametric smoothing, but transformations on the quantile models are more natural due to the equivariance property of the quantiles under monotone transformations. We propose an estimation procedure and establish its consistency and asymptotic normality under some regularity conditions. The objective function employed in the estimation can also be used to check inadequacy of a power-transformed linear quantile regression model and to obtain inference on the transformation parameter. The proposed approach is shown to be valuable through illustrative examples."
"10.1198/016214506000000979","2007","Quantile regression in reproducing kernel {H}ilbert spaces","2","In this article we consider quantile regression in reproducing kernel Hilbert spaces, which we call kernel quantile regression (KQR). We make three contributions: (1) we propose an efficient algorithm that computes the entire solution path of the KQR, with essentially the same computational cost as fitting one KQR model; (2) we derive a simple formula for the effective dimension of the KQR model, which allows convenient selection of the regularization parameter; and (3) we develop an asymptotic theory for the KQR model."
"10.1198/016214506000001301","2007","Minimum area confidence set optimality for confidence bands in simple linear regression","0","The average width of a simultaneous confidence band has been used by several authors (e.g., Naiman and Piegorsch) as a criterion for the comparison of different confidence bands. In this article the area of the confidence set that corresponds to a confidence band is used as a new criterion. For simple linear regression, comparisons have been carried out under this new criterion between hyperbolic bands, two-segment bands, and three-segment bands, which include constant width bands as special cases. It is found that if one requires a confidence band over the whole range of the covariate, then the best confidence band is given by the Working and Hotelling hyperbolic band. Furthermore, if one needs a confidence band over a finite interval of the covariate, then a restricted hyperbolic band can again be recommended, although a three-segment band may be very slightly superior in certain cases."
"10.1198/016214506000001103","2007","Efficient estimation of population-level summaries in general semiparametric regression models","3","This article considers a wide class of semiparametric regression models in which interest focuses on population-level quantities that combine both the parametric and the nonparametric parts of the model. Special cases in this approach include generalized partially linear models, generalized partially linear single-index models, structural measurement error models, and many others. For estimating the parametric part of the model efficiently, profile likelihood kernel estimation methods are well established in the literature. Here our focus is on estimating general population-level quantities that combine the parametric and nonparametric parts of the model (e.g., population mean, probabilities, etc.). We place this problem in a general context, provide a general kernel-based methodology, and derive the asymptotic distributions of estimates of these population-level quantities, showing that in many cases the estimates are semiparametric efficient. For estimating the population mean with no missing data, we show that the sample mean is serniparametric efficient for canonical exponential families, but not in general. We apply the methods to a problem in nutritional epidemiology, where estimating the distribution of usual intake is of primary interest and serniparametric methods are not available. Extensions to the case of missing response data are also discussed."
"10.1198/016214506000001266","2007","Optimal shrinkage estimation of variances with applications to microarray data analysis","3","Microarray technology allows a scientist to study genomewide patterns of gene expression. Thousands of individual genes are measured with a relatively small number of replications, which poses challenges to traditional statistical methods. In particular, the gene-specific estimators of variances are not reliable and gene-by-gene tests have Sow powers. In this article we propose a family of shrinkage estimators for variances raised to a fixed power. We derive optimal shrinkage parameters under both Stein and squared loss functions. Our results show that the standard sample variance is inadmissible under either loss function. We propose several estimators for the optimal shrinkage parameters and investigate their asymptotic properties under two scenarios: large number of replications and large number of genes. We conduct simulations to evaluate the finite sample performance of the data-driven optimal shrinkage estimators and compare them with some existing methods. We construct F-like statistics using these shrinkage variance estimators and apply them to detect differentially expressed genes in a microarray experiment. We also conduct simulations to evaluate performance of these F-like statistics and compare them with some existing methods."
"10.1198/016214506000001220","2007","Detecting differential expressions in {G}ene{C}hip microarray studies: a quantile approach","4","In this article we consider testing for differentially expressed genes in GeneChip studies by modeling and analyzing the quantiles of gene expression through probe level measurements. By developing a robust rank score test for linear quantile models with a random effect, we propose a reliable test for detecting differences in certain quantiles of the intensity distributions. By using a genomewide adjustment to the test statistic to account for within-array correlation, we demonstrate that the proposed rank score test is highly effective even when the number of arrays is small. Our empirical studies with real experimental data show that detecting differences in the quartiles for the probe level data is a valuable complement to the usual mixed model analysis based on Gaussian likelihood. The methodology proposed in this article is a first attempt to develop inferential tools for quantile regression in mixed models."
"10.1198/016214506000001211","2007","Correlation and large-scale simultaneous significance testing","13","Large-scale hypothesis testing problems, with hundreds or thousands of test statistics z(i) to consider at once, have become familiar in current practice. Applications of popular analysis methods, such as false discovery rate techniques, do not require independence of the z(i)'s, but their accuracy can be compromised in high-correlation situations. This article presents computational and theoretical methods for assessing the size and effect of correlation in large-scale testing. A simple theory leads to the identification of a single omnibus measure of correlation for the z(i)'s order statistic. The theory relates to the correct choice of a null distribution for simultaneous significance testing and its effect on inference."
"10.1198/016214508000000814","2008","Correlation-based functional clustering via subspace projection","0","A correlation-based functional clustering method is proposed for grouping curves with similar shapes. A correlation between two random functions defined through the functional inner product is used as a similarity measure. Curves with similar shapes are embedded in the cluster subspace spanned by a a mean shape function and eigenfunctions of the covariance kernel. The cluster membership prediction for each curve attempts to maximize the functional correlation between the observed and predicted curves via shape standardization and subspace projection among all possible clusters. The proposed method accounts for shape differentials through the functional multiplicative random-effects shape function model for each cluster, which regards random scales and intercept shifts as a nuisance. A consistent estimate is proposed for the random scale effect, whose sample variance estimate is also consistent. The derived identifiablility conditions for the clustering procedure unravel the predictability of cluster memberships. Simulation and a real data example illustrate the proposed method."
"10.1198/016214508000000779","2008","Properties and implementation of {J}effreys's prior in binomial regression models","0","We study several theoretical properties of Jeffreys's prior for binomial regression models. We show that Jeffreys's prior is symmetric and unimodal for a class of binomial regression models. We characterize the tail behavior of Jeffreys's prior by comparing it with the multivariate t and normal distributions under the commonly used logistic, probit, and complementary log-log regression models. We also show that the prior and posterior normalizing constants under Jeffreys's prior are linear transformation-invariant in the covariates. We further establish an interesting theoretical connection between the Bayes information criterion and the induced dimension penalty term using Jeffreys's prior for binomial regression models with general links in variable selection problems. Moreover, we develop an importance sampling algorithm for carrying out prior and posterior computations under Jeffreys's prior. We analyze a real data set to illustrate the proposed methodology."
"10.1198/016214508000001075","2008","Order selection in finite mixture models with a nonsmooth penalty","0","Order selection is a fundamental and challenging problem in the application of finite mixture models. We develop a new penalized likelihood approach that we call MSCAD. MSCAD deviates from information-based methods, such as Akaike information criterion and the Bayes information criterion, by introducing two penalty functions that depend on the mixing proportions and the component parameters. It is consistent in estimating both the order of the mixture model and the mixing distribution. Simulations show that MSCAD performs much better than some existing methods. Two real-data examples are examined to illustrate its performance."
"10.1198/016214508000000986","2008","On adaptive extensions of group sequential trials for clinical investigations","0","In group sequential trials, it is important to obtain adequate data to assess overall benefits and risks of an experimental treatment for patients To achieve this goal, we provide general, formal framework for adaptively extending a group sequential trial to stop at any interim analysis time. often after a significance boundary for a clinical endpoint is crossed. For statistical inference, we propose to order the sample space by a class of well-ordered group sequential tests. On this basis. we develop a unified sequential statistical inference approach that is applicable to both interim monitoring and final analysis. We also show that the new ordering provides the foundation for the repeated confidence intervals procedure of Jennison and Turnbull."
"10.1198/016214508000000940","2008","A few remarks on ``{A} capture-recapture approach for screening using two diagnostic tests with availability of disease status for the test positives only'' by {B}\""ohning and {P}atilea [MR2420228]","0",""
"10.1198/016214508000001039","2008","Bayesian inference on changes in response densities over predictor clusters","0","In epidemiology, it often is of interest to assess how individuals with different trajectories over time in an environmental exposure or biomarker differ with respect to a continuous response. For ease in interpretation and presentation of results. epidemiologists typically categorize predictors before analysis. To extend this approach to time-varying predictors, individuals can be clustered by then predictor trajectory, with the cluster index included as a predictor in a regression model for the response. This article develops, a semiparametric Bayes approach that avoids assuming a prespecified number of clusters and allows the response to vary nonparametrically over predictor clusters. This methodology is motivated by interest in relating trajectories in weight gain during pregnancy to the distribution of birth weight adjusted for gestational age at delivery. In this setting, the proposed approach allows the tails of the birth weight density to vary flexibly over weight gain clusters."
"10.1198/016214508000000841","2008","Multiple inference and gender differences in the effects of early intervention: a reevaluation of the {A}becedarian, {P}erry {P}reschool, and early training projects","0","The view that the returns to educational investments are highest for early. childhood interventions is widely held and steins primarily from several influential randomized trials-Abecedarian, Perry. and the Early Training Project-that point to super-normal returns to early interventions. This article presents it de novo analysis of these experiments, focusing on two core issues that have received limited attention in previous analyses: treatment effect heterogeneity by gender and overrejection of the null hypothesis due to multiple inference. To address the latter issue. a statistical framework thin combines summary index tests with familywise error rate and false discovery rate corrections is implemented The first technique reduces the number of tests conducted: the latter two techniques adjust the p values for multiple inference. The primary finding of the reanalysis is that girls garnered substantial short- and long-term benefits from the interventions, but there were no significant long-term benefits for boys. These conclusions. which have appeared ambiguous when using ""naive"" estimators that fail to adjust for multiple testing. contribute to a growing literature on the emerging female-male academic achievement gap. They also demonstrate that in complex studies where Multiple questions are asked of the same data set, it can be important to declare the family of tests under consideration and to either consolidate measures or report adjusted and unadjusted p values."
"10.1198/016214508000000832","2008","A case study in pharmacologic colon imaging using principal curves in single-photon emission computed tomography","0","In this article we are concerned with functional imaging of the colon to assess the kinetics of microbicide lubricants. The overarching goal IS to understand the distribution of the lubricants in the colon. Such information is crucial for understanding the potential impact of microbicides on human immunodeficiency virus transmission. The experiment was conducted by imaging a radiolabeled lubricant distributed in the subject's colon. The tracer imaging was conducted via single-photon emission computed tomography noninvasive in vivo functional imaging technique. We have developed a novel principal curve algorithm to construct a three-dimensional curve through the colon images. The algorithm was tested and debugged on several difficult two-dimensional images of familiar curves where the original principal curve algorithm does not apply. The final curve fit to the colon data is compared with experimental sigmoidoscope collection."
"10.1198/016214508000000823","2008","Hierarchical insurance claims modeling","0","This work describes statistical modeling of detailed. microlevel automobile insurance records. We consider 1993-2001 data a from it major insurance company in Singapore. By detailed microlevel records. we mean experience at the individual vehicle level, including vehicle and driver characteristics, insurance coverage, and claims experience. by year. The claims experience consists of detailed information oil the type of insurance claim such as whether the claim is due to injury to a third party, property damage to it third party, or claims for damage to the insured. as well its the corresponding claim amount. We propose a hierarchical model for three components. corresponding to the frequency. type. and severity of claims. The first model is it negative binomial regression model for assessing claim frequency. The driver's gender, age, and no claims discount, its well as vehicle age and type, turn out to be important variables for predicting the event of a claim. The second is a multinomial logit model to predict the type of insurance claim, whether it is third-party injury, third-party property damage. insured's own damage or some combination year vehicle age. and vehicle type turn out to be important predictors for this component. Our third model is for the severity component. Here we use a generalized beta of the second kind of long-tailed distribution for claim amounts and also incorporate predictor variables. Year. vehicle age, and person's age turn Out to be important predictors for this component. Not Surprisingly. we show it significant dependence among the different claim types we use a t-copula to account for this dependence. The three-component model provides justification for assessing the importance of it rating variable. When taken together. the integrated model allows more efficient prediction of automobile claims compared with than traditional methods. Using Simulation. we demonstrate this by developing predictive distributions and calculating premiums under alternative coverage limitations."
"10.1198/016214508000000869","2008","High-dimensional sparse factor modeling: applications in gene expression genomics","3","We describe Studies in molecular profiling and biological pathway analysis that use sparse latent factor and regression models for microarray gene expression data. We discuss breast cancer applications and key aspects of the modeling and computational methodology. Our case Studies aim to investigate and characterize heterogeneity of structure related to specific oncogenic pathways, its well as links between aggregate patterns in gene expression profiles and clinical biomarkers. Based on the metaphor of statistically derived ""factors"" as representing biological ""subpathway"" structure, we explore the decomposition of fitted sparse factor models into pathway subcomponents and investigate how these components overlay multiple aspects of known biological activity. Our methodology is based on sparsity modeling of multivariate regression, ANOVA, and latent factor models, as well as a class of models that combines all components. Hierarchical sparsity priors address questions of dimension reduction and multiple comparisons, as well its scalability of the methodology. The models include practically relevant non-Gaussian/nonparametric component,,. for latent structure. underlying often quite complex non-Gaussianity in multivariate expression patterns. Model search and fitting are addressed through stochastic simulation and evolutionary stochastic search methods that are exemplified in the oncogenic pathway Studies. Supplementary supporting material provides more details of the applications, its well as examples of the use of freely available software tools for implementing the methodology."
"10.1198/016214508000000724","2008","Domain-level covariance analysis for multilevel survey data with structured nonresponse","0","Health care quality surveys in the United States are administered to individual respondents (i.e.. hospital patients, health plan members) to evaluate the performance of health care organizations (i.e., hospitals, health plans). which thus Constitute estimation domains. For better understanding and more parsimonious reporting of dimensions of quality, we analyze relationships among., quality Measures at the domain level. Rather than specifying a full parametric model for the observed responses and the nonresponse patterns at the lower (patient) level. we first fit generalized variance-covariance functions that take into account nonresponse patterns in the survey responses, then specify a likelihood function for the domain mean responses using these generalized variance-covariance functions. This allows us to model directly the relationships among domain means for different items. Because the response scales are bounded. we assume that these means follow a truncated multivariate normal distribution. We calculate maximum likelihood (ML) estimates using the EM algorithm and sample under Bayesian models using Markov chain Monte Carlo. Finally. we perform factor analysis on the estimated or sampled between-domain covariance matrixes. Using posterior draws, we assess posterior distributions of the number of selected factors and the assignment of items to groups under conventional rules. We compare ML estimates of this factor structure with those from several Bayesian models with different prior distributions for the between-domain covariance. We present analyses of data from the Consumer Assessment of Healthcare Providers and Systems (CAHPS) survey of Medicare Advantage health plans."
"10.1198/016214508000000706","2008","Does finasteride affect the severity of prostate cancer? {A} causal sensitivity analysis","0","In 2003 Thompson and colleagues reported that daily use of finasteride reduced the prevalence of prostate cancer by 25% compared to placebo. These results were based oil the double-blind randomized Prostate Cancer Prevention Trial (PCPT), which followed 18.882 men with no prior Or Current indications of prostate cancer annually for 7 years. Enthusiasm for the risk reduction afforded by the chemopreventative agent and adoption Of its use in clinical practice. however. was severely dampened by (lie additional finding in the trial of an increased absolute number of high-grade (Gleason score >= 7) cancers oil the finasteride arm. The question arose as to whether this finding, truly implied that finasteride increased the risk of more severe prostate cancer or was a study artifact due to a series of possible postrandomization election biases, including differences among treatment arms in patient characteristics of cancer cases, differences in biopsy verification of cancer status due to increased sensitivity Of prostate-specific antigen under finasteride, differential grading by biopsy due to prostate volume reduction by finasteride, and nonignorable dropout. Via a causal inference approach implementing inverse probability weighted estimating equations. this, analysis addresses the question of whether finasteride caused more severe prostate cancer by estimating the mean treatment difference in prostate cancer severity between finasteride and placebo for the principal stratum of participants who would have developed prostate cancer regardless of treatment assignment. We perform sensitivity analyses that sequentially adjust for the numerous potential postrandomization biases conjectured in the PCPT."
"10.1198/016214508000001048","2008","Comparing stochastic optimization methods for variable selection in binary outcome prediction, with application to health policy","0","Traditional variable-selection strategies in generalized linear models (GLMs) seek to optimize a measure of predictive accuracy without regard for the cost of data collection. When the purpose of such model building is the creation of predictive scales to be used in future studies with constrained budgets. the standard approach may not be optimal. We propose a Bayesian decision-theoretic framework for variable selection in binary-outcome GLMs where the budget for data collection is constrained and potential predictors may vary considerably in cost. The method is illustrated using data from a large Study of quality of hospital care in the U.S. in the 1980s. Especially, when the number of available predictors p is large. it is important to use an appropriate technique for optimization (e.g. in an application presented here where p = 83. the space over which we search has 2(83), 10(25) elements, which is too large to explore using brute force enumeration). Specifically we investigate simulated annealing (SA), genetic algorithms (GAs), and the tabu search (TS) method used in operations research. and we develop a context-specific version of SA, improved simulated annealing (ISA), that performs better than its generic counterpart. When p was modest in our study. we found that GAs performed relatively poorly for all but the very best user-defined input configurations, generic SA did not perform well. and TS had excellent median performance and was much less sensitive to suboptimal choice of user-defined inputs. When p was large in our study. the best versions of GA and ISA outperformed TS and generic SA. Our results are presented in the context of health policy but can apply to other quality assessment settings with dichotomous outcomes as well."
"10.1198/016214508000001020","2008","Rejoinder [MR2655714; MR2655715; MR2655716; MR2655717]","0",""
"10.1198/016214508000000733","2008","Can nonrandomized experiments yield accurate answers? {A} randomized experiment comparing random and nonrandom assignments","0","A key justification for using nonrandomized experiments is that, with proper adjustment, their results can well approximate results from randomized experiments. This hypothesis has not been consistently supported by empirical studies; however, previous methods used to study this hypothesis have confounded assignment method with other study features. To avoid these confounding factors, this study randomly assigned participants to be in a randomized experiment or a nonrandomized experiment. In the randomized experiment, participants were randomly assigned to mathematics or vocabulary training; in the nonrandomized experiment, participants chose their training. The study held all other features of the experiment constant: it carefully measured pretest variables that might predict the condition that participants chose, and all participants were measured on vocabulary and mathematics outcomes. Ordinary linear regression reduced bias in the nonrandomized experiment by 84-94% using covariate-adjusted randomized results as the benchmark. Propensity score stratification, weighting, and covariance adjustment reduced bias by about 58-96%. depending on the outcome measure and adjustment method. Propensity score adjustment performed poorly when the scores were constructed from predictors of convenience (sex, age, marital status, and ethnicity) rather than from a broader set of predictors that might include these."
"10.1198/016214508000000931","2008","Statistics: harnessing the power of information","0","Statisticians are harnessing the power of information buried in today's massive and complex data. High-dimensional data sets, often the result of combining multiple databases, have led to important challenges in mathematical statistics. The increasing quantity of high-dimensional complex data and the ability of statisticians to uncover the information in it have led to ever greater demand for their expertise. For example, many businesses seek to build analytic capabilities, and statisticians are essential to this effort. To meet the increased demand, the academic community has been called on to provide more statistics courses, faculty, and graduates. Although statisticians who can meet these demands of modem data are highly sought after, the employment prospects for statisticians are not easy to track, because the term ""statistician"" often is not used for relevant positions. However, the trend is unmistakable for statisticians who can unlock the power of these new data sets. Statistics graduate students are uncovering the value of information in data for local government and community nonprofit organizations through Statistics in the Community (STATCOM). an American Statistical Association student volunteer organization for pro bono statistics. STATCOM provides statistics students with real-world experiences while providing invaluable community assistance."
"10.1198/016214508000000544","2008","Selection of variables for cluster analysis and classification rules","0","In this article we introduce two procedures for variable selection in cluster analysis and classification rules. One is mainly aimed at detecting the ''noisy'' noninformative variables, while the other also deals with multicolinearity and general dependence. Both methods are designed to be used after a ''satisfactory'' grouping procedure has been carried out. A forward-backward algorithm is proposed to make such procedures feasible in large datasets. A small simulation is performed and some real data examples are analyzed."
"10.1198/016214508000000454","2008","Statistical significance of clustering for high-dimension, low-sample size data","6","Clustering methods provide a powerful tool for the exploratory analysis of high-dimension, low-sample size (HDLSS) data sets, such as gene expression microarray data. A fundamental statistical issue in clustering is which clusters are ''really there'', as opposed to being artifacts of the natural sampling variation. We propose SigClust as a simple and natural approach to this fundamental statistical problem. In particular, we define a cluster as data coming from a single Gaussian distribution and formulate the problem of assessing statistical significance of clustering as a testing procedure. This Gaussian null assumption allows direct formulation of p values that effectively quantify the significance of a given clustering. HDLSS covariance estimation for SigClust is achieved by a combination of invariance principles, together with a factor analysis model. The properties of SigClust are studied. Simulated examples, as well as an application to a real cancer microarray data set, show that the proposed method works remarkably well for assessing significance of clustering. Some theoretical results also are obtained."
"10.1198/016214508000000526","2008","On consistent nonparametric intensity estimation for inhomogeneous spatial point processes","2","A common nonparametric approach to estimate the intensity function of an inhomogeneous spatial point process is through kernel smoothing. When conducting the smoothing, one typically uses events only in a local set around the point of interest. But the resulting estimator often is inconsistent, because the number of events in a fixed set is of order 1 for spatial point processes. In this article we propose a new covariance-based kernel smoothing method to estimate the intensity function. Our method defines the distance between any two points as the difference between their associated covariate values. Consequently, we determine the kernel weight for a given event of the process as a function of its new distance to the point of interest. Under some suitable conditions on the covariates and the spatial point process, we prove that our new estimator is consistent for the true intensity. To handle the situation with high-dimensional covariates, we also extend sliced inverse regression, a useful dimension-reduction tool in standard regression analysis, to spatial point processes. Simulations and an application to a real data example are used to demonstrate the usefulness of the proposed method."
"10.1198/016214508000000517","2008","Combining registration and fitting for functional models","1","A registration method can be defined as a process of aligning features of a sample of curves by monotone transformations of their domain. The aligned curves exhibit only amplitude variation, and the domain transformations, called warping functions, capture the phase variation in the original curves. In this article we precisely define a new type of registration process, in which the warping functions optimize the fit of a principal components decomposition to the aligned curves. The principal components are effectively the features that this process aligns. We discuss the relationship of registration to closure of a function space under convex operations, and define consistency for registration methods. We define an explicit decomposition of functional variation into amplitude and phase partitions, and develop an algorithm for combining registration with principal components analysis, and apply it to simulated and real data."
"10.1198/016214508000000616","2008","Rejoinder [MR2528831]","0",""
"10.1198/016214508000000553","2008","The nested {D}irichlet process","6",""
"10.1198/016214508000000689","2008","Bayesian treed {G}aussian process models with an application to computer modeling","5","Motivated by a computer experiment for the design of a rocket booster this article explores nonstationary modeling methodologies that couple stationary Gaussian processes with treed partioning is a simple but effective method for dealing with nonstationarity. The methodological developments and statistical computing details that make this approach efficient are described in detail. In addition to providing an analysis of the rocket booster, we show that our approach is effective in other areas as well."
"10.1198/016214508000000670","2008","Modeling price dynamics in e{B}ay auctions using differential equations","4","Empirical research of online auctions has grown dramatically in recent years. Studies using publicly available bid data from such wedsites as eBay.com have found many divergences of bidding behavior and auction outcomes compared with ordinary offline auctions and auction theory. Among the main differences between online and offline auctions are the former's longer duration, anonymity of bidders and sellers, and low barriers of entry. All of these factors lead to dynamics in the bid arrival and price process that change throughout the auction. In this work we examine the price process in a large and diverse set of eBay auctions, for both low- and high-valued items, in terms of item, auction,bidder, and seller characteristics. We propose a family of differential equation models that captures online auction dynamics. In particular, introduce a multiple-comparisons test for comparing dynamic models of auction subpopulations, which we use to compare subpopulations of auctions grouped by characteristics of the auction, item, seller and bidders. We find that price dynamics change throughout the auction and are influenced mostly by factors that affect the level of uncertainty about the outcome (e.g., seller rating, item condition) and the level of competitiveness (e.g., early bidding, number of bids). We accomplish the modeling tasks within the framework of principle differential analysis and functional data models."
"10.1198/016214508000000661","2008","Prediction of protein interdomain linker regions by a nonstationary hidden {M}arkov model","0","Proteins have tow types of regions: linker regions that connect active domains and nonlinker regions, which include domains and terminal residues. It is important to try to distinguish these regions using information in the sequence of amino acids that, when folded, form the protein. In this article we develop a nonstationary hidden Markov model to infer region boundaries. The approach is based on a latent variable defined on the sequence, this variable is a continuous index of the region type and affects the probabilities that specific amino acids will apear. We develop an efficient Bayesian estimate of the model using Markov chain Monte Carlo methods, particularly Gibbs sampling, to simulate the parameters from the posteriors. We apply our methods to protein sequences with domains and interdomain linkers delineated using the Pfam-A database. The prediction results are superior to simpler methods. Importantly, our method estimates the probability that each amino acid belongs to a linker region, giving insight into the properties of interdomain linkers."
"10.1198/016214507000001427","2008","Election forecasts using spatiotemporal models","0","There exists a large literature on the problem of forecasting election results. But none of the published methods take spatial information into account, although there is clear evidence of geographic trends. To fill this gap, we use geostatical procedures to build a spatial model of voting patterns. We test the model in three close elections and find that it outperforms rival methods in current use. We apply kriging (a spatial model) and cokriging (in a spatiotemporal model version) to improve the accuracy of election night forecasts. We compare the results with actual outcomes and also to predictions made using models that use only historical data from polling stations in previous elections. Despite the apparent volatility leading up to three elections in our study, the use of spatial information strongly improves the accuracy of the prediction. Compared with forecasts using historical data alone, the spatiotemporal models are better whenever the proportion of counted votes in the election night tally exceeds 5%."
"10.1198/016214507000001319","2008","A test for partial differential expression","0","Even in a single-tissue type cancer is often a collection of different diseases, each with its own genetic mechanism. Consequently, a gene may be expressed in some but not all of the tissues in a sample. Differentially expressed genes are commonly detected by methods that test for a shift in location that ignore the possibility of heterogeneous expression. This article proposes a two-sample test statistic designed to detect shifts that occur in only a part of the sample (partial shifts). The statistic is based on the mixing proportion in a nonparametric mixture and minimizes a weight distance function. The test is shown to be asymptotically distribution free and consistent, and an efficient permutation-based algorithm for estimating the p value is discussed. A simulation study shows that the tests is indeed more powerful than the two-sample t test and the Cramer-von Mises test for detecting partial shifts and is competitive for whole-sample shifts. The use os the test is illustrated on real-life cancer datasets, where the test is able to find genes with clear heterogeneous expression associated with reported subtypes of the cancer."
"10.1198/016214508000000139","2008","Imputing risk tolerance from survey responses","0","Economic theory assigns a central role to risk preferences. This article develops a measure of relative risk tolerance using response to hypothetical income gambles in the Health and Retirement Study. In contrast to most survey measures that produce an ordinal metric, this article shows how to construct a cardinal proxy for the risk tolerance of each survey respondent. The article also shows how to account for measurement error in estimating this proxy and how to obtain consistent regression estimates despite the measurement error. The risk tolerance proxy is shown to explain differences in asset allocation across households."
"10.1198/016214507000001049","2008","Exploring voting blocs within the {I}rish electorate: a mixture modeling approach","0","Irish elections use a voting system called proportion representation by means of a single transferable vote(PR-STV). Under this system, voters express their vote by ranking some (or all) of the candidates in order of preference. Which candidates are elected is determined through a series of counts where candidates are eliminated and surplus votes are distributed.The electorate in any election forms a heterogeneous population: that is voters with different political and ideological persuasions would be expected to have different preferences for the candidates. The purpose of this article is to establish the presence of voting bloes in the Irish electorate, to characterize these blocs and to estimate their size.A mixture modeling approach is used to explore the heterogenecity of the Irish electorate and to establish the existence of clearly defined voting blocs. The voting blocs are characterized by thier voting preferences which are described using a ranking data model. In addition the care with which voters choose lower tier preferences is estimated in the model.The methodology is used to explore data from two Irish election. Data from eight opinion polls taken during the six weeks prior to the 1997 Irish presidential election are analyzed. These data reveal the evolution of the structure of the electorate during the election campaign. In addition data that record the votes from the Dublin West constituency of the 2002 Irish general election are analyzed to reveal distinct voting blocs within the electoate these blocs are characterized by party politics, candidate profile and political ideology."
"10.1198/016214507000001030","2008","Analysis of smoking cessation patterns using a stochastic mixed-effects model with a latent cured state","0","We develop a mixed model to capture the complex stochastic nature of tobacco abuse and dependence. This model describes transition processes among addiction and nonaddiction stages. An important innovation of our model is allowing an unobserved cure state, or permanent quitting, in contrast to transient quitting. This distinction is necessary to model data from situations where censoring prevents unambiguous determination that a person has been ""cured,"" Moreover, the processes that described transient and permanent quitting are likely to be different and have different policy-making implications. For example, when analyzing factors that influence smoking and can be targeted by interventions, it is more important to target those factors that are associated with permanent quitting rather than transient quitting.We apply our methodology to the Alpha-Tocopherol, Beta-Carotene Cancer Prevention (ATBC) study, a large (29,133 participants) longitudinal cohort study. While ATBC was designed as a cancer prevention study. It contains unique information about the smoking status of each participant during every 4-month period of the study. These data are used to model smoking cessation patterns using a discrete-time stochastic mixed-effects model with threee states: smoking, transient cessation, and permanent cessation (absorbent state). Random participant-specific transition probabilities among these states are used to account for participant-to-participant heterogeneity. Another important innovation in our article is to design computationally practical methods for dealing with the size of the dataset and complexity of the models. This is achieved using the marginal likelihood obtained by integrating over the Beta distribution of random effects"
"10.1198/016214508000000120","2008","Breast cancer relative hazard estimates from case-control and cohort designs with missing data on mammographic density","0","We analyzed data from the Breast Cancer Detection Demonstration Project (BCDDP) to obtain multivariate relative hazard models for breast cancer that included mammographic density (MD) in addition to standard risk factors. Data from the BCDDP were collected from a stratified case-control study in the screening phase (1973-1980) and from follow-up of three subcohorts in the follow-up phase (1980-1995). For both phases. MD measurements were only available for about half the women who developed breast cancer (cases) and a small fraction of noncases. We used a logistic regression model for the stratified case-control study and developed a general pseudo-likelihood approach to accommodate missing covariate data (MD) by adapting the method of Scott and Wild and Breslow and Holubkov. We showed that this method was substantially more efficient than a previously proposed weghted-likelihood method. We assumed piecewise exponential models for the analysis of each subcohort, with the missing covariate (MD) distribution conditional on the observed information modeled with polytomous logistic regression. We developed an EM algorithm for estimation, which allowed for time-varying covariates, incomplete follow-up, and left truncation. We analyzed the three follow-up subcohorts separately and then combined the relative hazard models from the case-control and cohort data. The final model included main effects for MD, weight, age at first live birth, number of previous breast biopsies, and number of sisters or mother with breast cancer and was more discriminating (higher concordance) than the original model of Gail et al., which included standard risk jfactors but not MD. In a spearate work, we combined this relative hazard model with other data to project absolute breast cancer risk."
"10.1198/016214507000001256","2008","A {B}ayesian capture-recapture population model with simultaneous estimation of heterogeneity","0","We develop a Bayesian capture-recapture model that provides estimates of abundance as well as time-varying and heterogeneous survival and capture probability distributions. The model uses a state-space approach by incorporating an underlying population model and an observation model, and here is applied to photo-identification data to estimate trends in the abundance and survival of a population of bottlenose dolphins (Tursiops truncatus) in northeast Scotland. Novel features fo the mdoel include simultaneous estimation of time-varying survival and capture probability distributions, estimation of heterogeneity effects for survival and capture, use of separate data to inflate the number of identified animals to the total abundance, and integration of spearate observations of the same animals from right and left side photographs. A Bayesian approach using Markov chain Monte CArlo methods allows for uncertainty in measurement and parameters, and simulations confirm the model's validity."
"10.1198/016214507000001265","2008","Spatial analysis to quantify numerical model bias and dependence: how many climate models are there?","1","A limited number of complex numerical models that simulate the Earth's atmosphere, ocean, and land processes are the primary tool to study how climate may change over the next century due to anthropogenic emissions of greenhouse gases. A standard assumption is that these climate models are random samples from a distribution of possible models centered around the true climate. This implies that agreement with observations and the predictive skill of climate models will improve as more models are added to an average of the models. In this article we present a statistical methodology to quantify whether climate models are indeed unbiased and whether and where model biases are correlated across models. We consider the stimulated mean state and the simulated trend over the period 1970-1999 for Northern Hemisphere summer and winter temperature. The key to the statistical analysis is a spatial model for the bias of each climate model and the use of kernel smoothing to estimate the correlations of biases across different climate models. The spatial model is particularly important to determine statistical significance of the estimated correlation under the hypothesis of independent climate models. Our results suggest that most of the climate model bias patterns are indeed correlated. In particular, climate models developed by the same institution have highly correlated biases. Also, somewhat surprisingly, we find evidence that the model skills for simulating the mean climate and simulating the warming trends are not strongly related."
"10.1198/016214507000001247","2008","War and wages: the strength of instrumental variables and their sensitivity to unobserved biases","3","An instrument manipulates a treatment that it does not entirely control, but the instrument affects the outcome only indirectly through its maipulation of the treatment. The idealized prototype is the randomized encouragement design, in which subjects are randomly assigned to receive either encouragement to accept the treatment or no such encouragement, but not all subjects comply by doing what they are encouraged to do, and the situation is such that only the treatment itself, not disregarded encouragement alone, can affect the outome. An instrument is weak if it has only a slight impact on acceptance of the treatment, that is, if most people disregard encouragement to accept the treatment. Typical applications of instrumental variables are not ideal; encouragement is not randomized, although it may be assigned in a far less biased manner than the treatment itself. Using the concept of design sensitivity, we s tudy the sensitivity of instrumental variable analyses to departures from the ideal of random assignment of encouragement, with particular reference to the strength of the instrument. With these issues in mind, we reanalyze a clever study by Angrist and Krueger concerning the effects of military service during World War II on subsequent earnings, in which cohorts of very similar but not identical age were differently ""encouraged"" to serve in the war. A striking feature of this example is that those who served earned more, but the effect of service on earnings appears to be negative: that is the instrumental variables analysis reverses the sign of the naive comparison. For expository purposes, this example has the convenient feature of enabling, by selecting different birth cohorts, the creation of instruments of varied strength, from extremely weak to fairly strong, although separated by the same time interval and thus perhaps similarly biased. No matter how large the sample size becomes, even if the effect under study is quite large, studies with weak instruments are extremely sensitive to tiny biases, whereas studies with stronger instruments can be insensitive to moderate biases."
"10.1198/016214507000001003","2008","Nonparametric risk management with generalized hyperbolic distributions","0","In this article we propose the generalized hyperbolic adaptive volatility (GHADA) risk management model based on the generalized hyperbolic (GH) distribution and on a nonparametric adaptive methodology. Compared with the normal distribution, the GH distribution has semiheavy tails and represents the financial risk factors more apporpriately. Nonparametric adaptive methodology has the desirable property of being able to estimate homogeneous volatility over a short time interval and reflects a sudden change in the volatility process. For the German mark/U.S. dollar exchange rate and German bank portfolio data, the proposed GHADA model provides more accurate Value at risk calculations than the models with assumptions of the normal and t distributions."
"10.1198/016214507000001012","2008","Multiple model evaluation absent the gold standard through model combination","0","We describe a method for evaluating an ensemble of predictive models given a sample of observations comprising the model predictions and the outcome event measured with error. Our formulation allows us to simultaneously estimate measurement error parameters, true outcome-the ""gold standard""-and a relative weighting of the predictive scores. We describe conditions necessary to estimate the gold standard and to calibrate these estimates and detail how our approach is related to, but distinct from, standard model combination techniques. We apply our approach to data from a study to evaluate a collection of BRCA1/BRCA2 gene mutation prediction scores. In this example, genotype is measured with error by one or more genetic assays. We estimate true genotype for each individual in the data set, operating characteristics of the commonly used genotyping procedures, and a relative weighting of the scores. Finally, we compare the scores against the gold standard genotype and find that Mendelian scores, on average, the more refined and better calibrated of those considered and that the comaprison is sensitive to measurement error in the gold standard."
"10.1198/016214508000000229","2008","Fiducial intervals for variance components in an unbalanced two-component normal mixed linear model","0","In this article we propose a new method for constructing confidence intervals for sigma(2)(alpha), sigma(2)(epsilon), and the interclass correlation rho = sigma(2)(alpha)/(sigma(2)(alpha)+sigma(2)(epsilon)) in a two-component mixed-effects linear model. This method is based on an extension of R. A. Fisher's fiducial argument. We conducted a simulation study to compare the resulting interval estimates with other competing confidence interval procedures from the literature. Our results demonstrate that the proposed fiducial intervals have satisfactory performance in terms of coverage probability, as well as shorter average confidence interval lengths overall. We also prove that these fiducial intervals have asymptotically exact frequentist coverage probability. The computations for the proposed procedures are illustrated using real data examples."
"10.1198/016214508000000166","2008","Surveillance strategies for detecting changepoint in incidence rate based on exponentially weighted moving average methods","0","Surveillance is a major issue in the today's world. The need to develop adequate surveillance strategies is genuine in many spheres of human activity. In this article we focus on a specific problem of surveillance and discuss some related statistical issues. This specific problem deals with a possible change in the incidence rate of an event (to a higher value) when a system is studied at discrete time points. We apply the exponentially weighted moving average methods for detecting an increased incidence rate per exposure unit of an event. Different measures of evaluation, suitable in different types of applications, such as the expected delay, out-of-control average run length, and probability of successful detection, are studied. Analytical bounds are provided for those measures of evaluation. Results from intensive simulations indicate that the analytical bounds perform fairly well when the weight parameter is large."
"10.1198/016214508000000283","2008","Sufficient dimension reduction with missing predictors","1","In high-dimensional data analysis, sufficient dimension reduction (SDR) methods are effective in reducing the predictor dimension, while retaining full regression information and imposing no parametric models. However, it is common in high-dimensional data that a subset of predictors may have missing observations. Existing SDR methods resort to the complete-case analysis by removing all the subjects with missingness in any of the predictors under inquiry. Such an approach does not make effective use of the data and is valid only when missingness is independent of both observed and unobserved quantities. In this article, we propose a new class of SDR estimators under a more general missingness mechanism that allows missingness to depend on the observed data. We focus on a widely used SDR method, sliced inverse regression, and propose an augmented inverse probability weighted sliced inverse regression estimator (AIPW-SIR). We show that AIPW-SIR is doubly robust and asymptotically consistent and demonstrate that AIPW-SIR is more effective than the complete-case analysis through both simulations and real data analysis. We also outline the extension of the AIPW strategy to other SDR methods, including sliced average variance estimation and principal Hessian directions."
"10.1198/016214508000000418","2008","Sliced regression for dimension reduction","6","A new dimension-reduction method involving slicing the region of the response and applying local kernel regression to each slice is proposed. Compared with the traditional inverse regression methods [e.g., sliced inverse regression (SIR)], the new method is free of the linearity condition and has much better estimation accuracy. Compared with the direct estimation methods (e.g., MAVE), the new method is much more robust against extreme values and can capture the entire central subspace (CS) exhaustively. To determine the CS dimension, a consistent cross-validation criterion is developed. Extensive numerical studies, including a real example, confirm our theoretical findings."
"10.1198/016214508000000238","2008","Efficient and doubly robust imputation for covariate-dependent missing responses","0","In this article we study a well-known response missing-data problem. Missing data is an ubiquitous problem in medical and social science studies. Imputation is one of the most popular methods for dealing with missing data. The most commonly used imputation that makes use of covariates is regression imputation, in which the regression model can be parametric, semiparametric, or nonparametric. Parametric regression imputation is efficient but is not robust against misspecification of the regression model. Although nonparametric regression imputation (such as nearest-neighbor imputation and kernel regression imputation) is model-free, it is not efficient, especially if the dimension of covariate vector is high (the well-known problem of curse of dimensionality). Semiparametric regression imputation (such as partially linear regression imputation) can reduce the dimension of the covariate in nonparametric regression fitting but is not robust against misspecification of the linear component in the regression. Assuming that the missing mechanism is covariate-dependent and that the propensity function can be specified correctly, we propose a regression imputation method that has good efficiency and is robust against regression model misspecification. Furthermore, our method is valid as long as either the regression model or the propensity model is correct, a property known as the double-robustness property. We show that asymptotically the sample mean based on our imputation achieves the semiparametric efficient lower bound if both regression and propensity models are specified correctly. Our simulation results demonstrate that the proposed method outperforms many existing methods for handling missing data, especially when the regression model is misspecified. As an illustration, an economic observational data set is analyzed."
"10.1198/016214508000000409","2008","Partially collapsed {G}ibbs samplers: theory and methods","0","Ever-increasing computational power, along with ever-more sophisticated statistical computing techniques, is making it possible to fit ever-more complex statistical models. Among the more computationally intensive methods, the Gibbs sampler is popular because of its simplicity and power to effectively generate samples from a high-dimensional probability distribution. Despite its simple implementation and description, however, the Gibbs sampler is criticized for its sometimes slow convergence, especially when it is used to fit highly structured complex models. Here we present partially collapsed Gibbs sampling strategies that improve the convergence by capitalizing on a set of functionally incompatible conditional distributions. Such incompatibility generally is avoided in the construction of a Gibbs sampler, because the resulting convergence properties are not well understood. We introduce three basic tools (marginalization, permutation, and trimming) that allow us to transform a Gibbs sampler into a partially collapsed Gibbs sampler with known stationary distribution and faster convergence."
"10.1198/016214508000000193","2008","Learning causal {B}ayesian network structures from experimental data","1","We propose a method for the computational inference of directed acyclic graphical structures given data from experimental interventions. Order-space Markov chain Monte Carlo, equi-energy sampling, importance weighting, and stream-based computation are combined to create a fast algorithm for learning causal Bayesian network structures."
"10.1198/016214508000000391","2008","Second-order analysis of inhomogeneous spatial point processes with proportional intensity functions","0","We consider pairs of spatial point processes with intensity functions sharing a common multiplicative term. We introduce two novel approaches to estimating the pair correlation function for one of the point processes by treating the other as a baseline so as to account for the unspecified part of the intensity functions. The first approach is based on nonparametric kernel-smoothing, whereas the second approach uses a conditional likelihood estimation approach to fit a parametric model for the pair correlation function. A great advantage of the proposed methods is that they do not require the often difficult estimation of the unspecified part of the intensity functions. We establish the consistency of the resulting estimators and discuss how the parametric estimator can be applied in model diagnostics and inference on regression parameters for the intensity functions. We apply the proposed procedures to two spatial point patterns regarding the spatial distributions of birds in the U.K.'s Peak District in 1990 and 2004."
"10.1198/016214508000000319","2008","Cure rate model with mismeasured covariates under transformation","0","Cure rate models explicitly account for the survival fraction in failure time data. When the covariates are measured with errors, naively treating mismeasured covariates as error-free would cause estimation bias and thus lead to incorrect inference. Under the proportional hazards cure model, we propose a corrected score approach as well as its generalization, and implement a transformation on the mismeasured covariates toward error additivity and/or normality. The corrected score equations can be easily solved through the backfitting procedure, and the biases in the parameter estimates are successfully eliminated. We show that the proposed estimators for the regression coefficients are consistent and asymptotically normal. We conduct simulation studies to examine the finite-sample properties of the new method and apply it to a real data set for illustration."
"10.1198/016214508000000382","2008","Covariate bias induced by length-biased sampling of failure times","2","Although many authors have proposed different approaches to the analysis of length-biased survival data, a number of issues have not been fully addressed. The most important among these issues is perhaps that regarding inclusion of covariates into the analysis of length-biased lifetime data collected through cross-sectional sampling of a population. One aspect of this problem, which appears to have been neglected in the literature, concerns the effect of length bias on the sampling distribution of the covariates. In most regression analyses, it is conventional to condition on the observed covariate values; however, certain covariate values could be preferentially selected into the sample, being linked to the long-term survivors, who themselves are favored by the sampling mechanism. This observation raises two questions: (1) Does the conditional analysis of covariates lead to biased estimators of regression coefficients?; and (2) does inference through the joint I likelihood of covariates and failure times yield more efficient estimators of the regression parameters? We present a joint likelihood approach and study the large-sample behavior of the resulting maximum likelihood estimators (MLEs). We find that these MLEs are more efficient than their conditional counterparts even though the two MLEs are asymptotically equal. Our results are illustrated using data on survival with dementia, collected as part of the Canadian Study of Health and Aging."
"10.1198/016214508000000328","2008","Density estimation in the presence of heteroscedastic measurement error","3","We consider density estimation when the variable of interest is subject to heteroscedastic measurement error. The density is assumed to have a smooth but unknown functional form that we model with a penalized mixture of B-splines. We treat the situation in which multiple mismeasured observations of each variable of interest are observed for at least some of the subjects, and the measurement error is assumed to be additive and normal. The measurement error variance function is modeled with a second penalized mixture of B-splines. The article's main contributions are to address the effects of heteroscedastic measurement error effectively, explain the biases caused by ignoring heteroscedasticity, and present an equivalent kernel for a spline-based density estimator. Derivation of the equivalent kernel may be of independent interest. We use small-sigma asymptotics to approximate the biases incurred by assuming that the measurement error is homoscedastic when it actually is heteroscedastic. The biases incurred by misspecifying heteroscedastic measurement error as homoscedastic can be substantial. We fit the model using Bayesian methods and apply it to an example from nutritional epidemiology and a simulation experiment."
"10.1198/016214508000000373","2008","Wavelet-based nonparametric functional mapping of longitudinal curves","0","Functional mapping based on parametric and nonparametric modeling of functional data can estimate the developmental pattern of genetic effects on a complex dynamic or longitudinal process triggered by quantitative trait loci (QTLs). But existing functional mapping models have a limitation for mapping dynamic QTLs with irregular functional data characterized by many local features, such as peaks. We derive a statistical model for QTL mapping of longitudinal curves of any form based on wavelet shrinkage techniques. The fundamental idea of this model is a repeated splitting of an initial sequence into detail coefficients that quantify local fluctuations at a particular scale and smooth coefficients that quantify remaining low-frequency variation in the signal after the high-frequency detail is removed and, subsequently, QTL mapping with the smooth coefficients extracted from noisy longitudinal data. Compared with conventional full-dimensional functional mapping, wavelet-based nonparametric functional mapping provides consistent results, and better results in some circumstances, and is much more computationally efficient. This wavelet-based model is validated by the analysis of a real example for stem diameter growth trajectories in a forest tree, and its statistical properties are examined through extensive simulation studies. Wavelet-based functional mapping broadens the use of functional mapping to studying an arbitrary form of longitudinal curves and will have many implications for investigating the interplay between gene actions/interactions and developmental pathways in various complex biological processes and networks."
"10.1198/016214508000000364","2008","A unified approach to nonparametric comparison of receiver operating characteristic curves for longitudinal and clustered data","0","We present a unified approach to nonparametric comparisons of receiver operating characteristic (ROC) curves for a paired design with clustered data. Treating empirical ROC curves as stochastic processes, their asymptotic joint distribution is derived in the presence of both between-marker and within-subject correlations. A Monte Carlo method is developed to approximate their joint distribution without involving nonparametric density estimation. The developed theory is applied to derive new inferential procedures for comparing weighted areas under the ROC curves, confidence bands for the difference function of ROC curves, confidence intervals for the set of specificities at which one diagnostic test is more sensitive than the other, and multiple comparison procedures for comparing more than two diagnostic markers. Our methods demonstrate satisfactory small-sample performance in simulations. We illustrate our methods using clustered data from a glaucoma study and repeated-measurement data from a startle response study."
"10.1198/016214508000000175","2008","Robust and efficient adaptive estimation of binary-choice regression models","1","The binary-choice regression models, such as probit and logit, are used to describe the effect of explanatory variables on a binary response variable. Typically estimated by the maximum likelihood method, estimates are very sensitive to deviations from a model, such as heteroscedasticity and data contamination. At the same time, the traditional robust (high-breakdown point) methods, such as the maximum trimmed likelihood, are not applicable because, by trimming observations, they induce nonidentification of parameter estimates. To provide a robust estimation method for binary-choice regression, we consider a maximum symmetrically trimmed likelihood estimator (MSTLE) and design a parameter-free adaptive procedure for choosing the amount of trimming. The proposed adaptive MSTLE preserves the robust properties of the original MSTLE, significantly improves the finite-sample behavior of MSTLE, and also ensures the asymptotic equivalence of the MSTLE and maximum likelihood estimator under no contamination. The results concerning the trimming identification, robust properties, and asymptotic distribution of the proposed method are accompanied by simulation experiments and an application documenting the finite-sample behavior of some existing and the proposed methods."
"10.1198/016214508000000337","2008","The {B}ayesian lasso","10","The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant."
"10.1198/016214508000000346","2008","Variable selection and model averaging in semiparametric overdispersed generalized linear models","0","We express the mean and variance terms in a double-exponential regression model as additive functions of the predictors and use Bayesian variable selection to determine which predictors enter the model and whether they enter linearly or flexibly. When the variance term is null, we obtain a generalized additive model, which becomes a generalized linear model if the predictors enter the mean linearly. The model is estimated using Markov chain Monte Carlo simulation, and the methodology is illustrated using real and simulated data sets."
"10.1198/016214508000000210","2008","Semiparametric analysis of heterogeneous data using varying-scale generalized linear models","0","This article describes a class of heteroscedastic generalized linear regression models in which a subset of the regression parameters are rescaled nonparametrically, and develops efficient semiparametric inferences for the parametric components of the models. Such models provide a means to adapt for heterogeneity in the data due to varying exposures, varying levels of aggregation, and so on. The class of models considered includes generalized partially linear models and nonparametrically scaled link function models as special cases. We present an algorithm to estimate the scale function nonparametrically, and obtain asymptotic distribution theory for regression parameter estimates. In particular, we establish that the asymptotic covariance of the semiparametric estimator for the parametric part of the model achieves the semiparametric lower bound. We also describe bootstrap-based goodness-of-scale test. We illustrate the methodology with simulations, published data, and data from collaborative research on ultrasound safety."
"10.1198/016214508000000148","2008","Estimation of space-time branching process models in seismology using an {EM}-type algorithm","0","Maximum likelihood estimation of branching point process models via numerical optimization procedures can be unstable and computationally intensive. We explore an alternative estimation method based on the expectation-maximization algorithm. The method involves viewing the estimation of such branching processes as analogous to incomplete data problems. Using an application from seismology, we show how the epidemic-type aftershock sequence (ETAS) model can, in fact, be estimated this way, and we propose a computationally efficient procedure to maximize the expected complete data log-likelihood function. Using a space-time ETAS model, we demonstrate that this method is extremely robust and accurate and use it to estimate declustered background seismicity rates of geologically distinct regions in Southern California. All regions show similar declustered background intensity estimates except for the one covering the southern section of the San Andreas fault system to the east of San Diego in which a substantially higher intensity is observed."
"10.1198/016214507000000833","2008","Inference for a proton accelerator using convolution models","0","Proton beams present difficulties in analysis because of the limited data that can be collected. The study of such beams must depend on complex computer simulators that incorporate detailed physical equations. The statistical problem of interest is to infer the initial state of the beam from the limited data collected as the beam passes through a series of focusing magnets. We are thus faced with a classic inverse problem where the computer simulator links the initial state to the observables. We propose a new model for the initial distribution that is derived from the discretized process convolution approach. This model provides a computationally tractable method for this highly challenging problem. Taking a Bayesian perspective allows better estimation of the uncertainty and propagation of this uncertainty."
"10.1198/016214507000000905","2008","Statistical modeling and analysis for robust synthesis of nanostructures","0","We systematically investigate the best process conditions that ensure synthesis of different types of one-dimensional cadmium selenide nanostructures with high yield and reproducibility. Through a designed experiment and rigorous statistical analysis of experimental data, models linking the probabilities of obtaining specific morphologies to the process variables are developed. A new iterative algorithm for fitting a multinomial generalized linear model is proposed and used. The optimum process conditions, which maximize the preceding probabilities and make the synthesis process robust (i.e., less sensitive) to variations in process variables around set values, are derived from the fitted models using Monte Carlo simulations.Cadmium selenide has been found to exhibit one-dimensional morphologies of nanowires, nanobelts, and nanosaws, often with the three morphologies being intimately intermingled within the as-deposited material. A slight change in growth condition can result in a totally different morphology. To identify the optimal process conditions that maximize the yield of each type of nanostructure and, at the same time, make the synthesis process robust (i.e., less sensitive) to variations of process variables around set values, a large number of trials were conducted with varying process conditions. Here, the response is a vector whose elements correspond to the number of appearances of different types of nanostructures. The fitted statistical models would enable nanomanufacturers to identify the probability of transition from one nanostructure to another when changes, even tiny ones, are made in one or more process variables. Inferential methods associated with the modeling procedure help in judging the relative impact of the process variables and their interactions on the growth of different nanostructures. Owing to the presence of internal noise, that is, variation around the set value, each predictor variable is a random variable. Using Monte Carlo simulations, the mean and variance of transformed probabilities are expressed as functions of the set points of the predictor variables. The mean is then maximized to find the optimum nominal values of the process variables, with the constraint that the variance is under control."
"10.1198/016214507000001283","2008","Daytime {A}rctic cloud detection based on multi-angle satellite data with case studies","0","Global climate models predict that the strongest dependences of surface air temperatures on increasing atmospheric carbon dioxide levels will occur in the Arctic. A systematic study of these dependences requires accurate Arctic-wide measurements, especially of cloud coverage. Thus cloud detection in the Arctic is extremely important, but it is also challenging because of the similar remote sensing characteristics of clouds and ice- and snow-covered surfaces. This article proposes two new operational Arctic cloud detection algorithms using Multiangle Imaging SpectroRadiometer (MISR) imagery. The key idea is to identify cloud-free surface pixels in the imagery instead of cloudy pixels as in the existing MISR operational algorithms. Through extensive exploratory data analysis and using domain knowledge, three physically useful features to differentiate surface pixels from cloudy pixels have been identified. The first algorithm, enhanced linear correlation matching (ELCM), thresholds the features with either fixed or data-adaptive cutoff values. Probability labels are obtained by using ELCM labels as training data for Fisher's quadratic discriminant analysis (QDA), leading to the second (ELCM-QDA) algorithm. Both algorithms are automated and computationally efficient for operational processing of the massive MISR data set. Based on 5 million expert-labeled pixels, ELCM results are significantly in terms of both accuracy (92%) and coverage (100%) compared with two MISR operational algorithms, one with an accuracy of 80% and coverage of 27% and the other with an accuracy of 83% and a coverage of 70%. The ELCM-QDA probability prediction is also consistent with the expert labels and is more informative. In conclusion, ELCM and ELCM-QDA provide the best performance to date among all available operational algorithms using MISR data."
"10.1198/016214507000000888","2008","Computer model calibration using high-dimensional output","3","This work focuses on combining observations from field experiments with detailed computer simulations of a physical process to carry out statistical inference. Of particular interest here is determining uncertainty in resulting predictions. This typically involves calibration of parameters in the computer simulator as well as accounting for inadequate physics in the simulator. The problem is complicated by the fact that simulation code is sufficiently demanding that only a limited number of simulations can be carried out. We consider applications in characterizing material properties for which the field data and the simulator output are highly multivariate. For example, the experimental data and simulation output may be an image or may describe the shape of a physical object. We make use of the basic framework of Kennedy and O'Hagan. However, the size and multivariate nature of the data lead to computational challenges in implementing the framework. To overcome these challenges, we make use of basis representations (e.g., principal components) to reduce the dimensionality of the problem and speed up the computations required for exploring the posterior distribution. This methodology is applied to applications, both ongoing and historical, at Los Alamos National Laboratory."
"10.1198/016214507000000996","2008","Two-sided estimation of mate preferences for similarities in age, education, and religion","0","We propose a two-sided method to simultaneously estimate men's and women's preferences for relative age, education, and religious characteristics of potential mates using cross-sectional data on married couples and single individuals, in conjunction with a behavioral model developed in game theory and discrete choice estimation methods developed for simpler, one-sided choice situations. We use fixed effects to control for characteristics that are observed by the opposite sex but are missing from our data. Estimated mean preference coefficients determine the average degree to which measured characteristics of individuals affect others' evaluations of them as marital partners, whereas the model also accounts for variation of preferences around the means and for limitations in men's and women's information about members of the opposite sex. By assuming that each individual chooses freely from the set of potential partners that he or she finds available, we estimate preferences without having to observe these sets or specify any details of the matching process. This makes our method robust to unknown features of the process. Application of the method to data from the first wave of the National Survey of Families and Households indicates roughly symmetric or complementary preferences of men and women for age, education, and religious affiliation characteristics of potential mates and a much stronger preference for religious homogamy among conservative Protestants relative to mainline Protestants than was suggested by an earlier, retrospective study of religious differences in the temporal stability of marriages. Our method should be useful in many situations in which voluntary pairings have arisen through some complex process, the details of which have not been recorded. Besides marriage and cohabitation, data on employment, college attendance, and the coresidence of elderly parents with adult children often have this character, as do some biological data on nonhuman mating."
"10.1198/016214507000000572","2008","Statistical treatment choice: an application to active labor market programs","0","Choosing among a number of treatments the most suitable for a particular client is an issue of everyday concern. In this article two methodological advances for statistically assisted treatment choice are developed: First, it permits one to combine a dataset on previously treated clients with a dataset on new clients when the regressors available in these two datasets do not coincide. It thereby incorporates additional regressors on previously treated clients that are not available for the current clients. Such a situation often arises because of cost considerations, data confidentiality reasons, or time delays in data availability. Second, statistical inference on the recommended treatment choice is analyzed and conveyed to the agent or caseworker in a comprehensible and transparent way. The implementation of this methodology in a pilot study in Switzerland for choosing among active labor market programs for unemployed job seekers is described."
"10.1198/016214507000000554","2008","Bayesian selection and clustering of polymorphisms in functionally related genes","0","In epidemiologic studies, there is often interest in assessing the relationship between polymorphisms in functionally related genes and a health outcome. For each candidate gene, single nucleotide polymorphism (SNP) data are collected at a number of locations, resulting in a large number of possible genotypes. Because instabilities can result in analyses that include all the SNPs, dimensionality is typically reduced by conducting single SNP analyses or attempting to identify haplotypes. This article proposes an alternative Bayesian approach for reducing dimensionality. A multilevel Dirichlet process prior is used for the distribution of the SNP-specific regression coefficients within genes, incorporating a variable selection-type mixture structure to allow SNPs with no effect. This structure allows simultaneous selection of important SNPs and soft clustering of SNPs having similar impact on the health outcome. The methods are illustrated using data from a study of pro- and anti-inflammatory cytokine polymorphisms and spontaneous preterm birth."
"10.1198/016214507000000563","2008","Bayesian accelerated failure time model with multivariate doubly interval-censored data and flexible distributional assumptions","0","In this article we consider the relationship of covariates to the time to caries of permanent first molars. This involves an analysis of multivariate doubly interval-censored data. To describe this relationship, we suggest an accelerated failure time model with random effects, taking into account that the observations are clustered. Indeed, up to four permanent molars per child enter into the analysis, implying up to four caries times for each child. Each distributional part of the model is specified in a flexible way as a penalized Gaussian mixture with an overspecified number of mixture components. A Bayesian approach with the Markov chain Monte Carlo methodology is used to estimate the model parameters, and a software package in the R language has been written that implements it."
"10.1198/016214507000000473","2008","How useful is bagging in forecasting economic time series? {A} case study of {U}.{S}. consumer price inflation","0","This article focuses on the widely studied question of whether the inclusion of indicators of real economic activity lowers the prediction mean squared error of forecasting models of U.S. consumer price inflation. We propose three variants of the bagging algorithm specifically designed for this type of forecasting problem and evaluate their empirical performance. Although bagging predictors in our application are clearly more accurate than equally weighted forecasts, median forecasts, ARM forecasts, AFTER forecasts, or Bayesian forecast averages based on one extra predictor at a time, they are generally about as accurate as the Bayesian shrinkage predictor, the ridge regression predictor, the iterated LASSO predictor, or the Bayesian model average predictor based on random subsets of extra predictors. Our results show that bagging can achieve large reductions in prediction mean-squared errors even in such challenging applications as inflation forecasting; however, bagging is not the only method capable of achieving such gains."
"10.1198/016214507000000482","2008","Analysis of episodic data with application to recurrent pulmonary exacerbations in cystic fibrosis patients","0","We consider a special type of recurrent event data, termed ""recurrent episode"" data, arising in episodic illness studies. When an event occurs, it lasts for a random length of time. A naive recurrent-event analysis disregards the length of the episodes, which may contain important information about the severity of the disease, the associated medical costs, and quality of life. Bivariate gap time models have been suggested in which length of episodes and time between episodes are modeled jointly. These models are useful but may obscure the overall effects of treatment and other prognostic factors. The analysis can be further complicated if covariate effects change over time, as may occur when the effects vary across episodes. This article reviews the existing methods applied to recurrent episode data and approaches the problem using the recently developed temporal process regression. Novel endpoints are constructed that summarize both episode frequency and the length of episodes and time between episodes. Time-varying coefficient models, with inferences based on functional estimating equations, are proposed. Both existing and new methods are applied to a clinical trial to assess the efficacy of a treatment for patients with cystic fibrosis, many of whom experienced multiple episodes of pulmonary exacerbations."
"10.1198/016214507000000923","2008","Bayesian hidden {M}arkov modeling of array {CGH} data","3","Genomic alterations have been linked to the development and progression of cancer. The technique of comparative genomic hybridization (CGH) yields data consisting of fluorescence intensity ratios of test and reference DNA samples. The intensity ratios provide information about the number of copies in DNA. Practical issues such as the contamination of tumor cells in tissue specimens and normalization errors necessitate the use of statistics for learning about the genomic alterations from array CGH data. As increasing amounts of array CGH data become available, there is a growing need for automated algorithms for characterizing genomic profiles. Specifically, there is a need for algorithms that can identify gains and losses in the number of copies based on statistical considerations, rather than merely detect trends in the data.We adopt a Bayesian approach, relying on the hidden Markov model to account for the inherent dependence in the intensity ratios. Posterior inferences are made about gains and losses in copy number. Localized amplifications (associated with oncogene mutations) and deletions (associated with mutations of tumor suppressors) are identified using posterior probabilities. Global trends such as extended regions of altered copy number are detected. Because the posterior distribution is analytically intractable, we implement a Metropolis-within-Gibbs algorithm for efficient simulation-based inference. Publicly available data on pancreatic adenocarcinoma, glioblastoma multiforme, and breast cancer are analyzed, and comparisons are made with some widely used algorithms to illustrate the reliability and success of the technique."
"10.1198/016214507000000662","2008","Causal inference in hybrid intervention trials involving treatment choice","1","Randomized allocation of treatments is a cornerstone of experimental design but has drawbacks when a limited set of individuals are willing to be randomized, or the act of randomization undermines the success of the treatment. Choice-based experimental designs allow a subset of the participants to choose their treatments. We discuss here causal inferences for hybrid experimental designs in which some participants are randomly allocated to treatments and others receive their treatment preference. This work was motivated by the Women Take Pride (WTP) study, a doubly randomized preference trial (DRPT) that assessed behavioral interventions for women with heart disease. We propose a model for estimating the causal effects in the subpopulations defined by treatment preferences, and thus preference effects. We describe an EM algorithm for computing maximum likelihood estimates of the model parameters. We illustrate the method by analyzing sickness impact profile (SIP) scores and treatment adherence in the WTP data. Our results show some evidence that SIP scores were improved when women received their preferred treatment and strong preference effects on program adherence; that is, women assigned to their preferred treatment were more likely to adhere to the program. We also provide a framework for assessing the DRPT and other hybrid trial designs and discuss some alternative designs from the perspective of the strength of assumptions required to make causal inferences."
"10.1198/016214507000000374","2008","Longitudinal nested compliance class model in the presence of time-varying noncompliance","1","This article discusses a nested latent class model for analyzing longitudinal randomized trials when subjects do not always adhere to the treatment to which they are randomized. In the Prevention of Suicide in Primary Care Elderly: Collaborative Trial, subjects were randomized to either the control treatment, where they received standard care, or to the intervention, where they received standard care in addition to meeting with depression health specialists. The health specialists educate patients, their families, and physicians about depression and monitor their treatment. Those randomized to the control treatment have no access to the health specialists; however, those randomized to the intervention could choose not to meet with the health specialists, hence receiving only the standard care. Subjects participated in the study for two years where depression severity and adherence to meeting with health specialists were measured at each follow-up. The outcome of interest is the effect of meeting with the health specialists on depression severity. Traditional intention-to-treat and as-treated analyses may produce biased causal effect estimates in the presence of subject noncompliance. Utilizing a nested latent class model that uses subject-specific and time-invariant ""superclasses"" allows us to summarize longitudinal trends of compliance patterns and to estimate the effect of the intervention using intention-to-treat contrasts within principal strata that correspond to longitudinal compliance behavior patterns. Analyses show that subjects with more severe depression are more likely to adhere to treatment randomization, and those that are compliant and meet with health specialists benefit from the meetings and show improvement in depression. Simulation results show that our estimation procedure produces reasonable parameter estimates under correct model assumptions."
"10.1198/016214507000000437","2008","Eye-movement analysis of search effectiveness","0","Advances in eye-tracking technology have promoted its widespread use to understand and improve target searches in psychology, industrial engineering, human factors, medical diagnostics, and marketing. Eye movements are the realization of a complex, unobserved spatiotemporal attention process with many sources of variation. Eye-tracking data often have been aggregated and/or summarized descriptively, because few adequate statistical models are available for their analysis. This article proposes a model that may serve to uncover the latent attention processes of people searching for targets in complex scenes. It recognizes the spatial nature of eye movements and represents two latent attention states, a localization state and an identification state, between which people may switch over time according to a Markov process. A saliency map, based on low-level perceptual features and the scene's organization, guide target searches in the localization state. In the identification state, people verify whether a selected candidate object is the target. The model is applied to analyze commercial eye-tracking data from more than 100 people engaged in a target search task on a computer-simulated retail shelf display. Rapid switching between attention states over time is revealed. Estimates of the feature and saliency maps are provided and found to be related to search performance. The results facilitate the evaluation of the effectiveness of alternative visual search strategies."
"10.1198/016214507000000653","2008","Rejoinder [MR2523980]","0",""
"10.1198/016214507000000626","2008","Estimating incumbency advantage and its variation, as an example of a before-after study","0","Incumbency advantage is one of the most widely studied features in American legislative elections. In this article we construct and implement an estimate that allows incumbency advantage to vary between individual incumbents. This model predicts that open-seat elections will be less variable than those with incumbents running, an observed empirical pattern that is not explained by previous models. We apply our method to the U.S. House of Representatives in the twentieth century. Our estimate of the overall pattern of incumbency advantage over time is similar to previous estimates (although slightly lower), and we also find a pattern of increasing variation. More generally, our multilevel model represents a new method for estimating effects in before-after studies."
"10.1198/016214507000000347","2008","Principal stratification for causal inference with extended partial compliance","4","Many double-blind placebo-controlled randomized experiments with active drugs suffer from complications beyond simple noncompliance. First, the compliance with assigned dose is often partial, with patients taking only part of the assigned dose, whether active or placebo. Second, the blinding may be imperfect in the sense that there may be detectable positive or negative side effects of the active drug, and consequently, simple compliance has to be extended to allow different compliances to active drug and placebo. Efron and Feldman presented an analysis of such a situation and discussed inference for dose-response from the nonrandomized data in the active treatment arm, which stimulated active discussion, including on the role of the intention-to-treat principle in such studies. Here, we formulate the problem within the principal stratification framework of Frangakis and Rubin, which adheres to the intention-to-treat principle, and we present a new analysis of the Efron-Feldman data within this framework. Moreover, we describe precise assumptions under which dose-response can be inferred from such nonrandomized data, which seem debatable in the setting of this example. Although this article only deals in detail with the specific Efron-Feldman data, the same framework can be applied to various circumstances in both natural science and social science."
"10.1198/016214507000001337","2008","Mixtures of {$g$} priors for {B}ayesian variable selection","10","Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures."
"10.1198/016214507000001472","2008","An approach to multivariate covariate-dependent quantile contours with application to bivariate conditional growth charts","3","Multivariate quantile contours are useful in numerous applications and have been studied in different contexts. However, no easy solutions exist when dynamic and conditional quantile contours are needed without strong distributional assumptions. In this article we propose a new form of bivariate quantile contours and a two-stage estimation procedure to take time effect into account. The proposed procedure relies on quantile regression for longitudinal data and is flexible to include potentially important covariates as necessary. In addition, we propose a visual model assessment tool and discuss a practical guideline for model selection. The performance of the proposed methodology is demonstrated by a simulation study, as well as an application to joint height-weight screening of young children in the United States. We construct bivariate growth charts by a nested sequence of age-dependent and covariate-varying quantile contours of height and weight, and use it to locate an individual subject's percentile rank with respect to a reference population. Our work shows that the proposed method is valuable for pediatric growth monitoring and provides more informative readings than the conventional approach based on univariate growth charts."
"10.1198/016214507000001463","2008","Equivalence between conditional and random-effects likelihoods for pair-matched case-control studies","0","Two approaches dominate the analysis of highly stratified data sets: use of conditional likelihood and random-effects models. Conditioning is more traditional and has attractive robustness properties. Contemporary random-effects approaches rely more on accurate assumptions but can be applied to a larger class of problems. For pair-matched studies with arbitrary numbers of categorical covariates, we reconcile these two approaches, showing that the conditional approach has an exact interpretation as a specific type of random-effects model. The random-effects models that provide equivalence with conditioning naturally inherit all the attractive properties of that approach, and we argue they are a pragmatic model choice. We also discuss desirable characteristics of the random-effects model that are entirely novel and that have foundational implications. For example, although our model is specified entirely within a full-likelihood framework, its assumed random-effects distribution does not need a parametric form; we make explicit the robustness that this provides. In addition, our model forces a priori exchangeability of case and control status, an attractive property for objective analyses. The equivalence of the two approaches justifies extensions of conditional likelihood methods to new situations. We give a motivating example from a real problem concerning misclassification error in a genotyping process, and discuss refinements specific to genetic applications."
"10.1198/016214507000001382","2008","Efficient local estimation for time-varying coefficients in deterministic dynamic models with applications to {HIV}-1 dynamics","4","Recently deterministic dynamic models have become very popular in biomedical research and other scientific areas; examples include modeling human immunodeficiency virus (HIV) dynamics, pharmacokinetic/pharmacodynamic analysis, tumor cell kinetics, and genetic network modeling. In this article we propose estimation methods for the time-varying coefficients in deterministic dynamic systems that are usually described by a set of differential equations. Three two-stage local polynomial estimators are proposed, and their asymptotic normality is established. An alternative approach, a discretization method that is widely used in stochastic diffusion models, is also investigated. We show that the discretization method that uses the simple Enter discretization approach for the deterministic dynamic model does not achieve the optimal convergence rate compared with the proposed two-stage estimators. We use Monte Carlo simulations to study the finite-sample performance, and use a real data application to HIV dynamics to illustrate the proposed methods."
"10.1198/016214507000001481","2008","Time-dependent predictive values of prognostic biomarkers with failure time outcome","2","In a prospective cohort study, information on clinical parameters, tests, and molecular markers often is collected. Such information is useful to predict patient prognosis and to select patients for targeted therapy. We propose a new graphical approach, the positive predictive value (PPV) curve, to quantify the predictive accuracy of prognostic markers measured on a continuous scale with censored failure time outcome. The proposed method highlights the need to consider both predictive values and the marker distribution in the population when evaluating a marker, and it provides a common scale for comparing different markers. We consider both semiparametric- and nonparametric-based estimating procedures. In addition, we provide asymptotic distribution theory and resampling-based procedures for making statistical inference. We illustrate our approach with numerical studies and data sets from the Seattle Heart Failure Study."
"10.1198/016214507000001391","2008","Point and interval estimation of variogram models using spatial empirical likelihood","0","We present a spatial blockwise empirical likelihood method for estimating variogram model parameters in the analysis of spatial data on a grid. The method produces point estimators that require no spatial variance estimates to compute, unlike least squares methods for variogram fitting, but are as efficient as the best least squares estimator in large samples. Our approach also produces confidence regions for the variogram, without requiring knowledge of the full joint distribution of the spatial data. In addition, the empirical likelihood formulation extends to spatial regression problems and allows simultaneous inference on both spatial trend and variogram parameters. We examine the asymptotic behavior of the estimator analytically, and investigate its behavior in finite samples through simulation studies."
"10.1198/016214507000001139","2008","Bayesian hierarchical curve registration","1","Functional data often exhibit a common shape, but with variations in amplitude and phase across curves. The analysis often proceeds by synchronization of the data through curve registration. In this article we propose a Bayesian hierarchical model for curve registration. Our hierarchical model provides a formal account of amplitude and phase variability while borrowing strength from the data across curves in the estimation of the model parameters. We discuss extensions of the model by using penalized B-splines in the representation of the shape and time-transformation functions, and by allowing temporal misalignment of the curves. We discuss applications of our model to simulated data, as well as to two data sets. In particular, we use our model in a nonstandard analysis aimed at investigating regulatory network in time course microarray data."
"10.1198/016214507000001364","2008","The matrix stick-breaking process: flexible {B}ayes meta-analysis","2","In analyzing data from multiple related studies, it often is of interest to borrow information across studies and to cluster similar studies. Although parametric hierarchical models are commonly used, of concern is sensitivity to the form chosen for the random-effects distribution. A Dirichlet process (DP) prior can allow the distribution to be unknown, while clustering studies; however, the DP does not allow local clustering of studies with respect to a subset of the coefficients without making independence assumptions. Motivated by this problem, we propose a matrix stick-breaking process (MSBP) as a prior for a matrix of random probability measures. Properties of the MSBP are considered, and methods are developed for posterior computation using Markov chain Monte Carlo. Using the MSBP as a prior for a matrix of study-specific regression coefficients, we demonstrate advantages over parametric modeling in simulated examples. The methods are further illustrated using a multinational uterotrophic bioassay study."
"10.1198/016214507000001373","2008","Hierarchical false discovery rate-controlling methodology","1","We discuss methodology for controlling the false discovery rate (FDR) in complex large-scale studies that involve testing multiple families of hypotheses; the tested hypotheses are arranged in a tree of disjoint subfamilies, and the subfamilies of hypotheses are hierarchically tested by the Benjamini and Hochberg FDR-controlling (BH) procedure. We derive an approximation for the multiple family FDR for independently distributed test statistics: q, the level at which the BH procedure is applied, times the number of families tested plus the number of discoveries, divided by the number of discoveries plus 1. We provide a universal bound for the FDR of the discoveries in the new hierarchical testing approach, 2 x 1.44 x q, and demonstrate in simulations that when the data has an hierarchical structure the new testing approach can be considerably more powerful than the BH procedure."
"10.1198/016214508000000012","2008","A uniform improvement of {B}onferroni-type tests by sequential tests","0","We propose a new method to uniformly improve cutoff tests (as, e.g., the Bonferroni test) by replacing them with a sequential testing procedure. The proposed procedure provides a uniform improvement-that is, it rejects the null hypothesis at least in all cases where the original procedure does, has level a, and never requires a sample size larger than that of the original procedure. Information on the often-unknown joint distribution of the test statistics across the hypotheses is not required. We discuss the applications to one- and two-sided tests for the intersection of multiple hypotheses, as well as to tests of elementary hypotheses. For the wide class of asymptotically linear test statistics and under unknown nuisance parameters, we propose an asymptotic approximation to our improved procedure and show that it exhausts the nominal type I error rate asymptotically. We present an application in a clinical study with multiple endpoints where the null hypothesis of no effect is tested against the alternative that at least one endpoint shows an effect, and give a real data example."
"10.1198/016214507000001346","2008","Sequential experimental designs for generalized linear models","0","We consider the problem of experimental design when the response is modeled by a generalized linear model (GLM) and the experimental plan can be determined sequentially. Most previous research on this problem has been limited either to one-factor, binary response experiments or to augmenting the design when there are already sufficient data to compute parameter estimates. We suggest a new procedure for the sequential choice of observations that offers five important advantages: (1) It can be applied to multifactor experiments and is not limited to the one-factor setting; (2) it can be used with any GLM, not just binary responses; (3) both fully sequential and group sequential settings are treated; (4) it enables efficient design from the outset of the experiment; and (5) the experimenter is not constrained to specify a single model and can use the prior to reflect uncertainty as to the link function and the form of the linear predictor. Our procedure is based on a D-optimality criterion and on a Bayesian analysis that exploits a discretization of the parameter space to efficiently represent the posterior distribution. In the one-factor setting, a simulation study shows that our method is superior in efficiency to commonly used procedures, such as the ""Bruceton"" test, Neyer's procedure, or Wu's improved Robbins-Monro method. We also present a comparison of results obtained with the new algorithm versus the ""Bruceton"" method on an actual sensitivity test conducted recently at an industrial plant. Source code for the algorithms and examples throughout the article is available at http://www.math.tau.ac.il/similar to dms/GLM-Design."
"10.1198/016214507000000897","2008","Randomization inference in a group-randomized trial of treatments for depression: covariate adjustment, noncompliance, and quantile effects","3","In the Prospect Study, in 10 pairs of two primary-care practices, one practice was picked at random to receive a ""depression care manager"" to treat its depressed patients. Randomization inference, properly performed, reflects the assignment of practices, not patients, to treatment or control. Yet, pertinent data describe individual patients: depression outcomes, baseline covariates, compliance with treatment. The methods discussed use only (i) the random assignment of clusters to treatment or control and (ii) the hypothesis about effects being tested or inverted for confidence intervals, so they are randomization inferences in Fisher's strict sense. There is no assumption that the covariance model generated the data, that compliers resemble noncompliers, that dependence is from additive random cluster effects, that individuals in a same cluster do not interfere with one another, or that units are sampled from a population. We contrast methods of covariance adjustment, never assuming the models are ""true,"" obtaining exact randomization inferences. We consider exact inference about effects proportional to doses with noncompliance and effects whose magnitude varies with the degree of improvement that would occur without treatment. A simulation examines power."
"10.1198/016214507000000356","2008","Modeling disease progression with longitudinal markers","0","In this article we propose a Bayesian natural history model for disease progression based on the joint modeling of longitudinal biomarker levels, age at clinical detection of disease, and disease status at diagnosis. We establish a link between the longitudinal responses and the natural history of the disease by using an underlying latent disease process that describes the onset of the disease and models the transition to an advanced stage of the disease as dependent on the biomarker levels. We apply our model to data from the Baltimore Longitudinal Study of Aging on prostate-specific antigen to investigate the natural history of prostate cancer."
"10.1198/016214507000001274","2008","Aberrant effects of treatment","0","Often, the protocol for a randomized controlled trial states that if a patient's health deteriorates substantially in certain specific ways, then the study treatments will no longer be considered adequate medical care, and the patient will be taken off the study treatments and given the needed care. Aberrant responses of this sort present special problems of interpretation. The few such patients may have received only limited exposure to the study treatments, but also may have experienced some of the worst outcomes observed in the trial. The most basic question is whether one of the treatments tends to cause such aberrant responses, quite apart from any other beneficial or harmful effects that treatment may have. Common practice is to reduce aberrant responses to binary events and apply Fisher's exact test for a 2 x 2 table, but this approach discards the available information about the magnitude of the aberration. We propose a new randomization test for this problem that uses information about both the number of aberrations and their magnitude. In extreme situations, the new test reduces to either Fisher's exact test or Wilcoxon's rank sum test, but in common situations, the new test has features of both methods. Importantly, this exact test reacts only to aberrant effects; it is completely unaffected by treatment effects that do not produce aberrations. When aberrant responses have several aspects, we examine the consequences of selecting, after the fact, the one aspect exhibiting the greatest aberration, and applying the test to that aspect. For a single response variable, a confidence interval is proposed for an additive aberrant effect; the interval is constructed by inverting the test, and a multivariate extension is briefly mentioned. We illustrate the use of the test in the ACE-Inhibitor After Anthracycline (AAA) randomized trial, which was aimed at preserving cardiac function in children treated for cancer. Seven of 135 children were removed from the trial due to substantial cardiac declines; 6 of these were on placebo, I of whom subsequently died."
"10.1198/016214507000000194","2008","A mixture model with dependent observations for the analysis of {CSFE}-labeling experiments","0","Recent advances in flow cytometry have resulted in the development of powerful bioassays to analyze the proliferation of cell populations. The carboxy-fluorescein diacetate succinimidyl ester (CFSE)-labeling experiment is one such assay that is widely used to study cell kinetics and as a standard tool for investigating lymphocyte proliferation. Several mathematical models have been proposed to describe cell proliferation during CFSE experiments. The statistical analysis of CFSE-labeling data has received little attention, but it poses a number of methodological issues. In this article we approach their analysis using a mixture model. The mixing proportions are specified through an age-dependent branching process that models the temporal organization of the cell population. Because the CFSE molecules are partitioned between daughter cells when their mother divides, the observations generated by this assay are dependent. The data structure can be compared with that of a partially observed random-effects model where the clusters cannot be identified. In this context, we propose three estimators. We prove their consistency and asymptotic normality, investigate their finite-sample properties in simulation studies, and contrast their relative merits for the analysis of CFSE-labeling data. We use the proposed methods to analyze the proliferation in vitro of CD4+ and CD8+ T lymphocytes. In particular, we compare their activation and proliferation rates, and also investigate the effect of two stimuli that activate resting lymphocytes: the nonspecific mitogen phytohemaglutanin (PHA) and ligation of both the T-cell receptor CD3 and the costimulatory receptor CD28 cell surface proteins with monoclonal antibodies."
"10.1198/016214507000000383","2008","A capture-recapture approach for screening using two diagnostic tests with availability of disease status for the test positives only","1","The article considers screening human populations with two screening tests. If any of the two tests is positive, then full evaluation of the disease status is undertaken; however, if both diagnostic tests are negative, then disease status remains unknown. This procedure leads to a data constellation in which, for each disease status, the 2 x 2 table associated with the two diagnostic tests used in screening has exactly one empty, unknown cell. To estimate the unobserved cell counts, previous approaches assume independence of the two diagnostic tests and use specific models, including the special mixture model of Walter or unconstrained capture-recapture estimates. Often, as is also demonstrated in this article by means of a simple test, the independence of the two screening tests is not supported by the data. Two new estimators are suggested that allow associations of the screening test, although the form of association must be assumed to be homogeneous over disease status. These estimators are modifications of the simple capture-recapture estimator and easy to construct. The estimators are investigated for several screening studies with fully evaluated disease status in which the superior behavior of the new estimators compared to the previous conventional ones can be shown. Finally, the performance of the new estimators is compared with maximum likelihood estimators, which are more difficult to obtain in these models. The results indicate the loss of efficiency as minor."
"10.1198/016214507000000428","2008","Application of multidimensional selective item response regression model for studying multiple gene methylation in {SV}40 oncogenic pathways","0","Alteration of gene methylation patterns has been reported to be involved in the early onsets of many human malignancies. Many exogenous risk factors, such as cigarette smoke, dietary additives, chemical exposures, radiation, and biologic agents including viral infection, are involved in the methylation pathways of cancers. We propose a multidimensional selective item response regression model to describe and test how a risk factor may alter molecular pathways involving aberrant methylation of multiple genes in oncogenesis. Our modeling framework is built on an item response model for multivariate dichotomous responses of high dimension, such as aberrant methylation of multiple tumor-suppressor genes, but we allow risk factors such as SV40 viral infection to alter the distribution of the latent factors that subsequently affect the outcome of cancer. We postulate empirical identification conditions under our model formulation. Moreover, we do not prespecify the links between the multiple dichotomous methylation responses and the latent factors, but rather conduct specification searches with a genetic algorithm to discover the links. Parameter estimation through maximum likelihood and specification searches in models with multidimensional latent factors for multivariate binary responses have become practical only recently, due to modern statistical computing development. We illustrate our proposal with the biological finding that simultaneous methylation of multiple tumor-suppressor genes is associated with the presence of SV40 viral sequences and with the cancer status of lymphoma/leukemia. We are able to test whether the data are consistent with the causal hypothesis that SV40 induces aberrant methylation of multiple genes in its oncogenic pathways. At the same time, we are able to evaluate the role of SV40 in the methylation pathway and to determine whether the methylation pathway is responsible for the development of leukemia/lymphoma."
"10.1198/016214507000000365","2008","Bayesian clustering of transcription factor binding motifs","2","Genes are often regulated in living cells by proteins called transcription factors that bind directly to short segments of DNA in close proximity to specific genes. These binding sites have a conserved nucleotide appearance, which is called a motif. Several recent studies of transcriptional regulation require the reduction of a large collection of motifs into clusters based on the similarity of their nucleotide composition. We present a principled approach to this clustering problem based on a Bayesian hierarchical model that accounts for both within- and between-motif variability. We use a Dirichlet process prior distribution that allows the number of clusters to vary and we also present a novel generalization that allows the core width of each motif to vary. This clustering model is implemented, using a Gibbs sampling strategy, on several collections of transcription factor motif matrices. Our stochastic implementation allows us to examine the variability of our results in addition to focusing on a set of best clusters. Our clustering results identify several motif clusters that suggest that several transcription factor protein families are actually mixtures of several smaller groups of highly similar motifs, which provide substantially more refined information compared with the full set of motifs in the family. Our clusters provide a means by which to organize transcription factors based on binding motif similarities and can be used to reduce motif redundancy within large databases such as JASPAR and TRANSFAC, which aides the use of these databases for further motif discovery. Finally, our clustering procedure has been used in combination with discovery of evolutionarily conserved motifs to predict co-regulated genes. An alternative to our Dirichlet process prior distribution is presented that differs substantially in terms of a priori clustering characteristics, but shows no substantive difference in the clustering results for our dataset. Despite our specific application to transcription factor binding motifs, our Bayesian clustering model based on the Dirichlet process has several advantages over traditional clustering methods that could make our procedure appropriate and useful for many clustering applications."
"10.1198/016214507000000400","2008","Individual prediction in prostate cancer studies using a joint longitudinal survival-cure model","0","Patients treated for prostate cancer are monitored by periodically measuring prostate-specific antigen (PSA) after treatment. Increases in PSA are suggestive of cancer recurrence and are used in making decisions about possible new treatments. The data from studies of such patients typically consist of longitudinal PSA measurements, censored event times, and baseline covariates. Methods for the combined analysis of both longitudinal and survival data have been developed in recent years, with the main emphasis on modeling and estimation. We analyze data from a prostate cancer study in which the patients are treated with radiation therapy, using a joint model extended by adding a mixture structure to the model. Here we focus on using this model to make individualized predictions of disease progression for censored and alive patients. In this model, each patient is assumed to be either cured by the treatment or susceptible to clinical recurrence. The cured fraction is modeled as a logistic function of baseline covariates, measured before the end of the radiation therapy period. The longitudinal PSA data is modeled as a nonlinear hierarchical mixed model, with different models for the cured and susceptible groups. To accommodate the heavy tail manifested by the data and possible outliers, a t distribution is used for the measurement error. The clinical recurrences are modeled using a time-dependent proportional hazards model for those in the susceptible group, where the time-dependent covariates include both the current value and the slope the of posttreatment PSA profile. The baseline hazard is assumed to have a generalized Weibull form. Estimates of the parameters in the model are obtained using a Markov chain Monte Carlo method. The model is used to give individual predictions of both future PSA values and the predicted probability of recurrence up to four years in the future. These predictions are compared with observed data from a validation data set consisting of further follow-up of the subjects in the study. There is good correspondence between the predictions and the validation data."
"10.1198/016214507000000842","2008","Spatial-temporal modeling of forest gaps generated by colonization from below- and above-ground bark beetle species","0","Studies of forest declines are important, because they both reduce timber production and affect successional trajectories of landscapes and ecosystems. Of particular interest is the decline of red pines, which is characterized by expanding areas of dead and chlorotic trees in plantations throughout the Great Lakes region. Here we examine the impact of two bark beetle groups, red turpentine beetles and pine engraver bark beetles, on tree mortality and the subsequent gap formation over time in a plantation in Wisconsin. We construct spatial-temporal statistical models that quantify the relations among red turpentine beetle colonization, pine engraver bark beetle colonization, and mortality of red pine trees while accounting for correlation across space and over time. We extend traditional Markov random-field models to include temporal terms and multiple-response variables aimed at developing a suitable set of statistical models for addressing the scientific questions about the forest ecosystem under study. For statistical inference, we adopt a Bayesian hierarchical modeling approach and devise Markov chain Monte Carlo algorithms for obtaining the posterior distributions of model parameters as well as posterior predictive distributions. In particular, we implement path sampling combined with perfect simulation for autologistic models while formally addressing the posterior propriety under an improper uniform prior. Our data analysis results suggest that red turpentine beetle colonization is associated with a higher likelihood of pine engraver bark beetle colonization and that pine engraver bark beetle colonization is associated with higher likelihood of red pine tree mortality, whereas there is no direct association between red turpentine beetle colonization and red pine tree mortality. There is strong evidence that red turpentine beetle colonization does not kill a red pine tree directly, but rather predisposes the tree to subsequent colonization by pine engraver bark beetles. The evidence is also strong that pine engraver bark beetles are the ultimate mortality agents of red pine trees."
"10.1198/016214507000000419","2008","A flexible method to measure synchrony in neuronal firing","0","Neurons can transmit information about the characteristics of a stimulus through the spike rate of neurons and synchronization of the neurons. Various association measure can be used to describe how ""synchronous"" two spike trains are. We propose a new measure of synchrony, the conditional synchrony measure, which is the probability of firing together given that at least one of the two neurons is active. Focus is on the specification of a flexible marginal model for multivariate correlated binary data together with a pseudolikelihood estimation approach, to adequately and directly describe the measures of interest. A joint model must allow different time- and covariate-dependent firing rates for each neuron, and also must account for the association between them. The association between neurons might depend on covariates as well."
"10.1198/016214507000000392","2008","Relating ambient particulate matter concentration levels to mortality using an exposure simulator","0","Since the U.S. Environmental Protection Agency began widespread monitoring of PM2.5 (particulate matter <2.5 mu in diameter) concentration levels in the late 1990s, the epidemiological community has performed several observational studies directly relating PM2.5 concentration to various health endpoints including mortality and morbidity. However, recent research suggests that human exposure to the constituents of PM2.5 may differ significantly from ambient (or outdoor) PM2.5 concentration measured by monitors because people spend a great deal of time in environments, such as various indoor environments, where they are partially shielded from ambient sources of PM and are exposed to nonambient sources of PM. Recent research has provided some ways to include exposure information, but little has been done to determine the impact of including such information in a statistical model. To address this concern, we develop a three-stage Bayesian hierarchical model based on the Poisson regression model that is traditionally used to characterize the relationship between PM2.5 concentration and health endpoints. Our approach includes a spatial model relating monitor readings to average county PM2.5 concentration and an exposure simulator that links average ambient PM2.5 concentration to average personal exposure using activity pattern data. We apply our model to a study population in North Carolina and explore the impact of various exposure modeling assumptions on the conclusions that can be drawn about the link between PM2.5 exposure and cardiovascular mortality."
"10.1198/016214507000000266","2008","A test for anchoring and {Y}ea-{S}aying in experimental consumption data","0","We analyze experimental survey data, with a random split into respondents who get an open-ended question on the amount of total family consumption (with follow-up unfolding brackets of the form ""Is consumption $X or more?"" for those who answer ""don't know"" or ""refuse"") and respondents who are immediately directed to unfolding brackets. In both cases, the entry point of the unfolding bracket sequence is randomized. Allowing for any type of selection into answering the open-ended or bracket questions, a nonparametric test is developed for errors in the answers to the first bracket question that are different from the usual reporting errors that will also affect open-ended answers. Two types of errors are considered explicitly: anchoring and yea-saying. Data are collected in the 1995 wave of the Assets and Health Dynamics survey, which is representative of the population in the United States that is 70 years and older. We reject the joint hypothesis of no anchoring and no yea-saying. Once yea-saying is taken into account, we find no evidence of anchoring at the entry point."
"10.1198/016214507000000257","2008","Estimating flight departure delay distributions---a statistical approach with long-term trend and short-term pattern","0","In this article we develop a model for estimating flight departure delay distributions required by air traffic congestion prediction models. We identify and study major factors that influence flight departure delays, and develop a strategic departure delay prediction model. This model employs nonparametric methods for daily and seasonal trends. In addition, the model uses a mixture distribution to estimate the residual errors. To overcome problems with local optima in the mixture distribution, we develop a global optimization version of the expectation-maximization algorithm, borrowing ideas from genetic algorithms. The model demonstrates reasonable goodness of fit, robustness to the choice of the model parameters, and good predictive capabilities. We use flight data from United Airlines and Denver International Airport from the years 2000/2001 to train and validate our model."
"10.1198/016214507000000338","2008","Bayesian {G}aussian mixture models for high-density genotyping arrays","0","Affymetrix's SNP (single-nucleotide polymorphism) genotyping chips have increased the scope and decreased the cost of gene-mapping studies. Because each SNP is queried by multiple DNA probes, the chips present interesting challenges in genotype calling. Traditional clustering methods distinguish the three genotypes of an SNP fairly well given a large enough sample of unrelated individuals or a training sample of known genotypes. This article describes our attempt to improve genotype calling by constructing Gaussian mixture models with empirically derived priors. The priors stabilize parameter estimation and borrow information collectively gathered on tens of thousands of SNPs. When data from related family members are available, our models capture the correlations in signals between relatives. With these advantages in mind, we apply the models to Affymetrix probe intensity data on 10,000 SNPs gathered on 63 genotyped individuals spread over eight pedigrees. We integrate the genotype-calling model with pedigree analysis and examine a sequence of symmetry hypotheses involving the correlated probe signals. The symmetry hypotheses raise novel mathematical issues of parameterization. Using the Bayesian information criterion, we select the best combination of symmetry assumptions. Compared to Affymetrix's software, our model leads to a reduction in no-calls with little sacrifice in overall calling accuracy."
"10.1198/016214507000000310","2008","Skill, luck, and streaky play on the {PGA} tour","0","In this study, we implement a random-effects model that estimates cubic spline-based time-dependent skill functions for 253 active PGA Tour golfers over the 1998-2001 period. Our model controls for the first-order autocorrelation of residual 18-hole scores and adjusts for round-course and player-course random effects. Using the model, we are able to estimate time-varying measures of skill and luck for each player in the sample. Estimates of spline-based player-specific skill functions provide strong evidence that the skill levels of PGA Tour players change through time. Using the model, we are able to rank golfers at the end of 2001 on the basis of their estimated mean skill levels and identify the players who experienced the most improvement and deterioration in their skill levels over the 1998-2001 period. We find that some luck is required to win PGA Tour events, even for the most highly skilled players. Player-course interactions contribute very little to variability in golfer performance, but the particular course rotations to which players are assigned can have a significant effect on the outcomes of the few tournaments played on more than one course. We also find evidence that a small number of PGA Tour participants experience statistically significant streaky play."
"10.1198/016214507000000329","2008","On estimating diagnostic accuracy from studies with multiple raters and partial gold standard evaluation","1","We are often interested in estimating sensitivity and specificity of a group of raters or a set of new diagnostic tests in situations in which gold standard evaluation is expensive or invasive. Numerous authors have proposed latent modeling approaches for estimating diagnostic error without a gold standard. Albert and Dodd showed that, when modeling without a gold standard, estimates of diagnostic error can be biased when the dependence structure between tests is misspecified. In addition, they showed that choosing between different models for this dependence structure is difficult in most practical situations. While these results caution against using these latent class models, the difficulties of obtaining gold standard verification remain a practical reality. We extend two classes of models to provide a compromise that collects gold standard information on a subset of subjects but incorporates information from both the verified and nonverified subjects during estimation. We examine the robustness of diagnostic error estimation with this approach and show that choosing between competing models is easier in this context. In our analytic work and simulations, we consider situations in which verification is completely at random as well as settings in which the probability of verification depends on the actual test results. We apply our methodological work to a study designed to estimate the diagnostic error of digital radiography for gastric cancer."
"10.1198/016214507000000185","2008","Joint modeling of time series measures and recurrent events and analysis of the effects of air quality on respiratory symptoms","0","Exposure to ambient pollutants at concentrations above defined standards is a risk factor for respiratory symptoms, especially in sensitive children. Many studies have been undertaken to monitor air quality and to assess its association with respiratory symptoms. We propose a joint mixed-effects regression model of time series measures and recurrent events to analyze the air quality and respiratory symptom data from the Yale Mothers and Infants Health Study. Three mothers' symptoms (runny nose, cough, and sore throat) and three infants' symptoms (runny nose, cough, and general sickness) were investigated. To alleviate the computational complexity, a two-stage maximum likelihood-based estimation procedure is introduced to estimate the parameters, and simulation studies are conducted to assess the validity of this estimation procedure. Our analysis reveals differences in the etiology of respiratory symptoms between mothers and infants. Most notably, coarse particles of mass between 2.5 and 10 mu in diameter increased the risks of mothers' runny nose and cough symptoms but had no significant effect on any of the three infants' symptoms. The sulfate level was negatively associated with the risk of infants' runny nose and cough symptoms but had no significant effect on any of the three mothers' symptoms. High level of humidity is negatively associated with the mothers' cough incidence but had no significant association with any of the three infants' symptoms. Such differences reveal not only the sensitivity of the mothers and infants to the air quality, but also call for further understanding of the differences. It is possible that actions taken to overcome humidity by mothers may inadvertently affect their infants."
"10.1198/016214507000001454","2008","Rejoinder [MR2420211]","0",""
"10.1198/016214507000001409","2008","Bivariate binomial spatial modeling of {\it {L}oa loa} prevalence in tropical {A}frica","3","We present a state-of-the-art application of smoothing for dependent bivariate binomial spatial data to Loa loa prevalence mapping in West Africa. This application starts with the nonspatial calibration of survey instruments, continues with the spatial model building and assessment, and ends with robust, tested software intended for use by field workers for online prevalence map updating. From a statistical perspective, we address several important methodological issues: building spatial models that are sufficiently complex to capture the structure of the data but remain computationally usable, reducing the computational burden in the handling of very large covariate data sets, and devising methods for comparing spatial prediction methods for a given exceedance policy threshold."
"10.1198/016214508000001084","2008","Multiply robust inference for statistical interactions","4","A primary focus of an increasing number of scientific studies is to determine whether two exposures interact in the effect that they produce on an outcome of interest. Interaction is commonly assessed by fitting regression models in which the linear predictor includes the product between those exposures. When the man interest lies in the interaction, this approach is not entirely satisfactory, because it is prone to (possibly severe) bias when the main exposure effects or the associations between outcome and extraneous factors are misspecified. In this article we consider conditional mean models with identity or log link that postulate the statistical interaction in terms of a finite-dimensional parameter but are otherwise unspecified. We show that estimation of the interaction parameter often is not feasible in this model, because it requires nonparametric estimation of auxiliary conditional expectations given high-dimensional variables. We thus consider multiply robust estimation under a union model that assumes that at least one of several working submodels holds. Our approach is novel in that it makes use of information on the joint distribution of the exposures conditional on the extraneous factors in making inferences about the interaction parameter of interest. In the special case of a randomized trial or a family-based genetic study in which the joint exposure distribution is known by design or by Mendelian inheritance, the resulting multiply robust procedure leads to asymptotically distribution-free tests of the null hypothesis of no interaction on an additive scale. We illustrate the methods through simulation and analysis of a randomized follow-up study."
"10.1198/016214508000001066","2008","Smoothly clipped absolute deviation on high dimensions","5","The smoothly clipped absolute deviation (SCAD) estimator, proposed by Fan and Li, has many desirable properties, including continuity, sparsity, and unbiasedness. The SCAD estimator also has the (asymptotically) oracle property when the dimension of covariates is fixed or diverges more slowly than the sample size. In this article we study the SCAD estimator in high-dimensional settings where the dimension of covariates can be much larger than the sample size. First, we develop and efficient optimization algorithm that is fast and always converges to a local minimum. Second, we prove that the SCAD estimator still has the oracle property on high-dimensional problems. We perform numerical studies to compare the SCAD estimator with the LASSO and SIS-SCAD estimators in terms of prediction accuracy and variable selectivity when the true model is sparse. Through the simulation, we show that the variance estimator of Fan and Li still works well for some limited high-dimensional cases where the true nonzero coefficients are not too small and the sample size is moderately large. We apply the proposed algorithm to analyze a high-dimensional microarray data set."
"10.1198/016214508000001057","2008","Model selection criteria for missing-data problems using the {EM} algorithm","0","We consider novel methods for the Computation of model selection criteria in missing-data problems based on the output of the EM algorithm The methodology is very general and can be applied to numerous simulations involving incomplete data within an EM framework, from covariates missing at random in arbitrary regression models to nonignorably missing longitudinal responses and/or covariates. Toward this goal, we develop a class of information criteria for missing-data problems called ICH,Q, which yields the Akaike information criterion and the Bayesian information criterion as special cases. The computation of ICH,Q requires an analytic approximation to a complicated function. called the H-function, along with output from the EM algorithm used in obtaining maximum likelihood estimates. The approximation to the H-function leads to a large class of information criteria, called IC(H) over tilde (k),Q. Theoretical properties of IC(H) over tilde (k),Q, including consistency, are investigated in detail. To eliminate the analytic approximation to the H-function, a computationally simpler approximation to ICH,Q. called ICQ, is proposed, the computation of which depends solely on the Q-function of the EM algorithm. Advantages and disadvantages of IC(H) over tilde (k),Q and ICQ are discussed and examined in detail in the context of missing-data problems. Extensive simulations are given to demonstrate the methodology and examine the small-sample and large-sample performance of IC(H) over tilde (k),Q and ICQ in missing-data problems. An AIDS data set also is presented to illustrate the proposed methodology."
"10.1198/016214508000000760","2008","Optimal crossover designs for two treatments in the presence of mixed and self-carryover effects","0","The main purpose of this article is to identify optimal crossover designs for two treatments under a model that includes mixed and self carryover effects. In addition, results are reported for optimal two-treatment crossover designs under several closely related models, and the performance of various designs for three and four periods is studied under the different models"
"10.1198/016214508000000805","2008","A multiple-index model and dimension reduction","0","Dimension reduction can be used as an initial step in statistical modeling. Further specification of model structure is imminent and important when the reduced dimension is still greater than 1. In this article we investigate one method of specification that involves separating the linear component front the nonlinear components, leading to further dimension reduction in the unknown link function and. thus, better estimation and easier interpretation of the model. The specified Model includes the popular econometric multiple-index model and the partially linear single-index model as its special cases. A criterion is developed to validate the model specification. An algorithm is proposed to estimate the Model directly. Asymptotic distributions for the estimators of the parameters and the nonparametric link function are derived. Air pollution data in Chicago are used to illustrate the modeling procedure and to demonstrate its advantages over the existing dimension reduction approaches."
"10.1198/016214508000000878","2008","Nonparametric variable selection: the {EARTH} algorithm","0","We consider regression experiments involving a response variable Y and a large number of predictor variables X(1),...,X(d), many of which may be irrelevant for the prediction of Y and thus must be removed before Y can be predicted from the X's. We consider two procedures that select variables by using importance scores that measure the strength of the relationship between predictor variables and a response and keep those variables whose importance scores exceed a threshold. In the first of these procedures, scores are obtained by randomly drawn subregions (tubes) of the predictor space that constrain all but one predictor and in each subregion computing a signal-to-noise ratio (efficacy) based on a nonparametric univariate regression of Y on the unconstrained variable. The subregions are adapted to boost weak variables iteratively by searching (hunting) for the subregions in which the efficacy is maximized. The efficacy can be viewed as an approximation to a one-to-one function of the probability of identifying features. By using importance scores based on averages of maximized efficacies. We develop a variable selection algorithm called EARTH (efficacy adaptive regression tube hunting) based on examining the conditional expectation of the response given all but one of the predictor variables for a collection of randomly, adaptively, and iteratively selected regions. The second importance score method (RFVS) is based on using random forest importance values to select variable. Computer simulations show that EARTH and RFVS are successful variable selection methods compared with other procedures in nonparametric situations with a large number of irrelevant predictor variables, and that when each is combined with the model selection and prediction procedure MARS, the tree-based prediction procedure GUIDE, and the random forest method, the combinations lead to improved prediction accuracy for certain models with many irrelevant variables. We give conditions under which a version of the EARTH algorithm selects the correct model with probability lending to 1 as the sample size n tends to infinity even if d -> infinity as n -> infinity. We include the analysis of a real data set in which we show how a training set can be used to find a threshold for the EARTH importance scores."
"10.1198/016214508000000977","2008","Nonparametric quantile estimations for dynamic smooth coefficient models","2","We suggest quantile regression methods for a class of smooth coefficient time series models. We use both local polynomial and local constant litting schemes to estimate the smooth coefficients in a quantile framework. We establish the asymptotic properties of both the local polynomial and local constant estimators for alpha-mixing time series. We also suggest a bandwidth selector based on the nonparametric version of the Akaike information criterion. along with a consistent estimate of the asymptotic covariance matrix. We evaluate the asymptotic behaviors of the estimators at boudaries and compare the local polynomial quantile estimator and the local constant estimator. A simulation study is carried out to illustrate the performance of estimates. An empirical application of the model to real data further demonstrate the potential of the proposed modeling procedures."
"10.1198/016214508000000968","2008","Bandwidth selection in nonparametric kernel testing","0","We propose a sound approach to bandwidth selection in nonparametric kernel testing. The main idea is to find an Edgeworth expansion of the asymptotic distribution of the test concerned Due to the involvement of a kernel bandwidth in the leading term of the Edgeworth expansion. we dire able to establish closed-form expressions to explicitly represent the leading terms of both the size and power functions and then determine how the bandwidth should be chosen according to certain requirements for both the size and power functions. For example, when a significance level is given. we can choose the bandwidth such that the Power function is maximized while the size function is controlled by the significance level. Both asymptotic theory and methodology are established. In addition. we develop an easy implementation procedure for the practical realization of the established methodology and illustrate this on two simulated examples and a real data example."
"10.1198/016214508000000797","2008","Parameter estimation for differential equation models using a framework of measurement error in regression models","3","Differential equation (DE) models are widely used in many scientific fields, including engineering, physics, and biomedical sciences. The so-called ""forward problem,"" the problem of simulations and predictions of state variables for given parameter values in the DE models. has been extensively studied by mathematicians, physicists, engineers, and other scientists. However, the ""inverse problem"" the problem of parameter estimation based on the measurements of output variables, has not been well explored using modern statistical method, although some least squares-based approaches have been proposed and studied. In this article we propose parameter estimation methods for ordinary differential equation (ODE) models based on the local smoothing approach and a pseudo-least squares (PsLS) principle under a framework of measurement error in regression models. The asymptotic properties of the Proposed PsLS estimator are established. We also compare the PsLS method to the corresponding simulation-extrapolation) (SIMEX) method and evaluate their finite-sample performances via simulation studies. We illustrate the proposed approach using an application example from an HIV dynamic study."
"10.1198/016214508000000788","2008","Variable selection in nonparametric varying-coefficient models for analysis of repeated measurements","4","Nonparametric varying-coefficient models are commonly used for analyzing data measured repeatedly over time, including longitudinal and functional response data. Although many procedures have been developed for estimating varying coefficients. the problem of variable selection for such models has rot been addressed to date. la this article we present a regularized estimation procedure for variable selection that combines basis function approximations and the smoothly clipped absolute deviation penalty. The proposed I)procedure Sill)simultaneously selects significant variables with time-varying, effects and estimates the nonzero smooth coefficient functions. Under suitable conditions. we establish the theoretical properties of our procedure, including consistency in variable selection and the oracle property in estimation. Here the oracle property means that the asymptotic distribution of an estimated coefficient function is the same as that when it is known a priori which variables are in the model. The method is illustrated with simulations and tow real data examples, one for identifying risk factors in the study of AIDS and one using microarray time-course gene expression data to identify the transcription factors related to the yeast cell-cycle process."
"10.1198/016214508000000959","2008","Covariance tapering for likelihood-based estimation in large spatial data sets","5","Maximum likelihood is an attractive method of estimating covariance parameters in spatial models based oil Gaussian processes. But calculating, the likelihood can he computationally infeasible for large data sets, requiring O(m(3)) calculations for a data set with a observations. This article proposes the method of covariance tapering to approximate the likelihood in this setting. In this approach. covariance matrixes are ""tapered."" or multiplied element wise by a sparse correlation matrix. The resulting matrixes can their be manipulated using efficient sparse matrix algorithms. We propose two approximations to the Gaussian likelihood using tapering. One of the approximations simply replaces the model covariance with a tapered version, whereas the other is motivated by the theory of unbiased estimating equations. Focusing on the particular case of the Matern class of covariance functions, we give conditions under which estimators maximizing the tapering approximations are. like the maximum likelihood estimator, Strongly consistent. Moreover, we show in a simulation study that the tapering estimators can have sampling densities quite similar to that of the maximum likelihood estimator even when the degree of tapering is severe. We illustrate the accuracy and computational gains of the tapering methods in an analysis of yearly total precipitation anomalies at weather stations in the United States."
"10.1198/016214508000000751","2008","Functional additive models","2","In commonly used functional regression models. the regression of it scalar or functional response oil the functional predictor is assumed to be linear. This means that the response is a linear function of the functional principal component scores of the predictor process. We relax the linearity assumption and propose to replace it by an additive structure. leading to it more widely applicable and much more flexible framework for functional regression models. The proposed functional additive regression models are Suitable for both scalar and functional response. The regularization needed for effective estimation of the regression parameter function is implemented through it projection on the eigenbasis of the covariance operator of the functional components in the model. The use of functional principal components in an additive rather than linear way leads to substantial broadening of the scope of functional regression models and emerges as a natural an additive rather than linear way leads to substantial broadening of the scope of functional regression models and emerges as a natural approach, because the uncorrelatedness of the functional principal components is shown to lead to a straightforward implementation of the functional additive model, based solely oil a sequence of one-dimensional smoothing, steps and without the need for backfitting. This facilitates the theoretical analysis. and we establish the asymptotic consistency of the estimates of the components of the functional additive model. We illustrate the empirical performance of the proposed modeling framework and estimation methods through simulation studies and in applications to gene expression time course data."
"10.1198/016214508000000742","2008","Semiparametric estimation of covariance matrixes for longitudinal data","3","Estimation of longitudinal data covariance structure Poses significant challenges because the data usually are collected at irregular time points. A viable semiparametric model for covariance matrixes has been proposed that allows one to estimate the variance function nonparametrically and to estimate the correlation function parametrically by aggregating information front irregular and sparse data points within each subject. But the asymptotic properties of the quasi-maximum likelihood estimator (QMLE) of parameters in the covariance model are largely, unknown. We address this problem in the context of more general models for the conditional (QMLE) of parameters in the covariance. including parametric, nonparametric, or semiparametric. We also consider the possibility of rough mean regression function and introduce the difference-based method to reduce biases in the context of varying-coefficient partially linear mean regression Models. This provides a more robust estimator of the covariance function under a wider range of situation,. Under some technical conditions. consistency and asymptotic normality are obtained for the QMLE of of the Parameters ill the correlation function. Simulation Studies and a real data example are used to illustrate the Proposed approach."
"10.1198/016214508000000850","2008","Assessing the effect of selection at the amino acid level in malaria antigen sequences through {B}ayesian generalized linear models","0","We present it statistical approach for identifying residues in DNA sequences for which diversity may be maintained by natural selection. Bayesian generalized linear models (GLMs) are used to describe patterns of mutation in it DNA Sequence alignment. Posterior distributions of key quantities. such as probabilities of nonsynonymous and synonymous mutations per site, are studied. Inference in this class of models is achieved through customary Markov chain Monte Carlo methods. Model selection is dealt with by means of a minimum posterior predictive loss approach. We describe how information on the evolutionary process underlying the sequences can be formally incorporated into the models through sturctured priors. The proposed methodology was designed to analyze several DNA sequences encoding the vaccine candidate apical membrane antigen-1 (AMA-1) of the human malaria parasite The study of genetic Variability in antigen Sequences is relevant to determining whether it particular antigen is a viable target for a vaccine construct. Using a simulation study. We first compare the GLM-based approach to existing methods for detecting sites Under selection that are based on stochastic methods of sequence evolution. We then apply the proposed models to the AMA-1 sequence data, which allows us to identify residues with the greatest disparities between nonsynonymous and synonymous changes. Recent experimental evidence suggests that several of these residues re immunologically relevant, indicating at the proposed models may be Used predictively to identify functionally significant residues in antigens for which experimental results are not yet available."
"10.1198/016214508000000922","2008","The dynamics of economic functions: modeling and forecasting the yield curve","0","The class of functional signal plus noise (FSN) models is introduced that provides a new, general method for modeling and forecasting time series of economic functions. The underlying, continuous economic function (or ""signal"") is a natural cubic spline whose dynamic evolution is driven by a cointegrated vector autoregression for the ordinates (or ""gamma-values"") at the knots of the spline. The natural cubic spline provides flexible cross-sectional tit and results in a linear state-space model. This FSN model achieves dimension reduction. provides a coherent description of the observed yield curve and its dynamics as the cross-sectional dimension N becomes large. and call be feasibly estimated and used for forecasting when N is large. The integration and cointegration properties of the model are derived. The FSN models are then applied to forecasting 36-dimensional yield curves for U.S. Treasury bonds at the 1-month-ahead horizon. The method consistently outperforms the dynamic Nelson-Siegel and random walk forecasts on the basis of both mean squared forecast error criteria and economically relevant loss functions derived front the realized profits of pairs trading algorithms. The analysis also highlights in a concrete setting the dangers of attempting to infer the relative economic value of model forecasts oil the basis of their associated mean squared forecast errors."
"10.1198/016214508000000698","2008","Records in athletics through extreme-value theory","1","We are interested in two questions on extremes relating to world records in athletics. The first question is: What is the ultimate world record in a specific athletic event (such as the 100-m race for men or the high jump for women), given today's state of the art? Our second question is: How ""good"" is a current athletic world record? An answer to the second question also enables us to compare the quality of world records in different athletic events. We consider these questions for each of 28 events (14 for both men and women).We approach the two questions with the probability theory of extreme values and the corresponding statistical techniques. The statistical model is of a nonparametric nature only some ""weak regularity"" of the tail of the distribution function is assumed. We derive the limiting distribution of the estimated quality of a world record.While almost all attempts to predict an ultimate world record are based on the development of world records over time. this is not our method. Instead, we use all top performances. Our estimated ultimate world record tells us what, in principle, is possible in the near future, given the present knowledge, material (shoes, suits, equipment), and drug laws."
"10.1198/016214508000000913","2008","Branching processes as models of progenitor cell populations and estimation of the offspring distributions","0","We consider two new models of reducible age-dependent branching processes with emigration ill coil junction with estimation problems arising in cell biology. Methods of statistical inference are developed using the relevant embedded discrete branching Structure. Based on observations of the branching process with emigration. estimators of the offspring probabilities are proposed for the hidden Unobservable process without emigration. which is of prime interest to investigators. The problem under consideration is motivated by experimental data generated by time-lap se video recording of cultured cells, which Provides abundant information oil their individual evolutions and thus on the basic parameters of their life cycle in tissue culture. Some parameters. such its the mean and variance of the mitotic cycle time, can be estimated nonparametrically without resorting to any mathematical model of cell population kinetics. For other parameters. such as the offspring distribution. a model-based inference is needed. Age-dependent branching processes have proven to be useful models For that purpose. A special feature of the data generated by time-lapse experiments is the presence of censoring effects due to the migration of cells out of the field of observation. For the time-to-event observations, such as the mitotic cycle time. the effects,' of data censoring can be accounted for by standard methods of survival analysis. No methods are available to accommodate Such effects in the statistical inference on the offspring distribution. Within the framework of branching processes, the loss of cells to Follow-up can be modeled its a process of emigration. Incorporating the emigration process into a pertinent branching model of cell evolution provides the basis for the proposed estimation techniques. Statistical inference oil the offspring distribution is illustrated with :in application to the development of oligodendrocytes in cell culture."
"10.1198/016214508000000481","2008","Variable inclusion and shrinkage algorithms","3","The Lasso is a popular and computationally efficient procedure for automatically performing both variable selection and coefficient shrinkage on linear regression models. Oner limitation of the Lasso is that the same tuning parameter is used for both variable selection and shrinkage. As a result, it typically ends up selecting a model with too many variables to prevent overshrinkage of the regression coefficients. we suggest an improved class of methods called variable inclusion and shrinkage algorithms (VISA). Our approach is capable of selecting sparse models while avoiding overshrinkage problems and uses a path algorithm, and so also is computationally efficient. We show through extensive simulations that VISA significantly outperforms the Lasso and also provides improvements over more recent procedures, such as the Dantzig selector, relaxed Lasso, and adaptive Lasso. In addition, we provide theoretical justification for VISA in terms of nonasymptotic bounds on the estimation error that suggest it should exhibit good performance even for large numbers of predictors. Finally, we extend the VISA methodolog, path algorithm, and the theoretical bounds to the generalized linear models framework."
"10.1198/016214508000000625","2008","Empirical likelihood-based estimation of the treatment effect in a pretest-posttest study","1","The pretest-posttest study design is commonly used in medical and social science research to assess the effect of a treatment or an intervention. Recently, interest has been rising in developing inference procedures that improve efficiency while relaxing assumptions used in the pretest-posttest data analysis, especially when the posttest measurement might be missing. In this article we propose a semiparametric estimation procedure based on empirical likelihood (EL) that incorporates the common baseline covariate information to improve efficiency. The proposed method also yields an asymptotically unbiased estimate of the response distribution. Thus functions of the response distribution, such as the median, can be estimated straightforwardly, and the EL method can provide a more appealing estimate of the treatment effect for skewed data.We show that, compared with existing methods, the proposed EL estimator has appealing theoretical properties, especially when the working model for the underlying relationship between the pretest and posttest measurements is misspecified. A series of simulation studies demonstrates that the EL-based estimator outerperforms its competitors when the working model is misspecified and the data are missing at random. We illustrate the methods by analyzing data from an AIDS clinical trial (ACTG 175)."
"10.1198/016214508000000436","2008","Tests based on intrinsic priors for the equality of two correlated proportions","1","Correlated proportions arise in longitudinal (panel) studies. A typical example is the ''opinion swing'' problem: ''Has the proportion of people favoring a politician changed after his recent speech to the nation on TV?'' Because the same group of individuals is interviewed before and after the speech, the two proportions are correlated. A natural null hypothesis is to be tested is whether the corresponding population proportions are equal. A standard Bayesian approach to this problem has already been considered in the literature, based on a Dirichlet prior for the cell probabilities of the underlying 2 x 2 table under the alternative hypothesis, together with an induced prior under the null. With a lack of specific prior information, a diffuse (e.g., uniform) distribution may be used. We claim that this approach is not satisfactory, because in a testing problem one should make sure that the prior under the alternative is adequately centered around the region specified by the null, in order to obtain a fairer comparision between the two hypothesis, especially when the data are in reasonable agreement with the null. Following an intrinsic prior methodology, we develop two strategies for the construction of a collection of objective priors increasingly peaked around the null. We provide a simple interpretation of their structure in terms of weighted imaginary sample scenarios. We illustrate our method by means of three examples, carrying out sensitivity analysis and providing comparision with existing results."
"10.1198/016214508000000508","2008","Binary time series modeling with application to adhesion frequency experiments","0","Repeated adhesion frequency assay is the only published method for measuring the kinetic rates of cell adhesion. Cell adhesion plays an important role in many physiological and pathological processes. Traditional analysis of adhesion frequency experiments assumes that the adhesion test cycles are independent Bernoulli trials. This assumption often can be violated in practice. Motivated by the analysis of repeated adhesion tests, a binary time series model incorporating random effects is developed. A goodness-of-fit statistic is introduced to assess the adequacy of distribution assumptions on the dependent binary data with random effects. The asymptotic distribution of the goodness-of-fit statistic is derived, and its finite-sample performance is examined through a simulation study. Application of the proposed methodology to real data from a T-cell experiment reveals some interesting information, including the dependency between repeated adhesion tests."
"10.1198/016214508000000427","2008","Optimal designs for dose-finding studies","5","Understanding and properly characterizing the dose-response relationship is a fundamental step in the investigation of a new compound, be it a herbicide or fertilizer, a molecular entity, an environmental toxin, or an industrial chemical. In this article we investigate the problem of deriving efficient designs for the estimation of target doses in the context of clinical dose finding. We propose methods to determine the appropriate number and actual levels of the doses to be administered to patients, as well as their relative sample size allocations. More specifically, we derive local optimal designs that minimize the asymptotic variance of the minimum effective dose estimate under a particular dose-response model. We investigate the small-sample properties of these designs, together with their sensitivity to a missspecification of the true parameter values and of the underlying dose-response model, through simulation. Finally, we demonstrate that the designs derived for a fixed model are rather sensitive with respect to this assumption and construct robust optimal designs that take into account a set of potential dose-response profiles within classes of models commonly used in drug development practice."
"10.1198/016214508000000490","2008","Power-transformed linear quantile regression with censored data","1","We propose a class of power-transformed linear quantile regression models for survival data subject to random censoring. The estimation procedure follows two sequential steps. First, for a given transformation parameter, we can easily obtain the estimates for the regression coefficients by minimizing a well-defined convex objective function. Second, we can estimate the transformation parameter based on a model discrepancy measure by constructing cumulative sum processes. We show that both the regression and transformation parameter estimates are strongly consistant and asymptotically normal. The variance-covariance matrix depends on the unknown density function of the error term, so we estimate the variance by the usual bootstrap approach. We examine the performance of the proposed method for finite sample sizes through simulation studies and illustrate it with as real data example."
"10.1198/016214508000000463","2008","Partially linear additive hazards regression with varying coefficients","1","To explore the nonlinear interactions between some covariates and an exposure variable, we propose the partially linear additive hazards model for survival data. In a semiparametric setting, we construct a local pseudoscore function to estimate the varying and constant coefficients and establish the asymptotic normality of the proposed estimators. Moreover, we develop the weak convergence property for the local estimator of the baseline cumulative hazard function. We conduct simulation studies to empirically examine the finite-sample performance of the proposed methods and use real data from a breast cancer study for illustration."
"10.1198/016214508000000535","2008","Estimating equations inference with missing data","1","There is a large and growing body of literature on estimating equation (EE) as an estimation approach. One basic property of EE that has been universely adopted in practice is that of unbiasedness, and there are deep conceptual reasons why unbiasedness is a desirable EE characteristic. This article deals with inference from EEs generally leads to EEs that are biased and thus, violates a basic assumption of the EE approach. The main contribution of this article is that it goes beyond existing imputation methods and proposes a procedure whereby one mitigates the effects of missing data through a reformation of EEs imputed through a kernel regression method. These (modified) EEs then constitute a basis for inference by the generalized method of moments (GMM) and empirical likelihood (EL). Asymptotic properties of the GMM and EL estimators of the unknown parameters are derived and analyzed. Unlike most of the literature, which deals with missingness in either covariate values or response data, our method allows for missingness in both sets of variables. Another important strength of our approach is that it allows auxiliary information to be handled successfully. We illustrate the method using a well-known wormy-fruits dataset and data from a study on Duchenne muscular dystrophy detection and compare our results with several existing methods via a simulation study."
"10.1198/016214508000000445","2008","On a projective resampling method for dimension reduction with multivariate responses","4","Consider the dimension reduction problem where both the response and the predictor are vectors. Existing estimators of this problem take one of the following routes: (1) targeting the part of the dimension reduction space that is related to the conditional mean (or moments) of the response space directly by multivariate slicing. However, the first two approaches do not fully recover the dimension reduction space, and the third is hampered by the fact that the accuracy of estimators based on multivariate slicing drops sharply as the dimension of response increases-a phenomenon often called the ''curse of dimensionality''. We propose a new method that overcomes both difficulties, in that it involves univariate slicing only and it is guaranteed to fully recover the dimension reduction space under reasonable conditions. The method will be compared with the existing estimators by simulation and applied to a dataset."
"10.1198/016214508000000472","2008","An informational measure of association and dimension reduction for multiple sets and groups with applications in morphometric analysis","0","In this article we propose a new general index that measures relationships between multiple sets of random vectors. This index is based on Kullback-Leibler (KL) information, which measures linear or nonlinear dependence between multiple sets using joint and marginal densities of affine transformations of the random vectors. Estimates of the matrixes are obtained by maximizing the KL information and are shown to be consistent. The motivation for introducing such an index comes from morphological integration studies, a topic in biological science. As a special case of this index, we define an overall measure of association and two other measures for dimension reduction. The use of these measures is illustrated through real data analysis in morphometric studies and extensive simulations, and their performance is compared with that of approaches based on canonical correlation analysis, our general index not only provides an overall measure of association, but also determines nonlinear relationships, thereby making it useful in many other applications."
"10.1198/016214508000000652","2008","Model-independent estimates of dark matter distributions","0","A new nonparametric method is described to estimate the distribution of mass within spherical galaxies. The problem of estimating the mass, M(r), within radius r is converted into a problem of estimating a regression function nonparametrically, subject to shape restriction. We represent the restrictions by the interception of quadratic cones and use the second-order cone programming to estimate the unknown parameters. We establish asymptotic results that are used to construct confidence intervals M(r). We apply the technique to new kinematic data for four dwarf galaxies. Results indicate that dark matter dominates the stellar kinematics of these systems at all radii."
"10.1198/016214508000000643","2008","A directional model for the estimation of the rotation axes of the ankle joint","2","This article is motivated by the estimation of the direction of the two rotation axes of the ankle. These axes carry information on individual ankles: they are useful in the construction of biomechanical models and the treatment of orthopedic problems. In biomechanics, the rotation axes of the ankle often are estimated using optimization techniques. This work investigates a statistical model for carrying out the estimation. The data set for analysis is a time-ordered sequence of 3x3 rotation matrixes giving the ankle's orientations as the foot moves with respect to the lower leg. These rotation matrixes are assumed to follow Fisher-von Mises distributions. The predicted values for the observed rotations features four angles for the orientation of two rotation axes and two series of time-varying rotation angles about the two axes. Maximum likelihood estimators of the parameters are derived. Approximations to their sampling distributions are obtained when the errors are clustered around the identity matrix. Sandwich variance estimators, accounting for autocorrelation in the errors, are proposed for the estimators of the tow axes. An extension of the model that uses the traslational motion in the estimation of the tow axes is presented. Compared with the traditional biomechanical techniques for estimating the anatomic axes of the ankle, the proposed model has two advantages: It allows the calculation of standard errors for the estimates, and can distinguish the parameters whose estimations is affected by a limited range of motion about the two axes. This is illustrated by the estimation of the rotation axes of the ankles of tow subjects."
"10.1198/016214507000001328","2008","Assessing identification risk in survey microdata using log-linear models","0","This article considers the assessment of the risk of identification of respondents in survey microdata, in the context of applications at the United Kingdom (UK) Office for National Statistics (ONS). The threat comes from the matching of categorical ""key"" variables between microdata records and external data sources and from the use of log-linear models to facilitate matching. While the potential use of such statistical models is well established in the literature, little consideration has been given to model specification or to the sensitivity of risk assessment to this specification. In numerical work not reported here, we have found that standard techniques for selecting log-linear models, such as chi-squared goodnmess-of-fit tests, provide little guidance regarding the accuracy of risk estimation for teh very sparse tables generated by typical applications at ONS, for example, tables with millions of cells formed by cross-classifying six key variables, with sample sizes of 10 or 100,000. In this article we develop new criteria for assessing the specification of a log-linear model in relation to the accuracy of risk estimates. We find that, withing a class of ""reasonable"" models, risk estiamtes tend to decrease as the complexity fo the model increases. We develop criteria that detect ""underlitting"" (associated with overestimation of the risk). The criteria may also reveal ""overfitting"" (associated with underestiamtion) lathough not so clearly, so we suggest employing a forward model selection approach. our criteria turn out to be related to established methods of testing for overdispersion in Poisson log-linear models. We show how our approach may be used for file-level and record-leveol measures of risk. We evaluate the proposed procedures using samples drawn from the 2001 UK Census where the true risks can be determined and show that a forward selection approach leads to good risk estimates. There are several ""good"" models between which our approach provides little discrimination. The risk estimates are found to be stable across these models, implying a form of robustness. We also apply our approach to a large survey dataset. There is no indication that increasing the sample size necessarily leads to the selection of a more complec model. The risk estimates for this application display more variation but suggest a suitable upper bound."
"10.1198/016214507000001021","2008","Stochastic networks in nanoscale biophysics: modeling enzymatic reaction of a single protein","0","Advances in nanotechnology enable scientists for the first time to study biological processes on a nanoscale molecule-by-molecule basis. A surprising discovery from recent nanoscale single-molecule biophysics experiments is taht biological reactions involving enzymes behave fundamentally differently from what classical theory predicssts. In this article we introduce a stochastic network model to explain the experimental puzzle (by modeling enzymatic reactions as a stochastic network connected by different enzyme conformations). Detailed analyses of the model, including analyses of the first-passage-time distributions and goodness of fit, show that the stochastic network model is capable of explaining the experimental surprises. The model is analytically tractable and closely fits experimental data. The biological/chemical meaning of the model is discussed."
"10.1198/016214508000000201","2008","Current methods for recurrent events data with dependent termination: a {B}ayesian perspective","0","There has been a recent surge of interest in modeling and methods for analyzing recurrent events data with risk of termination dependent on the history of the recurrent events. To aid future users in understanding the implications of modeling assumptions and modeling properties, we review the state-of-the-art statistical methods and present novel theoretical properties, identifiability results, and practical consequences of key modeling assumptions of several fully specified stochastic models. After introducing stochastic models with 2 noninformative termination process, we focus on a class of models that allows both negative and positive association between the risk of termination and the rate of recurrent events through a frailty variable. We also discuss the relationship, as well as the major differences between these models in terms of their motivations and physical interpretations. We discuss associated Bayesian methods based on Markov chain Monte Carlo tools, and novel model diagnostic tools to perform inference based on fully specified models. We demonstrate the usefulness of the current methodology through an analysis of a data set from a clinical trial. Finally, we explore possible future extensions and limitations of the methodology."
"10.1198/016214508000000292","2008","Toward causal inference with interference","0","A fundamental assumption usually made in causal inference is that of no interference between individuals (or units); that is, the potential outcomes of one individual are assumed to be unaffected by the treatment assignment of other individuals. However, in many settings, this assumption obviously does not hold. For example, in the dependent happenings of infectious diseases, whether one person becomes infected depends on who else in the population is vaccinated. In this article, we consider a population of groups of individuals where interference is possible between individuals within the same group. We propose estimands for direct, indirect, total, and overall causal effects of treatment strategies in this setting. Relations among the estimands are established; for example, the total causal effect is shown to equal the sum of direct and indirect causal effects. Using an experimental design with a two-stage randomization procedure (first at the group level, then at the individual level within groups), unbiased estimators of the proposed estimands are presented. Variances of the estimators are also developed. The methodology is illustrated in two different settings where interference is likely: assessing causal effects of housing vouchers and of vaccines."
"10.1198/016214508000000265","2008","Laplace periodogram for time series analysis","0","A new type of periodogram, called the Laplace periodogram, is derived by replacing least squares with least absolute deviations in the harmonic regression procedure that produces the ordinary periodogram of a time series. An asymptotic analysis reveals a connection between the Laplace periodogram and the zero-crossing spectrum. This relationship provides a theoretical justification for use of the Laplace periodogram as a nonparametric tool for analyzing the serial dependence of time series data. Superiority of the Laplace periodogram in handling heavy-tailed noise and nonlinear distortion is demonstrated by simulations. A real-data example shows its great effectiveness in analyzing heart rate variability in the presence of ectopic events and artifacts."
"10.1198/016214508000000274","2008","Trimmed comparison of distributions","0","This article introduces an analysis of similarity of distributions based on the L-2-Wasserstein distance between trimmed distributions. Our main innovation is the use of the impartial trimming methodology, already considered in robust statistics, which we adapt to this setup. Instead of simply removing data at the tails to provide some robustness to the similarity analysis, we develop a data-driven trimming method aimed at maximizing similarity between distributions. Dissimilarity is then measured in terms of the distance between the optimally trimmed distributions. We provide illustrative examples showing the improvements over previous approaches and give the relevant asymptotic results to justify the use of this methodology in applications."
"10.1198/016214508000000184","2008","Penalized estimating functions and variable selection in semiparametric regression models","2","We propose a general strategy for variable selection in semiparametric regression models by penalizing appropriate estimating functions. Important applications include semiparametric linear regression with censored responses and semiparametric regression with missing predictors. Unlike the existing penalized maximum likelihood estimators, the proposed penalized estimating functions may not pertain to the derivatives of any objective functions and may be discrete in the regression coefficients. We establish a general asymptotic theory for penalized estimating functions and present suitable numerical algorithms to implement the proposed estimators. In addition, we develop a resampling technique to estimate the variances of the estimated regression coefficients when the asymptotic variances cannot be evaluated directly. Simulation studies demonstrate that the proposed methods perform well in variable selection and variance estimation. We illustrate our methods using data from the Paul Coverdell Stroke Registry."
"10.1198/016214508000000355","2008","Survival analysis with quantile regression models","5","Quantile regression offers great flexibility in assessing covariate effects on event times, thereby attracting considerable interests in its applications in survival analysis. But currently available methods often require stringent assumptions or complex algorithms. In this article we develop a new quantile regression approach for survival data subject to conditionally independent censoring. The proposed martingale-based estimating equations naturally lead to a simple algorithm that involves minimizations only of L-1-type convex functions. We establish uniform consistency and weak convergence of the resultant estimators. We develop inferences accordingly, including hypothesis testing, second-stage inference, and model diagnostics. We evaluate the finite-sample performance of the proposed methods through extensive simulation studies. An analysis of a recent dialysis study illustrates the practical utility of our proposals."
"10.1198/016214508000000247","2008","Penalized clustering of large-scale functional data with multiple covariates","1","In this article we propose a penalized clustering method for large-scale data with multiple covariates through a functional data approach. In our proposed method, responses and covariates are linked together through nonparametric multivariate functions (fixed effects), which have great flexibility in modeling various function features, such as jump points, branching, and periodicity. Functional ANOVA is used to further decompose multivariate functions in a reproducing kernel Hilbert space and provide associated notions of main effect and interaction. Parsimonious random effects are used to capture various correlation structures. The mixed-effects models are nested under a general mixture model in which the heterogeneity of functional data is characterized. We propose a penalized Henderson's likelihood approach for model fitting and design a rejection-controlled EM algorithm for the estimation. Our method selects smoothing parameters through generalized cross-validation. Furthermore, Bayesian confidence intervals are used to measure the clustering uncertainty. Simulation studies and real-data examples are presented to investigate the empirical performance of the proposed method. Open-source code is available in the R package MFDA."
"10.1198/016214508000000021","2008","Penalized normal likelihood and ridge regularization of correlation and covariance matrices","1","High dimensionality causes problems in various areas of statistics. A particular situation that rarely has been considered is the testing of hypotheses about multivariate regression models in which the dimension of the multivariate response is large. In this article a ridge regularization approach is proposed in which either the covariance or the correlation matrix is regularized to ensure nonsingularity irrespective of the dimensionality of the data. It is shown that the proposed approach can be derived through a penalized likelihood approach, which suggests cross-validation of the likelihood function as a natural approach for estimation of the ridge parameter. Useful properties of this likelihood estimator are derived, discussed, and demonstrated by simulation. For a class of test statistics commonly used in multivariate analysis, the proposed regularization approach is compared with some obvious alternative regularization approaches using generalized inverse and data reduction through principal components analysis. Essentially, the approaches considered differ in how they shrink eigenvalues of sample covariance and correlation matrices. This leads to predictable differences in power properties when comparing the use of different regularization approaches, as demonstrated by simulation. The proposed ridge approach has relatively good power compared with the alternatives considered. In particular, a generalized inverse is shown to perform poorly and cannot be recommended in practice. Finally, the proposed approach is used in analysis of data on macroinvertebrate biomasses that have been classified to species."
"10.1198/016214507000001355","2008","Using {SIMEX} for smoothing-parameter choice in errors-in-variables problems","4","SIMEX methods are attractive for solving curve estimation problems in errors-in-variables regression, using parametric or semiparametric techniques. However, nonparametric approaches are generally of quite a different type, being based on, for example, kernels, local-linear modeling, ridging, orthogonal series, or splines. All of these techniques involve the challenging (and not well studied) issue of empirical smoothing parameter choice. We show that SIMEX can be used effectively for selecting smoothing parameters when applying nonparametric methods to errors-in-variable regression. In particular, we suggest an approach based on multiple error-inflated (or remeasured) data sets and extrapolation."
"10.1198/016214507000000446","2008","Goodness of fit of social network models","1","We present a systematic examination of a real network data set using maximum likelihood estimation for exponential random graph models as well as new procedures to evaluate how well the models fit the observed networks. These procedures compare structural statistics of the observed network with the corresponding statistics on networks simulated from the fitted model. We apply this approach to the study of friendship relations among high school students from the National Longitudinal Study of Adolescent Health (AddHealth). We focus primarily on one particular network of 205 nodes, although we also demonstrate that this method may be applied to the largest network in the AddHealth study, with 2,209 nodes. We argue that several well-studied models in the networks literature do not fit these data well and demonstrate that the fit improves dramatically when the models include the recently developed geometrically weighted edgewise shared partner, geometrically weighted dyadic shared partner, and geometrically weighted degree network statistics. We conclude that these models capture aspects of the social structure of adolescent friendship relations not represented by previous models."
"10.1198/016214508000000030","2008","On {S}tudent's 1908 article ``{T}he probable error of a mean''","0","This month marks the 100th anniversary of the appearance of William Sealey Gusset's celebrated article, ""The Probable Error of a Mean"" (Student 1908a). Gosset's elegant result represented the first in a series of exact, ""small-sample"" results that were developed by Gusset, Fisher, and others to form a central component of the modern theory of statistical inference. This review celebrates the centenary of Gosset's article by discussing both its background and its impact on statistical theory and practice."
"10.1198/jasa.2009.tm09170","2009","Conditional quantile estimation for generalized autoregressive conditional heteroscedasticity models","0","Conditional quantile estimation is an essential ingredient in modem risk management. Although generalized autoregressive conditional heteroscedasticity (GARCH) processes have proven highly successful in modeling financial data, it is generally recognized that it would be useful to consider a broader class of processes capable of representing more flexibly both asymmetry and tail behavior of conditional returns distributions. In this article we study estimation of conditional quantiles for GARCH models using quantile regression. Quantile regression estimation of GARCH models is highly nonlinear; we propose a simple and effective two-step approach of quantile regression estimation for linear GARCH time series. In the first step, we use a quantile autoregression sieve approximation for the GARCH model by combining information over different quantiles. Then second-stage estimation for the GARCH model is carried out based on the firststage minimum distance estimation of the scale process of the time series. Asymptotic properties of the sieve approximation, the minimum distance estimators, and the final quantile regression estimators using generated regressors are studied. These results are of independent interest and have applications in other quantile regression settings. Monte Carlo and empirical application results indicate that the proposed estimation methods outperform some existing conditional quantile estimation methods."
"10.1198/jasa.2009.tm08603","2009","What are the limits of posterior distributions arising from nonidentified models and why should we care?","0","In health research and other fields. the observational data available to researchers often fall short of the data that ideally would be available, due to the inherent limitations of study design and data acquisition. Were they available, these ideal data might be readily analyzed via straightforward statistical models with such desirable properties as parameter identifiability. Conversely, realistic models for the available data that incorporate uncertainty about the link between ideal and available data may be nonidentified. While there is no conceptual difficulty in implementing Bayesian analysis with nonidentified models and proper prior distributions, it is important to know to what extent data can be informative about parameters of interest. Determining the large-sample limit of the posterior distribution is one way to characterize the informativeness of data. In some nonidentified models, it is relatively straightforward to determine the limit via a particular reparameterization of the model; however, in other nonidentified models there is no such obvious approach. Thus we have developed an algorithm for determining the limiting posterior distribution for at least some such more difficult models. The work is motivated by two specific nonidentified models that arise quite naturally, and the algorithm is applied to reveal how informative the data are for these models. This article has supplementary material online."
"10.1198/jasa.2009.tm08647","2009","{$p$}-values for high-dimensional regression","1","Assigning significance in high-dimensional regression is challenging. Most computationally efficient selection algorithms cannot guard against inclusion of noise variables. Asymptotically valid p-values are not available. An exception is a recent proposal by Wasserman and Roeder that splits the data into two parts. The number of variables is then reduced to a manageable size using the first split, while classical variable selection techniques can be applied to the remaining variables, using the data from the second split. This yields asymptotic error control under minimal conditions. This involves a one-time random split of the data, however. Results are sensitive to this arbitrary choice, which amounts to a ""p-value lottery"" and makes it difficult to reproduce results. Here we show that inference across multiple random splits can be aggregated while maintaining asymptotic control over the inclusion of noise variables. We show that the resulting p-values can be used for control of both family-wise error and false discovery rate. In addition, the proposed aggregation is shown to improve power while reducing the number of falsely selected variables substantially."
"10.1198/jasa.2009.tm08538","2009","Testing for efficacy in primary and secondary endpoints by partitioning decision paths","0","Testing for efficacy in multiple endpoints has emerged as an important statistical problem. The Food and Drug Administration (FDA) will issue a guidance on Multiple Endpoints in the near future.When there are primary and secondary endpoints, efficacy in the secondary endpoint is relevant only if efficacy in the primary endpoint has been shown. Thus, there are defined paths to decision making. Current approaches to this problem are based on closed testing, testing all possible intersection hypotheses, and collating the results. For decision making to follow predefined paths, strategic choices of test statistics and critical values must be made. As the number of doses and endpoints increase, such strategic choices become increasingly difficult.Partition testing is an alternative to closed testing. It provides insight into confidence sets for stepwise tests, and can be more powerful than closed testing. It also can simplify problem formulation when decision making follows specific paths. For the primary-secondary endpoints problem, we show that partition testing has advantages. When used to implement what we call the ""decision path principle,"" partition testing not only drastically reduces the number of hypotheses to be tested, but also guides decision making along predefined paths. With Our way of setting critical values, it has higher probabilities than gatekeeping methods of correctly inferring efficacious primary endpoints as being efficacious, while maintaining the same level of strong FWER control. These advantages are illustrated with a real data example and by simulation."
"10.1198/jasa.2009.tm08302","2009","Nonparametric {B}ayes conditional distribution modeling with variable selection","4","This article considers a methodology for flexibly characterizing the relationship between a response and multiple predictors. Goals are (1) to estimate the conditional response distribution addressing the distributional changes across the predictor space, and (2) to identify important predictors for the response distribution change both within local regions and globally. We first introduce the probit stick-breaking process (PSBP) as a prior for an uncountable collection of predictor-dependent random distributions and propose a PSBP mixture (PSBPM) of normal regressions for modeling the conditional distributions. A global variable selection structure is incorporated to discard unimportant predictors, while allowing estimation of posterior inclusion probabilities. Local variable selection is conducted relying on the conditional distribution estimates at different predictor points. An efficient stochastic search sampling algorithm is proposed for posterior computation. The methods are illustrated through simulation and applied to an epidemiologic study."
"10.1198/jasa.2009.tm08024","2009","The analysis of two-way functional data using two-way regularized singular value decompositions","0","Two-way functional data consist of a data matrix whose row and column domains are both structured, for example, temporally or spatially, as when the data are time series collected at different locations in space. We extend one-way functional principal component analysis (PCA) to two-way functional data by introducing regularization of both left and right singular vectors in the singular value decomposition (SVD) of the data matrix. We focus oil a penalization approach and solve the nontrivial problem of constructing proper two-way penalties from one-way regression penalties. We introduce conditional cross-validated smoothing parameter selection whereby left-singular vectors are cross-validated conditional on right-singular vectors, and vice versa. The concept can be realized as part of an alternating optimization algorithm. In addition to the penalization approach, we briefly consider two-way regularization with basis expansion. The proposed methods are illustrated with one simulated and two real data examples. Supplemental materials available online show that several ""natural"" approaches to penalized SVDs are flawed and explain why so."
"10.1198/jasa.2009.tm08128","2009","Multivariate statistical process control using {LASSO}","0","This article develops a new multivariate statistical process control (SPC) methodology based on adapting the LASSO variable selection method to the SPC problem. The LASSO method has the sparsity property of being able to select exactly the set of nonzero regression coefficients in multivariate regression modeling, which is especially useful in cases where the number of nonzero coefficients is small. In multivariate SPC applications, process mean vectors often shift in a small number of components. Our primary goals are to detect such a shift as soon as it occurs and to identify the shifted mean components. Using this connection between the two problems, we propose a LASSO-based multivariate test statistic, and then integrate this statistic into the multivariate EWMA charting scheme for Phase II multivariate process monitoring. We show that this approach balances protection against various shift levels and shift directions, and thus provides an effective tool for multivariate SPC applications. This article has supplementary material online."
"10.1198/jasa.2009.tm08287","2009","Efficient global approximation of generalized nonlinear {$\ell_1$}-regularized solution paths and its applications","0","We consider efficient construction of nonlinear solution paths for general l(1)-regularization. Unlike the existing methods that incrementally build the solution path through a combination of local linear approximation and recalibration, we propose an efficient global approximation to the whole solution path. With the loss function approximated by a quadratic spline, we show that the solution path can be computed using a generalized Lars algorithm. The proposed methodology avoids high-dimensional numerical optimization and thus provides faster and more stable computation. The methodology also can be easily extended to more general regularization framework. We illustrate such flexibility with several examples, including a generalization of the elastic net and a new method that effectively exploits the so-called ""support vectors"" in kernel logistic regression."
"10.1198/jasa.2009.tm08564","2009","Generalized multilevel functional regression","2","We introduce Generalized Multilevel Functional Linear Models (GMFLMs), a novel statistical framework for regression models where exposure has a multilevel functional structure. We show that GMFLMs are, in fact, generalized multilevel mixed models. Thus, GMFLMs can be analyzed using the mixed effects inferential machinery and can be generalized within a well-researched statistical framework. We propose and compare two methods for inference: (1) a two-stage frequentist approach: and (2) a joint Bayesian analysis. Our methods are motivated by and applied to the Sleep Heart Health Study, the largest community cohort study of sleep. However, our methods are general and easy to apply to a wide spectrum of emerging biological and medical datasets. Supplemental materials for this article are available online."
"10.1198/jasa.2009.tm08273","2009","Maximum likelihood estimation of the multivariate normal mixture model","0","The Hessian of the multivariate normal mixture model is derived, and estimators of the information matrix are obtained, thus enabling consistent estimation of all parameters and their precisions. The usefulness of the new theory is illustrated with two examples and some simulation experiments. The newly proposed estimators appear to be superior to the existing ones."
"10.1198/jasa.2008.tm08516","2009","Forward regression for ultra-high dimensional variable screening","8","Motivated by the seminal theory of Sure Independence Screening (Fan and Lv 2008, SIS), we investigate here another popular and classical variable screening method, namely, forward regression (FR). Our theoretical analysis reveals that FR can identify all relevant predictors consistently, even if the predictor dimension is substantially larger than the sample size. In particular, if the dimension of the true model is finite, FR can discover all relevant predictors within a finite number of steps. To practically select the ""best"" candidate from the models generated by FR, the recently proposed BIC criterion of Chen and Chen (2008) can be used. The resulting model can then serve as an excellent starting point, from where many existing variable selection methods (e.g., SCAD and Adaptive LASSO) can be applied directly. FR's outstanding finite sample performances are confirmed by extensive numerical studies."
"10.1198/jasa.2009.tm08368","2009","Sequential implementation of {M}onte {C}arlo tests with uniformly bounded resampling risk","0","This paper introduces an open-ended sequential algorithm for computing the p-value of a test using Monte Carlo simulation. It guarantees that the resampling risk, the probability of a different decision than the one based on the theoretical p-value, is uniformly bounded by an arbitrarily small constant. Previously suggested sequential or nonsequential algorithms, using a bounded sample size, do not have this property. Although the algorithm is open-ended, the expected number of steps is finite, except when the p-value is on the threshold between rejecting and not rejecting. The algorithm is suitable as standard for implementing tests that require (re)sampling. It can also be used in other situations: to check whether a test is conservative, iteratively to implement double bootstrap tests, and to determine the sample size required for a certain power. An R-package implementing the sequential algorithm is available online."
"10.1198/jasa.2009.tm08163","2009","Empirical likelihood in missing data problems","1","Missing data is a ubiquitous problem in medical and social sciences. It is well known that inferences based only on the complete data may not only lose efficiency, but may also lead to biased results if the data is not missing completely at random (MCAR). The inverse-probability weighting method proposed by Horvitz and Thompson (1952) is a popular alternative when the data is not MCAR. The Horvitz-Thompson method, however, is sensitive to the inverse weights and may suffer from loss of efficiency. In this paper, we propose a unified empirical likelihood approach to missing data problems and explore the use of empirical likelihood to effectively combine unbiased estimating equations when the number of estimating equations is greater than the number of unknown parameters. One important feature of this approach is the separation of the complete data unbiased estimating equations from the incomplete data unbiased estimating equations. The proposed method can achieve semiparametric efficiency if the probability of missingness is correctly specified. Simulation results show that the proposed method has better finite sample performance than its competitors. Supplemental materials for this paper, including proofs of the main theoretical results and the R code used for the NHANES example, are available online on the journal website."
"10.1198/jasa.2009.tm08541","2009","On nonparametric variance estimation for second-order statistics of inhomogeneous spatial point processes with a known parametric intensity form","0","We introduce new variance estimation procedures for second-order statistics that are computed from a single realization of intensity reweighted stationary spatial point processes. The statistics are defined either on a subset B of the observation window or on the whole window. For the former, we use subblocks that have the same size and shape as B as ""replicates"" of B in order to estimate the target variance. For the latter, we develop a subsampling estimator for a key component in the target variance and estimate its other components by method-of-moment methods. Under some suitable conditions, we prove that the proposed variance estimators are consistent for the target variances in both cases. Simulations and an application to a real data example are used to demonstrate the usefulness of the proposed methods. This article has supplementary material online."
"10.1198/jasa.2009.tm08393","2009","Learn from thy neighbor: parallel-chain and regional adaptive {MCMC}","0","Starting with the seminal paper of Haario, Saksman, and Tamminen (Haario, Saksman, and Tamminen 2001), a substantial amount of work has been done to validate adaptive Markov chain Monte Carlo algorithms. In this paper we focus on two practical aspects of adaptive Metropolis samplers. First, we draw attention to the deficient performance of standard adaptation when the target distribution is multimodal. We propose a parallel chain adaptation strategy that incorporates multiple Markov chains which are run in parallel. Second, we note that the current adaptive MCMC paradigm implicitly assumes that the adaptation is uniformly efficient on all regions of the state space. However, in many practical instances, different ""optimal"" kernels are needed in different regions of the state space. We propose here a regional adaptation algorithm in which we account for possible errors made in defining the adaptation regions. This corresponds to the more realistic case in which one does not know exactly the optimal regions for adaptation. The methods focus on the random walk Metropolis sampling algorithm but their scope is much wider. We provide theoretical justification for the two adaptive approaches using the existent theory build for adaptive Markov chain Monte Carlo. We illustrate the performance of the methods using simulations and analyze a mixture model for real data using an algorithm that combines the two approaches."
"10.1198/jasa.2009.tm08332","2009","A factor model approach to multiple testing under dependence","1","The impact of dependence between individual test statistics is currently among the most discussed topics in the multiple testing of high-dimensional data literature, especially since Benjamini and Hochberg (1995) introduced the false discovery rate (FDR). Many papers have first focused on the impact of dependence on the control of the FDR. Sonic more recent works have investigated approaches that account for common information shared by all the variables to stabilize the distribution of the error rates. Similarly, we propose to model this sharing of information by a factor analysis structure for the conditional variance of the test statistics. It is shown that the variance of the number of false discoveries increases along with the fraction of common variance. Test statistics for general linear contrasts are deduced, taking advantage of the common factor structure to reduce the variance of the error rates, A conditional FDR estimate is proposed and the overall performance of multiple testing procedure is shown to be markedly improved, regarding the nondiscovery rate, with respect to classical procedures. The present methodology is also assessed by comparison with leading multiple testing methods."
"10.1198/jasa.2009.tm08470","2009","Amplification of sensitivity analysis in matched observational studies","5","A sensitivity analysis displays the increase in uncertainty that attends an inference when a key assumption is relaxed. In matched observational studies of treatment effects, a key assumption in some analyses is that subjects matched for observed covariates are comparable, and this assumption is relaxed by positing a relevant covariate that was not observed and not controlled by matching. What properties would such an unobserved covariate need to have to materially alter the inference about treatment effects? For ease of calculation and reporting, it is convenient that the sensitivity analysis be of low dimension, perhaps indexed by a scalar sensitivity parameter, but for interpretation in specific contexts, a higher dimensional analysis may be of greater relevance. An amplification of a sensitivity analysis is defined as a map from each point in a low-dimensional sensitivity analysis to a set of points, perhaps a ""curve,"" in a higher dimensional sensitivity analysis such that the possible inferences are the same for all points in the set. Possessing an amplification, an investigator may calculate and report the low-dimensional analysis, yet have available the interpretations of the higher dimensional analysis."
"10.1198/jasa.2009.ap08527","2009","Semiparametric efficient estimation for incomplete longitudinal binary data, with application to smoking trends","0","Parametric option pricing models are widely used in finance. These models capture several features of asset price dynamics; however, their pricing performance can be significantly enhanced when they are combined with nonparametric learning approaches that learn and correct empirically the pricing errors. In this article we propose a new nonparametric method for pricing derivatives assets. Our method relies on the state price distribution instead of the state price density, because the former is easier to estimate nonparametrically than the latter. A parametric model is used as an initial estimate of the state price distribution. Then the pricing errors induced by the parametric model are fitted nonparametrically. This model-guided method, called automatic correction of errors (ACE), estimates the state price distribution nonparametrically. The method is easy to implement and can be combined with any model-based pricing formula to correct the systematic biases of pricing errors. We also develop a nonparametric test based on the generalized likelihood ratio to document the efficacy of the ACE method. Empirical studies based on S&P 500 index options show that our method outperforms several competing pricing models in terms of predictive and hedging abilities."
"10.1198/jasa.2009.ap07466","2009","Bayesian calibration of microsimulation models","0","Microsimulation models that describe disease processes synthesize information from multiple sources and can be used to estimate the effects of screening and treatment on cancer incidence and mortality at a population level. These models are characterized by simulation of individual event histories for an idealized population of interest. Microsimulation models are complex and invariably include parameters that are not well informed by existing data. Therefore, a key component of model development is the choice of parameter values. Microsimulation model parameter values are selected to reproduce expected or known results though the process of model calibration. Calibration may be done by perturbing model parameters one at a time or by using a search algorithm. As an alternative, we propose a Bayesian method to calibrate microsimulation models that uses Markov chain Monte Carlo. We show that this approach converges to the target distribution and use a simulation study to demonstrate its finite-sample performance. Although computationally intensive, this approach has several advantages over previously proposed methods, including the use of statistical criteria to select parameter values, simultaneous calibration of multiple parameters to multiple data sources, incorporation of information via prior distributions, description of parameter identifiability, and the ability to obtain interval estimates of model parameters. We develop a microsimulation model for colorectal cancer and use our proposed method to calibrate model parameters. The microsimulation model provides a good fit to the calibration data. We find evidence that some parameters are identified primarily through prior distributions. Our results underscore the need to incorporate multiple sources of variability (i.e., due to calibration data, unknown parameters, and estimated parameters and predicted values) when calibrating and applying microsimulation models."
"10.1198/jasa.2009.ap08029","2009","Analysis of multifactor affine yield curve models","1","In finance and economics much work has been done on the theoretical modeling and statistical estimation of the yield curve, defined as the relationship between -1/tau logp(t)(tau) and tau, where p(t)(tau) is the time t price of a zero-coupon bond with payoff I at maturity date t + tau. Of considerable current interest are models of the yield curve in which a collection of observed and latent factors determine the market price of factor risks, the stochastic discount factor, and the arbitrage-free bond prices. The model is particularly interesting from a statistical perspective, because the yields are complicated nonlinear functions of the underlying parameters (e.g., those appearing in the evolution dynamics of the factors and those appearing in the model of the factor risks). This nonlinearity tends to produce a likelihood function that is multimodal. In this article we revisit the question of how such models should be fit from the Bayesian viewpoint. Key aspects of the inferential framework include (a) a prior on the parameters of the model that is motivated by economic considerations, in particular, those involving the slope of the implied yield curve; (b) posterior simulation of the parameters in ways to improve the efficiency of the MCMC output, for example, through sampling of the parameters marginalized over the factors and tailoring of the proposal densities in the Metropolis-Hastings steps using information about the mode and curvature of the current target based on the output of a simulating annealing algorithm; and (c) measures to mitigate numerical instabilities in the fitting through reparameterizations and square root filtering recursions. We apply the techniques to explain the monthly yields on nine U.S. Treasury Bills (with maturities ranging from 1 month to 120 months) over the period January 1986-December 2005. The model contains three factors, one latent and two observed. We also consider the problem of predicting the nine yields for each month of 2006. We show that the (multi-step-ahead) prediction regions properly bracket the actual yields in those months, thus highlighting the practical value of the fitted model."
"10.1198/jasa.2009.ap09575","2009","Rejoinder [MR2750563]","0",""
"10.1198/jasa.2009.ap07611","2009","A {B}ayesian model for cross-study differential gene expression","0","In this article we define a hierarchical Bayesian model for microarray expression data collected from several studies and use it to identify genes that show differential expression between two conditions. Key features include shrinkage across both gene.; and studies, and flexible modeling that allows for interactions between platforms and the estimated effect, as well as concordant and discordant differential expression across studies. We evaluate the performance of our model in a comprehensive Fashion, using both artificial data, and a ""split-study"" validation approach that provides an agnostic assessment of the model's behavior under both the null hypothesis and a realistic alternative. The simulation results from the artificial data demonstrate the advantages of the Bayesian model. Furthermore, the simulations provide guidelines for when the Bayesian model is most likely to be useful. Most notably, in small studies the Bayesian model generally outperforms other methods when evaluated based on several performance measures across a range of simulation parameters, with the differences diminishing for larger sample sizes in the individual Studies. The split-study validation illustrates appropriate shrinkage of the Bayesian model in the absence of platform, sample, and annotation differences that otherwise complicate experimental data analyses. Finally, we fit our model to four breast cancer studies using different technologies (cDNA and Affymetrix) to estimate differential expression in estrogen receptor-positive tumors versus estrogen receptor-negative tumors. Software and data for reproducing our analysis are available publicly."
"10.1198/jasa.2009.0137","2009","Hunting for significance with the false discovery rate","0","When testing a single hypothesis. it is common knowledge that increasing the sample size after nonsignificant results and repeating the hypothesis test several times at unadjusted critical levels inflates tire overall Type I error rate severely. In contrast, if a large number of hypotheses are tested controlling the False Discovery Rate, such ""hunting for significance"" has asymptotically no impact on the error rate. More specifically. if the sample size is increased for all hypotheses simultaneously and only tire test at the final interim analysis determines which hypotheses are rejected, a data dependent increase or sample size does riot affect tire False Discovery Rate. This holds asymptotically (for an increasing number of hypotheses) for all scenarios but the global null hypothesis where all hypotheses are true. To control the False Discovery Rate also under the global null hypothesis, we consider stopping rules where stopping before a predefined maximum sample size is reached is possible only if sufficiently many null hypotheses can he rejected. The procedure is illustrated with several datasets from microarray experiments."
"10.1198/jasa.2009.0107","2009","A multivariate extension of the dynamic logit model for longitudinal data based on a latent {M}arkov heterogeneity structure","0","For the analysis of multivariate categorical longitudinal data, we propose an extension of the dynamic logit model. The resulting model is based oil a marginal parameterization of the conditional distribution of each vector of response variables given the covariates, the lagged response variables. and a set of subject-specific parameters for the unobserved heterogeneity. The latter ones are assumed to follow a first-order Markov chain. For the maximum likelihood estimation of the model parameters, we outline an EM algorithm. The data analysis approach based on the Proposed model is illustrated by a simulation study and in application to a dataset. which derives front the Panel Study on Income Dynamics and concerns fertility and female participation to the labor market."
"10.1198/jasa.2009.0237","2009","Bayesian mixture labeling by highest posterior density","0","A fundamental problem for Bayesian mixture model analysis is label switching, which occurs as a result of the nonidentifiability of the mixture components under symmetric priors. We propose two labeling methods to solve this problem. The first method, denoted by PM(ALG), is based on the posterior modes and an ascending algorithm generically denoted ALG. We use each Markov chain Monte Carlo sample as the starting point in an ascending algorithm, and label the sample based on the mode of the posterior to which it converges. Our natural assumption here is that the samples converged to the same mode should have the same labels. The PM(ALG) labeling method has some computational advantages over other popular labeling methods. Additionally, it automatically matches the ""ideal"" labels in the highest posterior density credible regions. The second method does labeling by maximizing the normal likelihood of the labeled Gibbs samples. Using a Monte Carlo simulation study and a real dataset, we demonstrate the success of our new methods in dealing with the label switching problem."
"10.1198/jasa.2009.0158","2009","Rejoinder [MR2751451; MR2751450; MR2751449; MR2751448]","0",""
"10.1198/jasa.2009.0121","2009","On consistency and sparsity for principal components analysis in high dimensions","10","Principal components analysis (PCA) is a classic method for the reduction of dimensionality of data in the form of n observations (or cases) of a vector with p variables. Contemporary datasets often have p comparable with or even much larger than n. Our main assertions, in such settings, are (a) that some initial reduction in dimensionality is desirable before applying any PCA-type search for principal modes, and (b) the initial reduction in dimensionality is best achieved by working in a basis in which the signals have a sparse representation. We describe a simple asymptotic model in which the estimate of the leading, principal component vector via standard PCA is consistent if and only if p(n)/n -> 0. We provide a simple algorithm for selecting it subset of coordinates with largest sample variances, and show that if PCA is done on the selected subset, then consistency is recovered, even if p(n) >> n."
"10.1198/jasa.2009.0042","2009","Intervention and causality: forecasting traffic flows using a dynamic {B}ayesian network","0","Real-time traffic flow data across entire networks can be used in a traffic management system to monitor current traffic flows so that traffic can be directed and managed efficient. Reliable short-term forecasting models of traffic flows are crucial for the success of any traffic management system. The model proposed in this article for forecasting traffic flows is a multivariate Bayesian dynamic model called the multiregression dynamic model (MDM). This model is an example of a dynamic Bayesian network and is designed to preserve the conditional independences and causal drive exhibited by the traffic flow series. Sudden changes can occur in traffic flow series in response to such events as traffic accidents or roadworks. A traffic management system is particularly useful at such times of change. To ensure that the associated forecasting model continues to produce reliable forecasts, despite the changes the MDM uses the technique of external intervention. This article will demonstrate how intervention works in the MDM and how it can improve forecast performance at times of change. External intervention has also been used in the context of Bayesian networks to identify causal relationships between variables, and in dynamic Bayesian networks to identify lagged causal relationships between time series. This article goes beyond the identification of lagged causal relationships previously addressed using intervention in dynamic Bayesian networks, to show how intervention in the MDM can be used to identify contemporaneous causal relationships between time series."
"10.1198/jasa.2009.0034","2009","Doubly robust internal benchmarking and false discovery rates for detecting racial bias in police stops","0","Allegations of racially biased policing are a contentious issue in many communities. Processes that flag potential problem officers have become a key component of risk management systems at major police departments. We present a statistical method to potential problem officers by blending three methodologies that are the focus of active research effort: propensity score weighting, doubly robust estimation, and false discovery rates. Compared with other systems currently in use, the proposed method reduces the risk of flagging a substantial number of false positives by more rigorously adjusting for potential for potential confounders and by using the false discovery rate as a measure to flag officers. We apply the methodology to data on 500,000 pedestrian stops in New York City in 2006. Of the nearly 3,000 New York City Police Department officers regularly involved in pedestrian stops, we flag 15 officers who stopped a substantially greater fraction of black and Hispanic suspects than our statistical benchmark predicts."
"10.1198/jasa.2009.0135","2009","Sequential design for microarray experiments","0","A critical aspect in the design of microarray studies is the determination of the sample size necessary to declare genes differentially expressed across different experimental conditions. In this article. we propose a sequential approach where the decision to stop the experiment depends on the accumulated microarray data. The study could stop whenever sufficient data have been accumulated to identify gene expression changes across several experimental conditions. The gene expression response is modeled by a robust linear regression model. We then construct a sequential confidence interval for the intercept of this model, which represents the median gene expression at a given experimental condition. We derive the stopping rule of the experiment for both continuous and discrete sequential approaches and give the asymptotic properties of the stopping variable. We demonstrate the desirable properties of our sequential approach, both theoretically and numerically. In our application to a study of hormone responsive breast cancer cell lines, we estimated the stopping variable for the sample size determination to be smaller than the actual sample size available to conduct the experiment. This means that we call obtain all accurate assessment of differential gene expression without compromising, the cost and size of the study. Altogether, we anticipate that this approach could have ail important contribution to microarray Studies by improving the usual experimental designs and methods of analysis."
"10.1198/jasa.2009.0030","2009","Intrinsically autoregressive spatiotemporal models with application to aggregated birth outcomes","0","A class of hierarchical Bayesian models is proposed for adverse birth outcomes such as preterm birth, which are conditional binomial distribution. The log-odds of an adverse outcome in a particular county, logit(p(i)), follow a linear model that includes observed covariates and normally-distributed random effects. Spatial dependence between neighboring regions is allowed for by including an intrinsically autoregressive (IAR) prior or air IAR convolution prior in the linear predictor. Temporal dependence is incorporated by including a temporal IAR term also. It is shown that the variance parameters underlying these random effects (IAR, convolution, convolution plus temporal IAR) are identifiable. The Deviance Information Criterion (DIC) is considered as a way to compare spatial hierarchical models. Simulations are performed to test whether the DIC call identify whether binomial outcomes come front a hierarchical model that includes different combinations of random and fixed effects. Having validated the DIC as a means of comparing models, we examine preterm birth and low birth weight counts in the state of Arkansas from 1994-2005. We find that preterm birth and low birth weight have different spatial patterns of risk, and that rates of low birth weight can be fit with a relatively simple model that includes a constant spatial effect for all periods. a linear trend. and three covariates (multiple birth. black mother, smoking). It is also found that the risks of each outcome are increasing over time, even with adjustment for covariate."
"10.1198/jasa.2009.0029","2009","Regression models for identifying noise sources in magnetic resonance images","1","Stochastic noise susceptibility artifacts, magnetic field and radiofrequency inhomogeneities, and other noise components in magnetic resonance images (MRIs) call introduce serious bias into any Measurements made with those images. We formally introduce three regression models including a Rician regression model and two associated normal models to characterize stochastic noise in various magnetic resonance imaging modalities, including diffusion-weighted imaging (DWI) and functional MRI (fMRI). Estimation algorithms are introduced to maximize the likelihood function of tire three regression models. We also develop a diagnostic procedure for systematically exploring MR images to identify noise components other than simple stochastic noise, and to detect discrepancies between the fitted regression models and MRI data. The diagnostic procedure includes goodness-of-fit statistics. measures of influence. and tools for graphical display. The goodness-of-fit statistics call assess the key assumptions of the three regression models. whereas measures of influence can isolate outliers caused by certain noise components. including motion artifacts. The tools for graphical display permit graphical visualization of the values for the goodness-of-fit statistic and influence measures. Finally, we conduct simulation studies to evaluate performance of these methods, and we analyze a real dataset to illustrate how our diagnostic procedure localizes subtle image artifacts by detecting intravoxel variability that is not captured by the regression models."
"10.1198/jasa.2009.0026","2009","Mapping ancient forests: {B}ayesian inference for spatio-temporal trends in forest composition using the fossil pollen proxy record","1","Ecologists use the relative abundance of fossil pollen tit sediments to estimate how tree species abundances change over space and time. To predict historical forest composition and quantify the available information, we build a Bayesian hierarchical model of forest composition in central New England, USA. based oil pollen in a network of ponds. The critical relationships between abundances of taxa in the pollen record and abundances as actual vegetation are estimated for the modern and colonial periods. for which both pollen and direct vegetation data are available, based on a latent multivariate spatial process representing forest composition. For time periods in the past with only pollen data, we use the estimated model parameters to constrain predictions about the latent spatio-temporal process conditional on the pollen data. We develop all innovative graphical assessment of feature significance to help to infer which Spatial patterns are reliably estimated. The model allows us to estimate the spatial distribution and relative abundances of tree species over the last 2,500 years, with an assessment of uncertainty, and to draw inference about how these patterns have changed over time. Cross-validation suggests that Our feature significance approach call reliably indicate certain large-scale spatial features for many taxa. but that features on scales smaller than 50km are difficult to distinguish, as are large-scale features for some taxa. We also use the model to quantitatively investigate ecological hypotheses, including covariate effects on taxa abundances and questions about pollen dispersal characteristics. The critical advantages of our modeling approach over current ecological analyses are the explicit spatio-temporal representation. quantification of abundance on the scale of trees rather than pollen, and uncertainty characterization."
"10.1198/jasa.2009.0025","2009","Log-linear models for gene association","0","We describe a class of log-linear models for the detection of interactions in high-dimensional genomic data. This class of models leads to a Bayesian model selection algorithm that can be applied to data that have been reduced to contingency tables using ranks of observations within subjects, and discretization of these ranks within gene/network components. Many normalization issues associated with the analysis of genomic data are thereby avoided. A prior density based on Ewens' sampling distribution is used to restrict the number of interacting components assigned high posterior probability, and the calculation of posterior model probabilities is expedited by approximations based on the likelihood ratio statistic. Simulations studies are used to evaluate the efficiency of the resulting structures. Finally, the algorithm is validated in a microarray study for which it was possible to obtain biological confirmation of detected interactions."
"10.1198/jasa.2009.0024","2009","Density estimation for protein conformation angles using a bivariate von {M}ises distribution and {B}ayesian nonparametrics","1","Interest in predicting protein backbone conformational angles has prompted the development of modeling and inference procedures for bivariate angular distributions. We present it Bayesian approach to density estimation for bivariate angular data that uses a Dirichlet process mixture model and a bivariate von Mises distribution. We derive the necessary full conditional distributions 10 fit the model, as well as the details for sampling from the posterior predictive distribution, We show how our density estimation method makes it possible to improve current approaches for protein structure prediction by comparing the performance of the so-called ""whole"" and ""half"" position distributions. Current methods in the field are based on whole position distributions. its density estimation for (he half positions requires techniques. such as ours, that can provide good estimates for small datasets. With our method we are able to demonstrate that half position data provides it better approximation for the distribution of confirmational angles at it given sequence position, therefore providing increased efficiency and accuracy in structure prediction."
"10.1198/jasa.2009.0023","2009","Modeling hazard rates as functional data for the analysis of cohort lifetables and mortality forecasting","0","As world populations age, the analysis of demographic mortality data and demographic predictions of future mortality have met with increasing interest. The study of mortality patterns and the forecasting of future mortality with its associated impacts on social welfare. health care, and societal planning has become a more pressing issue. An ideal set of data to study Patterns of change in long-term mortality is the well-known historical Swedish cohort mortality data. because of its high quality and long span of more than two Centuries. We explore the use of functional data analysis to model these data and to derive mortality forecasts. Specifically, we address the challenge of flexibly modeling these data while including, the effect of the birth year by regarding log-hazard functions, derived from observed cohort lifetable, as random functions. A functional model for the analysis of these cohort log-hazard functions, extending functional principal component approaches by introducing time-varying eigenfunctions, is found to adequately address these challenges. The associated analysis of the dependency Structure of the cohort log-hazard functions leads to the concept of time-varying principal components of mortality. We then extend this analysis to mortality forecasting, by combining prediction of incompletely, observed log-hazard functions with functional local extrapolation, and demonstrate these functional approaches for the Swedish cohort mortality data."
"10.1198/jasa.2009.0021","2009","Nonparametric residue analysis of dynamic {PET} data with application to cerebral {FDG} studies in normals","0","Kinetic analysis is used to extract metabolic information from dynamic positron emission tomography (PET) uptake data. The theory of indicator dilutions, developed in the seminal work of Meier and Zierler (1954), provides a probabilistic framework for representation of PET tracer uptake data in terms of a convolution between an arterial input function and a tissue residue. The residue is a scaled Survival function associated with tracer residence in the tissue. Nonparametric inference for the residue, a deconvolution problem, provides a novel approach to kinetic analysis-critically one that is not reliant on specific compartmental modeling assumptions. A practical computational technique based on regularized Cubic B-spline approximation of the residence time distribution is proposed. Nonparametric residue analysis allows formal statistical evaluation of specific parametric models to he considered. This analysis needs 10 Property account for the increased flexibility of the nonparametric estimator. The methodology is illustrated using data from a series of cerebral studies with PET and fluorodeoxyglucose (FDG) in normal Subjects. Comparisons are made between key functionals of the residue, tracer flux, flow, etc., resulting from a parametric (the standard two-compartment of Phelps et al. 1979) and a nonparametric analysis. Strong statistical evidence against the compartment model is found. Primarily these differences relate to the representation of the early temporal structure of the tracer residence-largely a function of the vascular supply network. There are convincing physiological arguments against the representations implied by the compartmental approach hut this is the first time that a rigorous statistical confirmation using PET data has been reported. The compartmental analysis produces suspect values for flow but, notably, the impact on the metabolic flux, though statistically significant, is limited to deviations on the order of 3%-4%. The general advantage of the nonparametric residue analysis is the ability to provide a valid kinetic quantitation in the context of studies where there may be heterogeneity or other uncertainty about the accuracy of a compartmental model approximation of the tissue residue."
"10.1198/jasa.2009.0020","2009","Nonparametric signal extraction and measurement error in the analysis of electroencephalographic activity during sleep","3","We introduce methods for signal and associated variability estimation based on hierarchical nonparametric smoothing with application to the Sleep Heart Health Study (SHHS). SHHS is the largest electroencephalographic (EEG) collection of sleep-related data, which contains, at each visit, two quasi-continuous EEG signals for each subject. The signal features extracted from EEG data are then used in second level analyses to investigate the relation between health, behavioral, or biometric outcomes and sleep. Using subject specific signals estimated with known variability in a second level regression becomes a nonstandard measurement error problem. We propose and implement methods that take into account cross-sectional and longitudinal measurement error. The research presented here forms the basis for EEG signal processing for the SHHS."
"10.1198/jasa.2009.0019","2009","Repeated measurements on distinct scales with censoring---a {B}ayesian approach applied to microarray analysis of maize","0","We analyze data collected in a somatic embryogenesis experiment carried out on Zea mays at Iowa state university. The main objective of the Study was to identify the set of genes in maize that actively participate in embryo development. Embryo tissue was sampled and analyzed at various time periods and under different mediums and light conditions. As is the case in many microarray experiments. the operator scanned each Slide multiple times to find the slide-specific 'optimal' laser and sensor settings. The multiple readings of each slide are repeated measurements oil different scales with differing censoring they cannot be considered to be replicate measurements in the traditional sense. Yet it has been shown that the choice of reading can have an impact on genetic inference. We propose a hierarchical modeling approach to estimating gene expression that combines all available readings on each spot and accounts for censoring in the observed values. We assess the statistical properties of the proposed expression estimates using a simulation experiment. As expected, combining all available scans using each with good statistical properties results in expression estimates with noticeably lower bias and root mean squared error relative to other approaches that have been proposed in the literature. Inferences drawn from the somatic embryogenesis experiment, which motivated this work changed drastically when data were analyzed using the standard approaches or using the methodology we propose."
"10.1198/jasa.2009.0017","2009","Random effects models in a meta-analysis of the accuracy of two diagnostic tests without a gold standard","0","In studies of the accuracy of diagnostic tests, it is common that both the diagnostic test itself and the reference test are imperfect. This is the case for the microsatellite instability test, which is routinely used as a prescreening procedure to identify individuals with Lynch syndrome, the most common hereditary colorectal cancer syndrome. The microsatellite instability test is known to have imperfect sensitivity and specificity. Meanwhile, the reference test, mutation analysis, is also imperfect. We evaluate this test via a random effects meta-analysis of 17 studies. Study-specific random effects account for between-study heterogeneity in mutation prevalence, test sensitivities and specificities under a nonlinear mixed effects model and a Bayesian hierarchical model. Using model selection techniques, we explore a range of random effects models to identify a best-fitting, model. We also evaluate sensitivity to the conditional independence assumption between the microsatellite instability test and the Mutation analysis by allowing for correlation between them. Finally. we use simulations to illustrate the importance of including appropriate random effects and the impact of overfitting. underfitting and misfitting on model performance. Our approach can be used to estimate the accuracy of two imperfect diagnostic tests from a meta-analysis of multiple studies or a multicenter study when the prevalence of disease, test sensitivities and/or specificities may be heterogeneous among studies or centers."
"10.1198/jasa.2009.0016","2009","Sensitivity analysis for equivalence and difference in an observational study of neonatal intensive care units","4","In randomized experiments, it is sometimes important to demonstrate that two treatments do not differ greatly in their effects, or to demonstrate that the treatments have very different effects on one outcome similar effects on another outcome. These ""demonstrations"" may take the form of rejecting a null hypothesis of inequivalence in an equivalence test, or rejecting a null hypothesis of equal and inequivalent effects on two outcomes. The procedures often express a complex hypothesis in terms of component hypotheses, and combine the component significance levels to test the complex hypothesis. If used in a randomized trial, randomization provides valid significance levels for each component test, and hence also for the combined procedure. In an observational study-that is, in a study of treatments that were not randomly assigned-there is typically concern that significance levels for testing hypotheses about treatment effects may be distorted by failure to control for some unobserved pretreatment covariate. This concern is raised in the evaluation of virtually all observational studies. The possible impact of such an unobserved covariate is clarified and displayed by a sensitivity analysis that, for various possible magnitudes of potential bias. yields a corresponding interval of possible significance levels. Some observational studies are sensitive to very small unobserved biases, whereas others are insensitive to large biases. Here, existing sensitivity analyses for component hypotheses are used to generate sensitivity analyses for complex hypotheses tested by combining component significance levels. We apply the procedure to our study of the timing of discharges of premature babies from neonatal intensive care units, focusing on the possible impact of delayed discharge on use of unplanned medical care after discharge."
"10.1198/jasa.2009.0015","2009","Inferring optimal peer assignment from experimental data","0","This articles studies the problem of optimally dividing individuals into peer groups to maximize social gains from heterogeneous peer effects. The specific setting analyzed here concerns efficient ways of allocating roommates in college dormitories. Using confidential data on a sample of Dartmouth College freshmen who were randomly assigned to be roommates. the article derives efficient roommate pairing rules, based on demographic and academic background. which maximize different aggregate outcomes. Segregation by precollege academic standing, and by race are seen to minimize mean enrolment into sororities, and maximize mean enrolment into fraternities. Segregation has no effect oil mean and median freshman year grade point average (GPA) but increases the higher and decreases the lower percentiles of the GPA distribution for both men and women. Efficiency loss due to legal constraints on allocations (e.g., race-blindness) is shown to be significant and also larger for women for whom peer effects are more nonlinear. The article develops large-sample inference methods for these optimal solutions and the resulting optimized values by using and extending insights from the mathematical programming, literature. Applicability of these techniques extends beyond linear maximands such as the mean to other important policy objectives such as outcome quantiles, which, though nonlinear, are shown to be quasi-convex in the allocation probabilities."
"10.1198/jasa.2009.0013","2009","Assessing sexual attitudes and behaviors of young women: a joint model with nonlinear time effects, time varying covariates, and dropouts","0","Understanding human sexual behaviors is essential for the effective prevention of sexually transmitted infections (STI). Analysis of longitudinally measured sexual behavioral data, however, is often complicated by zero-inflation of event counts, nonlinear time trend, time-varying covariates, and informative dropouts. Ignoring these complicating factors could undermine the validity of the study findings. In this article, we put forth a unified joint modeling structure that accommodates these features of the data. Specifically, we propose a pair of simultaneous models for the zero-inflated event counts: Each of these models contains an auto-regressive structure for the accommodation of the effect of recent event history, and a nonparametric compenent for the modeling of nonlinear time effect. Informative dropout and time varying covariates are modeled explicitly in the process. Model fitting and parameter estimation are carried out in a Bayesian paradigm by the use of a Markov chain Monte Carlo (MCMC) method. Analytical results showed that adolescent sexual behaviors tended to evolve nonlinearly over time, and they were strongly influenced by the day-to-day variations in mood and sexual interests. These findings suggest that adolescent sex is. to a large extent, driven by intrinsic factors rather than being compelled by circumstances. thus highlighting the need of education on self-protective measures against infection risks."
"10.1198/jasa.2009.0037","2009","A statistical framework to inter functional gene relationships from biologically interrelated microarray experiments","1","A major task in understanding biological processes is to elucidate the relationships between genes involved in the underlying biological pathways. Microarray data from all increasing number of biologically interrelated experiments now allows for more complete portrayals of functional gene relationships in the pathways. In current studies of gene relationships, the presence of expression dependencies attributable to the biologically interrelated experiments, however, has been widely ignored. When unaccounted for, these (experimental) dependencies can result in inaccurate inferences of functional gene relationships, and hence incorrect biological conclusions. This article contributes a framework consisting of a model and an estimation procedure to infer gene relationships when there are two-day dependencies in the gene expression matrix (the gene-wise and experiment-wise dependencies). The main aspect of the framework is the use of a Kronecker product covariance matrix to model the gene-experiment interactions. The resulting novel gene coexpression measure, trained Knorm correlation. can be understood as a natural extension of the widely used Pearson coefficient when the experiment correlation matrix is known. Compared with the Pearson coefficient, the Knorm correlation has a smaller estimation variance. The Knorm is also asymptotically consistent with the Pearson coefficient. When the experiment correlation matrix is unknown, the Knorm correlation is computed based on the estimated experiment correlation matrix by an iterative estimation procedure. We demonstrate the advantages of the Knorm correlation in both simulation studies, and real datasets. The Knorm correlation estimation procedure is implemented in an R package (Knorm) that is freely available from the Bioconductor website."
"10.1198/jasa.2009.0039","2009","Nonparametric priors for ordinal {B}ayesian social science models: specification and estimation","2","A generalized linear mixed model, ordered probit, is used to estimate levels of stress in presidential political appointees as a means of understanding their surprisingly short tenures. A Bayesian approach is developed, where the random effects are modeled with a Dirichlet process mixture prior, allowing for useful incorporation of prior information, but retaining some vagueness in the form of the prior. Applications of Bayesian models in the social sciences are typically done with ""uninformative"" priors, although some use of informed versions exists. There has been disagreement over this, and our approach may be a step in the direction of satisfying both camps. We give a detailed description of the data, show how to implement the model, and describe some interesting conclusions. The model utilizing a nonparametric prior fits better and reveals more information in the data than standard approaches."
"10.1198/jasa.2009.0038","2009","Bayesian analysis of cancer rates from {SEER} program using parametric and semiparametric joinpoint regression models","0","Cancer is the second leading cause of death in the United States. Cancer incidence and mortality rates measured the progress against cancer: these rates are obtained from the surveillance, Epidemiology, and End Results (SEER) Program of the National Cancer Institute (NCI). Lung cancer has the highest mortality rate among all cancers, whereas prostate cancer has the highest number of new cases among males. In this article, we analyze the incidence rates of these two cancers, as well as colon and rectal cancer. The NCI reports trends in cancer age-adjusted mortality and incidence rates in its annual report to the nation and analyzes them using the Joinpoint software. The location of the joinpoints signifies changes in cancer trends, whereas changes in the regression slope measure the degree of change. The Joinpoint software uses a numerical search to detect the joinpoints, fits regression within two consecutive joinpoints by least squares, and finally selects the number of joinpoints by either a series of permutation tests or the Bayesian information criterion. We propose Bayesian joinpoint models and provide statistical estimates of the joinpoints and the regression slopes. While the Joinpoint software and other work in this area assumes that the joinpoints occur on the discrete time grid, we allow a continous prior for the joinpoints induced by the Dirichlet distribution on the spacings in between. This prior further allows the user to impose prespecified minimum Laps in between two consecutive joinpoints. We develop parametric as well as semiparametric Bayesian joinpoint models: the semiparametric framework relaxes parametric distributional assumptions by modeling the distribution of regression slopes and error variances using Dirichlet process mixtures. These Bayesian models provide statistical inference with finite sample validity. Through a simulation study, We demonstrate performance of the proposed parametric and semiparametric Joinpoint models and compare the results with the ones front the Joinpoint software. We analyze age-adjusted cancer incidence rates from the SEER Program using these Bayesian models with different numbers of joinpoints by employing the deviance information criterion and the cross-validated predictive criterion. In addition, we model the lung cancer incidence rates and the smoking rates jointly and explore the relation between the two longitudinal processes."
"10.1198/016214508000000904","2009","Joint models for the association of longitudinal binary and continuous processes with application to a smoking cessation trial","1","Joint models for the association of a logitudinal binary and a longitudinal continuous process are proposed for situations in which their association is of direct interest. The models are parametrized such that the dependence between the two processes is characterized by unconstrained regression coefficients. Bayesian variable selection techniques are used to parsimoniously model these coefficients. A Markov chain Monte Carlo (MCMC) sampling algorithm is developed for sampling from the posterior distribution, using data augmentation steps to handle missing data. Several technical issues are addressed to implement the MCMC algorithm efficiently. The models are motivated by, and are used for, the analysis of a smoking cessation clinical trial in which an important question of interest was the effect of the (exercise) treatment on the relationship between smoking cessation and weight gain."
"10.1198/jasa.2009.tm08106","2009","Assessing robustness of intrinsic tests of independence in two-way contingency tables","1","For testing nested hypotheses from a Bayesian standpoint, a desirable condition is that the prior for the alternative model concentrates mass around the smaller, or null, model. For testing independence in contingency tables, the intrinsic priors satisfy this requirement. Furthermore. the degree of concentration of the priors is controlled by a discrete parameter, t, the training sample size, which plays an important role in the resulting answer. In this article we report on the robustness of the tests of independence for small or moderate sample sizes in contingency tables with respect to intrinsic priors with different degrees of concentration around the null. We compare these tests to frequentist tests and other robust Bayes tests. For large sample sizes, robustness is achieved because the intrinsic Bayesian tests are consistent. Examples using real and simulated data are given. Supplemental materials (technical details and data sets) are available online."
"10.1198/jasa.2009.tm08507","2009","An incomplete-data quasi-likelihood approach to haplotype-based genetic association studies on related individuals","0","We propose an incomplete-data, quasi-likelihood framework for estimation and score tests that accommodates both dependent and partially observed data. The motivation comes from genetic association studies, where we address the problems of estimating haplotype frequencies and testing association between a disease and haplotypes of multiple. tightly linked genetic markers, using case-control samples containing, related individuals. We consider a more general setting in which the complete data are dependent with marginal distributions following a generalized linear model. We form a vector, Z, whose elements are conditional expectations of the elements of the complete-data vector. given selected functions of the incomplete data. Assuming that the covariance matrix of Z is available. we create in optimal linear estimating function based on Z. which we solve by an iterative method. This approach addresses key difficulties in haplotype frequency estimation and testing problems in related individuals: (a) dependence that is known but can be complicated (b) data that are incomplete for structural reasons, as well as possibly missing, with different amounts of information for different observations; (c) the need for computational speed to analyze large numbers of markers; and (d) a well-established null model but an alternative model that is unknown and is difficult to Specify fully in related individuals. For haplotype analysis, we give sufficient conditions for consistency and asymptotic normality of the estimator and asymptotic chi(2) null distribution of the score test. We apply the method to test for association of haplotypes with alcoholism in the GAW 14 COGA data set."
"10.1198/jasa.2009.tm08033","2009","A class of semiparametric mixture cure survival models with dependent censoring","0","Modern cancer treatments have substantially improved cure rates and have generated a great interest in and need for proper statistical tools to analyze survival data with nonnegligible cure fractions. Data with Cure fractions often ire complicated by dependent censoring, and the analysis of this type of data typically involves untestable parametric assumptions on the dependence of the censoring mechanism and the true survival times. Motivated by the analysis of prostate cancer survival trends, we propose a class of semiparametric transformation cure models that allows for dependent censoring without making parametric assumptions on the dependence relationship. The proposed class of models encompasses a number of common models for the latency survival function, including the proportional hazards model and the proportional odds model, and also allows for time-dependent covariates. An inverse censoring probability reweighting scheme is used to derive unbiased estimating equations. Small-sample properties with simulations are derived, and the method is demonstrated with a data application."
"10.1198/jasa.2009.tm08458","2009","Tail index regression","0","In extreme value statistics, the tail index is an important measure to gauge the heavy-tailed behavior of a distribution, Under Pareto-type distributions, we employ the logarithmic function to link the tail index to the linear predictor induced by covariates, which constitutes the tail index regression model. We then propose an approximate log-likelihood function to obtain regression parameter estimators, and Subsequently show the asymptotic normality of those estimators. Numerical studies are presented to illustrate theoretical findings."
"10.1198/jasa.2009.tm08096","2009","Intrinsic regression models for positive-definite matrices with applications to diffusion tensor imaging","1","The aim of this paper is to develop an intrinsic regression model for the analysis of positive-definite matrices its responses in it Riemannian manifold and their association with a set of covariates, such as age and gender, in a Euclidean space, The primary motivation and application of the proposed methodology is in medical imaging. Because the set of positive-definite matrices do not form a vector space, directly applying classical multivariate regression may be inadequate ill establishing the relationship between positive-definite matrices and covariates of interest, such as age and gender, in real applications. Our intrinsic regression model. which is a semiparametric model, uses it link function to map from the Euclidean space of covariates to the Riemannian manifold of positive-definite matrices. We develop an estimation procedure to calculate parameter estimates and establish their limiting distributions. We develop score statistics to test linear hypotheses oil unknown parameters and develop it test procedure based on a resampling method to simultaneously assess the statistical significance of linear hypotheses across a large region of interest. Simulation Studies are used to demonstrate the methodology and examine the finite sample performance of the test procedure for controlling the family-wise error rate. We apply our methods to the detection of statistical significance of diagnostic effects on the integrity of white matter in a diffusion tensor study of human immunodeficiency virus. Supplemental materials for this article are available online."
"10.1198/jasa.2009.tm08614","2009","Analyzing length-biased data with semiparametric transformation and accelerated failure time models","0","Right-censored time-to-event data are often observed from a cohort of prevalent cases that are subject to length-biased sampling. Informative right censoring of data from the prevalent cohort within the population often makes it difficult to model risk factors on the unbiased failure times for the general population. because the observed failure times are length biased. In this paper. we consider two classes of flexible semiparametric models: the transformation models and the accelerated failure time models, to assess covariate effects on the population failure times by modeling the length-biased times. We develop unbiased estimating equation approaches to obtain the consistent estimators of the regression coefficients. Large sample properties for the estimators are derived. The methods are confirmed through simulations and illustrated by application to data from a study of a prevalent cohort of dementia patients."
"10.1198/jasa.2009.tm08430","2009","Statistical estimation in generalized multiparameter likelihood models","1","Multiparameter likelihood models (MLMs) with multiple covariates have a wide range of applications: however. they encounter the ""curse of dimensionality"" problem when the dimension of the covariates is large. We develop a generalized multiparameter likelihood model that copes with multiple covariates and adapts to dynamic structural changes well. It includes some popular models, such as the partially linear and varying-coefficient models, as special cases. We present a simple, effective two-step method to estimate both the parametric and the nonparametric components when the model is fixed. The proposed estimator of the parametric component has the n(-1/2) convergence rate, and the estimator of the nonparametric component enjoys an adaptivity property. We suggest a data-driven procedure for selecting the bandwidths. and propose an initial estimator in profile likelihood estimation of the parametric part to ensure stability of the approach in general settings. We further develop an automatic procedure to identify constant parameters in the underlying model. We provide a simulation study and an application to infant mortality data of China to demonstrate the performance of our proposed method."
"10.1198/jasa.2009.tm07494","2009","A semiparametric regression cure model for interval-censored data","0","Motivated by medical Studies in which patients could be Cured of disease but the disease event time may be subject to interval censoring, we present a semiparametric nonmixture cure model for the regression analysis of interval-censored time-to-event data. We develop semiparametric maximum likelihood estimation for the model using the expectation-maximization method for interval-censored data. The maximization step for the baseline function is nonparametric and numerically challenging. We develop an efficient and numerically stable algorithm via modern convex optimization techniques, yielding a self-consistency algorithm for the maximization step. We prove the strong consistency of the maximum likelihood estimators under the Hellinger distance, which is an appropriate metric for the asymptotic property of the estimators for interval-censored data. We assess the performance of the estimators in a simulation study with small to moderate sample sizes. To illustrate the method, we also analyze a real dataset from a medical study for the biochemical recurrence of prostate cancer among patients who have undergone radical prostatectomy. Supplemental materials for the computational algorithm are available online."
"10.1198/jasa.2009.tm07172","2009","Reweighting estimators for {C}ox regression with missing covariates","0","Missingness in covariates is a common problem in survival data. In this article we propose a reweighting method for estimating the regression parameters in the Cox model with missing covariates. We also consider the augmented reweighting method by subtracting the projection term onto the nuisance tangent space. The proposed method provides consistent and asymptotically normally distributed estimators when the missing-data mechanism depends on the outcome variables, its well as on the observed covariates with either monotone or arbitrary nonmonotone missingness patterns. Simulation results indicate that in most Situations, the proposed reweighting estimators are more efficient than the inverse probability weighting estimators for the regression coefficients of the missing covariates and are as efficient its or more efficient than the inverse probability weighting estimators for the regression coefficients of the completely observed covariates. The simple reweighting estimators can be easily implemented in standard statistical packages."
"10.1198/jasa.2009.tm08160","2009","Cox models with smooth functional effect of covariates measured with error","0","We propose, develop, and implement a fully Bayesian inferential approach for the Cox model when the log hazard function contains unknown smooth functions of the variables measured with error. Our approach is to model nonparametrically both the log-baseline hazard and the smooth components of the log-hazard functions using low-rank penalized splines. Careful implementation of the Bayesian inferential machinery is shown to produce remarkably better results than the naive approach. Our methodology was motivated by and applied to the study of progression time to chronic kidney disease as a function of baseline kidney function and applied to the Atherosclerosis Risk in Communities study, a large epidemiological cohort study. This article has supplementary material online."
"10.1198/jasa.2009.tm08198","2009","Nonparametric transition-based tests for jump diffusions","2","We develop a specification test for the transition density of a discretely sampled continuous-time jump-diffusion process, based on a comparison of a nonparametric estimate of the transition density or distribution function with their corresponding parametric counterparts assumed by the null hypothesis. As a special case. our method applies to pure diffusions. We provide a direct comparison of the two densities for an arbitrary specification of the null parametric model using three different discrepancy measures between the null and alternative transition density and distribution functions. We establish the asymptotic null distributions of proposed test statistics and compute their power functions. We investigate the finite-sample properties through simulations and compare them with those of other tests. This article has supplementary material online."
"10.1198/jasa.2009.tm08338","2009","Split samples and design sensitivity in observational studies","2","An observational or nonrandomized study of treatment effects may be biased by failure to control for some relevant covariate that was not measured. The design of an observational study is known to strongly affect its sensitivity to biases from covariates that were not observed. For instance. the choice of an outcome to study, or the decision to combine several outcomes in a test for coherence, can materially affect the sensitivity to unobserved biases. Decisions, that shape the design are, therefore, critically important, but they are also difficult decisions to make in the absence of data. We consider the possibility of randomly splitting the data from an observational study into a smaller planning sample and a larger analysis sample, where the planning sample is used to guide decisions about design. After reviewing the concept of design sensitivity. we evaluate sample splitting in theory, by numerical computation, and by simulation, comparing it to several methods that use all of the data. Sample splitting is remarkably effective, much more so in observational studies than in randomized experiments: splitting 1,000 matched pairs into 100 planning pairs and 900 analysis pairs often materially improves the design sensitivity. An example from genetic toxicology is used to illustrate the method."
"10.1198/jasa.2009.tm08086","2009","Semiparametric estimation methods for panel count data using monotone {$B$}-splines","0","We study semiparametric likelihood-based methods for panel count data with proportional mean model E[N(t)vertical bar Z] = Lambda(0)(t)exp(beta(T)(0)Z), where Z is a vector of covariates and Lambda(0)(t) is the baseline mean function. We propose to estimate Lambda(0)(t) and beta(0) jointly with Lambda(0)(t) approximated by monotone B-splines and to compute the estimators using generalized Rosen algorithm proposed by Jamshidian (2004). We show that the proposed spline-based likelihood estimators of Lambda(0)(t) are consistent with a possibly better than n(1/3) convergence rate if Lambda(0)(t) is sufficiently smooth. The normality of the estimators of beta(0) is also established. Comparisons between the proposed estimators and their alternatives studied in Wellner and Zhang (2007) are made through simulations studies. regarding their finite sample performance and computational complexity. A real example from a bladder tumor clinical trial is used to illustrate the methods."
"10.1198/jasa.2009.tm08213","2009","Testing for the supremacy of a multinomial cell probability","0","Tests for the supremacy of a multinomial cell probability are developed. The tested null hypothesis states that a particular cell of interest is not more probable than all others, Rejection of this null leads to the conclusion that the cell of interest has a strictly greater probability than all other cells. The null hypothesis constrains the multinomial probability vector to a nonconvex region that is a union of closed convex cones. The likelihood ratio test for this problem is derived and shown to be equivalent to an intersection-union test. The least favorable configuration of the multinomial probability vector in the null parameter space is derived. and the limiting null distribution of the test statistic that is stochastically greatest is shown to be a mixture of point mass at zero and a chi-square distribution with a single degree of freedom. Asymptotic and valid finite-sample testing procedures are proposed and examined via a simulation study and the analysis of two datasets. The proposed procedures are extended to test whether the cell with the largest observed frequency is uniquely most probable. An equivalence between a likelihood ratio test for this problem and a Union-intersection test is demonstrated."
"10.1198/jasa.2009.tm08047","2009","The multiset sampler","1","We introduce the multiset sampler (MSS), a new Metropolis-Hastings algorithm for drawing samples from a posterior distribution. The MSS is designed to be effective when the posterior has the feature that the parameters can be divided into two. sets, X, the parameters of interest and Y, the nuisance parameters. We contemplate a sampler that iterates between X move,; and Y moves. We consider the case where either (a) Y is discrete and lives on a finite set or (b) Y is continuous and lives on a bounded set. After presenting some background, we define a multiset and show how to construct a distribution on one. The construction may seem artificial and pointless at first, but several small examples illustrate its value. Finally, we demonstrate the MSS in several realistic examples and compare it with alternatives."
"10.1198/jasa.2009.tm09398","2009","Rejoinder [MR2562002; MR2750230; MR2750231; MR2750232; \refcno 2750233]","0",""
"10.1198/jasa.2009.ap07625","2009","Active learning through sequential design, with applications to detection of money laundering","0","Money laundering is a process designed to conceal the true origin of funds that were originally derived front illegal activities. Because money laundering often involves criminal activities, financial institutions have the responsibility to detect and report it to the appropriate government agencies in a timely manner. But the huge number of transactions occurring each day make detecting money laundering difficult. The usual approach adopted by financial institutions is to extract some summary statistics from the transaction history and conduct a thorough and time-consuming investigation on those suspicious accounts. In this article we propose an active learning through sequential design method for prioritization to improve the process of money laundering detection. The method uses a combination of stochastic approximation and D-optimal designs injudiciously select the accounts for investigation. The sequential nature of the method helps identify the optimal prioritization criterion with minimal time and effort. A case study with real banking data demonstrates the performance of the proposed method. A simulation study shows the method's efficiency and accuracy, as well as its robustness to model assumptions."
"10.1198/jasa.2009.ap08425","2009","Bayesian model averaging continual reassessment method in phase {I} clinical trials","1","The continual reassessment method (CRM) is a popular dose-finding design for phase I clinical trials. This method requires that practitioners prespecify the toxicity probability at each dose. Such prespecification can be arbitrary, and different specifications of toxicity probabilities may lead to very different design properties. To overcome the arbitrariness and further enhance the robustness of the design, we propose using multiple parallel CRM models, each with a different set of prespecified toxicity probabilities. In the Bayesian paradigm. we assign a discrete probability mass to each CRM model as the prior model probability. The posterior probabilities of toxicity can be estimated by the Bayesian model averaging (BMA) approach. Dose escalation or deescalation is determined by comparing the target toxicity rate and the BMA estimates of the (lose toxicity probabilities. We examine the properties of the BMA-CRM approach through extensive simulation studies, and also compare this new method and its variants with the original CRM. The results demonstrate that our BMA-CRM is competitive and robust, and eliminates the arbitrariness of the prespecification of toxicity probabilities,"
"10.1198/jasa.2009.ap08283","2009","Singular value decomposition-based alternative splicing detection","0","Altered alternative splicing has been identified as an important factor in tumorigenesis. The Affymetrix exon tiling array is designed for detecting alternative splicing events in a transcriptome-wide fashion; however. there are currently few analysis tools that have been well Studied for effective detection of alternative splicing events. We propose a new screening procedure based on singular value decomposition (SVD) of the residual matrix from a robust additive model fit to probe selection region (PSR) data. With this approach, we analyze the exon filing array data from a brain cancer Study conducted at the M. D. Anderson Cancer Center, and show that the proposed SVD-based approach is able to better accommodate outlying measures and capitalize on the multidimensional group-by-PSR gene expression profiles for more effective detection of group-specific alternative splicing events. as well as the PSRs most likely associated with the alternative splicing. Lab validation confirmed some of our findings. but the list of candidates detected with Our proposed method may provide a better signpost to guide further investigations."
"10.1198/jasa.2009.ap06623","2009","Predicting vehicle crashworthiness: validation of computer models for functional and hierarchical data","0","The CRASH computer model simulates the effect of a vehicle colliding against different barrier types. If it accurately represents real vehicle crashworthiness, the computer model can be of great value in various aspects of vehicle design, such as the setting of timing of air bag releases. The goal of this study is to address the problem of validating the computer model for such design goals, based on utilizing computer model runs and experimental data from real crashes. This task is complicated by the fact that (i) the output of this model consists of smooth functional data, and (ii) certain types of collision have very limited data. We address problem (iii) by extending existing Gaussian process-based methodology developed for models that produce real-valued Output. and resort to Bayesian hierarchical modeling to attack problem (ii). Additionally, we show how to formally test if the computer model reproduces reality. Supplemental materials for the article are available online."
"10.1198/jasa.2009.ap08423","2009","Joint modeling of self-rated health and changes in physical functioning","0","Self-rated health is an important indicator of future morbidity and mortality. Research has indicated that self-rated health is related to both levels of and changes in physical functioning. But no previous study has jointly modeled longitudinal functional Status and self-rated health trajectories. We propose a joint model for self-rated health and physical functioning that describes the relationship between perceptions of health and the rate of change of physical functioning or disability. Our joint model uses a non homogeneous, Markov process for discrete physical functioning states and connects this to a logistic regression model for ""healthy"" versus ""unhealthy"" self-rated health through parameters of the physical functioning model. We use simulation studies to establish finite-sample properties of our estimators and show that this model is robust to misspecification of the functional form of the relationship between self-rated health and rate of change of physical functioning, We also show that our joint model performs better than an empirical model based on observed changes in functional status. We apply our joint model to data from the Cardiovascular Health Study (CHS), a large multicenter longitudinal study of older adults. Our analysis indicates that self-rated health is associated both with level of functioning. as indicated by difficulty with activities of daily living (ADL) and instrumental activities of daily living (IADL). and with the risk of increasing difficulty with ADLs and IADLs."
"10.1198/jasa.2009.ap07058","2009","Modeling spatiotemporal forest health monitoring data","0","Forest health monitoring schemes were set Lip across Europe in the 1980s in response to concerns about air pollution-related forest dieback (Waldsterben) and have continued since then. Recent threats to forest health are climatic extremes likely due to global climate change and increased ground ozone levels and nitrogen deposition. We model yearly data on tree crown defoliation, an indicator of tree health, from a monitoring survey carried Out in Baden-Wurttemberg, Germany since 1983. On a changing irregular grid, defoliation and other sue-specific variables are recorded, In Baden-Wurttemberg, the temporal trend of defoliation differs among areas because of site characteristics and pollution levels, making it necessary to allow for space-time interaction in the model. For this purpose, we propose using generalized additive mixed models (GAMMs) incorporating scale-invariant tensor product smooths of the space-time dimensions. The space-time smoother allows separate smoothing parameters and penalties for the space and time dimensions and thus avoids the need to make arbitrary or ad hoc choices about the relative scaling of space and time. The approach of using a space-time smoother has intuitive appeal, making it easy to explain and interpret when communicating the results to nonstaticians, such as environmental policy makers. The model incorporates a nonlinear effect for mean tree age. the most important predictor. allowing the separation of trends in time. which may be pollution-related, from trends that relate purely to the aging of the survey population. In addition to a temporal trend due to site characteristics and other conditions modeled with the space-time Smooth, We account for random temporal correlation at site level by an autoregressive moving average (ARMA) process. Model selection is carried Out using the Bayes information criterion (BIC). and the adequacy of the assumed spatial and temporal error Structure is investigated with the empirical semivariogram and the empirical autocorrelation function."
"10.1198/jasa.2009.ap07613","2009","Weighted normal spatial scan statistic for heterogeneous population data","1","In geographical spatial epidemiology and disease surveillance, all the existing spatial scan methods for cluster detection using continuous data are designed for evaluating clusters of individuals and analyzing individual-level data. Motivated by growing demands to study the spatial heterogeneity of continuous measures in population data, such as mortality rates, survival rates, average body mass indexes and pollution at state, county, and census tract levels. we propose a weighted normal scan statistic for investigating the clusters of the cells (geographic units such as counties) with unusual high/low continuous regional measures, where the weights reflect the uncertainty of the regional measures or sample size (number of observed cases) in the cells. Power, precision, the effect of the weights, and the sensitivity of the proposed test statistic to data from various distributions are investigated through intensive simulation. The method is applied to 1988-2002 stage I and II lung cancer survival data in Los Angeles County in order to search for clusters of geographic units with high/low survival rates in a short-term/long-term survival after diagnosis, and to 1999-2003 breast cancer age-adjusted mortality rate data in the U.S. collected by the Surveillance, Epidemiology and End Results (SEER) program in order to evaluate the clustering pattern of counties with high mortality rate, The proposed method is included in the latest release of the SaTScan software (www.satscan.org)."
"10.1198/jasa.2009.0116","2009","Rank-based estimation and associated inferences for linear models with cluster correlated errors","1","R estimators based on the joint ranks (JR) of all the residuals have been developed over the last 20 years for fitting linear models with independently distributed errors. In this article, we extend these estimators to estimating the fixed effects in a linear model with cluster correlated continuous error distributions for general score functions. We discuss the asymptotic theory of the estimators and standard errors of the estimators. For the related mixed model with a single random effect, we discuss robust estimators of the variance components. These are used to obtain Studentized residuals for the JR fit. A real example is discussed, which illustrates the efficiency of the JR analysis over the traditional analysis and the efficiency of a prudent choice of a score function. Simulation studies over situations similar to the example confirm the validity and efficiency of the analysis."
"10.1198/jasa.2009.0109","2009","Robust response transformations based on optimal prediction","0","Nonlinear regression problems can often be reduced to linearity by transforming the response variable (e.g., using the Box-Cox family of transformations). The classic estimates of the parameter defining the transformation as well as of the regression coefficients are based on the maximum likelihood criterion, assuming homoscedastic normal errors for the transformed response. These estimates are nonrobust in the presence of outliers and can be inconsistent when the errors are nonnormal or heteroscedastic. This article proposes new robust estimates that are consistent and asymptotically normal for any unimodal and homoscedastic error distribution. For this purpose, a robust version of conditional expectation is introduced for which the prediction mean squared error is replaced with an M scale. This concept is then used to develop a nonparametric criterion to estimate the transformation parameter as well as the regression coefficients. A finite sample estimate of this criterion based on a robust version of smearing is also proposed. Monte Carlo experiments show that the new estimates compare favorably with respect to the available competitors."
"10.1198/jasa.2009.0113","2009","Testing dependence among serially correlated multicategory variables","0","The contingency table literature On tests for dependence among discrete multicategory variables is extensive. Standard tests assume, however, that draws are independent and only limited results exist Oil the effect of serial dependency-a problem that is important in areas Such as economics, finance, medical trials, and meteorology. This article proposes new tests of independence based on canonical correlations from dynamically augmented reduced rank regressions. The tests allow for an arbitrary number of categories as well as multiway tables of arbitrary dimension and are robust in the presence of serial dependencies that take the form of finite-order Markov processes. For three-way or higher order tables we propose new tests of joint and marginal independence. Monte Carlo experiments show that the proposed tests have good finite sample properties. An empirical application to microeconomic survey data on firms' forecasts of changes to their production and prices demonstrates the importance of correcting for serial dependencies in predictability tests."
"10.1198/jasa.2009.0125","2009","A {B}ayesian reassessment of nearest-neighbor classification","0","The k-nearest-neighbor (knn) procedure is a well-known deterministic method used in supervised classification. This article proposes a reassessment of this approach as a statistical technique derived from a proper probabilistic model; in particular, we modify the assessment found in Holmes and Adams, and evaluated by Manocha and Girolami, where the underlying probabilistic model is not completely well defined. Once provided with a clear probabilistic basis for the knn procedure, we derive computational tools for Bayesian inference on the parameters of the corresponding model. In particular, we assess the difficulties inherent to both pseudo-likelihood and path sampling approximations of an intractable normalizing constant. We implement a correct MCMC sampler based on perfect sampling. When perfect sampling is not available, we use instead a Gibbs sampling approximation. Illustrations of the performance of the corresponding Bayesian classifier are provided for benchmark datasets, demonstrating in particular the limitations of the pseudo-likelihood approximation in this set up."
"10.1198/jasa.2009.0104","2009","Shrinkage estimators for robust and efficient inference in haplotype-based case-control studies","2","Case-control association studies often aim to investigate the role of genes and gene-environment interactions in terms of the underlying haplotypes (i.e., the combinations of alleles at multiple genetic loci along chromosomal regions). The goal of this article is to develop robust but efficient approaches to the estimation of disease odds-ratio parameters associated with haplotypes and haplotype-environment interactions. We consider ""shrinkage"" estimation techniques that can adaptively relax the model assumptions of Hardy-Weinberg-Equilibrium and gene-environment independence required by recently proposed efficient ""retrospective"" methods. Our proposal involves first development of a novel retrospective approach to the analysis of case-control data, one that is robust to the nature of the gene-environment distribution in the underlying population. Next, it involves shrinkage of the robust retrospective estimator toward a more precise, but model-dependent, retrospective estimator using novel empirical Bayes and penalized regression techniques. Methods for variance estimation are proposed based on asymptotic theories. Simulations and two data examples illustrate both the robustness and efficiency of the proposed methods."
"10.1198/jasa.2009.0012","2009","Likelihood-based analysis of causal effects of job-training programs using principal stratification","2","Government-sponsored job-training programs must be subject to evaluation to assess whether their effectiveness justifies their cost to the public. The evaluation usually focuses on employment and total earnings, although the effect on wages is also of interest, because this effect reflects the increase in human capital due to the training program, whereas the effect on total earnings may be simply reflecting the increased likelihood of employment without any effect on wage rates. Estimating the effects of training programs on wages is complicated by the fact that, even in a randomized experiment, wages are ""truncated"" (or less accurately ""censored"") by nonemployment, that is, they are only observed and well-defined for individuals who are employed. In this article, we develop a likelihood-based approach to estimate the wage effect of the US federally-funded Job Corps training program using ""Principal Stratification"". Our estimands are formulated in terms of: (1) the effect of the training program on wages for those who would be employed whether they were trained or not, also called the survivor average causal effect (SACE), and the proportion of people in this category; (2) the wages when trained for those who would be employed only when trained, and the proportion of people in this category; (3) the wages when not trained for those who would be employed only when not trained, and the proportion of people in this category; (4) the proportion of people who would be not employed whether trained or not. We conduct likelihood-based analysis using the EM algorithm, and investigate the plausibility of important submodels with scaled log-likelihood ratio statistics. We also conduct a sensitivity analysis with respect to specific parametric assumptions. Our results suggest that all four types of people [(1)-(4) previously] exist, which is impossible under the usual monotonicity assumptions made in traditional econometric evaluation methods."
"10.1198/jasa.2009.0011","2009","Estimating response-maximized decision rules with applications to breastfeeding","0","To estimate the sequence of actions that optimizes response in a longitudinal setting, it is important to study the actions as part of a set of decision rules rather than in a series of single-action comparisons. We take as our motivating example the estimation of the set of decision rules for the duration of breastfeeding with a view to maximizing infant growth. Breastfeeding has many well-recognized beneficial effects on health and development. However observational evidence has suggested that breastfeeding is associated with reduced infant growth, although the long-term consequences for stature and adiposity remain controversial. The Promotion of Breastfeeding Intervention Trial (PROBIT) recruited 17,046 women in which hospitals and their affiliated polyclinics in Belarus were randomized to a breastfeeding promotion intervention or to standard care. In this article, we propose Structural Mean Models and estimate their parameters using G-estimation to obtain unbiased estimates of the effect of continued breastfeeding on infant growth (weight or length) at one year of age. We also implement a modified version of the G-estimation algorithm that is asymptotically unbiased; this is the first real-data application of the algorithm. Finally, we compare the decision rules implied by the G-estimates with the decision implied by a myopic optimization estimation approach, that is, we compare with decision rules that maximize response in the short-term. The breastfeeding regimes selected by each of the three models are optimal in the sense that specific criteria were optimized; the criteria considered here (maximizing weight or length) were chosen for simplicity, but may not lead to better overall health. We demonstrate in the context of a breastfeeding promotion intervention trial that optimal myopic decision strategies do not coincide with strategies that optimize a longer-term response."
"10.1198/jasa.2009.0009","2009","Bayesian analysis of isochores","1","Statistical identification of isochore structure, the variation in large-scale GC composition (proportion of DNA bases that are G or C as opposed to A or T), of mammalian genomes is a necessary requirement for understanding both the evolution of base composition and the many genomic features such as mutation and recombination rates, which covary with base composition. We have developed a Bayesian method for isochore analysis that we demonstrate to be more accurate than the commonly used binary segmentation approach implemented within the program IsoFinder. The method accounts for both fine-scale and large-scale structure. We adapt direct simulation methods to allow for iid samples from the posterior distribution of our model, and provide an accurate approximation to this that can analyze data from a chromosome in a matter of seconds. We apply our method to human chromosome 1. The resulting estimate of how GC content varies across this region is shown to be a better predictor of local recombination rates than IsoFinder, and we are able to detect regions consistent with the classic definition of isochores that cover 85% of the chromosome. We also show a measure of relative GC content to be particularly predictive of local recombination rates."
"10.1198/jasa.2009.0008","2009","On the determination of general scientific models with application to asset pricing","0","We consider a consumption-based asset pricing model that uses habit persistence to overcome the known statistical inadequacies of the classical consumption-based asset pricing model. We find that the habit model fits reasonably well and agrees with results reported in the literature if conditional heteroskedasticity is suppressed but that it does not fit nor do result,.; agree if conditional heteroskedasticity, well known to be present in financial market data, is allowed to manifest itself. We also find that it is the preference parameters of the model that are most affected by the presence or absence of conditional heteroskedasticity, especially the risk aversion parameter. The habit model exhibits four characteristics that are often present in models developed from scientific considerations: (1) a likelihood is not available; (2) prior information is available; (3) a portion of the prior information is expressed in terms of functionals of the model that cannot be converted into an analytic prior on model parameters; (4) the model can be simulated. The underpinning Of Our approach is that, in addition, (5) a parametric statistical model for the data, determined without reference to the scientific model, is known. In general one can expect to be able to determine a model that satisfies (5) because very richly parameterized statistical models are easily accommodated. We develop a computationally intensive, generally applicable, Bayesian strategy for estimation and inference for scientific models that meet this description together with methods for assessing model adequacy. An important adjunct to the method is that a map from the parameters of the scientific model to functionals of the scientific and statistical models becomes available. This map is a powerful tool for understanding the properties of the scientific model."
"10.1198/jasa.2009.0007","2009","Bayesian modeling of uncertainty in ensembles of climate models","0","Projections of future climate change caused by increasing greenhouse gases depend critically on numerical climate model, coupling the ocean and atmosphere (global climate models [GCMs]). However, different models differ substantially in their projections, which raises the question of how the different models can best be combined into a probability distribution of future climate change. For this analysis, we have collected both Current and future projected mean temperatures produced by nine climate models for 22 regions of the earth. We also have estimates of current mean temperatures from actual observations, together with standard errors, that can be used to calibrate the climate models. We propose a Bayesian analysis that allows us to combine the different climate models into a posterior distribution of future temperature increase, for each of the 22 regions, while allowing for the different climate models to have different variances. Two versions of the analysis are proposed: a univariate analysis in which each region is analyzed separately, and a multivariate analysis in which the 22 regions are combined into an overall statistical model. A cross-validation approach is proposed to confirm the reasonableness of our Bayesian predictive distributions. The results, of this analysis allow for a quantification of the uncertainty of climate model projections as a Bayesian posterior distribution, substantially extending previous approaches to uncertainty in climate models."
"10.1198/jasa.2009.0006","2009","Estimating the time-varying rate of transmission of {SARS} in {S}ingapore and {H}ong {K}ong under two environments","0","In modeling disease transmission there is much emphasis on how many people are infected but less attention is paid to when the infections occur, and the unrealistic assumption of constant infectiousness is often made. We propose a missing data formulation that enables us to apply the ECME algorithm to estimate a discretized intensity function of an inhomogeneous Poisson process. This approach requires interval-censored data only, but known infection times can be incorporated as well. We apply the proposed method to transmission data on severe respiratory syndrome (SARS) collected in Singapore and Hong Kong. The resulting estimates show that the rate of infection as a function of time may have more than one peak. By fitting a two-environment proportional intensity model to the Singapore data, we estimate that the rate of infection in an (unisolated) hospital environment is almost ten times greater than occurs in a nonhospital environment. This lends support to the theory that the SARS epidemic in Singapore was mainly driven by hospital-acquired infections. Estimates of individual infectivity reveal that three persons commonly regarded as ""superspreaders"" actually do not have unusually high individual infectiousness. The observed superspreading events seem to have been caused by environmental rather than biological factors."
"10.1198/jasa.2009.0005","2009","Bayesian emulation and calibration of a stochastic computer model of mitochondrial {DNA} deletions in substantia nigra neurons","0","This article considers the problem of parameter estimation for a stochastic biological model of mitochondrial DNA population dynamics using experimental data on deletion mutation accumulation. The stochastic model is an attempt to describe the hypothesized link between deletion accumulation and neuronal loss in the substantia nigra region of the human brain. Inference for the parameters of the model is complicated by the fact that the model is both analytically intractable and slow to sample from. We show how the stochastic model can be approximated using a simple parametric statistical model with smoothly varying parameters. These parameters are treated as unknown functions and modeled using Gaussian process priors. Several simplifications of our Bayesian model are implemented to ease the computational burden. Throughout the article, we validate our models using predictive simulations. We demonstrate the validity of our fitted model on an independent dataset of substantia nigra neuron survival."
"10.1198/jasa.2009.0003","2009","Estimating the effect of a time-dependent treatment by levels of an internal time-dependent covariate: application to the contrast between liver wait-list and posttransplant mortality","0","Few health policy issues in the U.S. are scrutinized as intensely as the distribution of organs for transplantation, with much effort currently underway to examine the efficacy of existing organ allocation algorithms. The design of efficient organ a I location policies depends strongly upon accurate estimates of the Survival benefit of transplantation. Many organ allocation policies order patients on the wait-list based on time-dependent health status measures (internal time-dependent covariates), such that priority is given to candidates at the greatest risk for wait-list mortality. It is of great interest to measure the transplant benefit by levels of such health status measures to identify the status levels that represent either futile or unnecessary transplants. In the presence of observational data, the survival benefit of transplantation has, to date, been quantified through the parameter corresponding to a binary time-dependent transplant indicator variable. Parameters from a standard time-dependent analysis using existing methods (i.e., separate transplant indicator for each status level) are difficult to interpret because they apply while the patient is at a particular level; i.e., they account for the patient's current condition, but do not account for the possibility that the patient's condition may worsen. We propose a novel method for estimating the effect of a time-dependent treatment by levels of in internal time-dependent covariate. The method yields parameter estimates which, rather than applying to the patient's current health status, average over future potential changes in health status. The proposed method is applied to end-stage liver disease data obtained from a national organ failure registry."
"10.1198/jasa.2009.0002","2009","A case study in exploratory functional data analysis: geometrical features of the internal carotid artery","0","This pilot study is a product of the AneuRisk Project, a scientific program that aims at evaluating the role of vascular geometry and hemodynamics in the pathogenesis of cerebral aneurysms. By means of functional data analyses, we explore the AneuRisk dataset to highlight the relations between the geometric features of the internal carotid artery, expressed by its radius profile and centerline curvature. and the aneurysm location. After introducing a new similarity index for functional data, we eliminate ancillary variability of vessel radius and curvature profiles through an iterative registration procedure. We then reduce data dimension by means of functional principal components analysis. Last, a quadratic discriminant analysis of functional principal components scores allows us to discriminate patients with aneurysms in different districts."
"10.1198/jasa.2009.0001","2009","Bayesian semiparametric joint models for functional predictors","1","Motivated by the need to understand and predict early pregnancy loss using hormonal indicators of pregnancy health, this article proposes a semiparametric Bayesian approach for assessing the relationship between functional predictors and a response. A multivariate, adaptive spline model is used to describe the functional predictors, and a generalized linear model with a random intercept describes the response. Through specifying the random intercept to follow a Dirichlet process jointly with the random spline coefficients, we obtain a procedure that clusters trajectories according to shape and according to the parameters of the response model for each cluster. This very flexible method allows for the incorporation of covariates in the models for both the response and the trajectory. We apply the method to postovulatory progesterone data from the Early Pregnancy Study and find that the model successfully predicts early pregnancy loss."
"10.1198/jasa.2009.0041","2009","Rejoinder [MR2662306]","0",""
"10.1198/jasa.2009.0018","2009","A spatio-temporal model for mean, anomaly, and trend fields of {N}orth {A}tlantic sea surface temperature","1","We consider the problem of fitting a statistical model to 30 years of sea Surface temperature records collected over a large portion of the Northern Atlantic. The observations were collected sparsely in space and time with different levels of accuracy. The purpose of the model is to produce an atlas of oceanic properties, including climatological mean fields, estimates of historical trends, and a spatio-temporal reconstruction of the anomalies, i.e., the transient deviations from the climatological mean. These products are of interest to climate change and climate variability research, numerical modeling, and remote sensing analyses. Our model improves upon the current tools used by oceanographers in that it constructs instantaneous temperature fields before averaging them into the climatology, thus giving equal weight to all years in the time frame, regardless of the temporal distribution of data. It also accounts for nonisotropic and nonstationary space and time dependencies, owing to its use of discrete process convolutions. Particular attention is given to the handling of massive datasets such as the one under study. This is achieved by considering compact support kernels that allow an efficient parallelization of the Markov chain Monte Carlo method used in the estimation of the model parameters. Resulting monthly climatologics are compared with those of the World Ocean Atlas 2001, version 2. Different water masses appear better separated in our climatology, and a close link emerges between the kernels' shape and the dominating patterns of ocean currents. The subpolar and the temperate North Atlantic display opposite trends, with the former mainly cooling over the years and the latter mainly warming, especially in the Gulf Stream region. Long-term changes in annual cycles are also detected. As in any hierarchical Bayesian model, parameter estimates come with credibility intervals, which are useful to compare results with other approaches and detect areas where sampling campaigns are needed the most."
"10.1198/jasa.2009.0033","2009","Communicating statistics and developing professionals: the 2008 {ASA} presidential address","0",""
"10.1198/jasa.2009.0103","2009","Order selection in finite mixture models with a nonsmooth penalty","0","Order selection is a fundamental and challenging problem in the application of finite mixture models. In this article, we develop a new penalized likelihood approach. The new method, modified smoothly clipped absolute deviation (MSCAD), deviates from information-based methods such as Akaike information criterion (AIC) and Bayesian information criterion (BIC) by introducing two penalty functions that depend on the mixing proportions and the component parameters. It is consistent at estimating both the order of the mixture model and the mixing distribution. Simulations show that MSCAD has much better performance than a number of existing methods. Two real-data examples are examined to illustrate the performance of MSCAD."
"10.1198/jasa.2009.tm09055","2009","Local rank inference for varying coefficient models","1","By allowing the regression coefficients to change with certain covariates, the class of varying coefficient models offers a flexible approach to modeling nonlinearity and interactions between covariates. This article proposes a novel estimation procedure for the varying coefficient models based on local ranks. The new procedure provides a highly efficient and robust alternative to the local linear least squares method. and can be conveniently implemented using existing R software package. Theoretical analysis and numerical simulations both reveal that the gain of the local rank estimator over the local linear least squares estimator, measured by the asymptotic mean squared error or the asymptotic mean integrated squared error, can be substantial. In the normal error case, the asymptotic relative efficiency for estimating both the coefficient functions and the derivative of the coefficient functions is above 96%; even in the worst case scenarios, the asymptotic relative efficiency has a lower bound 88.96% for estimating the coefficient functions, and a lower bound 89.91% for estimating their derivatives. The new estimator may achieve the nonparametric convergence rate even when the local linear least squares method fails due to infinite random error variance. We establish the large sample theory of the proposed procedure by utilizing results from generalized U-statistics, whose kernel function may depend on the sample size. We also extend a resampling approach, which perturbs the objective function repeatedly, to the generalized U-statistics setting, and demonstrate that it can accurately estimate the asymptotic covariance matrix."
"10.1198/jasa.2009.tm08349","2009","Empirical likelihood methods based on characteristic functions with applications to {L}\'evy processes","0","Levy processes have been receiving increasing attention in financial modeling. One distinctive feature of such models is that their characteristic functions are readily available. Inference based on characteristic functions is very useful for studying Levy processes. By incorporating the recent advances in nonparametric approaches, empirical likelihood methods based on characteristic functions are developed in this paper for parameter estimation, testing a particular parametric class including the presence of a jump component in the Levy process and testing for symmetry of a distribution. Simulation and case studies confirm the effectiveness of the proposed method."
"10.1198/jasa.2009.tm08107","2009","Median-based classifiers for high-dimensional data","1","Conventional distance-based classifiers use standard Euclidean distance, and so can suffer from excessive volatility if vector components have heavy-tailed distributions. This difficulty can be alleviated by replacing the L-2 distance by its L-1 counterpart. For example, the L-1 version of the popular centroid classifier would allocate a new data value to the population to whose centroid it was closest in L-1 terms. However, this approach can lead to inconsistency, because the centroid is defined using L-2, rather than L-1, distance. In particular, by mixing L-1 and L-2 approaches, we produce a classifier that can seriously misidentify data in cases where the means and medians of marginal distributions take different values. These difficulties motivate replacing centroids by medians. However, in the very-high-dimensional settings commonly encountered today, this can be problematic if we attempt to work with a conventional spatial median. Therefore, we suggest using componentwise medians to construct a robust classifier that is relatively insensitive to the difficulties caused by heavy-tailed data and entails straightforward computation. We also consider generalizations and extensions of this approach based on, for example, using data truncation to achieve additional robustness. Using both empirical and theoretical arguments, we explore the properties of these methods, and show that the resulting classifiers can be particularly effective. Supplementary materials are available online."
"10.1198/jasa.2009.tm08496","2009","Logistic regression with {B}rownian-like predictors","1","This article introduces a new type of logistic regression model involving functional predictors of binary responses, and provides an extension of this approach to generalized linear models. The predictors are trajectories that have certain sample path properties in common with Brownian motion. Time points are treated as parameters of interest, and confidence intervals are developed tinder prospective and retrospective (case-control) sampling designs. In an application to functional magnetic resonance imaging data, signals from individual subjects are used to find the portion of the time course that is most predictive of the response. This allows the identification of sensitive time points specific to a brain region and associated with a certain task, which can be used to distinguish between responses. A second application concerns gene expression data in a case-control study involving breast cancer, where the aim is to identify genetic loci along a chromosome that best discriminate between cases and controls."
"10.1198/jasa.2009.tm09047","2009","On multivariate runs tests for randomness","0","This paper proposes several extensions of the concept of runs to the multivariate setup, and studies the resulting tests of multivariate randomness against serial dependence. Two types of multivariate runs are defined: (i) an elliptical extension of the spherical runs proposed by Marden (1999), and (ii) an original concept of matrix-valued runs. The resulting runs tests themselves exist in various versions, one of which is a function of the number of data-based hyperplanes separating pairs of observations only. All proposed multivariate runs tests are affine-invariant and highly robust: in particular, they allow for heteroscedasticity and do not require any moment assumption. Their limiting distributions are derived under the null hypothesis and under sequences of local vector ARMA alternatives. Asymptotic relative efficiencies with respect to Gaussian Portmanteau tests are computed, and show that, while Marden-type runs tests suffer severe consistency problems, tests based on matrix-valued runs perform uniformly well for moderate-to-large dimensions. A Monte Carlo study confirms the theoretical results and investigates the robustness properties of the proposed procedures. A real-data example is also treated, and shows that combining both types of runs tests may provide some insight on the reason why rejection occur,, hence that Marden-type runs tests, despite their lack of consistency, also are of interest for practical purposes."
"10.1198/jasa.2009.tm08415","2009","Simultaneous testing of grouped hypotheses: finding needles in multiple haystacks","1","In large-scale multiple testing problems, data are often collected from heterogeneous sources and hypotheses form into groups that exhibit different characteristics. Conventional approaches, including the pooled and separate analyses, fail to efficiently utilize the external grouping information. We develop a compound decision theoretic framework for testing grouped hypotheses and introduce an oracle procedure that minimizes the false nondiscovery rate subject to a constraint on the false discovery rate. It is shown that both the pooled and separate analyses can be uniformly improved by the oracle procedure. We then propose a data-driven procedure that is shown to be asymptotically optimal. Simulation studies show that our procedures enjoy superior performance and yield the most accurate results in comparison with both the pooled and separate procedures. A real-data example with grouped hypotheses is studied in detail using different methods. Both theoretical and numerical results demonstrate that exploiting external information of the sample can greatly improve the efficiency of a multiple testing procedure. The results also provide insights on how the grouping information is incorporated for optimal simultaneous inference."
"10.1198/jasa.2009.tm08228","2009","Competing risks quantile regression","0","Quantile regression has emerged as a significant extension of traditional linear models and its potential in survival applications has recently been recognized. In this paper we study quantile regression with competing risks data, formulating the model based on conditional quantiles defined using the cumulative incidence function, which includes as a special case an analog to the usual accelerated failure time model. The proposed competing risks quantile regression model provides meaningful physical interpretations of covariate effects and, moreover, relaxes the constancy constraint on regression coefficients, thereby providing a useful, perhaps more flexible, alternative to the popular subdistribution proportional hazards model. We derive an unbiased monotone estimating equation for regression parameters in the quantile model. The uniform consistency and weak convergence of the resulting estimators are established across a quantile continuum. We develop inferences, including covariance estimation, second-stage exploration, and model diagnostics, which can be stably implemented using standard statistical software without involving smoothing or resampling. Our proposals are illustrated via simulation studies and an application to a breast cancer clinical trial."
"10.1198/jasa.2009.tm08270","2009","Poisson autoregression","0","In this article we consider geometric ergodicity and likelihood-based inference for linear and nonlinear Poisson autoregression. In the linear case, the conditional mean is linked linearly to its past values, as well as to the observed values of the Poisson process. This also applies to the conditional variance, making possible interpretation as an integer-valued generalized autoregressive conditional heteroscedasticity process. In a nonlinear conditional Poisson model, the conditional mean is a nonlinear function of its past values and past observations. As a particular example, we consider an exponential autoregressive Poisson model for time series. Under geometric ergodicity, the maximum likelihood estimators are shown to be asymptotically Gaussian in the linear model. In addition, we provide a consistent estimator of their asymptotic covariance matrix. Our approach to verifying geometric ergodicity proceeds via Markov theory and irreducibility. Finding transparent conditions for proving ergodicity turns out to be a delicate problem in the original model formulation. This problem is circumvented by allowing a perturbation of the model. We show that as the perturbations can be chosen to be arbitrarily small, the differences between the perturbed and nonperturbed versions vanish as far as the asymptotic distribution of the parameter estimates is concerned. This article has supplementary material online."
"10.1198/jasa.2009.tm08400","2009","Local polynomial quantile regression with parametric features","0","We propose a new approach to conditional quantile function estimation that combines both parametric and nonparametric techniques. At each design point, a global, possibly incorrect, pilot parametric model is locally adjusted through a kernel smoothing fit. The resulting quantile regression estimator behaves like a parametric estimator when the latter is correct and converges to the nonparametric solution as the parametric start deviates from the true underlying model. We give a Bahadur-type representation of the proposed estimator from which consistency and asymptotic normality are derived under an a-mixing assumption. We also propose a practical bandwidth selector based on the plug-in principle and discuss the numerical implementation of the new estimator. Finally, we investigate the performance of the proposed method via simulations and illustrate the methodology with a data example."
"10.1198/jasa.2009.ap08741","2009","Modeling and inference for measured crystal orientations and a tractable class of symmetric distributions for rotations in three dimensions","0","Electron backscatter diffraction (EBSD) is a technique used in materials science to study the microtexture of metals, producing data that measure the orientations of crystals in a specimen. We examine the precision of such data based on a useful class of distributions on orientations in three dimensions (as represented by 3 x 3 orthogonal matrices with positive determinants). Although such modeling has received attention in the statistical literature, the approach taken typically has been based on general ""special manifold"" considerations, and the resulting methodology may not be easily accessible to nonspecialists. We take a more direct modeling approach, beginning from a simple, intuitively appealing mechanism for generating random orientations specifically in three-dimensional space. The resulting class of distributions has many desirable properties, including directly interpretable parameters and relatively simple theory. We investigate the basic properties of the entire class and one-sample quasi-likelihood-based inference for one member of the model class, producing a new statistical methodology that is practically useful in the analysis of EBSD data. This article has supplementary material online."
"10.1198/jasa.2009.ap08171","2009","Option pricing with model-guided nonparametric methods","0","Parametric option pricing models are widely used in finance. These models capture several features of asset price dynamics; however, their pricing performance can be significantly enhanced when they are combined with nonparametric learning approaches that learn and correct empirically the pricing errors. In this article we propose a new nonparametric method for pricing derivatives assets. Our method relies on the state price distribution instead of the state price density, because the former is easier to estimate nonparametrically than the latter. A parametric model is used as an initial estimate of the state price distribution. Then the pricing errors induced by the parametric model are fitted nonparametrically. This model-guided method, called automatic correction of errors (ACE), estimates the state price distribution nonparametrically. The method is easy to implement and can be combined with any model-based pricing formula to correct the systematic biases of pricing errors. We also develop a nonparametric test based on the generalized likelihood ratio to document the efficacy of the ACE method. Empirical studies based on S&P 500 index options show that our method outperforms several competing pricing models in terms of predictive and hedging abilities."
"10.1198/jasa.2009.tm08260","2009","Jackknife empirical likelihood","1","Empirical likelihood has been found very useful in many different occasions. However, when applied directly to some more complicated statistics such as U-statistics, it runs into serious computational difficulties. In this paper, we introduce a so-called jackknife empirical likelihood (JEL) method. The new method is extremely simple to use in practice. In particular. the JEL is shown to be very effective in handling one and two-sample U-statistics. The JEL can be potentially useful for other nonlinear statistics."
"10.1198/jasa.2009.tm08084","2009","On large margin hierarchical classification with multiple paths","0","Hierarchical classification is critical to knowledge management and exploration. as is gene function prediction and document categorization. In hierarchical classification, an input is classified according to a structured hierarchy. In such a situation, the central issue is how to effectively utilize the interclass relationship to improve the generalization performance of flat classification ignoring such dependency. In this article, we propose a novel large margin method through constraints characterizing a multipath hierarchy, where class membership can be nonexclusive. The proposed method permits a treatment of various losses for hierarchical classification. For implementation. we focus on the symmetric difference loss and two large margin classifiers: support vector machines and psi-learning. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method achieves the desired objective and outperforms strong competitors ill the literature."
"10.1198/jasa.2009.tm08420","2009","Quantile regression with measurement error","3","Regression quantiles can be substantially biased when the covariates are measured with error. In this paper we propose a new method that produces consistent linear quantile estimation in the presence of covariate measurement error. The method corrects the measurement error induced bias by constructing joint estimating equations that simultaneously hold for all the quantile levels. An iterative EM-type estimation algorithm to obtain the Solutions to Such joint estimation equations is provided. The finite sample performance of the proposed method is investigated in a simulation study, and compared to the standard regression calibration approach. Finally, we apply our methodology to part of the National Collaborative Perinatal Project growth data, a longitudinal study with an unusual measurement error structure."
"10.1198/jasa.2009.tm08230","2009","Locally weighted censored quantile regression","1","Censored quantile regression offers a valuable supplement to Cox proportional hazards model for survival analysis. Existing work in the literature often requires stringent assumptions, such as unconditional independence of the survival time and the censoring variable or global linearity at all quantile levels. Moreover, some of the work uses recursive algorithms. making it challenging to derive asymptotic normality. To overcome these drawbacks, we propose a new locally weighted censored quantile regression approach that adopts the redistribution-of mass idea and employs a local reweighting scheme. Its validity only requires conditional independence of the survival time and the censoring variable given the covariates, and linearity at the particular quantile level of interest. Our method leads to a simple algorithm that can be conveniently implemented with R software. Applying recent theory of M-estimation with infinite dimensional parameters, we establish the consistency and asymptotic normality of the proposed estimator. The proposed method is studied via simulations and is illustrated with the analysis of in acute myocardial infarction dataset."
"10.1198/jasa.2009.tm07420","2009","Constructing confidence regions of optimal expected size","0","This article presents a Monte Carlo method for approximating the minimax expected size (MES) confidence set for a parameter known to lie in a compact set. The algorithm is motivated by problems in the physical sciences in which parameters are unknown physical constants related to the distribution of observable phenomena through complex numerical models. The method repeatedly draws parameters at random from the parameter space and simulates data as if each of those values were the true value of the parameter. Each set of simulated data is compared to the observed data using a likelihood ratio test. Inverting the likelihood ratio test minimizes the probability of including false values in the confidence region, which in turn minimizes the expected size of the confidence region. We prove that as the size of the Simulations grows, this Monte Carlo confidence set estimator converges to the Gamma-minimax procedure, where Gamma is a polytope of priors. Fortran-90 implementations of the algorithm for both serial and parallel computers are available. We apply the method to an inference problem in cosmology."
"10.1198/jasa.2009.tm08152","2009","Confidence regions for the multinomial parameter with small sample size","0","Consider the observation of it iid realizations of an experiment with d >= 2 possible outcomes, which corresponds to a single observation of a multinomial distribution M(d)(n, p) where p is an unknown discrete distribution on {1,...,d}. In many applications, the construction of a confidence region for p when it is small is crucial. This challenging concrete problem has a long history. It is well known that the confidence regions built from asymptotic statistics do not have good coverage when n is small. On the other hand, most available methods providing nonasymptotic regions with controlled coverage are limited to the binomial case d = 2. Here we propose a new method valid for any d >= 2 that provides confidence regions with controlled coverage and small Volume. The method involves inversion of the ""covering collection"" associated with level sets of the likelihood. The behavior when d/n tends to infinity remains an interesting open problem beyond the scope of this work."
"10.1198/jasa.2009.tm08439","2009","Nonparametric {B}ayes modeling of multivariate categorical data","1","Modeling of multivariate unordered categorical (nominal) data is a challenging problem, particularly in high dimensions and cases in which one wishes to avoid strong assumptions about the dependence structure. Commonly used approaches rely on the incorporation of latent Gaussian random variables or parametric latent class models. The goal of this article is to develop a nonparametric Bayes approach, which defines a prior with full support on the space of distributions for multiple unordered categorical variables. This support condition ensures that we are not restricting the dependence structure a priori. We show this can be accomplished through a Dirichlet process mixture of product multinomial distributions, which is also a convenient form for posterior computation. Methods for nonparametric testing of violations of independence are proposed, and the methods are applied to model positional dependence within transcription factor binding motifs."
"10.1198/jasa.2009.tm08523","2009","Empirical {B}ayes estimates for large-scale prediction problems","3","Classical prediction methods, such as Fisher's linear discriminant function, were designed for small-scale problems in which the number of predictors N is much smaller than the number of observations n. Modern scientific devices often reverse this Situation. A microarray analysis, for example, might include n = 100 subjects measured on N = 10.000 genes, each of which is a potential predictor. This article proposes an empirical Bayes approach to large-scale prediction, where the optimum Bayes, prediction rule is estimated employing the data from all of the predictors. Microarray examples are used to illustrate the method. The results demonstrate a close connection with the shrunken centroids algorithm of Tibshirani et al. (2002), a frequentist regularization approach to large-scale Prediction. and also with false discovery rate theory."
"10.1198/jasa.2009.tm07543","2009","Nonparametric prediction in measurement error models","0","Predicting the value of it variable Y corresponding to a future value of an explanatory variable X, based on a sample of previously observed independent data pairs (X(1), Y(1)),...,(X(n), Y(n),) distributed like (X, Y), is very important in statistics. In the error-free case, where X is observed accurately, this problem is strongly related to that of standard regression estimation, since prediction of Y can be achieved via estimation of the regression Curve E(Y vertical bar X). When the observed Xis and the future observation of X are measured with error, prediction is of a quite different nature. Here, if T denotes the future (contaminated) available version of X, prediction of Y can be achieved via estimation of E(Y vertical bar T). In practice, estimating E(Y vertical bar T) can be quite challenging, as data may be collected under different conditions, making the measurement errors on X(i) and X nonidentically distributed. We take up this problem in the nonparametric setting and introduce estimators which allow a highly adaptive approach to smoothing. Reflecting the complexity of the problem, optimal rates of convergence of estimators can vary from the semiparametric n(-1/2) rate to much slower rates that are characteristic of nonparametric problems. Nevertheless, we are able to develop highly adaptive, data-driven methods that achieve very good performance in practice. This article has the supplementary materials online."
"10.1198/jasa.2009.ap08329","2009","Thresholding events of extreme in simultaneous monitoring of multiple risks","1","This article develops a threshold system for monitoring airline performance. This threshold system divides the sample space into regions with increasing levels of risk and allows instant assessments of risk level of any observed airline performance. Of particular concern is the performance with extreme risk. In this article. a multivariate extreme value theory approach is used to establish thresholds for signaling varying levels of extremeness in the context of simultaneous monitoring of multiple risk measures. The threshold system is justified in terms of multivariate extreme quantiles, and its sample estimator is shown to be consistent. This threshold system applies to general extreme risk management, Finally, a simulation and comparison study demonstrates the good performance of the proposed multivariate extreme quantile estimator. Supplemental materials providing technical details are available online."
"10.1198/jasa.2009.ap06589","2009","Attributing effects to a cluster-randomized get-out-the-vote campaign","1","Early in the twentieth century, Fisher and Neyman demonstrated how to infer effects of agricultural interventions using only the very weakest of assumptions, by randomly varying which plots were to be manipulated. Although the methods permitted uncontrolled variation between experimental units. the), required strict control over assignment of interventions; this hindered their application to field studies With human subjects, who ordinarily could not be compelled to comply with experimenters' instructions. In 1996, however, Angrist, Imbens. and Rubin showed that inferences from randomized studies could accommodate noncompliance without significant strengthening of assumptions. Political scientists A. Gerber and D. Green responded quickly, fielding a randomized study of voter turnout campaigns in the November 1998 general election. Noncontacts and refusals were frequent. but Gerber and Green analyzed their data in the style of Angrist et A.. avoiding the need to model nonresponse. They did use models for other purposes: to address complexities of the randomization schemed to permit heterogeneity among voters and campaigners; to account for deviations from experimental protocol: and to take advantage of highly informative covariates. Although the added assumptions seemed straightforward and unassailable. a later analysis by Imai found them to be at odds with Gerber and Green's data. Using a different model, he reaches the very opposite of Gerber and Green's central conclusion about getting out the vote. This article shows that neither of the models are necessary, addressing all of the complications of Gerber and Green's Study using methods in the tradition of Fisher and Neyman. To do this, it merges recent developments in randomization-based inference for comparative Studies with somewhat older development,., in design-based analysis of sample surveys. The method involves regression. but large-sample analysis and simulations demonstrate its lack of dependence on regression assumptions. Its substantive results have consequences both for the design of campaigns to increase voter participation and for theories of political behavior more generally"
"10.1198/jasa.2009.0155","2009","Adversarial risk analysis","0","Applications in counterterrorism and corporate competition have led to the development of new methods for the analysis of decision making when there are intelligent opponents and uncertain outcomes. This field represents a combination of statistical risk analysis and game theory, and is sometimes called adversarial risk analysis. In this article, we describe several formulations of adversarial risk problems, and provide a framework that extends traditional risk analysis' tools, such as influence diagrams and probabilistic reasoning, to adversarial problems. We also discuss file research challenges that arise when dealing with these models, illustrate the ideas with examples from business, and point out releveance to national defense."
"10.1198/jasa.2009.0130","2009","A class of transformed mean residual life models with censored survival data","1","The mean residual life function is an attractive alternative to the survival function or the hazard function of a survival time in practice. It provides the remaining life expectancy of a subject surviving Lip to time t. In this study. We propose a class of transformed mean residual life models for fitting survival data under right censoring. To estimate the model parameters. we make use of the inverse probability of censoring weighting approach and develop a system of estimating equations. Efficiency and robustness of the estimators are also studied. Both asymptotic and finite sample properties of the proposed estimators are established and the approach is applied to two real-life datasets collected from clinical trials."
"10.1198/jasa.2009.0122","2009","Computationally efficient nonparametric importance sampling","0","The variance reduction established by importance sampling strongly depends on the choice of the importance sampling distribution. A good choice is often hard to achieve especially for high-dimensional integration problems. Nonparametric estimation of the optimal importance sampling distribution (known as ""nonparametric importance sampling"") is a reasonable alternative to parametric approaches. In this article, nonparametric variants of both the self-normalized and the unnormalized importance sampling estimator are proposed and investigated. A common critique of nonparametric importance sampling is the increased computational burden compared with parametric methods. We solve this problem to a large degree by utilizing the linear blend frequency polygon estimator instead of a kernel estimator. Mean square error convergence properties are investigated, leading to recommendations for the efficient application of nonparametric importance sampling Particularly, we show that nonparametric importance sampling asymptotically attains optimal importance sampling variance. The efficiency of nonparametric importance sampling algorithms relies heavily on the computational efficiency of the nonparametric estimator used. The linear blend frequency polygon outperforms kernel estimators in terms of certain criteria such as efficient sampling and evaluation. Furthermore. it is compatible with the inversion method for sample generation. This allows one to combine nonparametric importance sampling with other variance reduction techniques such as stratified sampling. Empirical evidence for the usefulness of the suggested algorithms is obtained by means of three benchmark integration problems. We show empirically that these methods may work in higher dimensions, at least up to dimension eight. As air application. we estimate the distribution of the queue length of a spain filter queuing system based on real data."
"10.1198/jasa.2009.0142","2009","Confidence intervals for population ranks in the presence of ties and near ties","3","Frequentist confidence intervals for population ranks and their statistical justifications;ire not well established. even though here is a great need for such procedures in a practice. How do we assign confidence bounds for the ranks of health care facilities, school, and financial institution based on data that do not clearly separate the performance of different entities apart? The commonly used bootstrap-based frequentist confidence intervals and Bayesian intervals for population ranks may not achieve the intended coverage probability ill the frequentist sense, especially in the presence of unknown ties or ""near ties"" among the population to be ranked. Given random samples from k populations, we propose confidence bounds for population ranking parameters and develop rigorous frequentist theory and nonstandard bootstrap inference for population ranks, which allow ties and near ties. In the process, a notion of modified population rank is introduced that appears quite suitable for dealing with the population ranking problem. The proposed methodology and theoretical results are illustrated through simulations and real dataset from a health research study involving 79 Veteran Health Administration (VHA) facilities. The results tire extended to general risk adjustment models."
"10.1198/jasa.2009.0133","2009","Prior distributions from pseudo-likelihoods in the presence of nuisance parameters","0","Consider a mode parameterized by 0 = (psi, lambda), where psi is the parameter of interest. The problern of eliminating the nuisance parameter lambda the nuis can be tackled by resorting to a pseudo-likelihood function L*(psi) for psi-namely a function of psi only and the data y with properties similar to those of a likelihood function. If one treats L*(psi) as a true likelihood. the posterior distribution pi*(psi vertical bar y) alpha pi(psi)L*(psi) for psi can be considered. where pi(psi) is a prior distribution on psi. The goal of this article is to construct probability matching priors for a scalar parameter of interest only (i.e., priors for which Bayesian and frequentist inference agree to some order of approximation) to be used in pi*(psi vertical bar y). When L*(psi) is it margpinal, a conditional, or a modification of the profile likelihood. we show that pi(psi) is simply proportional to the square root of the inverse of the asymptotic variance of the pseudo-maxinium likelihood estimator. The proposed priors are compared with the reference or Jeffreys' priors in four examples."
"10.1198/jasa.2009.0138","2009","Shrinkage estimation of the varying coefficient model","2","The varying coefficient model is a useful extension of the linear regression model. Nevertheless, how to conduct variable selection for the varying coefficient model in a computationally efficient manner is poorly understood. To solve the problem, we propose here a novel method, which combines the ideas of the local polynomial smoothing and the Least Absolute Shrinkage and Selection Operator (LASSO). The new method can do nonparametric estimation and variable selection simultaneously. With a local constant estimator and the adaptive LASSO penalty the new method can identify the true model consistently, and that the resulting estimator can be as efficient as the oracle estimator Numerical studies clearly confirm our theories. Extension to other shrinkage methods(e.g. the SCAD. i.e., the Smoothly Clipped Absolute Deviation.) mid other smoothing methods is stiaightforward."
"10.1198/jasa.2009.0126","2009","Partial correlation estimation by joint sparse regression models","2","In this article, we propose a computationally efficient approach-space (Sparse PArtial Correlation Estimation)-for selecting nonzero partial correlations under the high-dimension-low-sample-size setting. This method assumes the overall sparsity of the partial correlation matrix and employs sparse regression techniques for model fitting. We illustrate the performance of space by extensive simulation studies. It is shown that space performs well in both nonzero partial correlation selection and the identification of hub variables, and also outperforms two existing methods. We then apply space to a microarray breast cancer dataset and identify a set of hub genes that may provide important insights on genetic regulatory networks. Finally, we prove that, under a set of suitable assumptions. the proposed procedure is asymptotically consistent in terms of model selection and parameter estimation."
"10.1198/jasa.2009.0108","2009","On the concept of depth for functional data","0","The statistical analysis of functional data is a growing need in many research areas. In particular, a robust methodology is important to study curves, which are the output of many experiments in applied statistics. As a starting point for this robust analysis. we propose, analyze. and apply a new definition of depth for functional based on the graphic observations based presentation of the curves. Given a collection of functions. it establishes the ""centrality"" of an observation and provides a natural center-outward ordering of the sample curves. Robust statistics. such as the median function or a trimmed mean function, can be defined from this depth definition. Its finite-dimensional version provides a new depth for multivariate data that is computationally feasible and useful for studying high-dimensional observations. Thus. this new depth is also suitable for complex observations such as microarray data, images, and those arising in some recent marketing and financial studies. Natural properties of these new concepts are established and the uniform consistency of the sample depth is proved. Simulation results show that the corresponding depth based trimmed mean presents better performance than other possible location estimators proposed in the literature for some contaminated models. Data depth can be also used to screen for outliers. The ability of the new notions of depth to detect ""shape"" outliers is presented. Several real datasets are considered to illustrate this new concept of depth. including applications to microarray observations, weather data. and growth curves. Finally, through this depth, we generalize to functions the Wilcoxon rank sum test. It allows testing, whether two groups of curves come from the same population. This functional rank test when applied to children growth curves shows different growth patterns for boys and girls."
"10.1198/jasa.2009.0115","2009","Estimating derivatives for samples of sparsely observed functions, with application to online auction dynamics","2","It is often of interest to recover derivatives of a sample of random functions front sparse and noise-contaminated measurements especially when the dynamics Of underlying processes is of interest. We propose a novel approach based on estimating derivatives of eigenfunctions and expansions of random functions into their eigenfunctions to obtain a representation for derivatives. In combination with estimates for functional principal component scores for sparse data, this leads to a friable solution of the challenging problem to recover derivatives for sparsely observed functions. We establish consistency result,. and demonstrate in simulations that the method is superior to alternative approaches (derivative estimation with random effects models based on B-spline bases, kernel smoothing, smoothing splines, or P-splines). Our study is motivated by an analysis of bidding histories for eBay auctions, for which bids are typically very sparse in the middle and somewhat more frequent near the beginning and end of an auction, We demonstrate the estimation of derivatives of price curves for individual auctions from the sparsely observed bidding histories and also derive a model-free first-order differential equation that applies in the case of Gaussian processes, This provides a data-driven dynamic model that we use to elucidate auction dynamics."
"10.1198/jasa.2009.0119","2009","Screening experiments for developing dynamic treatment regimes","0","Dynamic treatment regimes are time-varying treatments that individualize sequences of treatments to the patient. The construction of dynamic treatment regimes is challenging because a patient will be eligible for some treatment components only if he has not responded (or has responded) to other treatment components. In addition, there are usually a number of potentially useful treatment components and combinations thereof. In this article, we propose new methodology for identifying promising components and screening out negligible ones. First, we define causal factorial effects for treatment components that may be applied sequentially to a patient. Second, we propose experimental designs that can be used to study the treatment components. Surprisingly, modifications can be made to (fractional) factorial designs-more commonly found in the engineering statistics literature-for screening in this setting. Furthermore, we provide an analysis model that can be used to screen the factorial effects. We demonstrate the proposed methodology using examples motivated in the literature and also via a simulation study."
"10.1198/jasa.2009.0102","2009","Nonparametric quantile estimations for dynamic smooth coefficient models","2","In this article, quatile regression in methods are suggested for a class of smooth coefficient time series models. We use both local polynomial and local constant fitting schemes to estimate the smooth coefficients in a quantile framework. We establish the asymptotic propel-ties of both the local polynomial and local constant estimators For alpha-mixing time series. Also, a bandwidth selector based on the nonparametric version of the Akaike information criterion is sugggested. together with a consistent estimate of the asymptotic covariance matrix. Furthermore, the asymptotic behaviors of the estimators at boundaries are examined. A comparison of the local polynomial quantile estimator with the local constant estimator is presented. A simulation study is carried Out to illustrate the performance of estimates. An empirical application of the model to real data further demonstrates the potential of the proposed modeling procedures."
"10.1198/jasa.2009.0114","2009","A design-adaptive local polynomial estimator for the errors-in-variables problem","4","Local polynomial estimators are popular techniques for nonparametric regression estimation and have received great attention in the literature. Their simplest version, the local constant estimator, can be easily extended to the errors-in-variables context by exploiting its similarity with the deconvolution kernel density estimator. The generalization of the higher order versions of the estimator, however. is not straightforward and has remained an open problem for the last 15 years. We propose an innovative local polynomial estimator of any order in the errors-in-variables context, derive its design-adaptive asymptotic properties and study its finite sample performance on simulated examples. We provide not only a solution to a long-standing open problem, but also provide methodological contributions to error-invariable regression, including local polynomial estimation of derivative functions."
"10.1198/jasa.2009.0120","2009","Estimation of parameters subject to order restrictions on a circle with application to estimation of phase angles of cell cycle genes","0","Motivated by a problem encountered in the analysis of cell cycle gene expression data. this article deals with the estimation of parameters subject to order restrictions on a unit circle. A normal eukaryotic cell cycle has four major phases, during cell division, and a cell cycle gene has its peak expression (phase angle) during the phase that may correspond to its biological function. Because the phases are ordered along a circle, the phase angles of cell cycle genes are ordered unknown parameters on a unit circle. The problem of interest is to estimate the phase angles using the information regarding the order among them. We address this problem by developing a circular version of the well-known isotonic regression for Euclidean data. Because of the underlying geometry, the standard pool adjacent violator algorithm (PAVA) cannot be used for deriving the circular isotonic regression estimator (CIRE). However, PAVA can be modified to obtain a computationally efficient algorithm for deriving the CIRE. We illustrate the CIRE by estimating the phase angles of some of well-known cell cycle genes using the unrestricted estimators obtained in the literature."
"10.1198/jasa.2009.0117","2009","Testing the nullity of {GARCH} coefficients: correction of the standard tests and relative efficiency comparisons","0","This article is concerned with testing the nullity of coefficients in generalized autoregressive conditionally heteroscedastic (GARCH) models. The problem is nonstandard because the quasi-maximum likelihood estimator is subject to positivity constraints. This article establishes the asymptotic null and local alternative distributions of Wald, score, and quasi-likelihood ratio tests. Efficiency comparisons under fixed alternatives are considered. Two cases of special interest are tests of the null hypothesis of one coefficient equal to zero and tests of the null hypothesis of no conditional heteroscedasticity. Finally, the proposed approach is used in the analysis of financial data and suggests reconsidering the preeminence of GARCH(1, 1) among GARCH models."
"10.1198/jasa.2009.0110","2009","Consistent classification of nonstationary time series using stochastic wavelet representations","0","Consider the situation when we have training data containing many time series having known group membership and testing data with unknown group membership. The goals are to find timescale features (using training data) that can best separate the groups, and to use these highly discriminant features to classify test data. We propose a method for classification using a bias-corrected nondecimated wavelet transform. Wavelets are ideal for identifying highly discriminant local time and scale features. The observed signals will be treated as realizations of locally stationary wavelet processes, under which we define and rigorously estimate the evolutionary wavelet spectrum (timescale decomposition of variance). The evolutionary wavelet spectrum, which contains the second-moment information on the signals, is used as the classification signature. For each test time series, we compute the empirical wavelet spectrum and its divergence from the wavelet spectrum of each group. The test time series is then assigned to the group to which it is the least dissimilar. Under the locally stationary wavelet framework, we rigorously demonstrate that the classification procedure is consistent (i.e., misclassification probability goes to zero at the rate that is inversely proportional to divergence between the evolutionary wavelet spectra). The method is illustrated using,seismic signals (earthquake vs. explosion events) and is demonstrated to work very well in simulation studies."
"10.1198/jasa.2009.0105","2009","Time series modelling with semiparametric factor dynamics","2","High-dimensional regression problems, which reveal dynamic behavior, are typically analyzed by time propagation of a few number of factors. The inference on the whole system is then based on the low-dimensional time series analysis. Such high-dimensional problems occur frequently in many different fields of science. In this article we address the problem of inference when the factors and factor loadings are estimated by semiparametric methods. This more flexible modeling approach poses an important question: Is it justified, from an inferential point of view, to base statistical inference on the estimated times series factors? We show that the difference of the inference based on the estimated time series and ""true"" unobserved time series is asymptotically negligible. Our results justify fitting vector autoregressive processes to the estimated factors, which allows one to study the dynamics of the whole high-dimensional system with a low-dimensional representation. We illustrate the theory with a simulation study. Also, we apply the method to a study of the dynamic behavior of implied volatilities and to the analysis of functional magnetic resonance imaging (fMRI) data."
"10.1198/jasa.2009.0124","2009","A {B}ayesian hierarchical nonoverlapping random disc growth model","0","A methodology is proposed to efficiently model a random set via a multistage hierarchical Bayesian model. We define a NonOverlapping Random Disk Model (NORDM), which is similar in spirit to the well-known Poisson-Boolean model. This model is formulated in a conditional setting that facilitates Bayesian sampling of important parameters in the model. This framework can accommodate any object, not just those with disk shapes, although the model can be easily extended to include any known compact convex set instead of the disc (e.g., polygons or ellipses). We further propose a growth model that is conceptually simple and allows straightforward estimation of parameters, without the need for tedious calculations of hitting or inclusion probabilities. The model is applied to severe storm cell development as obtained from weather radar."
"10.1198/jasa.2009.0118","2009","Local spectral analysis via a {B}ayesian mixture of smoothing splines","0","In many practical problems, time series are realizations of nonstationary random processes. These processes can often be modeled as processes with slowly changing dynamics or as piecewise stationary processes. In these cases, various approaches to estimating the time-varying spectral density have been proposed. Our approach in this article is to estimate the log of the Dahlhaus local spectrum using a Bayesian mixture of splines. The basic idea of Our approach is to first partition the data into small sections. We then assume that the log spectral density of the evolutionary process in any given partition is a mixture of individual log spectra. We use a mixture of smoothing splines model with time varying mixing weights to estimate the evolutionary log spectrum. The mixture model is fit using Markov chain Monte Carlo techniques that yield estimates of the log spectra of the individual subsections. In addition to an estimate of the local log spectral density, the method yields pointwise credible intervals. We use a reversible jump step to automatically determine the number of different spectral components."
"10.1198/jasa.2009.0127","2009","Variable selection for partially linear models with measurement errors","1","This article focuses on variable selection for partially linear models when the covariates are measured with additive errors. We propose two classes of variable selection procedures, penalized least squares and penalized quantile regression, using the nonconvex penalized principle. The first procedure corrects the bias in the loss function caused by the measurement error by applying the so-called correction-for-attenuation approach, whereas the second procedure corrects the bias by using orthogonal regression. The sampling properties for the two procedures are investigated. The rate of convergence and the asymptotic normality of the resulting estimates are established. We further demonstrate that, with proper choices of the penalty functions and the regularization parameter, the resulting estimates perform asymptotically as well as an oracle property. Choice of smoothing parameters is also discussed. Finite sample performance of the proposed variable selection procedures is assessed by Monte Carlo simulation studies. We further illustrate the proposed procedures by an application."
"10.1198/jasa.2009.0111","2009","Local multidimensional scaling for nonlinear dimension reduction, graph drawing, and proximity analysis","0","In the past decade there has been a resurgence of interest in nonlinear dimension reduction. Among new proposals are ""Local Linear Embedding,"" ""Isomap,"" and Kernel Principal Components Analysis which all construct global low-dimensional embeddings from local affine or metric information, We introduce a competing method called ""Local Multidimensional Scaling"" (LMDS). Like LLE, Isomap, and KPCA, LMDS constructs its global embedding from local information, but it uses instead a combination of MDS and ""force-directed"" graph drawing. We apply the force paradigm to create localized versions of MDS stress functions with a timing parameter to adjust the strength of nonlocal repulsive forces. We solve the problem Of tuning parameter selection with a meta-criterion that measures how well the sets of K-nearest neighbors agree between the data and the embedding. Tuned LMDS seems to be able to outperform MDS, PCA, LLE, Isomap, and KPCA, as illustrated with two well-known image datasets. The meta-criterion can also be used in a pointwise version as a diagnostic tool for measuring the local adequacy of embeddings and thereby detect local problems in dimension reductions."
"10.1198/jasa.2009.0106","2009","Likelihood-based sufficient dimension reduction","3","We obtain the maximum likelihood estimator of the central subspace under conditional normality of the predictors given the response. Analytically and in simulations we found that our new estimator can preform much better than sliced inverse regression, sliced average variance estimation and directional regression, and that it seems quite robust to deviations from normality."
"10.1198/jasa.2009.0101","2009","Generalized thresholding of large covariance matrices","6","We propose a new class of generalized thresholding operators that combine thresholding with shrinkage, and Study generalized thresholding of the sample covariance matrix in high dimensions. Generalized thresholding of the covariance matrix has good theoretical properties and carries almost no computational burden. We obtain in explicit convergence rate in the operator norm that shows the tradeoff between the sparsity of the true model, dimension, and the sample size, and shows that generalized thresholding is consistent over a large class of models as long as the dimension p and the sample size it satisfy log p/n -> 0. In addition, we show that generalized thresholding has the ""sparsistency"" property, meaning it estimates true zeros a, zeros with probability tending to 1, and, under an additional mild condition, is sign consistent for nonzero elements. We show that generalized thresholding covers, as special cases, hard and soft thresholding, smoothly clipped absolute deviation, and adaptive lasso, and compare different types of generalized thresholding in a simulation study and in an example of gene clustering from a microarray experiment with tumor tissues."
"10.1198/jasa.2009.0010","2009","A {B}ayesian hierarchical model for analysis of single-nucleotide polymorphisms diversity in multilocus, multipopulation samples","0","The distribution of genetic variation among populations is conveniently measured by Wright's F(ST),. which is a scaled variance taking on values in [0,I]. For certain types of genetic markers and for single-nucleotide polymorphisms (SNPs) in particular, it is reasonable to presume that allelic differences at most loci are selectively neutral. For such loci, the distribution of genetic variation among populations is determined by the size of local populations, the pattern and rate of migration among those populations. and the rate of mutation. Because the demographic parameters (population sizes and migration rates) are common across all autosomal loci, locus-specific estimates of F(ST) Will depart from a common distribution only for loci with unusually high or low rates of mutation or for loci that are closely associated with genomic regions having a relationship with fitness. Thus, loci that are statistical outliers showing significantly more among-population differentiation than others may mark genomic regions subject to diversifying selection among the sample populations. Similarly, statistical outliers showing significantly less differentiation among populations than others may mark genomic regions subject to stabilizing selection across the sample populations. We propose several Bayesian hierarchical models to estimate locus-specific effects on F(ST), and we apply these models to single nucleotide polymorphism data from the HapMap project. Because loci that are physically associated with one another are likely to show similar patterns of variation, we introduce conditional autoregressive models to incorporate the local correlation among loci for high-resolution genomic data. We estimate the posterior distributions of model parameters using Markov chain Monte Carlo (MCMC) simulations. Model comparison using several criteria, including deviance information criterion (DIC) and pseudomarginal likelihood (LPML), reveals that a model with locus- and population-specific effects is superior to other models for the data used in the analysis. To detect statistical outliers we propose an approach that measures divergence between the posterior distributions of locus-specific effects and the common F(ST) with the Kullback-Leibler divergence measure. We calibrate this measure by comparing values with those produced from the divergence between a biased and a fair coin. We conduct a simulation study to illustrate the performance of our approach for detecting loci subject to stabilizing/divergent selection, and we apply the proposed models to low- and high-resolution SNP data from the HapMap project. Model comparison using DIC and LPML reveals that conditional autoregressive (CAR) models are superior to alternative models for the high-resolution data. For both low- and high-resolution data, we identify statistical outliers that are associated with known genes."
"10.1198/jasa.2009.0004","2009","Robust estimation of mean functions and treatment effects for recurrent events under event-dependent censoring and termination: application to skeletal complications in cancer metastatic to bone","0","In clinical trials featuring recurrent clinical events, the definition and estimation of treatment effects involves a number of interesting issues, especially when loss to follow-up may be event-related and when terminal events such as death preclude the occurrence of further events. This paper discusses a clinical trial of breast cancer patients with bone metastases where the recurrent events are skeletal complications, and where patients may die during the trial. We argue that treatment effects should be based on marginal rate and mean functions. When recurrent event data are subject to event-dependent censoring, however, ordinary marginal methods may yield inconsistent estimates. Incorporating correctly specified inverse probability of censoring weights into analyses can protect against dependent censoring and yield consistent estimates of marginal features. An alternative approach is to obtain estimates of rate and mean functions from models that involve some conditioning to render censoring conditionally independent. We consider three methods of estimating mean functions of recurrent event processes and examine the bias and efficiency of unweighted and inverse probability weighted versions of the methods with and without a terminating event. We compare the methods via simulation and use them to analyse the data from the breast cancer trial."
"10.1198/jasa.2010.ap09236","2010","A statistical approach to thermal management of data centers under steady state and system perturbations","0","Temperature control for a large data center is both important and expensive. On the one hand, many of the components produce a great deal of heat, and on the other hand, many of the components require temperatures below a fairly low threshold for reliable operation. A statistical framework is proposed within which the behavior of a large cooling system can be modeled and forecast under both steady state and perturbations. This framework is based upon an extension of multivariate Gaussian autoregressive hidden Markov models (HMMs). The estimated parameters of the fitted model provide useful summaries of the overall behavior of and relationships within the cooling system. Predictions under system perturbations are useful for assessing potential changes and improvements to be made to the system. Many data centers have far more cooling capacity than necessary under sensible circumstances, thus resulting in energy inefficiencies. Using this model, predictions for system behavior after a particular component of the cooling system is shut down or reduced in cooling power can be generated. Steady-state predictions are also useful for facility monitors. System traces outside control boundaries flag a change in behavior to examine. The proposed model is fit to data from a group of air conditioners within an enterprise data center from the IT industry. The fitted model is examined, and a particular unit is found to be underutilized. Predictions generated for the system under the removal of that unit appear very reasonable. Steady-state system behavior also is predicted well."
"10.1198/jasa.2010.ap08148","2010","Exploiting regional treatment intensity for the evaluation of labor market policies","0","We estimate the effects of active labor market policies (ALMPs) on subsequent employment and earnings by nonparametric instrumental variable estimators. Very informative administrative Swiss data with detailed regional information are combined with exogenous regional variation in program participation probabilities to generate an instrument within well-defined local labor markets. We find that implementation of an ALMP increased individual employment probabilities by about 15% for unemployed that might be considered ""marginal"" participants."
"10.1198/jasa.2010.tm08737","2010","Reduced rank mixed effects models for spatially correlated hierarchical functional data","1","Hierarchical functional data are widely seen in complex studies where subunits are nested within units. which in turn are nested within treatment groups We propose a general framework of functional mixed effects model for such data within-unit and within-subunit variations are modeled through two separate sets of principal components. the subunit level functions are allowed to be correlated Penalized splines are used to model both the mean functions and the principal components functions, where roughness penalties are used to regularize the spline fit An expectation maximization (EM) algorithm is developed to tit the model. while the specific covariance structure of the model is utilized lot computational efficiency to avoid storage and inversion of large matrices Our dimension reduction with principal components provides an effective solution to the difficult tasks of modeling the covariance kernel of a random function and modeling the correlation between functions The proposed methodology is illustrated using simulations and an empirical damsel from a colon carcinogenesis study Supplemental materials are available online"
"10.1198/jasa.2009.tm09124","2010","Linear mixed-effects modeling by parameter cascading","0","A linear mixed-effects model (LME) is a familial example of a multilevel parameter structure involving nuisance and structural parameters. as well as parameters that essentially control the model's complexity Marginalization Over nuisance parameters. such as the restricted maximization likelihood method, has been the usual estimation strategy, but it can Involve onerous and complex algorithms to achieve the integrations involved Parameter cascading Is described as a multicriterion optimization algorithm that is relatively simple to program and leads to fast and stable computation The method is applied 10 LME. where well-developed marginalization methods are already available Our results suggest that parameter cascading is at least as good as. if not better than. the available methods We also extend the LME model to multicurve data smoothing by introducing. a basis partitioning scheme and defining. toughness penalty terms for both functional fixed effect and random effects The results are substantially better than those obtained by using the previous LME methods A supplemental document Is available online"
"10.1198/jasa.2010.tm08551","2010","Weighted generalized estimating functions for longitudinal response and covariate data that are missing at random","2","Longitudinal studies of ten feature incomplete response and covariate data It is well known that biases can arise from naive analyses of available data. but the precise impact of Incomplete data depends on the frequency of missing data and the strength of the association between the response variables and emanates and the missing-data indicators Various factors may influence the availability of response and covariate data at scheduled assessment times, and at any given assessment time the response may be missing, covariate data may be missing. or both response and covariate data may he missing Here we show that ills important to take the association between the missing data indicators for these two processes into account through Joint models Inverse probability-weighted generalized estimating equations offer an appealing approach for doing this Mete we develop these equations for a particular model generating intermittently missing-at-random data Empirical studies demonstrate that the consistent estimators arising from the proposed methods have very small empirical biases in moderate samples Supplemental materials are available online"
"10.1198/jasa.2009.tm08030","2010","Variational inference for large-scale models of discrete choice","0","Discrete choice models are commonly used by applied statisticians in numerous fields. such as marketing. economics. finance. and operations research When agents in discrete choice models are assumed to have differing preferences. exact inference is often intractable Markov chain Monte Carlo techniques make approximate inference possible. but the computational cos is prohibitive on the large damsels now becoming untimely available Variational I methods provide a deterministic alternative for approximation of the posterior distribution We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice The algorithms require only that we solve a sequence of unconstrained optimization problems. which are shown to be convex One version (0 the procedures relies on a new approximation to the variational objective function. based on the multivariate delta method Extensive simulations. along with an analysis of real-world data, demonstrate that variational methods achieve accuracy competitive with Markov chain Monte Carlo at a small fraction of the computational cost Thus. variational methods permit inference on damsels that otherwise cannot be analyzed without possibly adverse simplifications of the underlying discrete choice model Appendices C through F are available as online supplemental materials"
"10.1198/jasa.2009.tm08577","2010","Likelihood-based inference for max-stable processes","2","The last decade has seen max-stable processes emerge as a common tool for the statistical modeling of spatial extremes However, their application is complicated due to the unavailability of the multivariate density function and so likehhood-based methods remain far from providing a complete and flexible framework kit inference In this article we develop inferentially practical likehhood-based methods for fitting max-stable processes derived from a composite-likehhood approach The procedure is sufficiently reliable and versatile to permit the simultaneous modeling of marginal and dependence parameters in the spatial context at a moderate computational cost The utility of this methodology is examined via simulation. and illustrated by the analysts of United States precipitation extremes"
"10.1198/jasa.2009.tm09036","2010","Statistical agent-based models for discrete spatio-temporal systems","0","Agent-based models have been used to mimic natural processes in a variety of fields. from biology to social science By specifying mechanistic models that describe how small-scale processes hi net and then scaling them up. agent-based approaches can result in very complicated large-scale behavior while often relying on only a small set of initial conditions and intuitive rules Although many agent-based models are used strictly la a Simulation context. statistical implementations are less common To characterize complex dynamic processes such as the spread of epidemics. we present a hierarchical Bayesian framework for formal statistical agent-based modeling using spatiotemporal binary data Our approach is based on an intuitive parameterization of the system dynamics and Call explicitly accommodate directionally varying dispersal. long distance dispersal. and spatial heterogeneity"
"10.1198/jasa.2009.tm08622","2010","High-dimensional variable selection for survival data","1","The minimal depth of a maximal subtree IN a dimensionless order statistic measuring the predictiveness of a variable in a survival tree We derive the distribution of the minimal depth and use it lot high-dimensional variable selection using random survival forests In big p and small n problems (where p is the dimension and n Is the sample size). the distribution of the minimal depth reveals a ""ceiling effect"" in which a tree simply cannot be grown deep enough to properly identify predictive variables Motivated by this limitation. we develop a new regularized algorithm. termed RSF-Variable Hunting This algorithm exploits maximal subtrees for effective variable selection under such scenarios Several applications are presented demonstrating the methodology. including the problem of gene selection using microarray data In this work we focus only on survival settings. although out methodology also applies to other random forests applications. including regression and classification settings All examples presented here use the R-software package randomSurvivalForest"
"10.1198/jasa.2009.tm08299","2010","Marginal and nested structural models using instrumental variables","0","The objective of many scientific studies is to evaluate the effect of a treatment on an outcome of interest ceterts pat thus In variables (I Vs) serve as an experimental handle. independent of potential outcomes and potential treatment status and affecting potential outcomes only through potential treatment status We propose maternal and nested structural models using IVs. In the spirit of marginal and nested structural models under no unmeasured confounding A marginal structural IV model parameterizes the expectations of two potential outcomes under an active treatment and the null treatment respectively, for those in a covariate-specific subpopulation who would take the act we treatment it the instrument were externally set to each specific level A nested structural IV model parameterizes the difference between the two expectations after transformed by a link function and hence the average treatment effect on the treated at each instrument level We develop IV outcome regression. IV propensity score weighting. and doubly robust methods for estimation. in parallel to those for structural node Is under no unmeasured confounding The regression method requires correctly specified models for the treatment propensity score and the outcome regression function The weighting method requires a correctly specified model for the instrument propensity score The doubly robust estimators depend on the two sets of models and remain consistent if either set of models are correctly specified We apply our methods to study returns to education using data from the National Longitudinal Survey of Young Men"
"10.1198/jasa.2009.tm09147","2010","Multivariate outlier detection with high-breakdown estimators","0","In this paper we develop multivariate outlier tests based on the high-breakdown Minimum Covariance Determinant estimator The rules that we propose have good performance under the null hypothesis of no outliers in the data and also appreciable power properties for the purpose of individual outlier detection This achievement is made possible by two orders of improvement over the currently available methodology First we suggest an approximation to the exact distribution of robust distances flour which cut-off values can be obtained even in small samples Our thresholds are accurate simple to implement and result in more powerful outlier identification rules than those obtained by calibrating the asymptotic distribution of distances The second power improvement comes from the addition of a new iteration step after one-step reweighting of the estimator The proposed methodology is motivated by asymptotic distributional results Its finite sample performance is evaluated through simulations and compared to that of available multivariate outlier tests"
"10.1198/jasa.2009.ap08070","2010","Demonstrating single and multiple currents through the {\it {E}. coli}-{S}ec{YEG}-pore: testing for the number of modes of noisy observations","0","We analyze a new damsel from an electrophysiological recording of transmembrane currents through a bacterial membrane channel to demonstrate the existence of single and multiple channel currents Protein channels mediate transport through biological membranes. knowledge of the channel properties gained from electrophysiological recordings is important for a targeted drug design We investigate the bacterial membrane protein SecYFG which is 01 essential importance for the secretory pathway for sorting of newly synthesized proteins to their place of function in the cell Our results strongly indicate that in the SecYEG pore the different modes of the density of channel currents are approximately equidistant and correspond to different numbers of open channels in the membrane A current of approximate to 12 pA under the present experimental conditions turns out to be characteristic of the presence of a single open SecYEG pore a fact that had not been electrophysiologically characterized so far Electrophysiological recordings of single protein channels show a substantial amount of background noise The data at our disposal can he modeled as the independent sum of an error variable and the realization of the ionic current Thus, we are led to deconvoluting the density of the observations in order to recover the density f of the ionic currents, and then investigating the number of modes off To this end we propose an extension of Silverman's (198 I) test for the number of modes to deconvolution kernel density estimation, and develop the relevant theory The finite sample performance of the test is investigated in a simulation studyTechnical details for the proofs in this article are available as supplementary material online"
"10.1198/jasa.2009.ap08404","2010","Local post-stratification in dual system accuracy and coverage evaluation for the {U}.{S}. census","0","We consider a local post-stratification approach to analyze the capture recapture dual system Accuracy and Coverage Evaluation (A C E) data associated with the 2000 U S Census The local post-stratification is carried out via a nonparantetnc regression estimation of the census enumeration and the correct enumeration functions We propose a nonparametric population size estimator that is designed to accommodate some key aspects of the A C E missing values, erroneous enumerations, and extra covariates affecting the missingness and correct enumeration The resulting estimates are compared with estimates from a convent tonal post-stratification and a logistic regression approach in an analysis on the 2000 Census A C E data"
"10.1198/jasa.2009.ap08117","2010","Powering up with space-time wind forecasting","3","The technology to harvest electricity from wind energy is now advanced enough to make enure cities powered by it a reality High-quality, short-term forecasts of wind speed are vital to making this a more reliable energy source Gneiting et at (2006) have introduced a model for the average wind speed two hours ahead based on both spatial and temporal in The forecasts produced by this model are accurate, and subject to accuracy, the predictive distribution Is sharp. that is, highly concentrated around Its center However, this model is split into nonunique regimes based on the wind direction at an offsite location This paper both generalizes and Improves upon this model by treating wind direction as a circular variable and including it in the model It is robust in many experiments. such as predicting wind at other locations We compare this with the more common approach of modeling wind speeds and directions in the Cartesian space and use a skew-t distribution for the errors The quality of the predictions from all of these models can be more realistically assessed with a loss measure that depends upon the power curve relating wind speed to power output This proposed loss measure yields more insight into the true value of each model's predictions"
"10.1198/jasa.2009.ap08640","2010","Resolving contested elections: the limited power of post-vote vote-choice data","0","In close elections, the losing side has an incentive to obtain evidence that the election result is incorrect Sometimes this evidence comes in the form of court testimony from a sample of invalid voters, and this testimony is used to adjust vote totals (Belcher v Mayor of Ann Arbor 1978, Borders v King County 2005) However, while courts may be reluctant to make explicit findings about out-of-sample data (e g Invalid voters that do not testify), when samples are used to adjust vote totals. the court is making such findings implicitly In this paper. we show that the practice of adjusting vote totals on the basis of potentially unrepresentative samples can lead to incorrectly voided election results More generally, we demonstrate that even when frame error and measurement error are minimal random samples of post-vote vote-choice data can have limited power to detect incorrect election results without high response rates, precinct level polarization. or the acceptance of large Type I error rates Therefore, in U S election disputes. even high-quality post-vote vote-choice data may he insufficient to resolve contested elections without the use of modeling assumptions (whether or not these assumptions are acknowledged)"
"10.1198/jasa.2009.ap08445","2010","Scaling the critics: uncovering the latent dimensions of movie criticism with an item response approach","0","We study the critical opinions of expert movie reviewers as an item response problem Building on earlier ""unfolding"" models, we develop a framework that models an individual's decision to approve of disapprove of an item Using this approach we are able to recover the locations of movies and ideal points of critics in the same multidimensional space We demonstrate that a three-dimensional model captures much of the variation in critical opinions The first dimension signifies movie ""quality"" while the other two connote the nature and subject matter of the films We then demonstrate that the dimensions uncovered from our ""utility threshold model"" are statistically significant predictors of a movie's success, and are particularly useful in predicting the success of independent films"
"10.1198/jasa.2009.ap08518","2010","How many people do you know? {E}fficiently estimating personal network size","0","In this article we develop a method to estimate both individual social network size (ie, degree) and die distribution of network sizes in a population by asking respondents how many people they know in specific subpopulations (c g people named Michael) Building on the scale-up method of Killworth ei al (1998b) and other previous attempts to estimate individual network size we propose a latent non-random mixing model which resolves three known problems with previous approaches As a byproduct our method also provides estimates of the rate of social nix tug between population groups We demonstrate the model using a sample of 1.370 adults orginally collected by McCarty et al (2001) Based on insights developed during the statistical modeling, we conclude by offering practical guidelines for the design of future surveys to estimate social network size Most importantly. we show that if the first names asked about rue chosen properly, the estimates from the simple scale-up model enjoy the same bias-reduction as the estimates front our more complex latent nonrandom mixing model"
"10.1198/jasa.2010.ap07175","2010","Bayesian and frequentist methods for provider profiling using risk-adjusted assessments of medical outcomes","0","""Provider profiling"" is the evaluation of the performance of hospitals, doctors, and other medical practitioners to enhance the quality of medical care We propose a new method and compare conventional and Bayesian methodologies that are used or proposed for use for such ""report cards"" Conventional statistical approaches to these provider assessments use likelihood-based frequentist methodologies and the new Bayesian method is patterned after these For each of three models, we compare the frequentist and Bayesian approaches using data used by the New York State Department of Health for its annually released reports that profile hospitals permitted to perform coronary artery bypass graft surgery We use additional. constructed data sets to sharpen our conclusions Comparisons across methods associated with different models are Important because of current proposals to use random-effects (exchangeable) models for provider profiling We also summarize and discuss important issues in the conduct of provider profiling. such as inclusion of provider characteristics in the model and choice of criteria for determining unsatisfactory performance"
"10.1198/jasa.2009.ap07576","2010","Regularized reconstruction of wave height and slope fields from refracted images of water","0","Refractive imaging of wave fields in an established experimental technique We consider the associated reconstruction problem and investigate some statistically motivated refinements. Including (a) bias correction of local slope estimates. (a) regularization of directional slopes (c) spatially weighted reconstruction using the estimated variability of local slope estimates and (d) more accurate estimates of reference light profiles from tune sequence data These refinements are based on a nonparametric observational model for refractive imaging data Simulation studies show that the refinement can result in substantial improvements in the mean squared error of reconstruction A computationally efficient algorithm that exploits sparsity is used to evaluate the regularized estimator Our approach is illustrated by an application to real image data"
"10.1198/jasa.2009.ap08615","2010","Probabilistic wind spread forecasting using ensembles and {B}ayesian model averaging","2","The current weather forecasting paradigm is deterministric, based on numerical models Multiple estimates of the curl em state of the atmosphere are used to generate an ensemble of deterministic predictions Ensemble forecasts. while providing information on forecast uncertainty. are often uncalibrated Bayesian model averaging (BMA) is a statistical ensemble postprocessing method that creates calibrated predictive probability density functions (PM's) Probabilistic wind forecasting offers two challenges a skewed distribution and observations that are coarsely discretized We extend BMA to wind speed, taking account of these challenges This method provides calibrated and sharp probabilistic forecasts Comparisons are made between several formulations"
"10.1198/jasa.2010.ap09781","2010","Rejoinder [MR2757185; MR2757186; MR2757187]","0",""
"10.1198/jasa.2009.ap08248","2010","A moving average approach for spatial statistical models of stream networks","0","In this article we use moving averages to develop new classes of models in a flexible modeling framework for stream networks Streams and rivers are among our most important resources, yet models with autocorrelated errors for spatially continuous stream networks have been described only recently We develop models based on stream distance rather than on Euclidean distance Spatial autocovariance models developed for Euclidean distance may not be valid when using stream distance We begin by describing a stream topology We then use moving averages to build several classes of valid models for streams Various models are derived depending on Whether the moving average has a ""tail-up"" stream, a ""tail-down"" stream, or a ""two-tail"" construction These models also can account for the volume and direction of flowing water The data tor this article come from the Ecosystem Health Monitoring Program in Southeast Queensland. Australia, an important national program alined at monitoring water quality We model two water chemistry variables. pH and conductivity, for sample sizes close to 100 We estimate fixed effects and make spatial predictions One interesting aspect of stream networks is the possible dichotomy of autocorrelation between flow-connected and flow-unconnected locations For this reason, it is important to have a flexible modeling framework. which we achieve on the example data using a variance component approach"
"10.1198/jasa.2010.ap09811","2010","Statistics: from evidence to policy","0","The discipline of statistics, we as individual statisticians, and the American Statistical Association (ASA) have been called to action We can and must play a vital part in the gathering. analysis. interpretation. and communication of evidence for informed policy decisions I ask you to answer ""yes"" and to take on the role of an honest broker of policy alternatives I ask you to support a proactive engagement by the ASA in informing policy While interaction in policy forums by the ASA is an opportunity, to integrate the science of statistics into policy, it also represents a responsibility. because the association serves not only its members. but also the public Statistics can have a major impact in the journey from evidence to policy A current illustration in the evidence-based medicine context is comparative effectiveness research. which received focus and funding from the American Recovery and Reinvestment Act of 2009 The time is now for us to take action as a discipline. as individuals. and as an association to ensure that policy decisions are driven by evidence"
"10.1198/jasa.2010.tm09295","2010","Letters to the editor","0",""
"10.1198/jasa.2009.ap08629","2010","Bayesian multiscale multiple imputation with implications for data confidentiality","0","Many scientific, sociological, and economic applications present data that are collected on multiple scales of resolution. One particular form of multiscale data arises when data are aggregated across different scales both longitudinally and by economic sector. Frequently, such datasets experience missing observations in a manner that they can be accurately imputed, while respecting the constraints imposed by the multiscale nature of the data, using the method we propose known as Bayesian multiscale multiple imputation. Our approach couples dynamic linear models with a novel imputation step based on singular normal distribution theory. Although our method is of independent interest, one important implication of such methodology is its potential effect on confidential databases protected by means of cell suppression. In order to demonstrate the proposed methodology and to assess the effectiveness of disclosure practices in longitudinal databases, we conduct a large-scale empirical study using the U.S. Bureau of Labor Statistics Quarterly Census of Employment and Wages (QCEW). During the course of our empirical investigation it is determined that several of the predicted cells are within 1% accuracy, thus causing potential concerns for data confidentiality."
"10.1198/jasa.2010.ap09258","2010","Using {DNA} fingerprints to infer familial relationships within {NHANES} {III} households","0","Developing, targeting, and evaluating genomic strategies for population-based disease prevention require population-based data. In response to this urgent need, genotyping has been conducted within the Third National Health and Nutrition Examination (NHANES III), a nationally representative household-interview health survey. However, before these genetic analyses can occur, family relationships within households must be accurately ascertained. Unfortunately, reported family relationships within NHANES III households based on questionnaire data are incomplete and inconclusive with regard to actual biological relatedness of family members. We inferred family relationships within households using DNA fingerprints (Identifiler (R)) that contain the DNA loci used by law enforcement agencies for forensic identification of individuals. The performance of these loci for relationship inference is not well understood, however. We evaluated two competing statistical methods for relationship inference on pairs of household members: an exact likelihood ratio relying on allele frequencies to an identical-by-state (IBS) likelihood ratio that only requires matching alleles. We modified these methods to account for genotyping errors and population substructure. The two methods usually agree on the rankings of the most likely relationships; however, the IBS method underestimates the likelihood ratio by not accounting for the informativeness of matching rare alleles. The likelihood ratio is sensitive to estimates of population substructure, and parent child relationships are sensitive to the specified genotyping error rate. These loci were unable to distinguish second-degree relationships and cousins from being unrelated. The genetic data also are useful for verifying reported relationships and identifying data quality issues. An important byproduct is the first explicitly nationally representative estimates of allele frequencies at these ubiquitous forensic loci."
"10.1198/jasa.2010.ap07441","2010","Spatio-temporal analysis of total nitrate concentrations using dynamic statistical models","0","Atmospheric concentrations of total nitrate (TNO3), defined here as gas-phase nitric acid plus particle-phase nitrate, are difficult to simulate in numerical air quality models due to the presence of a variety of formation pathways and loss mechanisms, some of which are highly uncertain. The goal of this study is to estimate the relative importance of these different pathways across the Eastern United States by identifying empirical relationships that exist between TNO3 concentrations and a set of covariates (ammonium, sulfate, ozone, wind speed, relative humidity, and precipitation) measured from January 1997 to July 2004. We develop two dynamic statistical models to quantify these relationships. A major advantage of these models over typical linear regression models is that their regression coefficients can vary temporally. Results show that TNO3 is sensitive to ozone throughout the year, indicating an importance of daytime photochemical production of TNO3, especially in the Southeast. Sensitivity of TNO3 to residual ammonium (NH4+-2SO(4)(2-)) is most pronounced during winter, indicating a seasonal importance of gas/particle partitioning that is accentuated in the Midwest. Using a number of physical and chemical explanations, confidence is established in the spatial and temporal patterns of several such empirical relationships. In the future, these relationships may be used quantitatively to improve our mechanistic understanding of TNO3 formation pathways and loss mechanisms in the atmosphere."
"10.1198/jasa.2009.ap08746","2010","Synthetic control methods for comparative case studies: estimating the effect of {C}alifornia's tobacco control program","1","Building on an idea in Abadie and Gardeazabal (2003), this article investigates the application of synthetic control methods to comparative case studies. We discuss the advantages of these methods and apply them to study the effects of Proposition 99, a large-scale tobacco control program that California implemented in 1988. We demonstrate that. following Proposition 99, tobacco consumption fell markedly in California relative to a comparable synthetic control region. We estimate that by the year 2000 annual per-capita cigarette sales in California were about 26 packs lower than what they would have been in the absence of Proposition 99. Using new inferential methods proposed in this article, we demonstrate the significance of our estimates. Given that many policy interventions and events of interest in social sciences take place at an aggregate level (countries, regions. cities, etc.) and affect a small number of aggregate units, the potential applicability of synthetic control methods to comparative case studies is very large, especially in situations where traditional regression methods are not appropriate."
"10.1198/jasa.2009.ap08105","2010","A {B}ayesian vector multidimensional scaling procedure for the analysis of ordered preference data","0","Multidimensional scaling (MDS) comprises a family of geometric models for the multidimensional representation of data and a corresponding set of methods for fitting such models to actual data. In this paper, we develop a new Bayesian vector MDS model to analyze ordered successive categories preference/dominance data commonly collected in many social science and business studies. A joint spatial representation of the row and column elements of the input data matrix is provided in a reduced dimensionality such that the geometric relationship of the row and column elements renders insight into the utility structure underlying the data. Unlike classical deterministic MDS procedures, the Bayesian method includes a probability based criterion to determine the number of dimensions of the derived joint space map and provides posterior interval as well as point estimates for parameters of interest. Also, our procedure models the raw integer successive categories data which ameliorates the need of any data preprocessing as required for many metric MDS procedures. Furthermore, the proposed Bayesian procedure allows external information in the form of an intractable posterior distribution derived from a related dataset to be incorporated as a prior in deriving the spatial representation of the preference data. An actual commercial application dealing with consumers' intentions to buy new luxury sport utility vehicles are presented to illustrate the proposed methodology. Favorable comparisons are made with more traditional MDS approaches."
"10.1198/jasa.2010.tm09757","2010","Estimability and likelihood inference for generalized linear mixed models using data cloning","0","Maximum likelihood estimation for Generalized Linear Mixed Models (GLMM), an important class of statistical models with substantial applications in epidemiology, medical statistics, and many other fields, poses significant computational difficulties. In this article, we use data cloning, a simple computational method that exploits advances in Bayesian computation, in particular the Markov Chain Monte Carlo method, to obtain maximum likelihood estimators of the parameters in these models. This method also leads to a simple estimator of the asymptotic variance of the maximum likelihood estimators. Determining estimability of the parameters in a mixed model is, in general, a very difficult problem. Data cloning provides a simple graphical test to not only check if the full set of parameters is estimable but also, and perhaps more importantly, if a specified function of the parameters is estimable. One of the goals of mixed models is to predict random effects. We suggest a frequentist method to obtain prediction intervals for random effects. We illustrate data cloning in the GLMM context by analyzing the Logistic Normal model for over-dispersed binary data, and the Poisson Normal model for repeated and spatial counts data. We consider Normal Normal and Binary Normal mixture models to show how data cloning can be used to study estimability of various parameters. We contend that whenever hierarchical models are used, estimability of the parameters should be checked before drawing scientific inferences or making management decisions. Data cloning facilitates such a check on hierarchical models."
"10.1198/jasa.2010.tm09541","2010","Bootstrapping robust estimates for clustered data","0","In mixed models, the use of robust estimates raises several interesting inferential challenges. One of these challenges arises from the realization that the effect of contamination is to increase the variability in the data, but robust estimates of variance components are usually smaller than their nonrobust counterparts. The robust estimates reflect the variability of the bulk of the data, which is not the same as the variability in the data-generating process. This means that the naive implementation of bootstrap procedures might not work. In this article we consider several bootstrap procedures, including random effect, transformation, and weighted bootstraps. We give conditions for the asymptotic validity of the bootstraps and assess their performance via a small simulation study. Both the transformation and generalized cluster bootstrap perform well and are asymptotically valid under reasonable conditions."
"10.1198/jasa.2010.tm09123","2010","Constrained factor models","0","This article considers estimation and applications of constrained and partially constrained factor models when the dimension of explanatory variables is high. Both the classical and approximate factor models are investigated. For estimation, we employ both the maximum likelihood and least squares methods. We show that the least squares estimation is based on constrained principal component analysis and provides consistent estimates for the model under certain conditions. The normality condition is not used in the derivation. We then propose likelihood ratio statistics to test the adequacy of factor constraints. The test statistic is developed under the normality assumption, but simulation results show that it continues to perform well even if the underlying distribution is Student-t. The constraints are useful tools to incorporate prior information or substantive theory in applications of factor models. In addition, the constraints also serve as a statistical tool to obtain parsimonious econometric models for forecasting, to simplify the interpretations of common factors, and to reduce the dimension. We use simulation and real examples to investigate the performance of constrained estimation in finite samples and to highlight the importance of noise-to-signal ratio in factor analysis. Finally, we compare the constrained model with its unconstrained counterpart both in estimation and in forecasting. This article has supplementary material online."
"10.1198/jasa.2010.tm10134","2010","Optimal and efficient crossover designs for test-control study when subject effects are random","0","We study crossover designs based on the criteria of A-optimality and MV-optimality under the model with random subject effects, for the purpose of comparing several test treatments with a standard control treatment. Optimal and efficient designs are proposed, and their efficiencies are also evaluated. A family of totally balanced test-control incomplete crossover designs based on a function of the ratio of the subject effect variance to the error variance are shown to be highly efficient and robust. The results have interesting connections with those in Hedayat and Yang (2005) and Hedayat, Stufken, and Yang (2006). The omitted proofs in the article are included in a supplemental material online."
"10.1198/jasa.2010.tm10068","2010","Weighted optimality in designed experimentation","1","An optimality framework is developed for designing experiments in which not all treatments are of equal interest, such as those including an established control. Differential interest in treatments is formalized by assignment of weights. incorporated into optimality measures through a weighted version of the information matrix. All conventional measures of design efficacy are shown to have weighted analogs. The properties of weighted measures are explored, some general theory is developed, and weighted optimal designs are determined for unblocked experimentation. This new approach includes ""test treatments versus control"" experiments as a special case. Supplementary materials for the article are available online."
"10.1198/jasa.2010.tm09549","2010","Combining nonparametric and optimal linear time series predictions","0","We introduce a semiparametric procedure for more efficient prediction of a strictly stationary process admitting an ARMA representation. The procedure is based on the estimation of the ARMA representation, followed by a nonparametric regression where the ARMA residuals are used as explanatory variables. Compared to standard nonparametric regression methods, the number of explanatory variables can be reduced because our approach exploits the linear dependence of the process. We establish consistency and asymptotic normality results for our estimator. Numerical experiments show that significant gains can be achieved with our approach. All the supplemental materials used by this article are available online."
"10.1198/jasa.2010.tm10130","2010","Variable selection using adaptive nonlinear interaction structures in high dimensions","0","Numerous penalization based methods have been proposed for fitting a traditional linear regression model in which the number of predictors, p, is large relative to the number of observations, n. Most of these approaches assume sparsity in the underlying coefficients and perform some form of variable selection. Recently, some of this work has been extended to nonlinear additive regression models. However, in many contexts one wishes to allow for the possibility of interactions among the predictors. This poses serious statistical and computational difficulties when p is large, as the number of candidate interaction terms is of order p(2). We introduce a new approach, ""Variable selection using Adaptive Nonlinear Interaction Structures in High dimensions"" (VANISH), that is based on a penalized least squares criterion and is designed for high dimensional nonlinear problems. Our criterion is convex and enforces the heredity constraint, in other words if an interaction term is added to the model, then the corresponding main effects are automatically included. We provide theoretical conditions under which VANISH will select the correct main effects and interactions. These conditions suggest that VANISH should outperform certain natural competitors when the true interaction structure is sufficiently sparse. Detailed simulation results are also provided, demonstrating that VANISH is computationally efficient and can be applied to nonlinear models involving thousands of terms while producing superior predictive performance over other approaches."
"10.1198/jasa.2010.tm09414","2010","Composite likelihood {B}ayesian information criteria for model selection in high-dimensional data","0","For high-dimensional data sets with complicated dependency structures, the full likelihood approach often leads to intractable computational complexity. This imposes difficulty on model selection, given that most traditionally used information criteria require evaluation of the full likelihood. We propose a composite likelihood version of the Bayes information criterion (BIC) and establish its consistency property for the selection of the true underlying marginal model. Our proposed BIC is shown to be selection-consistent under some mild regularity conditions, where the number of potential model parameters is allowed to increase to infinity at a certain rate of the sample size. Simulation studies demonstrate the empirical performance of this new BIC, especially for the scenario where the number of parameters increases with sample size. Technical proofs of our theoretical results are provided in the online supplemental materials."
"10.1198/jasa.2010.tm10128","2010","Consistent model selection for marginal generalized additive model for correlated data","0","We consider the generalized additive model when responses from the same cluster are correlated. Incorporating correlation in the estimation of nonparametric components for the generalized additive model is important because it improves estimation efficiency and increases statistical power for model selection. In our setting, there is no specified likelihood function for the generalized additive model, because the outcomes could be nonnormal and discrete, which makes estimation and model selection very challenging problems. We propose consistent estimation and model selection that incorporate the correlation structure. We establish an asymptotic property with L(2)-norm consistency for the nonparametric components, which achieves the optimal rate of convergence. In addition, the proposed model selection strategy is able to select the correct generalized additive model consistently. That is, with probability approaching to 1, the estimators for the zero function components converge to 0 almost surely. We illustrate our method using numerical studies with both continuous and binary responses, along with a real data application of binary periodontal data.Supplemental materials including technical details are available online."
"10.1198/jasa.2010.tm10163","2010","High-frequency covariance estimates with noisy and asynchronous financial data","1","This article proposes a consistent and efficient estimator of the high-frequency covariance (quadratic covariation) of two arbitrary assets, observed asynchronously with market microstructure noise. This estimator is built on the marriage of the quasi maximum likelihood estimator of the quadratic variation and the proposed generalized synchronization scheme and thus is not influenced by the Epps effect. Moreover, the estimation procedure is free of tuning parameters or bandwidths and is readily implementable. Monte Carlo simulations show the advantage of this estimator by comparing it with a variety of estimators with specific synchronization methods. The empirical studies of six foreign exchange future contracts illustrate the time-varying correlations of the currencies during the 2008 global financial crisis, demonstrating the similarities and differences in their roles as key currencies in the global market."
"10.1198/jasa.2010.tm09534","2010","Pseudo-empirical likelihood inference for multiple frame surveys","0","This article presents a pseudo-empirical likelihood approach to inference for multiple-frame surveys. We establish a unified framework for point and interval estimation of finite population parameters, and show that inferences on the parameters of interest making effective use of different types of auxiliary population information can be conveniently carried out through the constrained maximization of the pseudo-empirical likelihood function. Confidence intervals are constructed using either the asymptotic chi(2) distribution of an adjusted pseudo-empirical likelihood ratio statistic or a bootstrap calibration method. Simulation results based on Statistics Canada's Family Expenditure Survey data show that the proposed methods perform well in finite samples for both point and interval estimation. In particular, a multiplicity-based pseudo-empirical likelihood method is proposed. This method is easily used for multiple-frame surveys with more than two frames and does not require complete frame membership information. The proposed pseudo-empirical likelihood ratio confidence intervals have a clear advantage over the conventional normal approximation-based intervals in estimating population proportions of rare items, a scenario that often motivates the use of multiple-frame surveys. All related computational problems can be handled using existing algorithms for pseudo-empirical likelihood methods with single-frame surveys."
"10.1198/jasa.2010.tm09181","2010","Multiple change-point estimation with a total variation penalty","0","We propose a new approach for dealing with the estimation of the location of change-points in one-dimensional piecewise constant signals observed in white noise. Our approach consists in reframing this task in a variable selection context. We use a penalized least-square criterion with a L-1-type penalty for this purpose. We explain how to implement this method in practice by using the LARS/LASSO algorithm. We then prove that, in an appropriate asymptotic framework, this method provides consistent estimators of the change points with an almost optimal rate. We finally provide an improved practical version of this method by combining it with a reduced version of the dynamic programming algorithm and we successfully compare it with classical methods."
"10.1198/jasa.2010.tm09572","2010","Modeling longitudinal data using a pair-copula decomposition of serial dependence","1","Copulas have proven to be very successful tools for the flexible modeling of cross-sectional dependence. In this paper we express the dependence structure of continuous-valued time series data using a sequence of bivariate copulas. This corresponds to a type of decomposition recently called a ""vine"" in the graphical models literature, where each copula is entitled a ""pair-copula."" We propose a Bayesian approach for the estimation of this dependence structure for longitudinal data. Bayesian selection ideas are used to identify any independence pair-copulas, with the end result being a parsimonious representation of a time-inhomogeneous Markov process of varying order. Estimates are Bayesian model averages over the distribution of the lag structure of the Markov process. Using a simulation study we show that the selection approach is reliable and can improve the estimates of both conditional and unconditional pairwise dependencies substantially. We also show that a vine with selection outperforms a Gaussian copula with a flexible correlation matrix. The advantage of the pair-copula formulation is further demonstrated using a longitudinal model of intraday electricity load. Using Gaussian, Gumbel, and Clayton pair-copulas we identify parsimonious decompositions of intraday serial dependence, which improve the accuracy of intraday load forecasts. We also propose a new diagnostic for measuring the goodness of fit of high-dimensional multivariate copulas. Overall, the pair-copula model is very general and the Bayesian method generalizes many previous approaches for the analysis of longitudinal data. Supplemental materials for the article are also available online."
"10.1198/jasa.2010.tm09666","2010","Dimension reduction in regressions through cumulative slicing estimation","0","In this paper we offer a complete methodology of cumulative slicing estimation to sufficient dimension reduction. In parallel to the classical slicing estimation, we develop three methods that are termed, respectively, as cumulative mean estimation, cumulative variance estimation, and cumulative directional regression. The strong consistency for p = O(n(1/2)/log n) and the asymptotic normality for p = o(n(1/2)) are established, where p is the dimension of the predictors and n is sample size. Such asymptotic results improve the rate p = o(n(1/3)) in many existing contexts of semiparametric modeling. In addition, we propose a modified BIC-type criterion to estimate the structural dimension of the central subspace. Its consistency is established when p = o(n(1/2)). Extensive simulations are carried out for comparison with existing methods and a real data example is presented for illustration."
"10.1198/jasa.2010.tm10195","2010","A hidden {M}arkov model approach to testing multiple hypotheses on a tree-transformed gene ontology graph","0","Gene category testing problems involve testing hundreds of null hypotheses that correspond to nodes in a directed acyclic graph. The logical relationships among the nodes in the graph imply that only some configurations of true and false null hypotheses are possible and that a test for a given node should depend on data from neighboring nodes. We developed a method based on a hidden Markov model that takes the whole graph into account and provides coherent decisions in this structured multiple hypothesis testing problem. The method is illustrated by testing Gene Ontology terms for evidence of differential expression."
"10.1198/jasa.2010.tm10570","2010","Chronic disease prevention research methods and their reliability, with illustrations from the {W}omen's {H}ealth {I}nitiative","0","This article reviews the status of statistical methods for chronic disease prevention research, with emphasis on the reliability of findings and on future methodological needs and opportunities. Observational studies, especially cohort studies, play a major role in disease prevention research, but depend on adequate confounding control methods for a useful interpretation. Stratification and regression methods that are commonly used to control confounding are described, and comparative findings from the Women's Health Initiative (WHI) randomized controlled trial and companion cohort study of the benefits and risk of postmenopausal hormone therapy are used to illustrate the success of these methods. Measurement error in exposure assessment may be a potentially dominating source of bias in such important prevention research areas as nutrition and physical activity epidemiology. Statistical methods to correct for measurement error are briefly reviewed, and the need for methods to accommodate systematic bias in exposure assessment is described. Recent analysis using nutrient exposure biomarkers in WHI cohorts is used to illustrate the impact of such methods. Randomized, controlled intervention trials have the potential to obviate these biases and to reliably assess intervention benefits and risks. However, trials in healthy persons with chronic disease outcomes are typically large, long-term, and expensive. Statistical methods for randomized, controlled prevention trials are briefly reviewed, and the roles of trials in the overall chronic disease prevention research enterprise are examined. Given the logistical and cost challenges, a full-scale disease prevention trial needs to be preceded by careful hypothesis development and initial testing. The potential role of biomarkers, especially high-dimensional biomarkers, in disease prevention hypothesis development, is described and illustrated. The presentation concludes with comments on the methodological research and research infrastructure developments needed to invigorate the chronic disease prevention research agenda, with emphasis on the important role for statisticians in enhancing prevention research methods and applications."
"10.1198/jasa.2010.ap09485","2010","Correcting for survey nonresponse using variable response propensity","0","All surveys with less than full response potentially suffer from nonresponse bias. Poststratification weights can only correct for selection into the sample based on observables whose distribution is known in the population. Variables such as gender, race, income, and region satisfy this requirement because they are available from the U.S. Census Bureau, but poststratification based on these variables may not eliminate nonresponse bias. I develop an approach for correcting for nonignorable nonresponse bias. Survey respondents can be classified by their ""response propensity."" Proxies for response propensity include the number of attempted phone calls, indicators of temporary refusal, and interviewer-coded measures of cooperativeness. We can then learn about the population of nonrespondents by extrapolating from the low-propensity respondents. I apply this new estimator to correct for unit nonresponse bias in the American National Election Study and in a CBS/New York Times preelection poll. I find that nonresponse bias can be a serious problem, particularly for items that relate to political participation. I find that my method is successful in substantially reducing nonresponse bias."
"10.1198/jasa.2010.ap09655","2010","Autoregressive mixture models for dynamic spatial {P}oisson processes: application to tracking intensity of violent crime","0","This article develops a set of tools for smoothing and prediction with dependent point event patterns. The methodology is motivated by the problem of tracking weekly maps of violent crime events, but is designed to be straightforward to adapt to a wide variety of alternative settings. In particular, a Bayesian semiparametric framework is introduced for modeling correlated time series of marked spatial Poisson processes. The likelihood is factored into two independent components: the set of total integrated intensities and a series of process densities. For the former it is assumed that Poisson intensities arc realizations from a dynamic linear model. In the latter case, a novel class of dependent stick-breaking mixture models are proposed to allow nonparametric density estimates to evolve in discrete time. This, a simple and flexible new model for dependent random distributions, is based on autoregressive time series of marginally beta random variables applied as correlated stick-breaking proportions. The approach allows for marginal Dirichlet process priors at each time and adds only a single new correlation term to the static model specification. Sequential Monte Carlo algorithms are described for online inference with each model component, and marginal likelihood calculations form the basis for inference about parameters governing temporal dynamics. Simulated examples are provided to illustrate the methodology, and we close with results for the motivating application of tracking violent crime in Cincinnati."
"10.1198/jasa.2010.ap09323","2010","Estimating individual-level risk in spatial epidemiology using spatially aggregated information on the population at risk","0","We propose a novel alternative to case-control sampling for the estimation of individual-level risk in spatial epidemiology. Our approach uses weighted estimating equations to estimate regression parameters in the intensity function of an inhomogeneous spatial point process, when information on risk-factors is available at the individual level for cases, but only at a spatially aggregated level for the population at risk. We develop data-driven methods to select the weights used in the estimating equations and show through simulation that the choice of weights can have a major impact on efficiency of estimation. We develop a formal test to detect non-Poisson behavior in the underlying point process and assess the performance of the test using simulations of Poisson and Poisson cluster point processes. We apply our methods to data on the spatial distribution of childhood meningococcal disease cases in Merseyside, U.K. between 1981 and 2007."
"10.1198/jasa.2010.ap09039","2010","Localized realized volatility modeling","0","With the recent availability of high-frequency financial data the long-range dependence of volatility regained researchers' interest and has led to the consideration of long-memory models for volatility. The long-range diagnosis of volatility, however, is usually stated for long sample periods, while for small sample sizes, such as one year, the volatility dynamics appears to be better described by short-memory processes. The ensemble of these seemingly contradictory phenomena point towards short-memory models of volatility with nonstationarities, such as structural breaks or regime switches, that spuriously generate a long memory pattern. In this paper we adopt this view on the dependence structure of volatility and propose a localized procedure for modeling realized volatility. That is at each point in time we determine a past interval over which volatility is approximated by a local linear process. A simulation study shows that long memory processes as well as short memory processes with structural breaks can be well approximated by this local approach. Furthermore, using S&P500 data we find that our local modeling approach outperforms long-memory type models and models with structural breaks in terms of predictability."
"10.1198/jasa.2010.ap09250","2010","Bayesian random segmentation models to identify shared copy number aberrations for array {CGH} data","0","Array-based comparative genomic hybridization (aCGH) is a high-resolution, high-throughput technique for studying the genetic basis of cancer. The resulting data consist of log fluorescence ratios as a function of the genomic DNA location and provide a cytogenetic representation of the relative DNA copy number variation. Analysis of such data typically involves estimating the underlying copy number state at each location and segmenting regions of DNA with similar copy number states. Most current methods proceed by modeling a single sample/array at a time, and thus fail to borrow strength across multiple samples to infer shared regions of copy number aberrations. We propose a hierarchical Bayesian random segmentation approach for modeling aCGH data that uses information across arrays from a common population to yield segments of shared copy number changes. These changes characterize the underlying population and allow us to compare different population aCGH profiles to assess which regions of the genome have differential alterations. Our method, which we term Bayesian detection of shared aberrations in aCGH (BDSAScgh), is based on a unified Bayesian hierarchical model that allows us to obtain probabilities of alteration states as well as probabilities of differential alterations that correspond to local false discovery rates for both single and multiple groups. We evaluate the operating characteristics of our method via simulations and an application using a lung cancer aCGH data set. This article has supplementary material online."
"10.1198/jasa.2010.ap09480","2010","Sampling with synthesis: a new approach for releasing public use census microdata","0","Many statistical agencies disseminate samples of census microdata, that is, data on individual records, to the public. Before releasing the microdata, agencies typically alter identifying or sensitive values to protect data subjects' confidentiality, for example by coarsening, perturbing, or swapping data. These standard disclosure limitation techniques distort relationships and distributional features in the original data, especially when applied with high intensity. Furthermore, it can be difficult for analysts of the masked public use data to adjust inferences for the effects of the disclosure limitation. Motivated by these shortcomings, we propose an approach to census microdata dissemination called sampling with synthesis. The basic idea is to replace the identifying or sensitive values in the census with multiple imputations, and release samples from these multiply-imputed populations. We demonstrate that sampling with synthesis can improve the quality of public use data relative to sampling followed by standard statistical disclosure limitation; simulation results showing this are available online as supplemental material. We derive methods for analyzing the multiple datasets generated by sampling with synthesis. We present algorithms for selecting which census values to synthesize based on considerations of disclosure risk and data utility. We illustrate sampling with synthesis on a population constructed with data from the U.S. Current Population Survey."
"10.1198/jasa.2010.ap09321","2010","A {B}ayesian shrinkage model for incomplete longitudinal binary data with application to the breast cancer prevention trial","0","We consider inference in randomized longitudinal studies with missing data that is generated by skipped clinic visits and loss to followup. In this setting, it is well known that full data estimands are not identified unless unverified assumptions are imposed. We assume a non-future dependence model for the drop-out mechanism and partial ignorability for the intermittent missingness. We posit an exponential tilt model that links nonidentifiable distributions and distributions identified under partial ignorability. This exponential tilt model is indexed by nonidentified parameters, which are assumed to have an informative prior distribution, elicited from subject-matter experts. Under this model, full data estimands are shown to be expressed as functionals of the distribution of the observed data. To avoid the curse of dimensionality, we model the distribution of the observed data using a Bayesian shrinkage model. In a simulation study, we compare our approach to a fully parametric and a fully saturated model for the distribution of the observed data. Our methodology is motivated by, and applied to, data from the Breast Cancer Prevention Trial."
"10.1198/jasa.2010.ap09185","2010","Using a short screening scale for small-area estimation of mental illness prevalence for schools","0","We use data collected in the National Comorbidity Survey-Adolescent (NCS-A) to develop a methodology to estimate the small-area prevalence of serious emotional distress (SED) in schools in the United States, exploiting the clustering of the main NCS-A sample by school. The NCS-A instrument includes both a short screening scale, the K6, and extensive diagnostic assessments of the individual disorders and associated impairment that determine the diagnosis of SED. We fitted a Bayesian bivariate multilevel regression model with correlated effects for the probability of SED and a modified K6 score at the individual and school levels. Our results provide evidence for the existence of variation in the prevalence of SED across schools and geographical regions. Although the concordance between the modified K6 scale and SED is only modest for individuals, the school-level random effects for the two measures are strongly correlated. Under this model we obtain a prediction equation for the rate of SED based on the mean K6 score and covariates. This finding supports the feasibility of using short screening scales like the K6 as an alternative to more comprehensive lay assessments in estimating school-level rates of SED. These methods may be applicable to other studies aiming at small-area estimation for geographical units."
"10.1198/jasa.2010.ap09581","2010","Modeling competing infectious pathogens from a {B}ayesian perspective: application to influenza studies with incomplete laboratory results","0","In seasonal influenza epidemics, pathogens such as respiratory syncytial virus (RSV) often cocirculate with influenza and cause influenza-like illness (ILL) in human hosts. However, it is often impractical to test for each potential pathogen or to collect specimens for each observed ILI episode, making inference about influenza transmission difficult. In the setting of infectious diseases, missing outcomes impose a particular challenge because of the dependence among individuals. We propose a Bayesian competing-risk model for multiple cocirculating pathogens for inference on transmissibility and intervention efficacies under the assumption that missingness in the biological confirmation of the pathogen is ignorable. Simulation studies indicate a reasonable performance of the proposed model even if the number of potential pathogens is misspecified. They also show that a moderate amount of missing laboratory test results has only a small impact on inference about key parameters in the setting of close contact groups. Using the proposed model, we found that a nonpharmaceutical intervention is marginally protective against transmission of influenza A in a study conducted in elementary schools."
"10.1198/jasa.2010.ap08663","2010","Self-selectivity in firm's decision to withdraw {IPO}: {B}ayesian inference for hazard models of bankruptcy with feedback","0","Examination on firm performance subsequent to a chosen event is widely used in finance studies to analyze the motivation behind managerial decisions. However, results are often subject to bias when the self-selectivity behind managerial decisions is ignored and unspecified. This study investigates a unique corporate event of initial public offering (IPO) withdrawal, where a firm's subsequent likelihood of bankruptcy is specified in a system of switching hazard models, and the expected difference in post-IPO and postwithdrawal survival probabilities serves as a ""feedback"" on a firm's decision to cancel its offering. Our Bayesian inference procedure generates strong evidence that incidence of withdrawal unfavorably affects the subsequent performance of a firm, and that the ""feedback"" is an important determinant in managerial decisions. The econometric and statistical model specification and the accompanying estimation procedure we used can be widely applicable to study self-selective corporate transactions."
"10.1198/jasa.2010.ap09490","2010","Building a stronger instrument in an observational study of perinatal care for premature infants","0","An instrument is a random nudge toward acceptance of a treatment that affects outcomes only to the extent that it affects acceptance of the treatment. Nonetheless, in settings in which treatment assignment is mostly deliberate and not random, there may exist some essentially random nudges to accept treatment, so that use of an instrument might extract bits of random treatment assignment from a setting that is otherwise quite biased in its treatment assignments. An instrument is weak if the random nudges barely influence treatment assignment or strong if the nudges are often decisive in influencing treatment assignment. Although ideally an ostensibly random instrument is perfectly random and not biased, it is not possible to be certain of this; thus a typical concern is that even the instrument might be biased to some degree. It is known from theoretical arguments that weak instruments are invariably sensitive to extremely small biases; for this reason, strong instruments are preferred. The strength of an instrument is often taken as a given. It is not. In an evaluation of effects of perinatal care on the mortality of premature infants, we show that it is possible to build a stronger instrument, we show how to do it, and we show that success in this task is critically important. We also develop methods of permutation inference for effect ratios, a key component in an instrumental variable analysis."
"10.1198/jasa.2010.tm09016","2010","Using calibration weighting to adjust for nonignorable unit nonresponse","0","When calibration weighting is be used to adjust for unit nonresponse in a sample survey, the response/nonresponse mechanism is often assumed to be a function of a set of covariates, which we call ""model variables."" These model variables usually also serve as the benchmark variables in the calibration equation. In principle, however, the model variables do not have to coincide with the benchmark variables. Since the model-variable values need only be known for the respondents, this allows the treatment of what is usually considered nonignorable nonresponse in the prediction approach to survey sampling. One can invoke either a quasi-randomization or prediction approach to justify calibration weighting as a means for adjusting for nonresponse. Both frameworks rely on unverifiable model assumptions, and both require large samples to produce nearly unbiased estimators even when those assumptions hold. We will explore these issues theoretically using a joint framework and with an empirical study."
"10.1198/jasa.2010.tm09228","2010","Functional varying coefficient models for longitudinal data","1","The proposed functional varying coefficient model provides a versatile and flexible analysis tool for relating longitudinal responses to longitudinal predictors. Specifically, this approach provides a novel representation of varying coefficient functions through suitable auto and cross-covariances of the underlying stochastic processes, which is particularly advantageous for sparse and irregular designs, as often encountered in longitudinal studies. Existing methodology for varying coefficient models is not adapted to such data. The proposed approach extends the customary varying coefficient models to a more general setting, in which not only current but also recent past values of the predictor time course may have an impact on the current value of the response time course. The influence of past predictor values is modeled by a smooth history index function, while the effects on the response are described by smooth varying coefficient functions. The resulting estimators for varying coefficient and history index functions are shown to be asymptotically consistent for sparse designs. In addition, prediction of unobserved response trajectories from sparse measurements on a predictor trajectory is obtained, along with asymptotic pointwise confidence bands. The proposed methods perform well in simulations, especially when compared with commonly used local polynomial smoothing methods for varying coefficient models, and are illustrated with longitudinal primary biliary liver cirrhosis data. The data application and detailed assumptions and proofs can be found in online Supplemental Material."
"10.1198/jasa.2010.tm09754","2010","Spectral connectivity analysis","0","Spectral kernel methods are techniques or mapping data into a coordinate system that efficiently reveals the geometric structure-in particular, the ""connectivity""-of the data. These methods depend on tuning parameters. We analyze the dependence of the method on these tuning parameters. We focus on one particular technique-diffusion maps-but our analysis can be used for other spectral methods as well. We identify the key population quantities, we define an appropriate risk function for analyzing the estimators, and we explain how these methods relate to classical kernel smoothing. We also show that, in some cases, fast rates of convergence are possible even in high dimensions. The Appendix of the article is available online as supplementary materials."
"10.1198/jasa.2010.tm10103","2010","Testing for change points in time series","0","This article considers the CUSUM-based (cumulative sum) test for a change point in a time series. In the case of testing for a mean shift, the traditional Kolmogorov Smirnov test statistic involves a consistent long-run variance estimator, which is needed to make the limiting null distribution free of nuisance parameters. The commonly used lag-window type long-run variance estimator requires to choose a bandwidth parameter and its selection is a difficult task in practice. The bandwidth that is a fixed function of the sample size (e.g., n(1/3), where n is sample size) is not adaptive to the magnitude of the dependence in the series, whereas the data-dependent bandwidth could lead to nonmonotonic power as shown in previous studies. In this article, we propose a self-normalization (SN) based Kolmogorov Smirnov test, where the formation of the self-normalizer takes the change point alternative into account. The resulting test statistic is asymptotically distribution free and its power is monotonic. Furthermore, we extend the SN-based test to test for a change in other parameters associated with a time series, such as marginal median, autocorrelation at lag one, and spectrum at certain frequency bands. The use of the SN idea thus allows a unified treatment and offers a new perspective to the large literature of change point detection in the time series setting. Monte Carlo simulations are conducted to compare the finite sample performance of the new SN-based test with the traditional Kolmogorov Smirnov test. Illustrations using real data examples are presented."
"10.1198/jasa.2010.tm09329","2010","False discovery rate control with groups","0","In the context of large-scale multiple hypothesis testing, the hypotheses often possess certain group structures based on additional information such as Gene Ontology in gene expression data and phenotypes in genome-wide association studies. It is hence desirable to incorporate such information when dealing with multiplicity problems to increase statistical power. In this article, we demonstrate the benefit of considering group structure by presenting a p-value weighting procedure which utilizes the relative importance of each group while controlling the false discovery rate under weak conditions. The procedure is easy to implement and shown to be more powerful than the classical Benjamini-Hochberg procedure in both theoretical and simulation studies. By estimating the proportion of true null hypotheses. the data-driven procedure controls the false discovery rate asymptotically. Our analysis on one breast cancer dataset confirms that the procedure performs favorably compared with the classical method."
"10.1198/jasa.2010.tm08177","2010","Bayesian variable selection in structured high-dimensional covariate spaces with applications in genomics","0","We consider the problem of variable selection in regression modeling in high-dimensional spaces where there is known structure among the covariates. This is an unconventional variable selection problem for two reasons: (1) The dimension of the covariate space is comparable, and often much larger, than the number of subjects in the study. and (2) the covariate space is highly structured, and in some cases it is desirable to incorporate this structural information in to the model building process. We approach this problem through the Bayesian variable selection framework, where we assume that the covariates lie on an undirected graph and formulate an Ising prior on the model space for incorporating structural information. Certain computational and statistical problems arise that are unique to such high-dimensional, structured settings, the most interesting being the phenomenon of phase transitions. We propose theoretical and computational schemes to mitigate these problems. We illustrate our methods on two different graph structures: the linear chain and the regular graph of degree k. Finally, we use our methods to study a specific application in genomics: the modeling of transcription factor binding sites in DNA sequences."
"10.1198/jasa.2010.tm09643","2010","Groupwise dimension reduction","0","In many regression applications, the predictors fall naturally into a number of groups or domains, and it is often desirable to establish a domain-specific relation between the predictors and the response. In this article, we consider dimension reduction that incorporates such domain knowledge. The proposed method is based on the derivative of the conditional mean, where the differential operator is constrained to the form of a direct sum. This formulation also accommodates the situations where dimension reduction is focused only on part of the predictors; as such it extends Partial Dimension Reduction to cases where the blocked predictors are continuous. Through simulation and real data analyses, we show that the proposed method achieves greater accuracy and interpretability than the dimension reduction methods that ignore group information. Furthermore, the new method does not require the stringent conditions on the predictor distribution that are required by existing methods."
"10.1198/jasa.2010.tm09448","2010","Approximate {B}ayesian computation: a nonparametric perspective","1","Approximate Bayesian Computation is a family of likelihood-free inference techniques that are well suited to models defined in terms of a stochastic generating mechanism. In a nutshell, Approximate Bayesian Computation proceeds by computing summary statistics s(obs) from the data and simulating summary statistics for different values of the parameter Theta. The posterior distribution is then approximated by an estimator of the conditional density g(Theta vertical bar s(obs)). In this paper, we derive the asymptotic bias and variance of the standard estimators of the posterior distribution which are based on rejection sampling and linear adjustment. Additionally, we introduce an original estimator of the posterior distribution based on quadratic adjustment and we show that its bias contains a fewer number of terms than the estimator with linear adjustment. Although we find that the estimators with adjustment are not universally superior to the estimator based on rejection sampling, we find that they can achieve better performance when there is a nearly homoscedastic relationship between the summary statistics and the parameter of interest. To make this relationship as homoscedastic as possible, we propose to use transformations of the summary statistics. In different examples borrowed from the population genetics and epidemiological literature, we show the potential of the methods with adjustment and of the transformations of the summary statistics. Supplemental materials containing the details of the proofs are available online."
"10.1198/jasa.2010.tm09420","2010","Mat\'ern cross-covariance functions for multivariate random fields","2","We introduce a flexible parametric family of matrix-valued covariance functions for multivariate spatial random fields, where each constituent component is a Matern process. The model parameters are interpretable in terms of process variance, smoothness, correlation length, and colocated correlation coefficients, which can be positive or negative. Both the marginal and the cross-covariance functions are of the Matern type. In a data example on error fields for numerical predictions of surface pressure and temperature over the North American Pacific Northwest, we compare the bivariate Matern model to the traditional linear model of coregionalization."
"10.1198/jasa.2010.tm10083","2010","Optimal sparse segment identification with application in copy number variation analysis","1","Motivated by DNA copy number variation (CNV) analysis based on high-density single nucleotide polymorphism (SNP) data, we consider the problem of detecting and identifying sparse short segments in a long one-dimensional sequence of data with additive Gaussian white noise, where the number, length, and location of the segments are unknown. We present a statistical characterization of the identifiable region of a segment where it is possible to reliably separate the segment from noise. An efficient likelihood ratio selection (LRS) procedure for identifying the segments is developed, and the asymptotic optimality of this method is presented in the sense that the LRS can separate the signal segments from the noise as long as the signal segments are in the identifiable regions. The proposed method is demonstrated with simulations and analysis of a real dataset on identification of copy number variants based on high-density SNP data. The results show that the LRS procedure can yield greater gain in power for detecting the true segments than some standard signal identification methods."
"10.1198/jasa.2010.tm09365","2010","Nested lattice sampling: a new sampling scheme derived by randomizing nested orthogonal arrays","0","A nested orthogonal array is an orthogonal array that contains a smaller orthogonal array as a subarray. We construct a new sampling scheme, called nested lattice sampling, by randomizing nested orthogonal arrays with nested permutations. For a pair of nested lattice samples associated with a nested orthogonal array of strength t, the points in both samples achieve uniformity in t or lower dimensions. Some statistical properties of nested lattice samples based on strength two nested orthogonal arrays are derived. The proposed sampling scheme is useful for running multi fidelity computer experiments, sequential evaluation of computer models, and calibration and validation of computer models. This article has supplementary material online."
"10.1198/jasa.2010.tm08463","2010","Nonparametric regression with missing outcomes using weighted kernel estimating equations","0","We consider nonparametric regression of a scalar outcome on a covariate when the outcome is missing at random (MAR) given the covariate and other observed auxiliary variables. We propose a class of augmented inverse probability weighted (AIPW) kernel estimating equations for nonparametric regression under MAR. We show that AIPW kernel estimators are consistent when the probability that the outcome is observed, that is, the selection probability, is either known by design or estimated under a correctly specified model. In addition, we show that a specific AIPW kernel estimator in our class that employs the fitted values from a model for the conditional mean of the outcome given covariates and auxiliaries is double-robust, that is, it remains consistent if this model is correctly specified even if the selection probabilities are modeled or specified incorrectly. Furthermore, when both models happen to be right, this double-robust estimator attains the smallest possible asymptotic variance of all AIPW kernel estimators and maximally extracts the information in the auxiliary variables. We also describe a simple correction to the A IPW kernel estimating equations that while preserving double-robustness it ensures efficiency improvement over nonaugmented IPW estimation when the selection model is correctly specified regardless of the validity of the second model used in the augmentation term. We perform simulations to evaluate the finite sample performance of the proposed estimators, and apply the methods to the analysis of the AIDS Costs and Services Utilization Survey data. Technical proofs are available online."
"10.1198/jasa.2010.tm09160","2010","Goodness of fit for generalized linear latent variables models","0","Generalized Linear Latent Variables Models (GLLVM) enable the modeling of relationships between manifest and latent variables, where the manifest variables are distributed according to a distribution of the exponential family (e.g., binomial or normal) and to the multinomial distribution (for ordinal manifest variables). These models are widely used in social sciences. To test the appropriateness of a particular model, one needs to define a goodness-of-fit test statistic (GFI). In the normal case, one can use a likelihood ratio test or a modified version proposed by Satorra and Bender (2001) (S&B GFI) that compares the sample covariance matrix to the estimated covariance matrix induced by the model. In the binary case. Pearson-type test statistics can be used if the number of observations is sufficiently large. In the other cases, including the case of mixed type's of manifest variables, there exists GFI based on a comparison between a pseudo sample covariance and the model covariance of the manifest variables. These types of GFI are based on latent variable models that suppose that the manifest variables are themselves induced by underlying normal variables (underlying variable approach). The pseudo sample covariance matrices are then made of polychoric, tetrachoric or polyserial correlations. In this article, we propose an alternative GFI that is more generally applicable. It is based on some distance comparison between the latent scores and the original data. This GFI takes into account the nature of each manifest variable and can in principle be applied in various situations and in particular with models with ordinal, and both discrete and continuous manifest variables. To compute the p-value associated to our GFI, we propose a consistent resampling technique that can be viewed as a modified parametric bootstrap. A simulation study shows that our GFI has good performance in terms of empirical level and empirical power across different models with different types of manifest variables. This article has supplementary material online."
"10.1198/jasa.2010.tm09794","2010","Tests for error correlation in the functional linear model","0","The paper proposes two inferential tests for error correlation in the functional linear model, which complement the available graphical goodness-of-fit checks. To construct them, finite dimensional residuals are computed in two different ways, and then their autocorrelations are suitably defined. From these autocorrelation matrices, two quadratic forms are constructed whose limiting distribution are chi-squared with known numbers of degrees of freedom (different for the two forms). The asymptotic approximations are suitable for moderate sample sizes. The test statistics can be relatively easily computed using the R package f.da, or similar MATLAB software. Application of the tests is illustrated on magnetometer and financial data. The asymptotic theory emphasizes the differences between the standard vector linear regression and the functional linear regression. To understand the behavior of the residuals obtained from the functional linear model, the interplay of three types of approximation errors must be considered, whose sources are: projection on a finite dimensional subspace, estimation of the optimal subspace, and estimation of the regression kernel."
"10.1198/jasa.2010.tm09307","2010","Least absolute relative error estimation","1","Multiplicative regression model or accelerated failure time model, which becomes linear regression model after logarithmic transformation, is useful in analyzing data with positive responses, such as stock prices or life times, that are particularly common in economic/financial or biomedical studies. Least squares or least absolute deviation are among the most widely used criterions in statistical estimation for linear regression model. However, in many practical applications, especially in treating, for example, stock price data, the size of relative error, rather than that of error itself, is the central concern of the practitioners. This paper offers an alternative to the traditional estimation methods by considering minimizing the least absolute relative errors for multiplicative regression models. We prove consistency and asymptotic normality and provide an inference approach via random weighting. We also specify the error distribution, with which the proposed least absolute relative errors estimation is efficient. Supportive evidence is shown in simulation studies. Application is illustrated in an analysis of stock returns in Hong Kong Stock Exchange."
"10.1198/jasa.2010.tm09467","2010","A new approach to optimal design for linear models with correlated observations","0","We consider the problem of designing experiments for regression in the presence of correlated observations with the location model as the main example. For a fixed correlation structure approximate optimal designs are determined explicitly, and it is demonstrated that under the model assumptions made by Bickel and Herzberg (1979) for the determination of asymptotic optimal design, the designs derived in this article converge weakly to the measures obtained by these authors.We also compare the asymptotic optimal design concepts of Sacks and Ylvisaker (1966, 1968) and Bickel and Herzberg (1979) and point out some inconsistencies of the latter. Finally, we combine the best features of both concepts to develop a new approach for the design of experiments for correlated observations, and it is demonstrated that the resulting design problems are related to the (logarithmic) potential theory."
"10.1198/jasa.2010.tm09032","2010","Testing the order of a finite mixture","0","The order is an important parameter in applications of finite mixture models. Yet designing a valid and easy-to-use statistical test for the order is challenging. To date, most results on hypothesis tests have focused on homogeneity, a special case where the null model has order I. In this work, we designed an EM test for the general problem of testing the null hypothesis of order m(0) versus an alternative hypothesis of order larger than m(0). For any positive integer m(0), the null limiting distribution of the EM test is a mixture of chi(2) distributions. The weights in this mixture-limiting distribution can be conveniently computed. Compared with related results, the new result is obtained under much less strict requirements on the component distribution and the parameter space. Extensive simulation studies show that the limiting distributions closely match the finite sample distributions of the EM test. When m(0) = 2, the new EM test has more accurate type I errors and matches the power of the modified likelihood ratio test. When m(0) = 3, there is a clear indication that the test has good power properties. Supplementary materials for this article are available online."
"10.1198/jasa.2010.tm09590","2010","Robust data-driven inference for density-weighted average derivatives","0","This paper presents a novel data-driven bandwidth selector compatible with the small bandwidth asymptotics developed in Cattaneo, Crump, and Jansson (2009) for density-weighted average derivatives. The new bandwidth selector is of the plug-in variety, and is obtained based on a mean squared error expansion of the estimator of interest. An extensive Monte Carlo experiment shows a remarkable improvement in performance when the bandwidth-dependent robust inference procedures proposed by Cattaneo. Crump, and Jansson (2009) are coupled with this new data-driven bandwidth selector. The resulting robust data-driven confidence intervals compare favorably to the alternative procedures available in the literature. The online supplemental material to this paper contains further results from the simulation study."
"10.1198/jasa.2010.tm09129","2010","Correlated {$z$}-values and the accuracy of large-scale statistical estimates","4","We consider large-scale studies in which there are hundreds or thousands of correlated cases to investigate, each represented by its own normal variate, typically a z-value. A familiar example is provided by a microarray experiment comparing healthy with sick subjects' expression levels for thousands of genes. This paper concerns the accuracy of summary statistics for the collection of normal variates, such as their empirical cdf or a false discovery rate statistic. It seems like we must estimate an N by N correlation matrix, N the number of cases, but our main result shows that this is not necessary: good accuracy approximations can be based on the root mean square correlation over all N . (N - 1)/2 pairs, a quantity often easily estimated. A second result shows that z-values closely follow normal distributions even under nonnull conditions, supporting application of the main theorem. Practical application of the theory is illustrated for a large leukemia microarray study."
"10.1198/jasa.2010.ap09260","2010","Sensitivity analysis for the cross-match test, with applications in genomics","1","The cross-match test is an exact, distribution-free test of no treatment effect on a high-dimensional outcome in a randomized experiment. The test uses optimal nonbipartite matching to pair 2/ subjects into / pairs based on similar outcomes, and the cross-match statistic A is the number of times that a treated subject was paired with a control, rejecting for small values of A. If the test is applied in an observational study in which treatments are not randomly assigned, then it may be comparing treated and control subjects who are not comparable, and thus may falsely reject a true null hypothesis of no treatment effect. We develop a sensitivity analysis for the cross-match test and apply it in an observational study of the effects of smoking on gene expression levels. In addition, we develop a sensitivity analysis for several multiple testing procedures using the cross-match test and apply it to 1627 molecular function categories in Gene Ontology."
"10.1198/jasa.2010.ap08274","2010","Analysis of variance and {$F$}-tests for partial linear models with applications to environmental health data","0","Fish consumption during pregnancy exposes the fetus to both the neurotoxicant methylmercury and nutrients known to be beneficial for brain development. When nutrient status is not measured, maternal methylmercury levels may be a partial biomarker for both toxic and nutrient exposures. It is therefore necessary to employ a flexible model-such as the partial linear model-that will allow for possible nonlinear trends of methylmercury. To enhance interpretations of fitting a partial linear model, we propose analysis of variance (ANOVA) inference tools including ANOVA decomposition and significance tests. The ANOVA decomposition explicitly gives the proportion of variation explained by the model and separates the contributions from the parametric and nonparametric components. Semiparametric F-tests are constructed based on ANOVA decomposition with the normality assumption. The proposed F-tests are applicable to testing significance of the parametric, the nonparametric, and the combination of both. The ANOVA investigation also yields new byproduct estimators, which can be viewed as penalized least squares estimators. Simulation results demonstrate that the performance of the new estimators and ANOVA F-tests is comparable to alternative methods in practical applications. This methodology is applied to reanalyze the Seychelles Child Development Study Main Cohort data to explore nonlinear relationships of prenatal methylmercury exposure through maternal fish consumption with prenatal and postnatal child development."
"10.1198/jasa.2010.ap07636","2010","An ensemble {K}alman filter and smoother for satellite data assimilation","0","This paper proposes a methodology for combining satellite images with advection-diffusion models for interpolation and prediction of environmental processes. We propose a dynamic state-space model and an ensemble Kalman filter and smoothing algorithm for on-line and retrospective state estimation. Our approach addresses the high dimensionality, measurement bias, and nonlinearities inherent in satellite data. We apply the method to a sequence of SeaWiFS satellite images in Lake Michigan from March 1998, when a large sediment plume was observed in the images following a major storm event. Using our approach, we combine the images with a sediment transport model to produce maps of sediment concentrations and uncertainties over space and time. We show that our approach improves out-of-sample RMSE by 20%-30% relative to standard approaches. This article has supplementary material online."
"10.1198/jasa.2010.ap08713","2010","Optimal partitioning for linear mixed effects models: applications to identifying placebo responders","0","A longstanding problem in clinical research is distinguishing drug-treated subjects that respond due to specific effects of the drug from those that respond to nonspecific (or placebo) effects of the treatment. Linear mixed effect models are commonly used to model longitudinal clinical trial data. In this paper we present a solution to the problem of identifying placebo responders using an optimal partitioning methodology for linear mixed effects models. Since individual outcomes in a longitudinal study correspond to curves, the optimal partitioning methodology produces a set of prototypical outcome profiles. The optimal partitioning methodology can accommodate both continuous and discrete covariates. The proposed partitioning strategy is compared and contrasted with the growth mixture modeling approach. The methodology is applied to a two-phase depression clinical trial where subjects in a first phase were treated openly for 12 weeks with fluoxetine followed by a double blind discontinuation phase where responders to treatment in the first phase were randomized to either stay on fluoxetine or switched to a placebo. The optimal partitioning methodology is applied to the first phase to identify prototypical outcome profiles. Using time to relapse in the second phase of the study, a survival analysis is performed on the partitioned data. The optimal partitioning results identify prototypical profiles that distinguish whether subjects relapse depending on whether or not they stay on the drug or are randomized to a placebo."
"10.1198/jasa.2010.ap08327","2010","Bayesian modeling of {MPSS} data: gene expression analysis of bovine {\it {S}almonella} infection","0","Massively Parallel Signature Sequencing (MPSS) is a high-throughput, counting-based technology available for gene expression profiling. It produces output that is similar to Serial Analysis of Gene Expression and is ideal for building complex relational databases for gene expression. Our goal is to compare the in vivo global gene expression profiles of tissues infected with different strains of Salmonella obtained using the MPSS technology. In this article, we develop an exact ANOVA type model for this count data using a zero-inflated Poisson distribution, different from existing methods that assume continuous densities. We adopt two Bayesian hierarchical models one parametric and the other semiparametric with a Dirichlet process prior that has the ability to ""borrow strength"" across related signatures, where a signature is a specific arrangement of the nucleotides, usually 16-21 base pairs long. We utilize the discreteness of Dirichlet process prior to cluster signatures that exhibit similar differential expression profiles. Tests for differential expression are carried out using nonparametric approaches, while controlling the false discovery rate. We identify several differentially expressed genes that have important biological significance and conclude with a summary of the biological discoveries. This article has supplementary materials online."
"10.1198/jasa.2010.ap09231","2010","Informative retesting","0","In situations where individuals are screened for an infectious disease or other binary characteristic and where resources for testing are limited, group testing can offer substantial benefits. Group testing, where subjects are tested in groups (pools) initially, has been successfully applied to problems in blood bank screening, public health, drug discovery, genetics, and many other areas. In these applications, often the goal is to identify each individual as positive or negative using initial group tests and subsequent retests of individuals within positive groups. Many group testing identification procedures have been proposed; however, the vast majority of them fail to incorporate heterogeneity among the individuals being screened. In this paper, we present a new approach to identify positive individuals when covariate information is available on each. This covariate information is used to structure how retesting is implemented within positive groups; therefore, we call this new approach ""informative retesting."" We derive closed-form expressions and implementation algorithms for the probability mass functions for the number of tests needed to decode positive groups. These informative retesting procedures are illustrated through a number of examples and are applied to chlamydia and gonorrhea testing in Nebraska for the Infertility Prevention Project. Overall, our work shows compelling evidence that informative retesting can dramatically decrease the number of tests while providing accuracy similar to established noninformative retesting procedures. This article has supplementary material online."
"10.1198/jasa.2010.ap08631","2010","Prediction of functional status for the elderly based on a new ordinal regression model","0","The functional mobility of the elderly is a very important factor in aging research, and prognostic information is valuable in making clinical and health care policy decisions. We develop a predictive model for the functional status of the elderly based on data from the Second Longitudinal Study of Aging (LSOA II). The functional status is an ordinal response variable. The ordered probit model has been moderately successful in analyzing such data; however, its reliance on the normal distribution for its latent variable hinders its accuracy and potential. In this paper, we focus on the prediction of conditional quantiles of the functional status based on a more general transformation model. The proposed estimation procedure does not rely on any parametric specification of the conditional distribution functions, aiming to reduce model misspecification errors in the prediction. Cross-validation within the LSOA II data shows that our prediction intervals are more informative than those from the ordered probit model. Monte Carlo simulations also demonstrate the merits of our approach in the analysis of ordinal response variables."
"10.1198/jasa.2010.ap08739","2010","Causal effects of treatments for informative missing data due to progression/death","1","In longitudinal clinical trials, when outcome variables at later time points are only defined for patients who survive to those times, the evaluation of the causal effect of treatment is complicated. In this paper, we describe an approach that can be used to obtain the causal effect of three treatment arms with ordinal outcomes in the presence of death using a principal stratification approach. We introduce a set of flexible assumptions to identify the causal effect and implement a sensitivity analysis for nonidentifiable assumptions which we parameterize parsimoniously. Methods are illustrated on quality of life data from a recent colorectal cancer clinical trial. This article has supplementary material online."
"10.1198/jasa.2010.ap09379","2010","The value of multiproxy reconstruction of past climate","1","Understanding the dynamics of climate change in its full richness requires the knowledge of long temperature time series. Although long-term, widely distributed temperature observations are not available, there are other forms of data, known as climate proxies, that can have a statistical relationship with temperatures and have been used to infer temperatures in the past before direct measurements. We propose a Bayesian hierarchical model to reconstruct past temperatures that integrates information from different sources, such as proxies with different temporal resolution and forcings acting as the external drivers of large scale temperature evolution. Additionally, this method allows us to quantify the uncertainty of the reconstruction in a rigorous manner. The reconstruction method is assessed, using a global climate model as the true climate system and with synthetic proxy data derived from the simulation. The target is to reconstruct Northern Hemisphere temperature from proxies that mimic the sampling and errors from tree ring measurements, pollen indices, and borehole temperatures. The forcing series used as covariates are solar irradiance, volcanic aerosols, and greenhouse gas concentrations. The Bayesian model was successful in integrating these different sources of information in creating a coherent reconstruction. Within the context of this numerical testbed, a statistical process model that includes the external forcings can improve the quality of a hemispheric reconstruction when long time scale proxy information is not available. This article has supplementary material online."
"10.1198/jasa.2010.tm08545","2010","Nonparametric analysis of clustered multivariate data","0","There is wide interest in extending univariate and multivariate nonparametric procedures to clustered and hierarchical data. Traditionally, parametric mixed models have been used to account for the correlation structures among the dependent observational units. In this work we extend multivariate nonparametric procedures for one-sample and several-sample location problems to clustered data settings. The results are given for a general score function, but with an emphasis on spatial sign and rank methods. Mixed models notation involving design matrices for fixed and random effects is used throughout. The asymptotic variance formulas and limiting distributions of the test statistics under the null hypothesis and under a sequence of alternatives are derived, as are the limiting distributions for the corresponding estimates. The approach based on a general score function also shows, for example, how M-estimates behave with clustered data. Efficiency studies demonstrate the practical advantages and disadvantages of the use of spatial sign and rank scores, as well as their weighted versions. Small-sample procedures based on sign change and permutation principles are discussed. Further development of nonparametric methods for cluster-correlated data would benefit from the notation already familiar to statisticians working under normality assumptions. Supplemental materials for the article are available online."
"10.1198/jasa.2010.tm09165","2010","Simultaneous confidence bands for penalized spline estimators","0","In this article we construct simultaneous confidence bands for a smooth curve using penalized spline estimators. We consider three types of estimation methods: (a) as a standard (fixed effect) nonparametric model, (b) using the mixed-model framework with the spline coefficients as random effects, and (c) a full Bayesian approach. The volume-of-tube formula is applied for the first two methods and compared with Bayesian simultaneous confidence bands from a frequentist perspective. We show that the mixed-model formulation of penalized splines can help obtain, at least approximately, confidence bands with either Bayesian or frequentist properties. Simulations and data analysis support the proposed methods. The R package ConfBands accompanies the article."
"10.1198/jasa.2010.tm08243","2010","Validating stationarity assumptions in time series analysis by rolling local periodograms","1","We propose a simple and powerful procedure to validate the assumption of weak stationarity in time series analysis. Our focus is on processes with a slowly varying autocovariance structure. The procedure evaluates the supremum over time of the L-2-distance between the local sample spectral density (local periodogram) calculated using a segment of observations falling within a rolling window and an estimator of the spectral density obtained using the entire time series at hand. Large sample properties of a basic deviation process are investigated and critical values of a supremum type test are obtained using an appropriate bootstrap procedure. The finite sample size and power properties of the procedure are investigated by means of simulations. Real data examples demonstrate the ability of the procedure to detect (possible) changes in the autocovariance structure of a time series and to understand their nature."
"10.1198/jasa.2010.tm09057","2010","On generating {M}onte {C}arlo samples of continuous diffusion bridges","0","Diffusion processes are widely used in engineering, finance, physics, and other fields. Usually continuous-time diffusion processes can be observed only at discrete time points. For many applications, it is often useful to impute continuous-time bridge samples that follow the diffusion dynamics and connect each pair of the consecutive observations. The sequential Monte Carlo (SMC) method is a useful tool for generating the intermediate paths of the bridge. The paths often are generated forward from the starting observation and forced in some ways to connect with the end observation. In this article we propose a constrained SMC algorithm with an effective resampling scheme guided by backward pilots carrying the information of the end observation. This resampling scheme can be easily combined with any forward SMC sampler. Two synthetic examples are used to demonstrate the effectiveness of the resampling scheme."
"10.1198/jasa.2010.tm09560","2010","Tests for high-dimensional covariance matrices","1","We propose tests for sphericity and identity of high-dimensional covariance matrices. The tests are nonparametric without assuming a specific parametric distribution for the data. They can accommodate situations where the data dimension is much larger than the sample size, namely the ''large p, small n"" situations. We demonstrate by both theoretical and empirical studies that the tests have good properties for a wide range of dimensions and sample sizes. We applied the proposed test on a microarray dataset on Yorkshire Gilts and tested for the covariance structure for the expression levels for sets of genes."
"10.1198/jasa.2010.tm09404","2010","Using evidence of mixed populations to select variables for clustering very high-dimensional data","0","In this paper we develop a nonparametric approach to clustering very high-dimensional data, designed particularly for problems where the mixture nature of a population is expressed through multimodality of its density. Therefore, a technique based implicitly on mode testing can be particularly effective. In principle, several alternative approaches could be used to assess the extent of multimodality, but in the present problem the excess mass method has important advantages. We show that the resulting methodology for determining clusters is particularly effective in cases where the data are relatively heavy tailed or show a moderate to high degree of correlation, or when the number of important components is relatively small. Conversely, in the case of light-tailed, almost-independent components when there are many clusters, clustering in terms of modality can be less reliable than more conventional approaches. This article has supplementary material online."
"10.1198/jasa.2010.tm08756","2010","Inference in semiparametric regression models under partial questionnaire design and nonmonotone missing data","0","In epidemiologic studies, partial questionnaire design (PQD) can reduce cost, time, and other practical burdens associated with lengthy questionnaires by assigning different subsets of the questionnaire to different, but overlapping, subsets of the study participants. In this article, we describe methods for semiparametric inference for regression model under PQD and other study settings that can generate nonmonotone missing data in covariates. In particular, motivated from methods for multiphase designs, we develop three estimators, namely mean score, pseudo-likelihood, and semiparametric maximum likelihood, each of which has some unique advantages. We develop the asymptotic theory and a sandwich variance estimator for each of the estimators under the underlying semiparametric model that allows the distribution of the covariates to remain nonparametric. We study the finite sample performances and relative efficiencies of the methods using simulation studies. We illustrate the methods using data from a case-control study of non-Hodgkin's lymphoma where the data on the main chemical exposures of interest are collected using two different instruments on two different, but overlapping, subsets of the participants. This article has supplementary material online."
"10.1198/jasa.2010.tm09340","2010","Posterior simulation in countable mixture models for large datasets","0","Mixture models, or convex combinations of a countable number of probability distributions, offer an elegant framework for inference when the population of interest can be subdivided into latent clusters having random characteristics that are heterogeneous between, but homogeneous within, the clusters. Traditionally, the different kinds of mixture models have been motivated and analyzed from very different perspectives, and their common characteristics have not been fully appreciated. The inferential techniques developed for these models usually necessitate heavy computational burdens that make them difficult, if not impossible, to apply to the massive data sets increasingly encountered in real world studies. This paper introduces a flexible class of models called generalized polya urn (GPU) processes. Many common mixture models, including finite mixtures, hidden Markov models, and Dirichlet processes, are obtained as special cases of CPU processes. Other important special cases include finite-dimensional Dirichlet priors, infinite hidden Markov models, analysis of densities models, nested Chinese restaurant processes, hierarchical DP models, nonparametric density models, spatial Dirichlet processes, weighted mixtures of DP priors, and nested Dirichlet processes. An investigation of the theoretical properties of GPU processes offers new insight into asymptotics that form the basis of cost-effective Markov chain Monte Carlo (MCMC) strategies for large datasets. These MCMC techniques have the advantage of providing inferences from the posterior of interest, rather than an approximation, and are applicable to different mixture models. The versatility and impressive gains of the methodology are demonstrated by simulation studies and by a semiparametric Bayesian analysis of high-resolution comparative genomic hybridization data on lung cancer. The appendixes are available online as supplemental material."
"10.1198/jasa.2010.tm09426","2010","Dimension reduction and adaptation in conditional density estimation","0","An orthogonal series estimator of the conditional density of a response given a vector of continuous and ordinal/nominal categorical predictors is suggested. The estimator is based on writing a conditional density as a sum of orthogonal projections on all possible subspaces of reduced dimensionality and then estimating each projection via a shrinkage procedure. The shrinkage procedure uses a universal thresholding and a dyadic-blockwise shrinkage for low and high frequencies, respectively. The estimator is data-driven, is adaptive to underlying smoothness of a conditional density, and attains a minimax rate of the mean integrated squared error convergence. Furthermore, if a conditional density depends only on a subgroup of predictors, then the estimator seizes the opportunity and attains a corresponding minimax rate of convergence. The latter property relaxes the notorious ""curse of dimensionality."" Moreover, the estimator is fast, because neither projections nor shrinkages are computation-intensive. A numerical study for finite samples and a real example are presented. Our results indicate that the proposed estimation procedure is practical and has a rigorous theoretical justification."
"10.1198/jasa.2010.tm08636","2010","Global partial likelihood for nonparametric proportional hazards models","1","As an alternative to the local partial likelihood method of Tibshirani and Hastie and Fan, Gijbels, and King. a global partial likelihood method is proposed to estimate the covariate effect in a nonparametric proportional hazards model, lambda(t vertical bar x) = exp{psi(x)}lambda(0)(t). The estimator, (psi) over cap (x), reduces to the Cox partial likelihood estimator if the covariate is discrete. The estimator is shown to be consistent and semiparametrically efficient for linear functionals of psi(x). Moreover, Breslow-type estimation of the cumulative baseline hazard function, using the proposed estimator (psi) over cap (x), is proved to be efficient. The asymptotic bias and variance are derived under regularity conditions. Computation of the estimator involves an iterative but simple algorithm. Extensive simulation studies provide evidence supporting the theory. The method is illustrated with the Stanford heart transplant data set. The proposed global approach is also extended to a partially linear proportional hazards model and found to provide efficient estimation of the slope parameter. This article has the supplementary materials online."
"10.1198/jasa.2010.tm09061","2010","Likelihood ratio tests with three-way tables","0","Likelihood ratio (LR) tests for association and for interaction are examined for three-way contingency tables, particularly the widely used 2 x 2 x K table. Mutual information identities are used to characterize the information decomposition and the logical relationship between the omnibus LR test for conditional independence across K strata and its two independent components. LR tests for interaction and for uniform association. The latter two tests are logically connected to formulating a natural two-step test for conditional independence. The proposed two-step test with reduced nominal levels is suggested instead of the Breslow-Day test and the Cochran-Mantel-Haenszel test. This yields efficient interval estimation for both the interaction parameter and the common odds ratio compared with using the Mantel-Haenszel estimate. This allows the development of power analysis for testing general hypotheses of varied interactions, using an invariant Pythagorean law of relative entropy."
"10.1198/jasa.2010.tm09380","2010","Grouping pursuit through a regularization solution surface","0","Extracting grouping structure or identifying homogenous subgroups of predictors in regression is crucial for high-dimensional data analysis. A low-dimensional structure in particular-grouping, when captured in a regression model-enables to enhance predictive performance and to facilitate a model's interpretability. Grouping pursuit extracts homogenous subgroups of predictors most responsible for outcomes of a response. This is the case in gene network analysis, where grouping reveals gene functionalities with regard to progression of a disease. To address challenges in grouping pursuit, we introduce a novel homotopy method for computing an entire solution surface through regularization involving a piecewise linear penalty. This nonconvex and overcomplete penalty permits adaptive grouping and nearly unbiased estimation, which is treated with a novel concept of grouped subdifferentials and difference convex programming for efficient computation. Finally, the proposed method not only achieves high performance as suggested by numerical analysis, but also has the desired optimality with regard to grouping pursuit and prediction as showed by our theoretical results."
"10.1198/jasa.2010.tm09415","2010","A framework for feature selection in clustering","1","We consider the problem of clustering observations using a potentially large set of features. One might expect that the true underlying clusters present in the data differ only with respect to a small fraction of the features, and will be missed if one clusters the observations using the full set of features. We propose a novel framework for sparse clustering, in which one clusters the observations using an adaptively chosen subset of the features. The method uses a lasso-type penalty to select the features. We use this framework to develop simple methods for sparse K-means and sparse hierarchical clustering. A single criterion governs both the selection of the features and the resulting clusters. These approaches are demonstrated on simulated and genomic data."
"10.1198/jasa.2010.tm08383","2010","Infinitesimal robustness for diffusions","0","We develop infinitesimally robust statistical procedures for the general diffusion processes. We first prove the existence and uniqueness of the times-series influence function of conditionally unbiased M-estimators for ergodic and stationary diffusions, under weak conditions on the (martingale) estimating function used. We then characterize the robustness of M-estimators for diffusions and derive a class of conditionally unbiased optimal robust estimators. To compute these estimators, we propose a general algorithm, which exploits approximation methods for diffusions in the computation of the robust estimating function. Monte Carlo simulation shows a good performance of our robust estimators and an application to the robust estimation of the exchange rate dynamics within a target zone illustrates the methodology in a real-data application."
"10.1198/jasa.2010.tm09570","2010","Design sensitivity and efficiency in observational studies","2","An observational study attempts to draw inferences about the effects caused by a treatment when subjects are not randomly assigned to treatment or control as they would be in a randomized trial. After adjustments have been made for imbalances in measured covariates, the key source of uncertainty in an observational study is the possibility that subjects were not comparable prior to treatment in terms of some unmeasured covariate, so that differing outcomes in treated and control groups are not effects caused by the treatment. A sensitivity analysis asks about the magnitude of the departure from random assignment needed to alter the qualitative conclusions of the study, and the power of a sensitivity analysis and the design sensitivity anticipate the outcome of a sensitivity analysis under an assumed model for treatment effect. Lacking theoretical guidance, we tend to select statistical methods for use in observational studies based on their efficiency in randomized experiments. This turns out to be a mistake. A highly efficient method for detecting small treatment effects in randomized experiments need not, and often does not, have the highest power in a sensitivity analysis or the largest design sensitivity. That is, the best procedure assuming the observational study is a randomized experiment need not be the best procedure under more realistic assumptions. Small effects are sensitive to small biases, and methods targeted at detecting small effects in the absence of bias may not give due emphasis to evidence that the effect is stable and not small, and hence not easily attributed to small or moderate biases. The issue is illustrated in a practical example, is explored informally using a graphical heuristic, is studied asymptotically by determining the design sensitivity for a signed rank statistic with general scores, and is evaluated in a simulation. Among robust procedures with similar Pitman efficiencies for several distributions, there are large differences in design sensitivity and hence substantial differences in the power of a sensitivity analysis."
"10.1198/jasa.2010.tm09302","2010","On estimation of partially linear transformation models","0","We study a general class of partially linear transformation models, which extend linear transformation models by incorporating nonlinear covariate effects in survival data analysis. A new martingale-based estimating equation approach, consisting of both global and kernel-weighted local estimation equations, is developed for estimating the parametric and nonparametric covariate effects in a unified manner. We show that with a proper choice of the kernel bandwidth parameter, one can obtain the consistent and asymptotically normal parameter estimates for the linear effects. Asymptotic properties of the estimated nonlinear effects are established as well. We further suggest a simple resampling method to estimate the asymptotic variance of the linear estimates and show its effectiveness. To facilitate the implementation of the new procedure, an iterative algorithm is developed. Numerical examples are given to illustrate the finite-sample performance of the procedure. Supplementary materials are available online."
"10.1198/jasa.2010.tm09239","2010","Second-order comparison of {G}aussian random functions and the geometry of {DNA} minicircles","1","Given two samples of continuous zero-mean iid Gaussian processes on [0, 1], we consider the problem of testing whether they share the same covariance structure. Our study is motivated by the problem of determining whether the mechanical properties of short strands of DNA are significantly affected by their base-pair sequence; though expected to be true, had so far not been observed in three-dimensional electron microscopy data, The testing problem is seen to involve aspects of ill-posed inverse problems and a test based on a Karhunen-Loeve approximation of the Hilbert-Schmidt distance of the empirical covariance operators is proposed and investigated. When applied to a dataset of DNA minicircles obtained through the electron microscope, our test seems to suggest potential sequence effects on DNA shape. Supplemental material available online."
"10.1198/jasa.2010.tm08127","2010","Shortcuts for locally consonant closed test procedures","0","The closed testing principle provides a general and simple framework to construct powerful multiple test procedures for k elementary null hypotheses while controlling the familywise error rate in the strong sense. However, the closed testing principle has the disadvantage of leading to the evaluation of O(2(k)) intersection hypotheses. Multiple test procedures based on the closed testing principle may thus require substantial computational efforts. Consonant closed test procedures for unrestricted hypotheses have the advantage of rejecting at least one elementary null hypothesis whenever the global null hypothesis is rejected and thus admit shortcuts of size k. If the elementary null hypotheses are restricted by logical constraints, the closure of common tests, such as max-t or min-p tests, may not be consonant in the usual sense. In this article we introduce a weaker consonance property, denoted as local consonance, and show that many closed test procedures with restricted hypotheses satisfy this condition. We describe a general algorithm to construct related shortcuts and illustrate the results with several applications."
"10.1198/jasa.2010.tm08241","2010","Latent stick-breaking processes","0","We develop a model for stochastic processes with random marginal distributions. Our model relies on a stick-breaking construction for the marginal distribution of the process, and introduces dependence across locations by using a latent Gaussian copula model as the mechanism for selecting the atoms. The resulting latent stick-breaking process (LaSBP) induces a random partition of the index space, with points closer in space having a higher probability of being in the same cluster. We develop an efficient and straightforward Markov chain Monte Carlo (MCMC) algorithm for computation and discuss applications in financial econometrics and ecology. This article has supplementary material online."
"10.1198/jasa.2010.tm09132","2010","Tree-structured wavelet estimation in a mixed effects model for spectra of replicated time series","1","This article develops a method for estimating the spectrum of a stationary process using time series traces recorded from experimental designs. Our procedure estimates the ""common"" log-spectrum and the variability over the traces (or subjects) using a mixed effects model. We combine spatially adaptive smoothing methods with recursive dyadic partitioning to construct a model for predicting subject-specific effects. The method is easy to implement and can handle large datasets because it uses the discrete wavelet transform which is computationally efficient. Numerical studies confirm that the proposed method performs very well despite its simplicity. The method is also applied to a multisubject electroencephalogram dataset."
"10.1198/jasa.2010.tm09313","2010","Generalized functional linear models with semiparametric single-index interactions","0","We introduce a new class of functional generalized linear models, where the response is a scalar and some of the covariates are functional. We assume that the response depends on multiple covariates, a finite number of latent features in the functional predictor, and interaction between the two. To achieve parsimony, the interaction between the multiple covariates and the functional predictor is modeled semipara-metrically with a single-index structure. We propose a two-step estimation procedure based on local estimating equations, and investigate two situations: (a) when the basis functions are predetermined, e.g.. Fourier or wavelet basis functions and the functional features of interest are known: and (b) when the basis functions are data driven, such as with functional principal components. Asymptotic properties are developed. Notably, we show that when the functional features are data driven, the parameter estimates have an increased asymptotic variance due to the estimation error of the basis functions. Our methods are illustrated with a simulation study and applied to an empirical dataset where a previously unknown interaction is detected. Technical proofs of our theoretical results are provided in the online supplemental materials."
"10.1198/jasa.2010.tm09386","2010","Test of association between two ordinal variables while adjusting for covariates","1","We propose a new set of test statistics to examine the association between two ordinal categorical variables X and Y after adjusting for continuous and/or categorical covariates Z. Our approach first fits multinomial (e.g.. proportional odds) models of X and Y, separately, on Z. For each subject, we then compute the conditional distributions of X and Y given Z. If there is no relationship between X and Y after adjusting for Z. then these conditional distributions will be independent, and the observed value of (X, Y) for a subject is expected to follow the product distribution of these conditional distributions. We consider two simple ways of testing the null of conditional independence, both of which treat X and Y equally, in the sense that they do not require specifying an outcome and a predictor variable. The first approach adds these product distributions across all subjects to obtain the expected distribution of (X. Y) tinder the null and then contrasts it with the observed unconditional distribution of (X, Y). Our second approach computes ""residuals"" from the two multinomial models and then tests for correlation between these residuals; we define a new individual-level residual for models with ordinal outcomes. We present methods for computing p-values using either the empirical or asymptotic distributions of our test statistics. Through simulations, we demonstrate that our test statistics perform well in terms of power and Type I error rate when compared to proportional odds models which treat X as either a continuous or categorical predictor. We apply our methods to data from a study of visual impairment in children and to a study of cervical abnormalities in human immunodeficiency virus (HIV)-infected women. Supplemental materials for the article are available online."
"10.1198/jasa.2010.ap08397","2010","Bayesian inference with incomplete multinomial data: a problem in pathogen diversity","0","With recent advances in genetic analysis, it has become feasible to classify a pathogen into genetically distinct variants even though they apparently cause an infected subject similar symptoms. The availability of such data opens up the interesting problem of studying the spatiotemporal variation in the diversity of variants of a pathogen. Data on pathogen variants often suffer the problems of (i) low cell counts, (ii) incomplete classification due to laboratory problems (e.g., contamination), and (iii) unseen variants. Shannon's entropy may be used as a measure of variant diversity. A Bayesian approach can be used to deal with the problems of low cell counts and unseen variants. Bayesian analysis of incomplete multinomial data may be carried out by Markov chain Monte Carlo techniques. However, for pathogen-variant data, there often is only one source of missingness-namely, some subjects are known to be infected by some unidentified pathogen variant. We point out that for incomplete data with disjoint sources of missingness, Bayesian analysis can be done more efficiently using an iid sampling scheme from the posterior distribution. We illustrate the method by analyzing a data set on the prevalence of bartonella infection among individual colonies of prairie dogs at the study site in Colorado between 2003 and 2006. We compare the result from the proposed Monte Carlo method with the results from other methods, including a model that entertains within-variant spatial correlation but no between-variant spatial correlation. This article has supplementary material online."
"10.1198/jasa.2010.ap07291","2010","Group comparison of eigenvalues and eigenvectors of diffusion tensors","0","Diffusion tensor imaging (DID data differ from most medical images in that values at each voxel are not scalars, but 3 x 3 symmetric positive definite matrices called diffusion tensors (DTs). The anatomic characteristics of the tissue at each voxel are reflected by the DT eigenvalues and eigenvectors. In this article we consider the problem of testing whether the means of two groups of DT images are equal at each voxel in terms of the DT's eigenvalues, eigenvectors, or both. Because eigendecompositions are highly nonlinear, existing likelihood ratio statistics (LRTs) for testing differences in eigenvalues or eigenvectors of means of Gaussian symmetric matrices assume an orthogonally invariant covariance structure between the matrix entries. While retaining the form of the LRTs, we derive new approximations to their true distributions when the covariance between the DT entries is arbitrary and possibly different between the two groups. The approximate distributions are those of similar LRT statistics computed on the tangent space to the parameter manifold at the true value of the parameter, but plugging in an estimate for the point of application of the tangent space. The resulting distributions, which are weighted sums of chi-squared distributions, are further approximated by scaled chi-squared distributions by matching the first two moments. For validity of the Gaussian model, the positive definite constraints on the DT are removed via a matrix log transformation, although this is not crucial asymptotically. Voxelwise application of the test statistics leads to a multiple-testing problem, which is solved by false discovery rate inference. The foregoing methods are illustrated in a DTI group comparison of boys versus girls."
"10.1198/jasa.2010.ap09114","2010","A rank-based test for comparison of multidimensional outcomes","0","For comparison of multiple outcomes commonly encountered in biomedical research, Huang et al. (2005) improved O'Brien's (1984) rank-sum tests through the replacement of the ad hoc variance by the asymptotic variance of the test statistics. The improved tests control the type I error rate at the desired level and gain power when the differences between the two comparison groups in each outcome variable lie in the same direction; however, they may lose power when the differences are in different directions (e.g., some are positive and some are negative). These tests and the popular Bonferroni correction failed to show important significant differences when applied to compare heart rates from a clinical trial to evaluate the effect of a procedure to remove the cardioprotective solution HTK. We propose an alternative test statistic, taking the maximum of the individual rank-sum statistics, which controls the type I error rate and maintains satisfactory power regardless of the direction of the differences. Simulation studies show the proposed test to be of higher power than other tests in a certain alternative parameter space of interest. Furthermore, when used to analyze the heart rate data, the proposed test yields more satisfactory results."
"10.1198/jasa.2009.ap07184","2010","Probabilistic weather forecasting for winter road maintenance","1","Winter road maintenance is one of the main tasks for the Washington State Department of Transportation. Anti-icing, that is, the preemptive application of chemicals, is often used to keep the roadways free of ice. Given the preventive nature of anti-icing, accurate predictions of road ice are needed. Currently, anti-icing decisions are usually based on deterministic weather forecasts. However, the costs of the two kinds of errors are highly asymmetric because the cost of a road closure due to ice is much greater than that of taking anti-icing measures. As a result, probabilistic forecasts are needed to optimize decision making.We propose two methods for forecasting the probability of ice formation. Starting with deterministic numerical weather predictions, we model temperature and precipitation using distributions centered around the bias-corrected forecasts. This produces a joint predictive probability distribution of temperature and precipitation, which then yields the probability of ice formation, defined here as the occurrence of precipitation when the temperature is below freezing. The first method assumes that temperatures, as well as precipitation, at different spatial locations are conditionally independent given the numerical weather predictions. The second method models the spatial dependence between forecast errors at different locations. The model parameters are estimated using a Bayesian approach via Markov chain Monte Carlo.We evaluate both methods by comparing their probabilistic forecasts with observations of ice formation for Interstate Highway 90 in Washington State for the 2003-2004 and 2004-2005 winter seasons. The use of the probabilistic forecasts reduces costs by about 50% when compared to deterministic forecasts. The spatial method improves the reliability of the forecasts, but does not result in further cost reduction when compared to the first method."
"10.1198/jasa.2009.ap09068","2010","Hierarchical spatial process models for multiple traits in large genetic trials","2","This article expands upon recent interest in Bayesian hierarchical models in quantitative genetics by developing spatial process models for inference on additive and dominance genetic variance within the context of large spatially referenced trial datasets of multiple traits of interest. Direct application of such multivariate models to large spatial datasets is often computationally infeasible because of cubic order matrix algorithms involved in estimation. The situation is even worse in Markov chain Monte Carlo (MCMC) contexts where such computations are performed for several thousand iterations. Here, we discuss approaches that help obviate these hurdles without sacrificing the richness in modeling. For genetic effects, we demonstrate how an initial spectral decomposition of the relationship matrices negates the expensive matrix inversions required in previously proposed MCMC methods. For spatial effects we discuss a multivariate predictive process that reduces the computational burden by projecting the original process onto a subspace generated by realizations of the original process at a specified set of locations (or knots). We illustrate the proposed methods using a synthetic dataset with multivariate additive and dominant genetic effects and anisotropic spatial residuals, and a large dataset from a scots pine (Pinus sylvestris L.) progeny study conducted in northern Sweden. Our approaches enable us to provide a comprehensive analysis of this large trial which amply demonstrates that, in addition to violating basic assumptions of the linear model, ignoring spatial effects can result in downwardly biased measures of heritability."
"10.1198/jasa.2009.ap08387","2010","An association test for multiple traits based on the generalized {K}endall's tau","1","In many genetics studies, especially in the investigation of mental illness and behavioral disorders, it is common for researchers to collect multiple phenotypes to characterize the complex disease of interest. It may be advantageous to analyze those phenotypic measurements simultaneously if they share a similar genetic mechanism. In this study, we present a nonparametric approach to studying multiple traits together rather than examining each trait separately. Through simulation we compared the nominal Type I error and power of our proposed test to an existing test, that is, a generalized family-based association test. The empirical results suggest that our proposed approach is superior to the existing test in the analysis of ordinal traits. The advantage is demonstrated on a dataset concerning alcohol dependence. In this application, the use of our methods enhanced the signal of the association test."
"10.1198/jasa.2009.ap08497","2010","Dynamic nonparametric {B}ayesian models for analysis of music","0","The dynamic hierarchical Dirichlet process (dHDP) is developed to model complex sequential data, with a focus on audio signals from music. The music is represented in terms of a sequence of discrete observations, and the sequence is modeled using a hidden Markov model (HMM) with time-evolving parameters. The dHDP imposes the belief that observations that are temporally proximate are more likely to be drawn from HMMs with similar parameters, while also allowing for ""innovation"" associated with abrupt changes in the music texture. The sharing mechanisms of the time-evolving model are derived, and for inference a relatively simple Markov chain Monte Carlo sampler is developed. Segmentation of a given musical piece is constituted via the model inference. Detailed examples are presented on several pieces, with comparisons to other models. The dHDP results are also compared with a conventional music-theoretic analysis. All the supplemental materials used by this paper are available online."
"10.1198/jasa.2009.ap07115","2010","Identifying intraparty voting blocs in the {U}.{K}. {H}ouse of {C}ommons","0","Legislative voting records are an important source of information about legislator preferences, intraparty cohesiveness, and the divisiveness of various policy issues. Standard methods of analyzing a legislative voting record tend to have serious drawbacks when applied to legislatures, such as the United Kingdom House of Commons, that feature highly disciplined parties, strategic voting, and large amounts of missing data. We present a method (based on a Dirichlet process mixture model) for analyzing such voting records that does not suffer from these same problems. Our method is model-based and thus allows one to make probability statements about quantities of interest. It allows one to estimate the number of voting blocs within a party or any other group of members of parliament (MPs). Finally, it can be used as both a predictive model and an exploratory model. We illustrate these points through an application of the method to the voting records of Labour Party MPs in the 1997-2001 session of the U.K. House of Commons."
"10.1198/jasa.2010.tm09107","2010","Robust model-free multiclass probability estimation","0","Classical statistical approaches for multiclass probability estimation are typically based on regression technique, such as multiple logistic regression, or density estimation approaches such as linear discriminant analysis (LDA) and quadratic discriminant analysis (ODA) These methods often make certain assumptions on the form of probability functions or on the underlying distributions of subclasses In this article. we develop a model-free procedure to estimate multiclass probabilities based on large-margin classifiers In particular, the new estimation scheme is employed by solving a series of weighted large-mail:in classifiers and then systematically extracting the probability information from these multiple classification rules A main advantage of the proposed probability estimation technique is that it does not impose any strong parametric assumption on the underlying distribution and can be applied for a wide range of large-margin classification methods A general computational algorithm is developed for class probability estimation Furthermore, we establish asymptotic consistency of the probability estimates Both simulated and real data examples are presented to illustrate competitive performance of the new approach and compare it with several other existing methods"
"10.1198/jasa.2010.tm08532","2010","Indirect cross-validation for density estimation","2","A new method of bandwidth selection or kernel density estimators is proposed The method termed indirect cross-validation (ICY). makes use of so-called selection kernels Least-squares cross-validation (LSCV) is used to select the bandwidth of a selection-kernel estimator and this bandwidth is appropriately escaled for use in a Gaussian kernel estimator The proposed selection kernels are linear combinations of two Gaussian kennels and need not be unimodal or positive A theory is developed showing that the relative error of ICV bandwidths can converge to 0 at a rate of n(-1/4). which is substantially better than the n(-1/10) rate of LSCV Interestingly, the selection kernels that are best for purposes of bandwidth selection are very poor if used to actually estimate die density function This property appears to be part of the lamer and we paradox to the effect that ""the harder the estimation problem. the better cross-validation performs'. The ICV method urn form outperforms LSCV in a simulation study. a real data example and a simulated example in which bandwidths are chosen locally Supplemental materials for the article available online"
"10.1198/jasa.2010.tm08487","2010","Weighted distance weighted discrimination and its asymptotic properties","1","While Distance Weighted Discrimination (MD) Is an appealing approach to classification in high dimensions, it was designed for balanced damsels In the case of unequal costs, biased sampling, or unbalanced data. there ale major improvements available using appropriately weighted versions of DWD (wDWD) A inapt contribution of this paper is the development of optimal weighting schemes for various nonstandard classification problems In addition. we discuss several alternative criteria and propose an adaptive weighting scheme (awDWD) and demonstrate its advantages over nonadaptive weighting schemes under sonic. situations The second major contribution Is a theoretical study of weighted DWD Both high-dimensional low sample-size asymptotics and Fisher consistency of DWD are studied The performance of weighted DWD is evaluated using simulated examples and two real data examples The theoretical results are also confirmed by simulations"
"10.1198/jasa.2009.tm08651","2010","A statistical framework for differential privacy","0","One goal of statistical privacy research construct a data release mechanism that protects individual privacy while preset ving information content An example is a random mechanism that takes an input database X and outputs a random database Z according to a distribution Q(n) (vertical bar X) Differential privacy is a particular privacy requirement developed by computer scientists in which Q (vertical bar X) IS required to be insensitive to changes in one data point in X This makes it difficult to inter front Z whether a given individual is in the original database X We consider differential privacy front a statistical perspective We consider several data-release mechanisms that satisfy the differential privacy requirement We show that it is useful to compare these schemes by computing the rate at convergence of distributions and densities constructed from the released data We study a general privacy method. called the exponential mechanism, introduced by McSheiry and Talwar (2007) We show dial the accuracy of this method is intimately linked to the rate at which the probability that the empirical distribution concentrates in a small ball around the true distribution"
"10.1198/jasa.2010.tm08281","2010","Variable selection with the strong heredity constraint and its oracle property","1","In this paper. we extend the LASSO method (Tibshittant 1996) for simultaneously fitting a regression model and identifying important interaction terms Unlike most of the existing variable selection methods. our method automatically enforces the heredity constraint that in Interaction term can be included in the model only it the corresponding main terms are also included in the model Furthermore, we extend our method to generalized linear models, and show that It performs as well as if the true model were given in advance, that is, the oracle property as in Fan and Li (2001) and Fan and Peng (2004) The proof of the oracle property is given in online supplemental materials Numerical results on both simulation data and real data indicate that our method tends to remove irrelevant variables more effectively and provide better prediction performance than previous work (Yuan, Joseph. and Lin 2007 and Zhao. Rocha. and Yu 2009 as well is the classical LASSO method)"
"10.1198/jasa.2009.tm08013","2010","Regularization parameter selections via generalized information criterion","1","We apply the nonconcave penalized likelihood approach to obtain variable selections as well as shrinkage estimators This approach relies heavily on the choice of regularization parameter, which controls the model complexity In this paper, we propose employing the generalized in criterion. encompassing the commonly used Akaike in criterion (AIC) and Bayesian information criterion (BIC), for selecting the regularization parameter Our proposal makes a connection between the classical variable selection criteria and the regularization parameter selections for the nonconcave penalized likelihood approaches We show that the BIC-type selector enables identification of the true model consistently, and the resulting estimator possesses the oracle property in the terminology of Fan and Li (2001) In contrast, however, the AIC-type selector tends to overfit with positive probability We further show that the AIC-type selector is asymptotically loss efficient. while the BIC-type selector is not Our simulation results confirm these theoretical findings. and an empirical example is presented Some technical proofs are given in the online supplementary material"
"10.1198/jasa.2009.tm08459","2010","Semiparametric efficient estimation for a class of generalized proportional odds cure models","0","We present a mixture cure model with the survival time of the ""uncured"" group coming from a class of linear transformation models, which is an extension of the proportional odds model This class of model. first proposed by Dabrowska and Doksum ( 988). which we term ""generalized proportional odds model,"" is well suited for the mixture cure model setting due to a clear separation between long-term and short-term effects A standard expectation-maximization algorithm can he employed to locate the nonparametric likelihood estimators which are shown to he consistent and semiparametric efficient However. there are difficulties in the M-step due to the nonparametric component We overcome these difficulties by proposing two different algorithms The first is to employ an majorize-minimize (MM) algorithm in the M-step instead of the usual Newton-Raphson method, and the other is based on an alternative form to express the model as a proportional hazards frailty model The two new algorithms are compared in a simulation study with an existing estimating equation approach by DJ and Ying (2004) The MM algorithm provides both computational stability and efficiency A case study of leukemia dam is conducted to illustrate the proposed procedures"
"10.1198/jasa.2009.tm08697","2010","Alternative goodness-of-fit tests for linear models","0","Fan and Huang (2001) presented a goodness-of-fit test for linear models based on Fourier transformations of the residuals of the fitted model We present two mole theoretically appealing tests in which the Fourier transforms are incorporated into a tilted model We show that when suitably normalized. the new test statistics have the same as distribution as Fan and Huang's test We propose modifications to the asymptotic normalization constants to improve the small sample sizes of our tests while retaining their asymptotic distributions Small sample sizes and powers are examined via simulations An illustration is given"
"10.1198/jasa.2009.tm09372","2010","Dimension reduction and semiparametric estimation of survival models","0","In this paper. we propose a new dimension reduction method by introducing a nominal regression model with the hazard function as the conditional mean. vs Inch naturally retrieves information from complete data and censored data as well Moreover. without requiring the linearity condition, the new method can estimate the entire central subspace consistently and exhaustively The method also provides an alternative approach or the analysis of censored data assuming neither the link function nor the distribution Hence. it exhibits superior robustness properties Numerical studies show that the method can indeed he readily used to efficiently estimate survival node Is. explore the data structures and identity important variables"
"10.1198/jasa.2009.tm08313","2010","A family of distributions on the circle with links to, and applications arising from, {M}\""obius transformation","0","We propose a family of four-parameter distributions on the circle thin contains the von Mises and wrapped Cauchy distributions as special cases The faintly is derived by transforming the von Mises distribution via a Mobius transformation which maps the not circle onto itself The densities in the family have a symmetric or asymmetric. urn modal or bimodal shape. depending on the values of the parameters Conditions for ummodality are explored Further properties of the proposed model are obtained, many by applying the theory of Mobius transformation Properties of a three-parameter symmetric submodel are investigated as well. these include maximum likelihood estimation, its asymptotics, and a reparameterization that proves useful quite generally A three-parameter asymmetric subfamily. which of proves to be an adequate model. is also discussed. with emphasis on its mean direction and circular skewness The proposed family and subfamilies ate used to model an asymmetrically distributed data set and are then adopted as the angular error distribution of a circular circular regression model Two applications of the latter are given It is in this regression context that the Mobius transformation especially comes into its own Comparisons with other families of circular distributions are made Supplemental materials for this article are available online"
"10.1198/jasa.2009.tm08744","2010","The dependent wild bootstrap","0","We propose a new resampling procedure. the dependent wild bootstrap. for stationary time series As a natural extension of the traditional wild bootstrap to time series setting, the dependent wild bootstrap offers a viable alternative to the existing block-based bootstrap methods. whose properties have been extensively studied over the last two decades Unlike all of the block-based bootstrap methods. the dependent wild bootstrap can be easily extended to irregularly spaced time series with no implement:atonal difficulty Furthermore, it preserves the favorable bias and mean squared error property of the tapered block bootstrap. which is the state-of-the-art block-based method in terms of asymptotic accuracy of variance estimation and distribution approximation The consistency of the dependent wild bootstrap in distribution approximation is established tinder the framework of the smooth function model In addition, we obtain the bias and variance expansions of the dependent wild bootstrap variance estimator for irregularly spaced time series on a lattice For irregularly spaced nonlattice time series, we prove the consistency of the dependent wild bootstrap for variance estimation and distribution approximation in the mean case Simulation studies and an empirical data analysis illustrate the finite-sample performance of the dependent wild bootstrap Some technical details and tables are included in the online supplemental material"
"10.1198/jasa.2009.tm08506","2010","Highly efficient aggregate unbiased estimating functions approach for correlated data with missing at random","1","We develop a consistent and highly efficient marginal model for missing at random data using an estimating function approach Our approach differs from inverse weighted estimating equations (Robins. Rotnitzky. mid Zhao 1995) and the imputation method (Paik 1997) in that our approach does not require estimating the probability of missing or imputing the missing response based on assumed models The proposed method is based on an aggregate unbiased estimating function approach. which does not require the likelihood function. however, it is equivalent to the score equation if the likelihood is known The aggregate-unbiased approach is based on the best linear approximation of efficient scores from the lull dataset We provide comparisons of the three approaches using simulated data and also a human immunodeficiency virus (HIV) data example"
"10.1198/jasa.2009.tm08485","2010","Semiparametric mean-covariance regression analysis for longitudinal data","2","Efficient estimation of the regression coefficients in longitudinal data analysis requires a correct specification of the covariance structure Existing approaches usually focus on model no the mean with specification of certain covariance structures, which may lead to inefficient or biased estimators of parameters in the mean it misspecification occurs In this article. we propose a data-driven approach based on semiparametric regression models tot the mean and the covariance simultaneously. motivated by the modified Cholesky decomposition A regression spline-based approach using generalized estimating equations is developed to estimate the parameters in the mean and the covariance The resulting estimators for the regression coefficients in both the mean and the covariance are shown to be consistent and asymptotically normally distributed In addition. the nonparametric functions in these two structures are estimated at their optimal rate of convergence Simulation studies and a real data analysis show that the proposed approach yields highly efficient estimators for the parameters in the mean. and provides parsimonious estimation for the covariance structure Supplemental materials for the article are available online"
"10.1198/jasa.2009.tm08326","2010","Approximate methods for state-space models","0","State-space models provide an important body of techniques for analyzing time series. but their use requires estimating Unobserved states The optimal estimate of the state Is its conditional expectation given the observation histories. and computing this expectation is hard when there are nonlinearities Existing filtering methods, including sequential Monte Carlo. tend to be either inaccurate or slow In this paper, we study a nonlinear filter for nonlinear/non-Gaussian state-space models. which uses Laplace's method. an asymptotic series expansion, to approximate the state's conditional mean and variance, together with a Gaussian conditional distribution This Laplace Gaussian fillet (LGE) gives fast. recursive, deterministic state estimates, with an error which is set by the stochastic characteristics of the model and is, we show, stable over time We illustrate the estimation ability of the LGF by applying it to the problem of neural decoding and compare it to sequential Monte Carlo both in simulations and with real data We find that the LGE can deliver superior results in a small fraction of the computing time This article has supplementary material online"
"10.1198/jasa.2009.ap08443","2010","A {B}ayesian analysis of body mass index data from small domains under nonignorable nonresponse and selection","0","Here we analyze body mass index (BMI) data for addict) and adolescents from the Third National Health and Nutrition Examination Survey (NHANES III) Because of the lack of BMI vallies for a considerable number of the children and adolescents, and the differential probabilities of selection of these individuals, serious nonresponse and selection bias in inference can be present To analyze the NHANES III BMI data, a nonignorable nonresponse model has been proposed to estimate the finite population means of small domains formed by crossing age, race and sex within counties In this approach. the log-BMI values are used to obtain more normally distributed data, and the model includes a spline regression of log-BMI on age, adjusted for race, sex. and the interaction of race and sex In this work, to assess the status of overweight and obesity in children and adolescents. our new model predicts the more informative fume population percentiles of BMI for these small domains. Incorporating additional measures to minimize possible biases These measures are the most appropriate transformation for the skewed BMI data. Incorporation of an intraclass, correlation within the households. and inclusion of selection probabilities into the nonignorable nonresponse model to reflect the higher probabilities of selection among black non-Hispanics and Hispainc-Americans We also consider robustness and sensitivity to the assumptions of the non ignorable nonresponse model by fitting several versions of our proposed model. as well as a very different ignorable nonresponse model that uses a mixture of Student t densities. selection probabilities. and BMI values It is noteworthy that in the likehhood-based inference literature, we have seen no work by others that includes both nonignorable nonresponse and nonignorable selection Based on NHANES III data, we show that there are differences in the 85th or 95th percentile for overweight by county. race. and particularly age, and a small difference in sex"
"10.1198/jasa.2011.tm10383","2011","Building consistent regression trees from complex sample data","0","In the past several years a wide range of methods for the construction of regression trees and other estimators based on the recursive partitioning of samples have appeared in the statistics literature. Many applications involve data collected through a complex sample design. At present, however, relatively little is known regarding the properties of these methods under complex designs. This article proposes a method for incorporating information about the complex sample design when building a regression tree using a recursive partitioning algorithm. Sufficient conditions are established for asymptotic design L-2 consistency of these regression trees as estimators for an arbitrary regression function. The proposed method is illustrated with Occupational Employment Statistics establishment survey data linked to Quarterly Census of Employment and Wage payroll data of the Bureau of Labor Statistics. Performance of the nonparametric estimator is investigated through a simulation study based on this example."
"10.1198/jasa.2011.tm10576","2011","Nonparametric tests for homogeneity based on non-bipartite matching","0","Given a sequence of observations, has a change occurred in the underlying probability distribution with respect to observation order? This problem of detecting change points arises in a variety of applications including health prognostics for mechanical systems, syndromic disease surveillance in geographically dispersed populations, anomaly detection in information networks, and multivariate process control in general. Detecting change points in high-dimensional settings is challenging, and most change-point methods for multidimensional problems rely upon distributional assumptions or the use of observation history to model probability distributions. We present three new nonparametric statistical tests for heterogeneity based on the combinatorial properties of minimum non-bipartite matching (MNBM). The key idea underlying each of these tests is that if a sequence of independent random observations undergoes a change in distribution-either an abrupt ""shift"" or a gradual ""drift""-a MNBM based on inter-point distances tends to produce pairings that are closer in the sequence labeling than would be the case if the observations were drawn from the same distribution. Our tests follow on the work of Rosenbaum (2005) who used MNBM to derive a simple cross-match test statistic for the two-sample problem based on this idea. Similar ideas are present in the minimum spanning tree (MST) test derived by Friedman and Rafsky (1979, 1981). We extend these approaches by utilizing ensembles of orthogonal MNBMs which greatly increase information extraction from the data, leading to tests that compare favorably to parametric procedures while maintaining level and good power properties across distributions."
"10.1198/jasa.2011.tm11181","2011","Tweedie's formula and selection bias","0","We suppose that the statistician observes some large number of estimates z(i), each with its own unobserved expectation parameter mu(i). The largest few of the z(i)'s are likely to substantially overestimate their corresponding mu(i)'s, this being an example of selection bias, or regression to the mean. Tweedie's formula, first reported by Robbins in 1956, offers a simple empirical Bayes approach for correcting selection bias. This article investigates its merits and limitations. In addition to the methodology, Tweedie's formula raises more general questions concerning empirical Bayes theory, discussed here as ""relevance"" and ""empirical Bayes information."" There is a close connection between applications of the formula and James-Stein estimation."
"10.1198/jasa.2011.tm10483","2011","A framework for assessing broad sense agreement between ordinal and continuous measurements","0","Conventional agreement studies have been confined to addressing the sense of reproducibility, and therefore are limited to assessing measurements on the same scale. In this work, we propose a new concept, called ""broad sense agreement,"" which extends the classical framework of agreement to evaluate the capability of interpreting a continuous measurement in an ordinal scale. We present a natural measure for broad sense agreement. Nonparametric estimation and inference procedures are developed for the proposed measure along with theoretical justifications. We also consider longitudinal settings which involve agreement assessments at multiple time points. Simulation studies have demonstrated good performance of the proposed method with small sample sizes. We illustrate our methods via an application to a mental health study."
"10.1198/jasa.2011.tm10265","2011","Identifiability and estimation of causal effects by principal stratification with outcomes truncated by death","0","In medical studies, there are many situations where the final outcomes are truncated by death, in which patients die before outcomes of interest are measured. In this article we consider identifiability and estimation of causal effects by principal stratification when some outcomes are truncated by death. Previous studies mostly focused on large sample bounds, Bayesian analysis, sensitivity analysis. In this article, we propose a new method for identifying the causal parameter of interest under a nonparametric and semiparametric model. We show that the causal parameter of interest is identifiable under some regularity assumptions and the assumption that there exists a pretreatment covariate whose conditional distributions among two principal strata are not the same, but our approach does not need the assumption of a mixture normal distribution for outcomes as required by Zhang, Rubin, and Mealli (2009). Hence, the proposed method is applicable not only to a continuous outcome but also to a binary outcome. When some of the assumptions are violated, we discuss biases of estimators and propose methods to reduce these biases. We conduct several simulation studies to evaluate the finite-sample performance of the proposed approach. Finally, we apply the proposed approach to a real dataset from a Southwest Oncology Group (SWOG) clinical trial."
"10.1198/jasa.2011.tm11199","2011","A direct estimation approach to sparse linear discriminant analysis","0","This article considers sparse linear discriminant analysis of high-dimensional data. In contrast to the existing methods which are based on separate estimation of the precision matrix Omega and the difference delta of the mean vectors, we introduce a simple and effective classifier by estimating the product Omega delta directly through constrained l(1) minimization. The estimator can be implemented efficiently using linear programming and the resulting classifier is called the linear programming discriminant (LPD) rule.The LPD rule is shown to have desirable theoretical and numerical properties. It exploits the approximate sparsity of Omega delta and as a consequence allows cases where it can still perform well even when Omega and/or delta cannot be estimated consistently. Asymptotic properties of the LPD rule are investigated and consistency and rate of convergence results are given. The LPD classifier has superior finite sample performance and significant computational advantages over the existing methods that require separate estimation of Omega and delta. The LPD rule is also applied to analyze real datasets from lung cancer and leukemia studies. The classifier performs favorably in comparison to existing methods."
"10.1198/jasa.2011.tm10003","2011","Coupling optional {P}\'olya trees and the two sample problem","0","Testing and characterizing the difference between two data samples is of fundamental interest in statistics. Existing methods such as Kolmogorov-Smirnov and Cramer-von Mises tests do not scale well as the dimensionality increases and provide no easy way to characterize the difference should it exist. In this work, we propose a theoretical framework for inference that addresses these challenges in the form of a prior for Bayesian nonparametric analysis. The new prior is constructed based on a random-partition-and-assignment procedure similar to the one that defines the standard optional Polya tree distribution, but has the ability to generate multiple random distributions jointly. These random probability distributions are allowed to ""couple,"" that is, to have the same conditional distribution, on subsets of the sample space. We show that this ""coupling optional Polya tree"" prior provides a convenient and effective way for both the testing of two sample difference and the learning of the underlying structure of the difference. In addition, we discuss some practical issues in the computational implementation of this prior and provide several numerical examples to demonstrate its work. Supplementary materials for this article are available online."
"10.1198/jasa.2011.tm11015","2011","Large-scale correlation screening","0","This article addresses the problem of screening for variables with high correlations in high-dimensional data in which there can be many fewer samples than variables. We focus on threshold-based correlation screening methods for three related applications: screening for variables with large correlations within a single treatment (autocorrelation screening), screening for variables with large cross-correlations over two treatments (cross-correlation screening), and screening for variables that have persistently large autocorrelations over two treatments (persistent-correlation screening). The novelty of correlation screening is that it identifies a smaller number of variables that are highly correlated with others compared with identifying a number of correlation parameters. Correlation screening suffers from a phase transition phenomenon; as the correlation threshold decreases, the number of discoveries increases abruptly. We obtain asymptotic expressions for the mean number of discoveries and the phase transition thresholds as a function of the number of samples, the number of variables, and the joint sample distribution. We also show that under a weak dependency condition, the number of discoveries is dominated by a Poisson random variable giving an asymptotic expression for the false-positive rate. The correlation screening approach yields tremendous dividends in terms of the type and strength of the asymptotic results that can be obtained. It also overcomes some of the major hurdles faced by existing methods in the literature, because correlation screening is naturally scalable to high dimensions. Numerical results strongly validate the theory presented here. We illustrate the application of the correlation screening methodology on a large-scale gene-expression dataset, revealing a few influential variables that exhibit significant correlation over multiple treatments. This article has supplementary material online."
"10.1198/jasa.2011.tm10552","2011","Bayesian kernel mixtures for counts","0","Although Bayesian nonparametric mixture models for continuous data are well developed, the literature on related approaches for count data is limited. A common strategy is to use a mixture of Poissons, which unfortunately is quite restrictive in not accounting for distributions with variance less than the mean. Other approaches include mixing multinomials, which requires finite support, and using a Dirichlet process prior with a Poisson base measure, which does not allow for smooth deviations from the Poisson. We propose broad class of alternative models, nonparametric mixtures of rounded continuous kernels. We develop an efficient Gibbs sampler for posterior computation, and perform a simulation study to assess performance. Focusing on the rounded Gaussian case, we generalize the modeling framework to account for multivariate count data, joint modeling with continuous and categorical variables, and other complications. We illustrate our methods through applications to a developmental toxicity study and marketing data. Supplemental material is available online."
"10.1198/jasa.2011.tm09771","2011","Forecasting time series with complex seasonal patterns using exponential smoothing","0","An innovations state space modeling framework is introduced for forecasting complex seasonal time series such as those with multiple seasonal periods, high-frequency seasonality, non-integer seasonality, and dual-calendar effects. The new framework incorporates Box-Cox transformations, Fourier representations with time varying coefficients, and ARMA error correction. Likelihood evaluation and analytical expressions for point forecasts and interval predictions under the assumption of Gaussian errors are derived, leading to a simple, comprehensive approach to forecasting complex seasonal time series. A key feature of the framework is that it relies on a new method that greatly reduces the computational burden in the maximum likelihood estimation. The modeling framework is useful for a broad range of applications, its versatility being illustrated in three empirical studies. In addition, the proposed trigonometric formulation is presented as a means of decomposing complex seasonal time series, and it is shown that this decomposition leads to the identification and extraction of seasonal components which are otherwise not apparent in the time series plot itself."
"10.1198/jasa.2011.tm10012","2011","Predictive inference for integrated volatility","0","Numerous volatility-based derivative products have been engineered in recent years. This has led to interest in constructing conditional predictive densities and confidence intervals for integrated volatility. In this article we propose nonparametric estimators of the aforementioned quantities, based on model-free volatility estimators. We establish consistency and asymptotic normality for the feasible estimators and study their finite-sample properties through a Monte Carlo experiment. Finally, using data from the New York Stock Exchange, we provide an empirical application to volatility directional predictability."
"10.1198/jasa.2011.tm09294","2011","Semiparametric stochastic modeling of the rate function in longitudinal studies","0","In longitudinal biomedical studies, there is often interest in the rate functions, which describe the functional rates of change of biomarker profiles. This article proposes a semiparametric approach to model these functions as the realizations of stochastic processes defined by stochastic differential equations. These processes are dependent on the covariates of interest and vary around a specified parametric function. An efficient Markov chain Monte Carlo algorithm is developed for inference. The proposed method is compared with several existing methods in terms of goodness of fit and more importantly the ability to forecast future functional data in a simulation study. The proposed methodology is applied to prostate-specific antigen profiles for illustration. Supplementary materials for this article are available online."
"10.1198/jasa.2011.tm10670","2011","A new nuisance-parameter elimination method with application to the unordered homologous chromosome pairs problem","0","Motivated by applications of the case control model or exponential tilting model in the unordered homologous chromosome pairs problem in genetic studies and in the interim analysis in double-blinded clinical trials, we develop a new nuisance-parameter elimination method based on the empirical Shannon's mutual information. The asymptotic behaviors of the maximum empirical Shannon's mutual information estimation and the empirical Shannon's mutual information test are similar to those of the maximum likelihood estimation and the likelihood ratio test, respectively. Interestingly, we have found a connection between the empirical Shannon's mutual information and the profile empirical likelihood (Owen 1988) under some constraints. In the test of the null hypothesis that the unordered pairs come from the same distribution, the maximum Shannon's mutual information estimation has a degenerate information matrix. As a result, we have to expand the empirical Shannon's mutual information test statistic up to the fourth order to find the limiting distribution of the mixture of a distribution with point mass at zero and a chi-squared distribution with one degree of freedom. A real genetic dataset is employed for illustration. We also outline another application of Shannon's mutual information in general genetic mixture models."
"10.1198/jasa.2011.tm10563","2011","Model-free feature screening for ultrahigh-dimensional data","0","With the recent explosion of scientific data of unprecedented size and complexity, feature ranking and screening are playing an increasingly important role in many scientific studies. In this article, we propose a novel feature screening procedure under a unified model framework, which covers a wide variety of commonly used parametric and semiparametric models. The new method does not require imposing a specific model structure on regression functions, and thus is particularly appealing to ultrahigh-dimensional regressions, where there are a huge number of candidate predictors but little information about the actual model forms. We demonstrate that, with the number of predictors growing at an exponential rate of the sample size, the proposed procedure possesses consistency in ranking, which is both useful in its own right and can lead to consistency in selection. The new procedure is computationally efficient and simple, and exhibits a competent empirical performance in our intensive simulations and real data analysis."
"10.1198/jasa.2011.tm10616","2011","Dynamic orthogonal components for multivariate time series","0","We introduce dynamic orthogonal components (DOC) for multivariate time series and propose a procedure for estimating and testing the existence of DOCs for a given time series. We estimate the dynamic orthogonal components via a generalized decorrelation method that minimizes the linear and quadratic dependence across components and across time. We then use Ljung-Box type statistics to test the existence of dynamic orthogonal components. When DOCs exist, univariate analysis can be applied to build a model for each component. Those univariate models are then combined to obtain a multivariate model for the original time series. We demonstrate the usefulness of dynamic orthogonal components with two real examples and compare the proposed modeling method with other dimension-reduction methods available in the literature, including principal component and independent component analyses. We also prove consistency and asymptotic normality of the proposed estimator under some regularity conditions. We provide some technical details in online Supplementary Materials."
"10.1198/jasa.2011.tm10156","2011","Maximum likelihood estimations and {EM} algorithms with length-biased data","0","Length-biased sampling has been well recognized in economics, industrial reliability, etiology applications, and epidemiological, genetic, and cancer screening studies. Length-biased right-censored data have a unique data structure different from traditional survival data. The nonparametric and semiparametric estimation and inference methods for traditional survival data are not directly applicable for length-biased right-censored data. We propose new expectation-maximization algorithms for estimations based on full likelihoods involving infinite-dimensional parameters under three settings for length-biased data: estimating nonparametric distribution function, estimating nonparametric hazard function under an increasing failure rate constraint, and jointly estimating baseline hazards function and the covariate coefficients under the Cox proportional hazards model. Extensive empirical simulation studies show that the maximum likelihood estimators perform well with moderate sample sizes and lead to more efficient estimators compared to the estimating equation approaches. The proposed estimates are also more robust to various right-censoring mechanisms. We prove the strong consistency properties of the estimators, and establish the asymptotic normality of the semiparametric maximum likelihood estimators under the Cox model using modern empirical processes theory. We apply the proposed methods to a prevalent cohort medical study. Supplemental materials are available online."
"10.1198/jasa.2011.tm10465","2011","Bayesian inference for general {G}aussian graphical models with application to multivariate lattice data","0","We introduce efficient Markov chain Monte Carlo methods for inference and model determination in multivariate and matrix-variate Gaussian graphical models. Our framework is based on the G-Wishart prior for the precision matrix associated with graphs that can be decomposable or non-decomposable. We extend our sampling algorithms to a novel class of conditionally autoregressive models for sparse estimation in multivariate lattice data, with a special emphasis on the analysis of spatial data. These models embed a great deal of flexibility in estimating both the correlation structure across outcomes and the spatial correlation structure, thereby allowing for adaptive smoothing and spatial autocorrelation parameters. Our methods are illustrated using a simulated example and a real-world application which concerns cancer mortality surveillance. Supplementary materials with computer code and the datasets needed to replicate our numerical results together with additional tables of results are available online."
"10.1198/jasa.2011.tm10470","2011","Semiparametric approach to a random effects quantile regression model","1","We consider a random effects quantile regression analysis of clustered data and propose a semiparametric approach using empirical likelihood. The random regression coefficients are assumed independent with a common mean, following parametrically specified distributions. The common mean corresponds to the population-average effects of explanatory variables on the conditional quantile of interest, whereas the random coefficients represent cluster-specific deviations in the covariate effects. We formulate the estimation of the random coefficients as an estimating equations problem and use empirical likelihood to incorporate the parametric likelihood of the random coefficients. A likelihood-like statistical criterion function is proposed that, which we show is asymptotically concave in a neighborhood of the true parameter value and motivates its maximizer as a natural estimator. We use Markov chain Monte Carlo samplers in the Bayesian framework, and propose the resulting quasi-posterior mean as an estimator. We show that the proposed estimator of the population-level parameter is asymptotically normal, and that the estimators of the random coefficients are shrunk toward the population-level parameter in the first-order asymptotic sense. These asymptotic results do not require Gaussian random effects, and the empirical likelihood-based likelihood-like criterion function is free of parameters related to the error densities. This makes the proposed approach both flexible and computationally simple. We illustrate the methodology with two real data examples."
"10.1198/jasa.2011.tm10322","2011","Order-restricted inference for multivariate binary data with application to toxicology","0","In many applications, researchers collect multivariate binary response data under two or more naturally ordered experimental conditions. In such situations, one is often interested in using all binary outcomes simultaneously to detect an ordering among the experimental conditions. To make such comparisons, we develop a general methodology for testing for the multivariate stochastic order between K >= 2 multivariate binary distributions. Our proposed test uses order-restricted estimators, which, according to our simulation study, are more efficient than the unrestricted estimators in terms of their mean squared error. We compared the power of the proposed test with that of several alternative tests, including procedures that combine individual univariate tests for order, such as union-intersection tests and a Bonferroni-based test. We also compared the proposed test with an unrestricted Hotel ling T-2-type test. Our simulations suggest that the proposed method competes well with these alternatives. The gain in power is often substantial. The proposed methodology is illustrated by applying it to a two-year rodent cancer bioassay data obtained from the U.S. National Toxicology Program. Supplemental materials are available online."
"10.1198/jasa.2011.tm09241","2011","Elastic net regression modeling with the orthant normal prior","0","The elastic net procedure is a form of regularized optimization for linear regression that provides a bridge between ridge regression and the lasso. The estimate that it produces can be viewed as a Bayesian posterior mode under a prior distribution implied by the form of the elastic net penalty. This article broadens the scope of the Bayesian connection by providing a complete characterization of a class of prior distributions that generate the elastic net estimate as the posterior mode. The resulting model-based framework allows for methodology that moves beyond exclusive use of the posterior mode by considering inference based on the full posterior distribution. Two characterizations of the class of prior distributions are introduced: a properly normalized, direct characterization, which is shown to be conjugate for linear regression models, and an alternate representation as a scale mixture of normal distributions. Prior distributions are proposed for the regularization parameters, resulting in an infinite mixture of elastic net regression models that allows for adaptive, data-based shrinkage of the regression coefficients. Posterior inference is easily achieved using Markov chain Monte Carlo (MCMC) methods. Uncertainty about model specification is addressed from a Bayesian perspective by assigning prior probabilities to all possible models. Corresponding computational approaches are described. Software for implementing the MCMC methods described in this article, written in c++ with an R package interface, is available at http://www.stat.osu.edu/similar to hans/software/."
"10.1198/jasa.2011.tm10382","2011","A perturbation method for inference on regularized regression estimates","0","Analysis of high-dimensional data often seeks to identify a subset of important features and to assess the effects of these features on outcomes. Traditional statistical inference procedures based on standard regression methods often fail in the presence of high-dimensional features. In recent years, regularization methods have emerged as promising tools for analyzing high-dimensional data. These methods simultaneously select important features and provide stable estimation of their effects. Adaptive LASSO and SCAD, for instance, give consistent and asymptotically normal estimates with oracle properties. However, in finite samples, it remains difficult to obtain interval estimators for the regression parameters. In this article, we propose perturbation resampling-based procedures to approximate the distribution of a general class of penalized parameter estimates. Our proposal, justified by asymptotic theory, provides a simple way to estimate the covariance matrix and confidence regions. Through finite-sample simulations, we verify the ability of this method to give accurate inference and compare it with other widely used standard deviation and confidence interval estimates. We also illustrate our proposals with a dataset used to study the association of HIV drug resistance and a large number of genetic mutations."
"10.1198/jasa.2011.tm10747","2011","Instability, sensitivity, and degeneracy of discrete exponential families","0","A number of discrete exponential family models for dependent data, first and foremost relational data, have turned out to be near-degenerate and problematic in terms of Markov chain Monte Carlo (MCMC) simulation and statistical inference. I introduce the notion of instability with an eye to characterize, detect, and penalize discrete exponential family models that are near-degenerate and problematic in terms of MCMC simulation and statistical inference. I show that unstable discrete exponential family models are characterized by excessive sensitivity and near-degeneracy. In special cases, the subset of the natural parameter space corresponding to nondegenerate distributions and mean-value parameters far from the boundary of the mean-value parameter space turns out to be a lower-dimensional subspace of the natural parameter space. These characteristics of unstable discrete exponential family models tend to obstruct MCMC simulation and statistical inference. In applications to relational data, I show that discrete exponential family models with Markov dependence tend to be unstable, and that the parameter space of some curved exponential families contains unstable subsets."
"10.1198/jasa.2011.ap10599","2011","A regularized {H}otelling's {$T^2$} test for pathway analysis in proteomic studies","0","Recent proteomic studies have identified proteins related to specific phenotypes. In addition to marginal association analysis for individual proteins, analyzing pathways (functionally related sets of proteins) may yield additional valuable insights. Identifying pathways that differ between phenotypes can be conceptualized as a multivariate hypothesis testing problem: whether the mean vector mu of a p-dimensional random vector X is mu(0). Proteins within the same biological pathway may correlate with one another in a complicated way, and Type I error rates can be inflated if such correlations are incorrectly assumed to be absent. The inflation tends to be more pronounced when the sample size is very small or there is a large amount of missingness in the data, as is frequently the case in proteomic discovery studies. To tackle these challenges, we propose a regularized Hotelling's T-2 (RHT) statistic together with a nonparametric testing procedure, which effectively controls the Type I error rate and maintains good power in the presence of complex correlation structures and missing data patterns. We investigate asymptotic properties of the RHT statistic under pertinent assumptions and compare the test performance with four existing methods through simulation examples. We apply the RHT test to a hormone therapy proteomics dataset, and identify several interesting biological pathways for which blood serum concentrations changed following hormone therapy initiation. This article has supplementary material online."
"10.1198/jasa.2011.ap10425","2011","A {B}ayesian semiparametric approach to intermediate variables in causal inference","0","In causal inference studies, treatment comparisons often need to be adjusted for confounded post-treatment variables. Principal stratification (PS) is a framework to deal with such variables within the potential outcome approach to causal inference. Continuous intermediate variables introduce inferential challenges to PS analysis. Existing methods either dichotomize the intermediate variable, or assume a fully parametric model for the joint distribution of the potential intermediate variables. However, the former is subject to information loss and arbitrary choice of the cutoff point and the latter is often inadequate to represent complex distributional and clustering features. We propose a Bayesian semiparametric approach that consists of a flexible parametric model for the potential outcomes and a Bayesian nonparametric model for the potential intermediate outcomes using a Dirichlet process mixture (DPM) model. The DPM approach provides flexibility in modeling the possibly complex joint distribution of the potential intermediate outcomes and offers better interpretability of results through its clustering feature. Gibbs sampling based posterior inference is developed. We illustrate the method by two applications: one concerning partial compliance in a randomized clinical trial, and one concerning the causal mechanism between physical activity, body mass index, and cardiovascular disease in the observational Swedish National March Cohort study."
"10.1198/jasa.2011.ap10346","2011","Multi-domain sampling with applications to structural inference of {B}ayesian networks","0","When a posterior distribution has multiple modes, unconditional expectations, such as the posterior mean, may not offer informative summaries of the distribution. Motivated by this problem, we propose to decompose the sample space of a multimodal distribution into domains of attraction of local modes. Domain-based representations are defined to summarize the probability masses of and conditional expectations on domains of attraction, which are much more informative than the mean and other unconditional expectations. A computational method, the multi-domain sampler, is developed to construct domain-based representations for an arbitrary multimodal distribution. The multi-domain sampler is applied to structural learning of protein-signaling networks from high-throughput single-cell data, where a signaling network is modeled as a causal Bayesian network. Not only does our method provide a detailed landscape of the posterior distribution but also improves the accuracy and the predictive power of estimated networks. This article has supplementary material online."
"10.1198/jasa.2011.ap10017","2011","Rasch model and its extensions for analysis of aphasic deficits in syntactic comprehension","0","Aphasia is the loss of the ability to produce and/or comprehend language, due to injury to brain areas responsible for these functions. Aphasic patients' performance on comprehension tests has traditionally been related both to the patient's individual ability and to the difficulty of the test questions. The natural choice for analysis of these test results is the Rasch model. It assumes that the probability of a patient responding correctly to a question is the inverse-logit function of the difference between the individual patient's ability and the difficulty of the test question. This study first modeled the way aphasic patients process different sentence types, as well as their ability to accomplish tasks using Rasch models. However, several scientifically important features of the data, such as the correlation of correct responses between two different comprehension tasks, and the association between response patterns in control sentences and response patterns in experimental sentences, were found to be inadequately captured by such models. Alternatively, we used a full Bayesian approach, exploring a mixture of generalized linear mixed models that clustered patients into similar response patterns and abilities. The mixture model was found to better describe the experimental results than any other model examined. The mixture model also expresses the hypothesis that aphasic patients can be classified into different ability and response profile groups, and that patients utilize different cognitive resources in different comprehension tasks. These results are scientifically important and could not have been discovered by using the simple Rasch model. This article has supplementary material online."
"10.1198/jasa.2011.ap10433","2011","Geostatistical model averaging for locally calibrated probabilistic quantitative precipitation forecasting","0","Accurate weather benefit many key societal functions and activities, including agriculture, transportation, recreation, and basic human and infrastructural safety. Over the past two decades, ensembles of numerical weather prediction models have been developed, in which multiple estimates of the current state of the atmosphere are used to generate probabilistic forecasts for future weather events. However, ensemble systems are uncalibrated and biased, and thus need to be statistically postprocessed. Bayesian model averaging (BMA) is a preferred way of doing this. Particularly for quantitative precipitation, biases and calibration errors depend critically on local terrain features. We introduce a geostatistical approach to modeling locally varying BMA parameters, as opposed to the extant method that holds parameters constant across the forecast domain. Degeneracies caused by enduring dry periods are overcome by Bayesian regularization and Laplace approximations. The new approach, called geostatistical model averaging (GMA), was applied to 48-hour-ahead forecasts of daily precipitation accumulation over the North American Pacific Northwest, using the eight-member University of Washington Mesoscale Ensemble. GMA had better aggregate and local calibration than the extant technique, and was sharper on average."
"10.1198/jasa.2011.ap09283","2011","Semiparametric {B}ayesian modeling of income volatility heterogeneity","0","Research on income risk typically treats its proxy-income volatility, the expected magnitude of income changes-as if it were unchanged for an individual over time, the same for everyone at a point in time, or both. In reality, income risk evolves over time, and some people face more of it than others. To model heterogeneity and dynamics in (unobserved) income volatility, we develop a novel semiparametric Bayesian stochastic volatility model. Our Markovian hierarchical Dirichlet process (MHDP) prior augments the recently developed hierarchical Dirichlet process (HDP) prior to accommodate the serial dependence of panel data. We document dynamics and substantial heterogeneity in income volatility."
"10.1198/jasa.2011.ap10611","2011","Predicting viral infection from high-dimensional biomarker trajectories","0","There is often interest in predicting an individual's latent health status based on high-dimensional biomarkers that vary over time. Motivated by time-course gene expression array data that we have collected in two influenza challenge studies performed with healthy human volunteers, we develop a novel time-aligned Bayesian dynamic factor analysis methodology. The time course trajectories in the gene expressions are related to a relatively low-dimensional vector of latent factors, which vary dynamically starting at the latent initiation time of infection. Using a nonparametric cure rate model for the latent initiation times, we allow selection of the genes in the viral response pathway, variability among individuals in infection times, and a subset of individuals who are not infected. As we demonstrate using held-out data, this statistical framework allows accurate predictions of infected individuals in advance of the development of clinical symptoms, without labeled data and even when the number of biomarkers vastly exceeds the number of individuals under study. Biological interpretation of several of the inferred pathways (factors) is provided."
"10.1198/jasa.2011.ap10194","2011","High-dimensional {ODE}s coupled with mixed-effects modeling techniques for dynamic gene regulatory network identification","0","Gene regulation is a complicated process. The interaction of many genes and their products forms an intricate biological network. Identification of this dynamic network will help us understand the biological processes in a systematic way. However, the construction of a dynamic network is very challenging for a high-dimensional system. In this article we propose to use a set of ordinary differential equations (ODE), coupled with dimensional reduction by clustering and mixed-effects modeling techniques, to model the dynamic gene regulatory network (GRN). The ODE models allow us to quantify both positive and negative gene regulation as well as feedback effects of genes in a functional module on the dynamic expression changes of genes in another functional module, which results in a directed graph network. A five-step procedure-clustering, smoothing, regulation identification, parameter estimates refining, and function enrichment analysis (CSIEF)-is developed to identify the ODE-based dynamic GRN. In the proposed CSIEF procedure, a series of cutting-edge statistical methods and techniques are employed, that include nonparametric mixed-effects models with a mixture distribution for clustering, nonparametric mixed-effects smoothing-based methods for ODE models, the smoothly clipped absolute deviation (SCAD)-based variable selection, and stochastic approximation EM (SAEM) approach for mixed-effects ODE model parameter estimation. The key step, the SCAD-based variable selection, is justified by investigating its asymptotic properties and validated by Monte Carlo simulations. We apply the proposed method to identify the dynamic GRN for yeast cell cycle progression data. We are able to annotate the identified modules through function enrichment analyses. Some interesting biological findings are discussed. The proposed procedure is a promising tool for constructing a general dynamic GRN and more complicated dynamic networks. This article has supplementary material online."
"10.1198/jasa.2011.ap10243","2011","A likelihood ratio test based method for signal detection with application to {FDA}'s drug safety data","0","Several statistical methods that are available in the literature to analyze postmarket safety databases, such as the U.S. Federal Drug Administration's (FDA) adverse event reporting system (AERS), for identifying drug-event combinations with disproportionately high frequencies, are subject to high false discovery rates. Here, we propose a likelihood ratio test (LRT) based method and show, via an extensive simulation study, that the proposed method while retaining good power and sensitivity for identifying signals, controls both the Type I error and false discovery rates. The application of the LRT method to the AERS database is illustrated using two datasets; a small dataset consisting of suicidal behavior and mood change-related AE cases for the drug Montelukast, and a large dataset consisting of all possible AE cases reported to FDA during 2004-2008 for the drug Heparin. This article has supplementary material online."
"10.1198/jasa.2011.ap11592","2011","Statistics: an all-encompassing discipline","0",""
"10.1198/jasa.2011.tm10488","2011","Predicting false discovery proportion under dependence","0","We present a flexible framework for predicting error measures in multiple testing situations under dependence. Our approach is based on modeling the distribution of the probit transform of the p-values by mixtures of multivariate skew-normal distributions. The model can incorporate dependence among p-values and it allows for shape restrictions on the p-value density. A nonparametric Bayesian scheme for estimating the components of the mixture model is outlined and Markov chain Monte Carlo algorithms are developed. These lead to the prediction of false discovery proportion and related credible bands. An expression for the positive false discovery rate for dependent observations is also derived. The power of the mixture model in estimation of key quantities in multiple testing is illustrated by a simulation study. A dataset on kidney transplant is also analyzed using the methods developed."
"10.1198/jasa.2011.tm10573","2011","Fusion-refinement procedure for dimension reduction with missing response at random","0","Dimension reduction methods are useful for handling high-dimensional data. It is a common situation that responses of some subjects are not observed in practice. Generally, the missingness carries additional information about the central subspace. Here we propose a two-stage procedure known as the fusion-refinement (FR) procedure. In the first stage, we obtain a subspace including the central subspace by fusing information on regression and missingness. In the second stage, we refine the obtained subspace to recover the central subspace by imputation method. We use sliced inverse regression to illustrate the FR procedure. We conduct simulation studies, and suggest a data-driven procedure to choose from the complete-case analysis and the FR procedure for a purpose of a real application. A real data analysis is used to illustrate our methods."
"10.1198/jasa.2011.tm10798","2011","Density estimation in several populations with uncertain population membership","0","We devise methods to estimate probability density functions of several populations using observations with uncertain population membership, meaning from which population an observation comes is unknown. The probability of an observation being sampled from any given population can be calculated. We develop general estimation procedures and bandwidth selection methods for our setting. We establish large-sample properties and study finite-sample performance using simulation studies. We illustrate our methods with data from a nutrition study."
"10.1198/jasa.2011.tm10370","2011","Robust, adaptive functional regression in functional mixed model framework","0","Functional data are increasingly encountered in scientific studies, and their high dimensionality and complexity lead to many analytical challenges. Various methods for functional data analysis have been developed, including functional response regression methods that involve regression of a functional response on univariate/multivariate predictors with nonparametrically represented functional coefficients. In existing methods, however, the functional regression can be sensitive to outlying curves and outlying regions of curves, so is not robust. In this article, we introduce a new Bayesian method, robust functional mixed models (R-FMM), for performing robust functional regression within the general functional mixed model framework, which includes multiple continuous or categorical predictors and random effect functions accommodating potential between-function correlation induced by the experimental design. The underlying model involves a hierarchical scale mixture model for the fixed effects, random effect, and residual error functions. These modeling assumptions across curves result in robust nonparametric estimators of the fixed and random effect functions which down-weight outlying curves and regions of curves, and produce statistics that can be used to flag global and local outliers. These assumptions also lead to distributions across wavelet coefficients that have outstanding sparsity and adaptive shrinkage properties, with great flexibility for the data to determine the sparsity and the heaviness of the tails. Together with the down-weighting of outliers, these within-curve properties lead to fixed and random effect function estimates that appear in our simulations to be remarkably adaptive in their ability to remove spurious features yet retain true features of the functions. We have developed general code to implement this fully Bayesian method that is automatic, requiring the user to only provide the functional data and design matrices. It is efficient enough to handle large datasets, and yields posterior samples of all model parameters that can be used to perform desired Bayesian estimation and inference. Although we present details for a specific implementation of the R-FMM using specific distributional choices in the hierarchical model, 1D functions, and wavelet transforms, the method can be applied more generally using other heavy-tailed distributions, higher dimensional functions (e.g., images), and using other invertible transformations as alternatives to wavelets. Supplementary materials for this article are available online."
"10.1198/jasa.2011.tm09693","2011","G{LS} estimation of dynamic factor models","0","In this article a simple two-step estimation procedure of the dynamic factor model is proposed. The estimator allows for heteroscedastic and serially correlated errors. It turns out that the feasible two-step estimator has the same limiting distribution as the generalized least squares (GLS) estimator assuming that the covariance parameters are known. In a Monte Carlo study of the small sample properties, we find that the GLS estimators may be substantially more efficient than the usual estimator based on principal components. Furthermore, it turns out that the iterated version of the estimator may feature considerably improved properties in sample sizes usually encountered in practice."
"10.1198/jasa.2011.tm10229","2011","Multi-layer designs for computer experiments","0","Space-filling designs such as Latin hypercube designs (LHDs) are widely used in computer experiments. However, finding an optimal LHD with good space-filling properties is computationally cumbersome. On the other hand, the well-established factorial designs in physical experiments are unsuitable for computer experiments owing to the redundancy of design points when projected onto a subset of factor space. In this work, we present a new class of space-filling designs developed by splitting two-level factorial designs into multiple layers. The method takes advantage of many available results in factorial design theory and therefore, the proposed multi-layer designs (MLDs) are easy to generate. Moreover, our numerical study shows that MLDs can have better space-filling properties than optimal LHDs."
"10.1198/jasa.2011.tm09738","2011","{\it {S}parse{N}et}: coordinate descent with nonconvex penalties","1","We address the problem of sparse selection in linear models. A number of nonconvex penalties have been proposed in the literature for this purpose, along with a variety of convex-relaxation algorithms for finding good solutions. In this article we pursue a coordinate-descent approach for optimization, and study its convergence properties. We characterize the properties of penalties suitable for this approach, study their corresponding threshold functions, and describe a df-standardizing reparametrization that assists our pathwise algorithm. The MC+ penalty is ideally suited to this task, and we use it to demonstrate the performance of our algorithm. Certain technical derivations and experiments related to this article are included in the Supplementary Materials section."
"10.1198/jasa.2011.tm10811","2011","A measure of stationarity in locally stationary processes with applications to testing","0","In this article we investigate the problem of measuring deviations from stationarity in locally stationary time series. Our approach is based on a direct estimate of the L-2-distance between the spectral density of the locally stationary process and its best approximation by a spectral density of a stationary process. An explicit expression of the minimal distance is derived, which depends only on integrals of the spectral density of the locally stationary process and its square. These integrals can be estimated directly without estimating the spectral density, and as a consequence, the estimation of the measure of stationarity does not require the specification of a smoothing bandwidth. We show weak convergence of an appropriately standardized version of the statistic to a standard normal distribution. The results are used to construct confidence intervals for the measure of stationarity and to develop a new test for the hypothesis of stationarity. Finally, we investigate the finite sample properties of the resulting confidence intervals and tests by means of a simulation study and illustrate the methodology in two data examples. Parts of the proofs are available online as supplemental material to this article."
"10.1198/jasa.2011.tm10281","2011","Linear or nonlinear? {A}utomatic structure discovery for partially linear models","0","Partially linear models provide a useful class of tools for modeling complex data by naturally incorporating a combination of linear and nonlinear effects within one framework. One key question in partially linear models is the choice of model structure, that is, how to decide which covariates are linear and which are nonlinear. This is a fundamental, yet largely unsolved problem for partially linear models. In practice, one often assumes that the model structure is given or known and then makes estimation and inference based on that structure. Alternatively, there are two methods in common use for tackling the problem: hypotheses testing and visual screening based on the marginal fits. Both methods are quite useful in practice but have their drawbacks. First, it is difficult to construct a powerful procedure for testing multiple hypotheses of linear against nonlinear fits. Second, the screening procedure based on the scatterplots of individual covariate fits may provide an educated guess on the regression function form, but the procedure is ad hoc and lacks theoretical justifications. In this article, we propose a new approach to structure selection for partially linear models, called the LAND (Linear And Nonlinear Discoverer). The procedure is developed in an elegant mathematical framework and possesses desired theoretical and computational properties. Under certain regularity conditions, we show that the LAND estimator is able to identify the underlying true model structure correctly and at the same time estimate the multivariate regression function consistently. The convergence rate of the new estimator is established as well. We further propose an iterative algorithm to implement the procedure and illustrate its performance by simulated and real examples. Supplementary materials for this article are available online."
"10.1198/jasa.2011.tm10506","2011","Summarizing insurance scores using a {G}ini index","0","Individuals, corporations and government entities regularly exchange financial risks y at prices Pi. Comparing distributions of risks and prices can be difficult, particularly when the financial risk distribution is complex. For example, with insurance, it is not uncommon for a risk distribution to be a mixture of 0's (corresponding to no claims) and a right-skewed distribution with thick tails (the claims distribution). However, analysts do not work in a vacuum, and in the case of insurance they use insurance scores relative to prices, called ""relativities,"" that point to areas of potential discrepancies between risk and price distributions.Ordering both risks and prices based on relativities, in this article we introduce what we call an ""ordered"" Lorenz curve for comparing distributions. This curve extends the classical Lorenz curve in two ways, through the ordering of risks and prices and by allowing prices to vary by observation. We summarize the ordered Lorenz curve in the same way as the classic Lorenz curve using a Gini index, defined as twice the area between the curve and the 45-degree line. For a given ordering, a large Gini index signals a large difference between price and risk distributions.We show that the ordered Lorenz curve has desirable properties. It can be expressed in terms of weighted distributions functions. In special cases, curves can be ranked through a partial ordering. We show how to estimate the Gini index and give pointwise consistency and asymptotic normality results. A simulation study and an example using homeowners insurance underscore the potential applications of these methods."
"10.1198/jasa.2011.tm10183","2011","Hierarchical clustering with prototypes via minimax linkage","0","Agglomerative hierarchical clustering is a popular class of methods for understanding the structure of a dataset. The nature of the clustering depends on the choice of linkage-that is, on how one measures the distance between clusters. In this article we investigate minimax linkage, a recently introduced but little-studied linkage. Minimax linkage is unique in naturally associating a prototype chosen from the original dataset with every interior node of the dendrogram. These prototypes can be used to greatly enhance the interpretability of a hierarchical clustering. Furthermore, we prove that minimax linkage has a number of desirable theoretical properties; for example, minimax-linkage dendrograms cannot have inversions (unlike centroid linkage) and is robust against certain perturbations of a dataset. We provide an efficient implementation and illustrate minimax linkage's strengths as a data analysis and visualization tool on a study of words from encyclopedia articles and on a dataset of images of human faces."
"10.1198/jasa.2011.tm10067","2011","Permutation multiple tests of binary features do not uniformly control error rates","0","Multiple testing for significant association between predictors and responses has a wide array of applications. One such application is pharmacogenomics, where testing for association between responses and many genetic markers is of interest. Permuting response group labels to generate a reference distribution is often thought of as a convenient thresholding technique that automatically captures dependence in the data. In reality, nontrivial model assumptions are required for permutation testing to control multiple testing error rates. When binary predictors (such as genetic markers) are individually tested by standard tests such as Fisher's exact test, permutation multiple testing can give incorrect unconditional and, especially, conditional assessment of significances. Regardless of whether the sample sizes are equal, how misleading permutation assessment can be is primarily a function of the difference in the covariances among the genetic markers between the phenotype groups."
"10.1198/jasa.2011.tm09478","2011","Optimal weight choice for frequentist model average estimators","0","There has been increasing interest recently in model averaging within the frequentist paradigm. The main benefit of model averaging over model selection is that it incorporates rather than ignores the uncertainty inherent in the model selection process. One of the most important, yet challenging, aspects of model averaging is how to optimally combine estimates from different models. In this work, we suggest a procedure of weight choice for frequentist model average estimators that exhibits optimality properties with respect to the estimator's mean squared error (MSE). As a basis for demonstrating our idea, we consider averaging over a sequence of linear regression models. Building on this base, we develop a model weighting mechanism that involves minimizing the trace of an unbiased estimator of the model average estimator's MSE. We further obtain results that reflect the finite sample as well as asymptotic optimality of the proposed mechanism. A Monte Carlo study based on simulated and real data evaluates and compares the finite sample properties of this mechanism with those of existing methods. The extension of the proposed weight selection scheme to general likelihood models is also considered. This article has supplementary material online."
"10.1198/jasa.2011.tm10518","2011","Rao-{B}lackwellization for {B}ayesian variable selection and model averaging in linear and binary regression: a novel data augmentation approach","0","Choosing the subset of covariates to use in regression or generalized linear models is a ubiquitous problem. The Bayesian paradigm addresses the problem of model uncertainty by considering models corresponding to all possible subsets of the covariates, where the posterior distribution over models is used to select models or combine them via Bayesian model averaging (BMA). Although conceptually straightforward, BMA is often difficult to implement in practice, since either the number of covariates is too large for enumeration of all subsets, calculations cannot be done analytically, or both. For orthogonal designs with the appropriate choice of prior, the posterior probability of any model can be calculated without having to enumerate the entire model space and scales linearly with the number of predictors, p. In this article we extend this idea to a much broader class of nonorthogonal design matrices. We propose a novel method which augments the observed nonorthogonal design by at most p new rows to obtain a design matrix with orthogonal columns and generate the ""missing"" response variables in a data augmentation algorithm. We show that our data augmentation approach keeps the original posterior distribution of interest unaltered, and develop methods to construct Rao-Blackwellized estimates of several quantities of interest, including posterior model probabilities of any model, which may not be available from an ordinary Gibbs sampler. Our method can be used for BMA in linear regression and binary regression with nonorthogonal design matrices in conjunction with independent ""spike and slab"" priors with a continuous prior component that is a Cauchy or other heavy tailed distribution that may be represented as a scale mixture of normals. We provide simulated and real examples to illustrate the methodology. Supplemental materials for the manuscript are available online."
"10.1198/jasa.2011.tm10276","2011","Large volatility matrix inference via combining low-frequency and high-frequency approaches","2","It is increasingly important in financial economics to estimate volatilities of asset returns. However, most of the available methods are not directly applicable when the number of assets involved is large, due to the lack of accuracy in estimating high-dimensional matrices. Therefore it is pertinent to reduce the effective size of volatility matrices in order to produce adequate estimates and forecasts. Furthermore, since high-frequency financial data for different assets are typically not recorded at the same time points, conventional dimension-reduction techniques are not directly applicable. To overcome those difficulties we explore a novel approach that combines high-frequency volatility matrix estimation together with low-frequency dynamic models. The proposed methodology consists of three steps: (i) estimate daily realized covolatility matrices directly based on high-frequency data, (ii) fit a matrix factor model to the estimated daily covolatility matrices, and (iii) fit a vector autoregressive model to the estimated volatility factors. We establish the asymptotic theory for the proposed methodology in the framework that allows sample size, number of assets, and number of days go to infinity together. Our theory shows that the relevant eigenvalues and eigenvectors can be consistently estimated. We illustrate the methodology with the high-frequency price data on several hundreds of stocks traded in Shenzhen and Shanghai Stock Exchanges over a period of 177 days in 2003. Our approach pools together the strengths of modeling and estimation at both intra-daily (high-frequency) and inter-daily (low-frequency) levels."
"10.1198/jasa.2011.tm10332","2011","Independent component analysis involving autocorrelated sources with an application to functional magnetic resonance imaging","0","Independent component analysis (ICA) is an effective data-driven method for blind source separation. It has been successfully applied to separate source signals of interest from their mixtures. Most existing ICA procedures are carried out by relying solely on the estimation of the marginal density functions, either parametrically or nonparametrically. In many applications, correlation structures within each source also play an important role besides the marginal distributions. One important example is functional magnetic resonance imaging (fMRI) analysis where the brain-function-related signals are temporally correlated.In this article, we consider a novel approach to ICA that fully exploits the correlation structures within the source signals. Specifically, we propose to estimate the spectral density functions of the source signals instead of their marginal density functions. This is made possible by virtue of the intrinsic relationship between the (unobserved) sources and the (observed) mixed signals. Our methodology is described and implemented using spectral density functions from frequently used time series models such as autoregressive moving average (ARMA) processes. The time series parameters and the mixing matrix are estimated via maximizing the Whittle likelihood function. We illustrate the performance of the proposed method through extensive simulation studies and a real fMRI application. The numerical results indicate that our approach outperforms several popular methods including the most widely used fastICA algorithm. This article has supplementary material online."
"10.1198/jasa.2011.tm10294","2011","Correcting for population stratification in genomewide association studies","0","Genomewide association studies have become the primary tool for discovering the genetic basis of complex human diseases. Such studies are susceptible to the confounding effects of population stratification, in that the combination of allele-frequency heterogeneity with disease-risk heterogeneity among different ancestral subpopulations can induce spurious associations between genetic variants and disease. This article provides a statistically rigorous and computationally feasible solution to this challenging problem of unmeasured confounders. We show that the odds ratio of disease with a genetic variant is identifiable if and only if the genotype is independent of the unknown population substructure conditional on a set of observed ancestry-informative markers in the disease-free population. Under this condition, the odds ratio of interest can be estimated by fitting a semiparametric logistic regression model with an arbitrary function of a propensity score relating the genotype probability to ancestry-informative markers. Approximating the unknown function of the propensity score by B-splines, we derive a consistent and asymptotically normal estimator for the odds ratio of interest with a consistent variance estimator. Simulation studies demonstrate that the proposed inference procedures perform well in realistic settings. An application to the well-known Wellcome Trust Case-Control Study is presented. Supplemental materials are available online."
"10.1198/jasa.2011.tm09737","2011","Posterior probability of discovery and expected rate of discovery for multiple hypothesis testing and high throughput assays","0","Technologies of measuring millions of quantities at once have been rapidly developed and used in biological and biomedical research and other fields in the past decade. Yet a key issue remains unsettled, namely, how to control spurious findings for the ensuing massive number of hypothesis tests. An emerging consensus is to control false discovery rate (FDR), with FDR defined as the expected proportion of true nulls among discoveries, and a discovery the rejection of a null hypothesis. However, the very concept of counting true nulls, implicitly or explicitly, is problematic, for nulls are rarely true in reality. We propose an approach that is philosophically different from the FDR and other approaches. Taking advantage of the massive measurements, we can and should directly evaluate the reproducibility of a discovery by calculating the posterior probability of discovery (PPD) given observed data. A discovery with a great PPD is deemed to be highly reproducible, that is, not spurious. For a subset of hypotheses tested, mean PPD yields the expected rate of discovery (ERD), a measure useful for various applications such as subset enrichment analysis. We present here the rationale, theoretical basis, and an algorithm for computing PPDs and ERDs from data. Their validity, utility, and optimal performance are demonstrated using both simulated and real data. Supplementary material is available online."
"10.1198/jasa.2011.tm09680","2011","Bayesian inference for the spatial random effects model","0","Spatial statistical analysis of massive amounts of spatial data can be challenging because computation of optimal procedures can break down. The Spatial Random Effects (SRE) model uses a fixed number of known but not necessarily orthogonal (multiresolutional) spatial basis functions, which gives a flexible family of nonstationary covariance functions, results in dimension reduction, and yields optimal spatial predictors whose computations are scalable. By modeling spatial data in a hierarchical manner with a process model that includes the SRE model, the choice is whether to estimate the SRE model's parameters or to take a Bayesian approach and put a prior distribution on them. In this article, we develop Bayesian inference for the SRE model when the spatial basis functions are multiresolutional. Then the covariance matrix of the random effects decomposes naturally in terms of Givens angles and eigenvalues, for which a new class of prior distributions is developed. This approach to prior specification of a spatial covariance matrix offers remarkable improvement over other types of priors used in the random-effects literature (e.g., Wishart priors), as demonstrated in a simulation experiment. Further, a large remote-sensing dataset of aerosol optical depth (AOD), from the Multi-angle Imaging SpectroRadiometer (MISR) instrument on the Terra satellite, is analyzed in a fully Bayesian framework, using the new prior, and compared to an empirical-Bayesian analysis."
"10.1198/jasa.2011.tm10301","2011","Variational {B}ayesian inference for parametric and nonparametric regression with missing data","0","Bayesian hierarchical models are attractive structures for conducting regression analyses when the data are subject to missingness. However, the requisite probability calculus is challenging and Monte Carlo methods typically are employed. We develop an alternative approach based on deterministic variational Bayes approximations. Both parametric and nonparametric regression are considered. Attention is restricted to the more challenging case of missing predictor data. We demonstrate that variational Bayes can achieve good accuracy, but with considerably less computational overhead. The main ramification is fast approximate Bayesian inference in parametric and nonparametric regression models with missing data. Supplemental materials accompany the online version of this article."
"10.1198/jasa.2011.tm10226","2011","An asymptotically pivotal transform of the residuals sample autocorrelations with application to model checking","0","We propose an asymptotically distribution-free transform of the sample autocorrelations of residuals in general parametric time series models, possibly nonlinear in variables. The residuals autocorrelation function is the basic model checking tool in time series analysis, but it is not useful when its distribution is incorrectly approximated because the effects of parameter estimation and/or higher-order serial dependence have not been taken into account. The limiting distribution of the residuals sample autocorrelations may be difficult to derive, particularly when the underlying innovations are uncorrelated but not independent. In contrast, our proposal is easily implemented in fairly general contexts and the resulting transformed sample autocorrelations are asymptotically distributed as independent standard normals when innovations are uncorrelated, providing an useful and intuitive device for time series model checking in the presence of estimated parameters. We also discuss in detail alternatives to the classical Box Pierce test, showing that our transform entails no efficiency loss under Gaussianity in the direction of MA and AR departures from the white noise hypothesis. as well as alternatives to Bartlett's T-p-process test. The finite-sample performance of the procedures is examined in the context of a Monte Carlo experiment for the new goodness-of-fit tests discussed in the article. The proposed methodology is applied to modeling the autocovariance structure of the well-known chemical process temperature reading data already used for the illustration of other statistical procedures. Additional technical details are included in a supplemental material online."
"10.1198/jasa.2010.tm10053","2011","Adaptive confidence intervals for the test error in classification","0","The estimated test error of a learned classifier is the most commonly reported measure of classifier performance. However, constructing a high-quality point estimator of the test error has proved to be very difficult. Furthermore, common interval estimators (e.g., confidence intervals) are based on the point estimator of the test error and thus inherit all the difficulties associated with the point estimation problem. As a result, these confidence intervals do not reliably deliver nominal coverage. In contrast, we directly construct the confidence interval by using smooth data-dependent upper and lower bounds on the test error. We prove that, for linear classifiers, the proposed confidence interval automatically adapts to the nonsmoothness of the test error, is consistent under fixed and local alternatives, and does not require that the Bayes classifier be linear. Moreover, the method provides nominal coverage on a suite of test problems using a range of classification algorithms and sample sizes. This article has supplementary material online."
"10.1198/jasa.2011.ap09706","2011","A statistical framework for the analysis of {C}h{IP}-{S}eq data","0","Chromatin immunoprecipitation followed by sequencing (ChIP-Seq) has revolutionalized experiments for genome-wide profiling of DNA-binding proteins, histone modifications, and nucleosome occupancy. As the cost of sequencing is decreasing, many researchers are switching from microarray-based technologies (ChIP-chip) to ChIP-Seq for genome-wide study of transcriptional regulation. Despite its increasing and well-deserved popularity, there is little work that investigates and accounts for sources of biases in the ChIP-Seq technology. These biases typically arise from both the standard preprocessing protocol and the underlying DNA sequence of the generated data.We study data from a naked DNA sequencing experiment, which sequences noncross-linked DNA after deproteinizing and shearing, to understand factors affecting background distribution of data generated in a ChIP-Seq experiment. We introduce a background model that accounts for apparent sources of biases such as mappability and GC content and develop a flexible mixture model named MOSAiCS for detecting peaks in both one- and two-sample analyses of ChIP-Seq data. We illustrate that our model fits observed ChIP-Seq data well and further demonstrate advantages of MOSAiCS over commonly used tools for ChIP-Seq data analysis with several case studies. This article has supplementary material online."
"10.1198/jasa.2011.ap10217","2011","Distinct counting with a self-learning bitmap","0","Counting the number of distinct elements (cardinality) in a dataset is a fundamental problem in database management. In recent years, there has been significant interest to address the distinct counting problem in a data stream setting, where each incoming data can be seen only once and cannot be stored for long periods of time. Many probabilistic approaches based on either sampling or sketching have been proposed in the computer science literature that only require limited computing and memory resources. However, the performances of these methods are not scale invariant, in the sense that their relative root mean square estimation errors (RRMSE) depend on the unknown cardinalities. This is not desirable in many applications where cardinalities can be dynamic or inhomogeneous and many cardinalities need to be estimated. In this article, we develop a novel approach, called self-learning bitmap (S-bitmap) that is scale invariant for cardinalities in a specified range. S-bitmap uses a binary vector whose entries are updated from 0 to 1 by an adaptive sampling process for inferring the unknown cardinality, where the sampling rates are reduced sequentially as more and more entries change from 0 to I. We prove rigorously that the S-bitmap estimate is not only unbiased but scale invariant. We demonstrate that to achieve a small RRMSE value of E or less, our approach requires significantly less memory and uses similar or fewer operations than state-of-the-art methods for many common practice cardinality scales. Both simulation and experimental studies are reported."
"10.1198/jasa.2011.ap10111","2011","Functional principal component analysis of density families with categorical and continuous data on {C}anadian entrant manufacturing firms","0","This article investigates the evolution of firm distributions for entrant manufacturing firms in Canada using functional principal components analysis. This methodology describes the dynamics of firms by examining production variables, size, and labor productivity, and a financial variable, leverage (debt-to-asset ratio). We adapt the canonical functional principal components analysis to allow for the inclusion of qualitative information in the form of discrete variables, industry, and region, to capture market structure differences, which is shown to change the dynamics of firm size and labor productivity distributions only. We also perform various tests with the null hypothesis that the distributions are equal across time. When accounting for industry and regional categories, there is a substantial fall in the number of rejections of the null hypothesis of equality for size and labor productivity, which is not the case for leverage. These results show the importance of including qualitative information to account for potential heterogeneity when applying functional principal component analysis to firm-level data. Finally, the methodology finds a correlation between the evolution of variable distributions and macroeconomic factors. This article has supplementary material online."
"10.1198/jasa.2011.ap10657","2011","Fast and accurate approximation to significance tests in genome-wide association studies","0","Genome-wide association studies commonly involve simultaneous tests of millions of single nucleotide polymorphisms (SNP) for disease association. The SNPs in nearby genomic regions, however, are often highly correlated due to linkage disequilibrium (LD, a genetic term for correlation). Simple Bonferonni correction for multiple comparisons is therefore too conservative. Permutation tests, which are often employed in practice, are both computationally expensive for genome-wide studies and limited in their scopes. We present an accurate and computationally efficient method, based on Poisson de-clumping heuristics, for approximating genome-wide significance of SNP associations. Compared with permutation tests and other multiple comparison adjustment approaches, our method computes the most accurate and robust p-value adjustments for millions of correlated comparisons within seconds. We demonstrate analytically that the accuracy and the efficiency of our method are nearly independent of the sample size, the number of SNPs, and the scale of p-values to be adjusted. In addition, our method can be easily adopted to estimate false discovery rate. When applied to genome-wide SNP datasets, we observed highly variable p-value adjustment results evaluated from different genomic regions. The variation in adjustments along the genome, however, are well conserved between the European and the African populations. The p-value adjustments are significantly correlated with LD among SNPs, recombination rates, and SNP densities. Given the large variability of sequence features in the genome, we further discuss a novel approach of using SNP-specific (local) thresholds to detect genome-wide significant associations. This article has supplementary material online."
"10.1198/jasa.2011.ap08689","2011","Analysis of long period variable stars with nonparametric tests for trend detection","0","In astronomy the study of variable stars that is, stars characterized by showing significant variation in their brightness over time has made crucial contributions to our understanding of many phenomena, from stellar birth and evolution to the calibration of the extragalactic distance scale. In this article, we develop a method for analyzing multiple, (pseudo)-periodic time series with the goal of detecting temporal trends in their periods. We allow for nonstationary noise and for clustering among the various time series. We apply this method to the long-standing astronomical problem of identifying variable stars whose regular brightness fluctuations have periods that change over time. The results of our analysis show that such changes can be substantial, raising the possibility that astronomers' estimates of galactic distances can be refined. Two significant contributions of our approach, relative to existing methods for this problem, are as follows: 1. The method is nonparametric, making minimal assumptions about both the temporal trends themselves but also the covariance structure of the nonstationary noise. 2. Our proposed test has higher power than existing methods. The test is based on inference for a high-dimensional normal mean, with control of the false discovery rate to account for multiplicity. We present theory and simulations to demonstrate the performance of our method. We also analyze data from the American Association of Variable Star Observers and find a monotone relationship between mean period and strength of trend similar to that identified by Hart, Koen, and Lombard (2007)."
"10.1198/jasa.2011.ap09476","2011","Robust {EM} continual reassessment method in oncology dose finding","0","The continual reassessment method (CRM) is a commonly used dose-finding design for phase I clinical trials. Practical applications of this method have been restricted by two limitations: (1) the requirement that the toxicity outcome needs to be observed shortly after the initiation of the treatment; and (2) the potential sensitivity to the prespecified toxicity probability at each dose. To overcome these limitations, we naturally treat the unobserved toxicity outcomes as missing data, and use the expectation-maximization (EM) algorithm to estimate the dose toxicity probabilities based on the incomplete data to direct dose assignment. To enhance the robustness of the design, we propose prespecifying multiple sets of toxicity probabilities, each set corresponding to an individual CRM model. We carry out these multiple CRMs in parallel, across which model selection and model averaging procedures are used to make more robust inference. We evaluate the operating characteristics of the proposed robust EM-CRM designs through simulation studies and show that the proposed methods satisfactorily resolve both limitations of the CRM. Besides improving the MTD selection percentage, the new designs dramatically shorten the duration of the trial, and are robust to the prespecification of the toxicity probabilities."
"10.1198/jasa.2011.ap10058","2011","Nonparametric {B}ayes stochastically ordered latent class models","0","Latent class models (LCMs) are used increasingly for addressing a broad variety of problems, including sparse modeling of multivariate and longitudinal data, model-based clustering, and flexible inferences on predictor effects. Typical frequentist LCMs require estimation of a single finite number of classes, which does not increase with the sample size, and have a well-known sensitivity to parametric assumptions on the distributions within a class. Bayesian nonparametric methods have been developed to allow an infinite number of classes in the general population, with the number represented in a sample increasing with sample size. In this article, we propose a new nonparametric Bayes model that allows predictors to flexibly impact the allocation to latent classes, while limiting sensitivity to parametric assumptions by allowing class-specific distributions to be unknown subject to a stochastic ordering constraint. An efficient MCMC algorithm is developed for posterior computation. The methods are validated using simulation studies and applied to the problem of ranking medical procedures in terms of the distribution of patient morbidity."
"10.1198/jasa.2011.ap10089","2011","Population value decomposition, a framework for the analysis of image populations","0","Images, often stored in multidimensional arrays, are fast becoming ubiquitous in medical and public health research. Analyzing populations of images is a statistical problem that raises a host of daunting challenges. The most significant challenge is the massive size of the datasets incorporating images recorded for hundreds or thousands of subjects at multiple visits. We introduce the population value decomposition (PVD), a general method for simultaneous dimensionality reduction of large populations of massive images. We show how PVD can be seamlessly incorporated into statistical modeling, leading to a new, transparent, and rapid inferential framework. Our PVD methodology was motivated by and applied to the Sleep Heart Health Study, the largest community-based cohort study of sleep containing more than 85 billion observations on thousands of subjects at two visits. This article has supplementary material online."
"10.1198/jasa.2011.ap09508","2011","Global warming and local dimming: the statistical evidence","0","Two effects largely determine global warming: the well-known greenhouse effect and the less well-known solar radiation effect. An increase in concentrations of carbon dioxide and other greenhouse gases contributes to global warming: the greenhouse effect. In addition, small particles, called aerosols, reflect and absorb sunlight in the atmosphere. More pollution causes an increase in aerosols, so that less sunlight reaches the Earth (global dimming). Despite its name, global dimming is primarily a local (or regional) effect. Because of the dimming the Earth becomes cooler: the solar radiation effect. Global warming thus consists of two components: the (global) greenhouse effect and the (local) solar radiation effect, which work in opposite directions. Only the sum of the greenhouse effect and the solar radiation effect is observed, not the two effects separately. Our purpose is to identify the two effects. This is important, because the existence of the solar radiation effect obscures the magnitude of the greenhouse effect. We propose a simple climate model with a small number of parameters. We gather data from a large number of weather stations around the world for the period 1959-2002. We then estimate the parameters using dynamic panel data methods, and quantify the parameter uncertainty. Next, we decompose the estimated temperature change of 0.73 degrees C (averaged over the weather stations) into a greenhouse effect of 1.87 degrees C, a solar radiation effect of -1.09 degrees C, and a small remainder term. Finally, we subject our findings to extensive sensitivity analyses."
"10.1198/jasa.2011.ap09094","2011","Modeling partial compliance through copulas in a principal stratification framework","1","Within the principal stratification framework for causal inference, modeling partial compliance is challenging because the continuous nature of the principal strata raises subtle specification issues. In this context, we propose an approach based on the assumption that the joint distribution of the degree of compliance to the treatment and the degree of compliance to the control follows a Plackett copula, so that their association is modeled in a flexible way through a single parameter. Moreover, given the two compliances, the distribution of the outcomes is parameterized in a flexible way through a regression model which may include interaction and quadratic terms and may also be heteroscedastic. In order to estimate the parameters of the resulting model, and then the causal effect of the treatment, we adopt a maximum likelihood approach via the EM algorithm. In applying this approach, the marginal distributions of the two compliances are estimated by their empirical distribution functions, so that no constraints are posed on these distributions. Since the two compliances cannot be jointly observed, there is not direct empirical support for the association parameter. We describe a strategy for studying this parameter by a profile likelihood method and discuss an analysis of the sensitivity of the causal inference results to its value. We apply the proposed approach to data from a study about a drug for lowering cholesterol levels previously analyzed by Efron and Feldman and by Jin and Rubin. Estimated causal effects are in line with those of previous analyses, but the pattern of association between the compliances is qualitatively different, apparently due to the flexibility of the copula and to allowing regression equations in the proposed method to include interactions and heteroscedasticity."
"10.1198/jasa.2011.ap10323","2011","Malaria in {N}orthwest {I}ndia: data analysis via partially observed stochastic differential equation models driven by {L}\'evy noise","0","Many biological systems are appropriately described by partially observed Markov process (POMP) models, also known as state space models. Such models also arise throughout the physical and social sciences, in engineering, and in finance. Statistical challenges arise in carrying out inference on nonlinear, nonstationary, vector-valued POMP models. Methodologies that depend on the Markov process model only through numerical solution of sample paths are said to have the plug-and-play property. This property enables consideration of models for which the evaluation of transition densities is problematic. Our case study employs plug-and-play methodology to investigate malaria transmission in Northwest India. We address the scientific question of the respective roles of environmental factors, immunity, and nonlinear disease transmission dynamics in epidemic malaria. Previous debates on this question have been hindered by the lack of a statistical investigation that gives simultaneous consideration to the roles of human immunity and the fluctations in mosquito abundance associated with environmental or ecological covariates. We present the first time series analysis integrating these various components into a single vector-valued dynamic model. We are led to investigate a POMP involving a system of stochastic differential equations driven by Levy noise. We find a clear role for rainfall and evidence to support models featuring the possibility of clinical immunity.An online supplement presents details of the methodology implemented and two additional figures."
"10.1198/jasa.2011.ap09732","2011","Predictive macro-finance with dynamic partition models","0","Dynamic partition models are used to predict movements in the term structure of interest rates. This allows one to understand historic cycles in the performance of how interest rates behave, and to offer policy makers guidance regarding future expectations on their evolution. Our approach allows for a random number of possible change points in the term structure of interest rates. We use particle learning to learn about the unobserved state variables in a new class of dynamic product partition models that relate macro-variables to term structures. The empirical results, using data from 1970 to 2000, clearly identifies some of the key shocks to the economy, such as recessions. We construct a time series of Bayes factors that, surprisingly, could serve as a leading indicator of economic activity, validated via a Granger causality test. Finally, the in-sample and out-of-sample forecasts from our model are quite robust regardless of the time to maturity of interest rates."
"10.1198/jasa.2011.ap10108","2011","Self-controlled case series analysis with event-dependent observation periods","0","The self-controlled case series method may be used to study the association between a time-varying exposure and a health event. It is based only on cases, and it controls for fixed confounders. Exposure and event histories are collected for each case over a predefined observation period. The method requires that observation periods should be independent of event times. This requirement is violated when events increase the mortality rate, since censoring of the observation periods is then event dependent. In this article, the case series method for rare nonrecurrent events is extended to remove this independence assumption, thus introducing an additional term in the likelihood that depends on the censoring process. In order to remain within the case series framework in which only cases are sampled, the model is reparameterized so that this additional term becomes estimable from the distribution of intervals from event to end of observation. The exposure effect of primary interest may be estimated unbiasedly. The age effect, however, takes on a new interpretation, incorporating the effect of censoring. The model may be fitted in standard loglinear modeling software; this yields conservative standard errors. We describe a detailed application to the study of antipsychotics and stroke. The estimates obtained from the standard case series model are shown to be biased when event-dependent observation periods are ignored. When they are allowed for, antipsychotic use remains strongly positively associated with stroke in patients with dementia, but not in patients without dementia. Two detailed simulation studies are included as Supplemental Material."
"10.1198/jasa.2011.ap10415","2011","Multivariate regression analysis for the item count technique","0","The item count technique is a survey methodology that is designed to elicit respondents' truthful answers to sensitive questions such as racial prejudice and drug use. The method is also known as the list experiment or the unmatched count technique and is an alternative to the commonly used randomized response method. In this article, I propose new nonlinear least squares and maximum likelihood estimators for efficient multivariate regression analysis with the item count technique. The two-step estimation procedure and the Expectation Maximization algorithm are developed to facilitate the computation. Enabling multivariate regression analysis is essential because researchers are typically interested in knowing how the probability of answering the sensitive question affirmatively varies as a function of respondents' characteristics. As an empirical illustration, the proposed methodology is applied to the 1991 National Race and Politics survey where the investigators used the item count technique to measure the degree of racial hatred in the United States. Small-scale simulation studies suggest that the maximum likelihood estimator can be substantially more efficient than alternative estimators. Statistical efficiency is an important concern for the item count technique because indirect questioning means loss of information. The open-source software is made available to implement the proposed methodology."
"10.1198/jasa.2011.ap09392","2011","An approach to the estimation of chronic air pollution effects using spatio-temporal information","0","There is substantial observational evidence that long-term exposure to particulate air pollution is associated with premature death in urban populations. Estimates of the magnitude of these effects derive largely from cross-sectional comparisons of adjusted mortality rates among cities with varying pollution levels. Such estimates are potentially confounded by other differences among the populations correlated with air pollution, for example, socioeconomic factors. An alternative approach is to study covariation of particulate matter and mortality across time within a city, as has been done in investigations of short-term exposures. In either event, observational studies like these are subject to confounding by unmeasured variables. Therefore the ability to detect such confounding and to derive estimates less affected by confounding are a high priority.In this article, we describe and apply a method of decomposing the exposure variable into components with variation at distinct temporal, spatial, and time by space scales, here focusing on the components involving time. Starting from a proportional hazard model, we derive a Poisson regression model and estimate two regression coefficients: the ""global"" coefficient that measures the association between national trends in pollution and mortality; and the ""local"" coefficient, derived from space by time variation, that measures the association between location-specific trends in pollution and mortality adjusted by the national trends. Absent unmeasured confounders and given valid model assumptions, the scale-specific coefficients should be similar; substantial differences in these coefficients constitute a basis for questioning the model.We derive a backfitting algorithm to fit our model to very large spatio-temporal datasets. We apply our methods to the Medicare Cohort Air Pollution Study (MCAPS), which includes individual-level information on time of death and age on a population of 18.2 million for the period 2000-2006.Results based on the global coefficient indicate a large increase in the national life expectancy for reductions in the yearly national average of PM2.5. However, this coefficient based on national trends in PM2.5 and mortality is likely to be confounded by other variables trending on the national level. Confounding of the local coefficient by unmeasured factors is less likely, although it cannot be ruled out. Based on the local coefficient alone, we are not able to demonstrate any change in life expectancy for a reduction in PM2.5. We use additional survey data available for a subset of the data to investigate sensitivity of results to the inclusion of additional covariates, but both coefficients remain largely unchanged."
"10.1198/jasa.2011.ap09764","2011","Estimating the term structure with a semiparametric {B}ayesian hierarchical model: an application to corporate bonds","0","The term structure of interest rates is used to price defaultable bonds and credit derivatives, as well as to infer the quality of bonds for risk management purposes. We introduce a model that jointly estimates term structures by means of a Bayesian hierarchical model with a prior probability model based on Dirichlet process mixtures. The modeling methodology borrows strength across term structures for purposes of estimation. The main advantage of our framework is its ability to produce reliable estimators at the company level even when there are only a few bonds per company. After describing the proposed model, we discuss an empirical application in which the term structure of 197 individual companies is estimated. The sample of 197 consists of 143 companies with only one or two bonds. In-sample and out-of-sample tests are used to quantify the improvement in accuracy that results from approximating the term structure of corporate bonds with estimators by company rather than by credit rating, the latter being a popular choice in the financial literature. A complete description of a Markov chain Monte Carlo (MCMC) scheme for the proposed model is available as Supplementary Material."
"10.1198/jasa.2011.r10138","2011","Making and evaluating point forecasts","1","Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance."
"10.1198/jasa.2011.tm10221","2011","Best predictive small area estimation","0","We derive the best predictive estimator (BPE) of the fixed parameters under two well-known small area models, the Fay-Herriot model and the nested-error regression model. This leads to a new prediction procedure, called observed best prediction (OBP), which is different from the empirical best linear unbiased prediction (EBLUP). We show that BPE is more reasonable than the traditional estimators derived from estimation considerations, such as maximum likelihood (ML) and restricted maximum likelihood (REML), if the main interest is estimation of small area means, which is a mixed-model prediction problem. We use both theoretical derivations and empirical studies to demonstrate that the OBP can significantly outperform EBLUP in terms of the mean squared prediction error (MSPE), if the underlying model is misspecified. On the other hand, when the underlying model is correctly specified, the overall predictive performance of the OBP is very similar to that of the EBLUP if the number of small areas is large. A general theory about OBP, including its exact MSPE comparison with the BLUP in the context of mixed-model prediction, and asymptotic behavior of the BPE, is developed. A real data example is considered. A supplementary appendix is available online."
"10.1198/jasa.2011.tm09654","2011","An outlier-robust fit for generalized additive models with applications to disease outbreak detection","0","We are interested in a class of unsupervised methods to detect possible disease outbreaks, that is, rapid increases in the number of cases of a particular disease that deviate from the pattern observed in the past. The motivating application for this article deals with detecting outbreaks using generalized additive models (GAMs) to model weekly counts of certain infectious diseases. We can use the distance between the predicted and observed counts for a specific week to determine whether an important departure has occurred. Unfortunately, this approach may not work as desired because GAMs can be very sensitive to the presence of a small proportion of observations that deviate from the assumed model. Thus, the outbreak may affect the predicted values causing these to be close to the atypical counts, and thus mask the outliers by having them appear not to be too extreme or atypical. We illustrate this phenomenon with influenza-like-illness doctor-visits data from the United States for the 2006-2008 flu seasons. One way to avoid this masking problem is to derive an algorithm to fit GAM models that can resist the effect of a small number of atypical observations. In this article we discuss such an outlier-robust fit for GAMs based on the backfitting algorithm. The basic idea is to replace the maximum likelihood based weights used in the generalized local scoring algorithm with those derived from robust quasi-likelihood equations (Cantoni and Ronchetti 2001b). These robust estimators for generalized linear models work well for the Poisson family of distributions, and also for binomial distributions with relatively large numbers of trials. We show that the resulting estimated mean function is resistant to the presence of outliers in the response variable and that it also remains close to the usual GAM estimator when the data do not contain atypical observations. We illustrate the use of this approach on the detection of the recent outbreak of H1N1 flu by looking at the weekly counts of influenza-like-illness (ILI) doctor visits, as reported through the U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet), and also apply our method to the numbers of requested isolates in Canada. Weeks with a sudden increase in ILI visits or requested isolates are much more clearly identified as atypical by the robust fit because the observed counts are far from the ones predicted by the fitted GAM model."
"10.1198/jasa.2011.tm08687","2011","Do-validation for kernel density estimation","0","Bandwidth selection in kernel density estimation is one of the fundamental model selection problems of mathematical statistics. The study of this problem took major steps forward with the articles of Hall and Marron (1987) and Hall and Johnstone (1992). Since then, the focus seems to have been on various versions of implementing the so-called plug-in method aimed at estimating the minimum mean integrated squared error (MISE). The most successful of these efforts still seems to be the plug-in method of Sheather and Jones (1991) or Park and Marron (1990) that we also use as a benchmark in this article. In this article we derive a new theorem deriving the asymptotic theory for linear combinations of bandwidths obtained from different selectors as, for example, direct and indirect cross-validation and plug-in, where we take advantage of recent advances in the study of indirect cross-validation; see Hart and Yi (1998), Hart and Lee (2005), and Savchuk, Hart, and Sheather (2008, 2010). We conclude that the slow convergence of data-driven bandwidths implies that once asymptotic theory is close to that of the plug-in, then it is the practical implementation that counts. This insight led us to a bandwidth selector search with the symmetrized version of one-sided cross-validation as a clear winner."
"10.1198/jasa.2011.tm10520","2011","Nonparametric regression analysis for group testing data","0","Group testing is a procedure used to reduce the cost and increase the speed of large screening studies in which infection or contamination of individuals is detected by a test carried out on a sample of, for example, blood, urine, or water. Instead of testing the sample of each individual, the method involves pooling samples of groups of several individuals and testing those pooled samples. We construct a nonparametric procedure for estimating the conditional probability of contamination given an explanatory variable when the observations are pooled data of this type. We investigate asymptotic theoretical properties of the estimator and establish its consistency. The procedure requires selecting an important smoothing parameter, and we suggest a way to do this automatically from the data. We illustrate the numerical performance of the method on some simulated examples and on data from the National Health and Nutrition Examination Survey. We discuss extensions of the procedure to cases where the test is imprecise and the covariates are observed inaccurately, and to the multivariate setting. Supplemental materials including proofs, R codes, and additional simulation results are available from the online JASA website."
"10.1198/jasa.2011.tm10390","2011","Outlier detection using nonconvex penalized regression","0","This article studies the outlier detection problem from the standpoint of penalized regression. In the regression model, we add one mean shift parameter for each of the n data points. We then apply a regularization favoring a sparse vector of mean shift parameters. The usual L-1 penalty yields a convex criterion, but fails to deliver a robust estimator. The L-1 penalty corresponds to soft thresholding. We introduce a thresholding (denoted by Theta) based iterative procedure for outlier detection (Theta-IPOD). A version based on hard thresholding correctly identifies outliers on some hard test problems. We describe the connection between Theta-IPOD and M-estimators. Our proposed method has one tuning parameter with which to both identify outliers and estimate regression coefficients. A data-dependent choice can be made based on the Bayes information criterion. The tuned Theta-IPOD shows outstanding performance in identifying outliers in various situations compared with other existing approaches. In addition, Theta-IPOD is much faster than iteratively reweighted least squares for large data, because each iteration costs at most O(np) (and sometimes much less), avoiding an O(np(2)) least squares estimate. This methodology can be extended to high-dimensional modeling with p >> n if both the coefficient vector and the outlier pattern are sparse."
"10.1198/jasa.2011.tm10159","2011","Bootstrapping lasso estimators","1","In this article, we consider bootstrapping the Lasso estimator of the regression parameter in a multiple linear regression model. It is known that the standard bootstrap method fails to be consistent. Here, we propose a modified bootstrap method, and show that it provides valid approximation to the distribution of the Lasso estimator, for all possible values of the unknown regression parameter vector, including the case where some of the components are zero. Further, we establish consistency of the modified bootstrap method for estimating the asymptotic bias and variance of the Lasso estimator. We also show that the residual bootstrap can be used to consistently estimate the distribution and variance of the adaptive Lasso estimator. Using the former result, we formulate a novel data-based method for choosing the optimal penalizing parameter for the Lasso using the modified bootstrap. A numerical study is performed to investigate the finite sample performance of the modified bootstrap. The methodology proposed in the article is illustrated with a real data example."
"10.1198/jasa.2011.tm10155","2011","A constrained {$\ell_1$} minimization approach to sparse precision matrix estimation","1","This article proposes a constrained l(1) minimization method for estimating a sparse inverse covariance matrix based on a sample of n iid p-variate random variables. The resulting estimator is shown to have a number of desirable properties. In particular, the rate of convergence between the estimator and the true s-sparse precision matrix under the spectral norm is s root logp/n when the population distribution has either exponential-type tails or polynomial-type tails. We present convergence rates under the elementwise l(infinity) norm and Frobenius norm. In addition, we consider graphical model selection. The procedure is easily implemented by linear programming. Numerical performance of the estimator is investigated using both simulated and real data. In particular, the procedure is applied to analyze a breast cancer dataset and is found to perform favorably compared with existing methods."
"10.1198/jasa.2011.tm10356","2011","Randomization-based inference within principal strata","0","In randomized studies, treatment comparisons conditional on intermediate postrandomization outcomes using standard analytic methods do not have a causal interpretation. An alternative approach entails treatment comparisons within principal strata defined by the potential outcomes for the intermediate outcome that would be observed under each treatment assignment. In this article we develop methods for randomization-based inference within principal strata. We compare our proposed methods with existing large-sample methods as well as traditional intent-to-treat approaches. This research is motivated by HIV prevention studies, where few infections are expected and inference is desired within the always-infected principal stratum, that is, all individuals who would become infected regardless of randomization assignment."
"10.1198/jasa.2011.tm09807","2011","Nonparametric evaluation of biomarker accuracy under nested case-control studies","0","To evaluate the clinical utility of new risk markers, a crucial step is to measure their predictive accuracy with prospective studies. However, it is often infeasible to obtain marker values for all study participants. The nested case-control (NCC) design is a useful cost-effective strategy for such settings. Under the NCC design, markers are only ascertained for cases and a fraction of controls sampled randomly from the risk sets. The outcome dependent sampling generates a complex data structure and therefore a challenge for analysis. Existing methods for analyzing NCC studies focus primarily on association measures. Here, we propose a class of nonparametric estimators for commonly used accuracy measures. We derived asymptotic expansions for accuracy estimators based on both finite population and Bernoulli sampling and established asymptotic equivalence between the two. Simulation results suggest that the proposed procedures perform well in finite samples. The new procedures were illustrated with data from the Framingham Offspring study."
"10.1198/jasa.2011.tm10021","2011","Estimating the jump activity index under noisy observations using high-frequency data","0","It is widely accepted that the high-frequency data are contaminated by microstructure noise, whose effect on the statistical inference has been of increasing interest in the literature. Much of it, however, has focused on the integrated volatility. In this article, we investigate another important characteristic, namely, the jump activity index (JAI) of a discretely sampled semi-martingale corrupted by microstructure noise. We point out that ignoring the microstructure noise can have a disastrous effect on the estimation of the JAI. Consequently, we propose a two-stage procedure to estimate the JAI. It first reduces the effect of noise by local smoothing and then estimates the index from the smoothed data. The asymptotic properties such as consistency and asymptotic normality are given. Simulations are conducted to evaluate the performance of the procedure. Finally, we implement our estimators to some real datasets."
"10.1198/jasa.2011.tm09779","2011","Nonparametric independence screening in sparse ultra-high-dimensional additive models","1","A variable screening procedure via correlation learning was proposed by Fan and Lv (2008) to reduce dimensionality in sparse ultra-high-dimensional models. Even when the true model is linear, the marginal regression can be highly nonlinear. To address this issue, we further extend the correlation learning to marginal nonparametric learning. Our nonparametric independence screening (NIS) is a specific type of sure independence screening. We propose several closely related variable screening procedures. We show that with general nonparametric models, under some mild technical conditions, the proposed independence screening methods have a sure screening property. The extent to which the dimensionality can be reduced by independence screening is also explicitly quantified. As a methodological extension, we also propose a data-driven thresholding and an iterative nonparametric independence screening (INIS) method to enhance the finite- sample performance for fitting sparse additive models. The simulation results and a real data analysis demonstrate that the proposed procedure works well with moderate sample size and large dimension and performs better than competing methods."
"10.1198/jasa.2011.tm09767","2011","A direct bootstrap method for complex sampling designs from a finite population","0","In complex designs, classical bootstrap methods result in a biased variance estimator when the sampling design is not taken into account. Resampled units are usually rescaled or weighted in order to achieve unbiasedness in the linear case. In the present article, we propose novel resampling methods that may be directly applied to variance estimation. These methods consist of selecting subsamples under a completely different sampling scheme from that which generated the original sample, which is composed of several sampling designs. In particular, a portion of the subsampled units is selected without replacement, while another is selected with replacement, thereby adjusting for the finite population setting. We show that these bootstrap estimators directly and precisely reproduce unbiased estimators of the variance in the linear case in a time-efficient manner, and eliminate the need for classical adjustment methods such as rescaling, correction factors, or artificial populations. Moreover, we show via simulation studies that our method is at least as efficient as those currently existing, which call for additional adjustment. This methodology can be applied to classical sampling designs, including simple random sampling with and without replacement, Poisson sampling, and unequal probability sampling with and without replacement."
"10.1198/jasa.2011.tm08601","2011","Inference for quantitation parameters in polymerase chain reactions via branching processes with random effects","0","The quantitative polymerase chain reaction (qPCR) is a widely used tool for gene quantitation and has been applied extensively in several scientific areas. The current methods used for analyzing qPCR data fail to account for multiple sources of variability present in the PCR dynamics, leading to biased estimates and incorrect inference. In this article, we introduce a branching process model with random effects to account for within-reaction and between-reaction variability in PCR experiments. We describe, in terms of the observed fluorescence data, new statistical methodology for gene quantitation. Using simulations, PCR experiments, and asymptotic theory we demonstrate the improvements achieved by our methodology compared to existing methods. This article has supplemental materials online."
"10.1198/jasa.2011.ap10604","2011","Using split samples and evidence factors in an observational study of neonatal outcomes","0","During a few years around the turn of the millennium, a series of local hospitals in Philadelphia closed their obstetrics units, with the consequence that many mothers-to-be arrived unexpectedly at the city's large, regional teaching hospitals whose obstetrics units remained open. Nothing comparable happened in other United States cities, where there were only sporadic changes in the availability of obstetrics units. What effect did these closures have on mothers and their newborns? We study this question by comparing Philadelphia before and after the closures to a control Philadelphia constructed from elsewhere in Pennsylvania, California, and Missouri, matching mothers for 59 observed covariates including year of birth. The analysis focuses on the period 1995-1996, when there were no closures, and the period 1997-1999 when five hospitals abruptly closed their obstetrics units. Using a new sensitivity analysis for difference-in-differences with binary outcomes, we examine the possibility that Philadelphia mothers differed from control mothers in terms of some covariate not measured, and perhaps the distribution of that unobserved covariate changed in a different way in Philadelphia and control-Philadelphia in the years before and after the closures. We illustrate two recently proposed techniques for the design and analysis of observational studies, namely split samples and evidence factors. To boost insensitivity to unmeasured bias, we drew a small random planning sample of about 26,000 mothers in 13,000 pairs and used them to frame hypotheses that promised to be less sensitive to bias; then these hypotheses were tested on the large, independent complementary analysis sample of nearly 240,000 mothers in 120,000 pairs. The splitting was successful twice over: (i) it successfully identified an interesting and moderately insensitive conclusion, (ii) by comparison of the planning and analysis samples, it is clearly seen to have avoided a exaggerated claim of insensitivity to unmeasured bias that might have occurred by focusing on the least sensitive of many findings. Also, we identified two approximate evidence factors and one test for unmeasured bias: (i) factor 1 compared Philadelphia to control before and after the closures, (ii) factor 2 focused on the years 1997-1999 of abrupt closures and compared zip codes with closures to zip codes without closures, (iii) and the test for bias focused on the years 1995-1996 prior to closures and compared zip codes which would have closures in 1997-1999 to zip codes without closures in 1997-1999-any ostensible effect found in that last comparison is surely bias from the characteristics of Philadelphia zip codes in which closures took place. Approximate evidence factors provide nearly independent tests of a null hypothesis such that the evidence in each factor would be unaffected by certain biases that would invalidate the other factor."
"10.1198/jasa.2011.ap09272","2011","Identifying risk factors for severe childhood malnutrition by boosting additive quantile regression","0","We investigated the risk factors for childhood malnutrition in India based on the 2005/2006 Demographic and Health Survey by applying a novel estimation technique for additive quantile regression. Ordinary linear and generalized linear regression models relate the mean of a response variable to a linear combination of covariate effects, and, as a consequence, focus on average properties of the response. The use of such a regression model for analyzing childhood malnutrition in developing or transition countries implies that the estimated effects describe the average nutritional status. However, it is of even greater interest to analyze quantiles of the response distribution, such as the 5% or 10% quantile, which relate to the risk of extreme malnutrition. Our investigation is based on a semiparametric extension of quantile regression models where different types of nonlinear effects are included in the model equation, leading to additive quantile regression. We addressed the variable selection and model choice problems associated with estimating such an additive quantile regression model using a novel boosting approach. Our proposal allows for data-driven determination of the amount of smoothness required for the nonlinear effects and combines model choice with an automatic variable selection property. In an empirical evaluation, we compared our boosting approach with state-of-the-art methods for additive quantile regression. The results suggest that boosting is an appropriate tool for estimation and variable selection in additive quantile regression models and helps to identify yet unknown risk factors for childhood malnutrition. This article has supplementary material online."
"10.1198/jasa.2011.ap10291","2011","Cocaine dependence treatment data: methods for measurement error problems with predictors derived from stationary stochastic processes","0","In a cocaine dependence treatment study, we use linear and nonlinear regression models to model posttreatment cocaine craving scores and first cocaine relapse time. A subset of the covariates are summary statistics derived from baseline daily cocaine use trajectories, such as baseline cocaine use frequency and average daily use amount. These summary statistics are subject to estimation error and can therefore cause biased estimators for the regression coefficients. Unlike classical measurement error problems, the error we encounter here is heteroscedastic with an unknown distribution, and there are no replicates for the error-prone variables or instrumental variables. We propose two robust methods to correct for the bias: a computationally efficient method-of-moments-based method for linear regression models and a subsampling extrapolation method that is generally applicable to both linear and nonlinear regression models. Simulations and an application to the cocaine dependence treatment data are used to illustrate the efficacy of the proposed methods. Asymptotic theory and variance estimation for the proposed subsampling extrapolation method and some additional simulation results are described in the online supplementary material."
"10.1198/jasa.2011.tm09748","2011","Robust and efficient one-way {MANOVA} tests","0","We propose robust tests as alternatives to the classical Wilks' Lambda test in one-way MANOVA. The robust tests use highly robust and efficient multisample multivariate S-estimators or MM-estimators instead of the empirical covariances. The properties of several robust test statistics are compared. Under the null hypothesis, the distribution of the test statistics is proportional to a chi-square distribution. As an alternative to the asymptotic distribution, we develop a fast robust bootstrap method to estimate the distribution under the null hypothesis. We show when it is asymptotically correct to estimate the null distribution in this way and we use simulations to verify the performance of the bootstrap based tests in finite samples. We also investigate the power of the new tests, as well as their robustness against outliers. Finally, we illustrate the use of these robust test statistics on a real data example. Some additional results are provided as supplemental material."
"10.1198/jasa.2011.tm10107","2011","The degrees of freedom of partial least squares regression","0","The derivation of statistical properties for partial least squares regression can be a challenging task. The reason is that the construction of latent components from the predictor variables also depends on the response variable. While this typically leads to good performance and interpretable models in practice, it makes the statistical analysis more involved. In this work, we study the intrinsic complexity of partial least squares regression. Our contribution is an unbiased estimate of its degrees of freedom. It is defined as the trace of the first derivative of the fitted values, seen as a function of the response. We establish two equivalent representations that rely on the close connection of partial least squares to matrix decompositions and Krylov subspace techniques. We show that the degrees of freedom depend on the collinearity of the predictor variables: The lower the collinearity, the higher the complexity. In particular, they are typically higher than the naive approach that defines the degrees of freedom as the number of components. Further, we illustrate how our degrees of freedom estimate can be used for the comparison of different regression methods. In the experimental section, we show that our degrees of freedom estimate in combination with information criteria is useful for model selection."
"10.1198/jasa.2011.tm10592","2011","A semiparametric threshold model for censored longitudinal data analysis","0","Motivated by an investigation of the relationship between blood pressure change and progression of microalbuminuria (MA) among individuals with type I diabetes, we propose a new semiparametric threshold model for censored longitudinal data analysis. We also study a new semiparametric Bayes information criterion-type criterion for identifying the parametric component of the proposed model. Cluster effects in the model are implemented as unknown fixed effects. Asymptotic properties are established for the proposed estimators. A quadratic approximation used to implement the estimation procedure makes the method very easy to implement by avoiding the computation of multiple integrals and the need for iterative algorithms. Simulation studies show that the proposed methods work well in practice. An illustration using the Wisconsin Diabetes Registry dataset suggests some interesting findings."
"10.1198/jasa.2011.tm10560","2011","Adaptive thresholding for sparse covariance matrix estimation","2","In this article we consider estimation of sparse covariance matrices and propose a thresholding procedure that is adaptive to the variability of individual entries. The estimators are fully data-driven and demonstrate excellent performance both theoretically and numerically. It is shown that the estimators adaptively achieve the optimal rate of convergence over a large class of sparse covariance matrices under the spectral norm. In contrast, the commonly used universal thresholding estimators are shown to be suboptimal over the same parameter spaces. Support recovery is discussed as well. The adaptive thresholding estimators are easy to implement. The numerical performance of the estimators is studied using both simulated and real data. Simulation results demonstrate that the adaptive thresholding estimators uniformly outperform the universal thresholding estimators. The method is also illustrated in an analysis on a dataset from a small round blue-cell tumor microarray experiment. A supplement to this article presenting additional technical proofs is available online."
"10.1198/jasa.2011.tm09774","2011","Projection estimators for generalized linear models","0","We introduce a new class of robust estimators for generalized linear models which is an extension of the class of projection estimators for linear regression. These projection estimators are defined using an initial robust estimator for a generalized linear model with only one unknown parameter. We found a bound for the maximum asymptotic bias of the projection estimator caused by a fraction epsilon of outlier contamination. For small epsilon, this bias is approximately twice the maximum bias of the initial estimator independently of the number of regressors. Since these projection estimators are not asymptotically normal, we define one-step weighted M-estimators starting at the projection estimators. These estimators have the same asymptotic normal distribution as the M-estimator and a degree of robustness close to the one of the projection estimator. We perform a Monte Carlo simulation for the case of binomial and Poisson regression with canonical links. This study shows that the proposed estimators compare favorably with respect to other robust estimators. Supplemental Material containing the proofs and the numerical algorithm used to compute the P-estimator is available online."
"10.1198/jasa.2011.ap10508","2011","Statistics: a key to innovation in a data-centric world!","0",""
"10.1198/jasa.2011.tm10036","2011","Model selection by testing for the presence of small-area effects, and application to area-level data","0","The models used in small-area inference often involve unobservable random effects. While this can significantly improve the adaptivity and flexibility of a model, it also increases the variability of both point and interval estimators. If we could test for the existence of the random effects, and if the test were to show that they were unlikely to be present, then we would arguably not need to incorporate them into the model, and thus could significantly improve the precision of the methodology. In this article we suggest an approach of this type. We develop simple bootstrap methods for testing for the presence of random effects, applicable well beyond the conventional context of the natural exponential family. If the null hypothesis that the effects are not present is not rejected then our general methodology immediately gives us access to estimators of unknown model parameters and estimators of small-area means. Such estimators can be substantially more effective, for example, because they enjoy much faster convergence rates than their counterparts when the model includes random effects. If the null hypothesis is rejected then the next step is either to make the model more elaborate (our methodology is available quite generally) or to turn to existing random effects models. This article has supplementary material online."
"10.1198/jasa.2011.tm09599","2011","Multivariate matching methods that are monotonic imbalance bounding","0","We introduce a new ""Monotonic Imbalance Bounding"" (MIB) class of matching methods for causal inference with a surprisingly large number of attractive statistical properties. MIB generalizes and extends in several new directions the only existing class, ""Equal Percent Bias Reducing"" (EPBR), which is designed to satisfy weaker properties and only in expectation. We also offer strategies to obtain specific members of the MIB class, and analyze in more detail a member of this class, called Coarsened Exact Matching, whose properties we analyze from this new perspective. We offer a variety of analytical results and numerical simulations that demonstrate how members of the MIB class can dramatically improve inferences relative to EPBR-based matching methods."
"10.1198/jasa.2011.tm09784","2011","Small sample {$LD50$} confidence intervals using saddlepoint approximations","0",""
"10.1198/jasa.2011.tm09803","2011","Confidence distributions and a unifying framework for meta-analysis","0","This article develops a unifying framework, as well as robust meta-analysis approaches, for combining studies from independent sources. The device used in this combination is a confidence distribution (CD), which uses a distribution function, instead of a point (point estimator) or an interval (confidence interval), to estimate a parameter of interest. A CD function contains a wealth of information for inferences, and it is a useful device for combining studies from different sources. The proposed combining framework not only unifies most existing meta-analysis approaches, but also leads to development of new approaches. We illustrate in this article that this combining framework can include both the classical methods of combining p-values and modern model-based meta-analysis approaches. We also develop, under the unifying framework, two new robust meta-analysis approaches, with supporting asymptotic theory. In one approach each study size goes to infinity. and in the other approach the number of studies goes to infinity. Our theoretical development suggests that both these robust meta-analysis approaches have high breakdown points and are highly efficient for normal models. The new methodologies are applied to study-level data from publications on prophylactic use of lidocaine in heart attacks and a treatment of stomach ulcers. The robust methods performed well when data are contaminated and have realistic sample sizes and number of studies."
"10.1198/jasa.2011.tm09506","2011","Nonparametric regression with predictors missing at random","0","Nonparametric regression with predictors missing at random (MAR), where the probability of missing depends only on observed variables, is considered. Univariate predictor is the primary case of interest. A new adaptive orthogonal series estimator is developed. Large sample theory shows that the estimator is rate-minimax and it is also sharp-minimax whenever predictors are missing completely at random (MCAR). Furthermore, confidence bands, estimation of nuisance functions, including conditional probability of observing the predictor, design density and scale, and multiple regression are also considered. Numerical study and a real example show feasibility of the proposed methodology for small samples. Supplementary materials, containing results of the numerical study, are available online."
"10.1198/jasa.2011.tm10098","2011","E{EB}oost: a general method for prediction and variable selection based on estimating equations","0","The modem statistical literature is replete with methods for performing variable selection and prediction in standard regression problems. However, simple models may misspecify or fail to capture important aspects of the data generating process such as missingness, correlation, and over/underdispersion. In this article we describe EEBoost. a strategy for variable selection and prediction which can be applied in high-dimensional settings where inference for low-dimensional parameters would typically be based on estimating equations. The method is simple, flexible, and easily implemented using existing software. The EEBoost algorithm is obtained as a modification of the standard boosting (or functional gradient descent) technique. We show that EEBoost is closely related to a class of L-1 constrained projected likelihood ratio minimizations, and therefore produces similar variable selection paths to penalized methods without the need to apply constrained optimization algorithms. The flexibility of EEBoost is illustrated by applying it to simulated examples with correlated outcomes and time-to-event data with missing covariates. In both cases, EEBoost outperforms variable selection methods which do not account for the relevant data characteristics. Furthermore, it is shown to be substantially faster to compute than competing methods based on penalized estimating equations. We also apply a version of EEBoost based on the Buckley James estimating equations to data from an HIV treatment trial, where the aim is to identify mutations which confer resistance to antiretroviral medications. Proofs of the main results appear in the Supplemental Materials (available online)."
"10.1198/jasa.2011.tm10422","2011","Some approximate evidence factors in observational studies","1","An observational study or nonrandomized experiment has exact evidence factors if it permits several statistically independent tests of the same null hypothesis Ho of no treatment effect, where these several tests depend upon different assumptions about bias from nonrandom treatment assignment. In an observational study, we are typically uncertain about what assumptions truly describe treatment assignment. If independent tests each reject Ho, and if each of these tests is valid under assumptions about treatment assignment that would invalidate the others, then rejection of Ho does not depend upon the truth of any one of these assumptions. Although exact evidence factors do exist, the requirement of exact independence may limit the class of test statistics in ways that reduce Pitman efficiency and design sensitivity, so one might wish to be free of these limitations. A much larger class of statistics permits approximate evidence factors which are not independent, but which preserve some of the important consequences of independence. Each of the several tests may be subjected to a sensitivity analysis, and the tests may be combined to yield a single inference. Results are proved for a large class of statistics and they are illustrated by following a suggestion of Maritz which uses the randomization distribution of tests based on statistics that are equated to zero in defining Huber's m-estimates. A study of damage to DNA from occupational exposures to chromium is used as an example."
"10.1198/jasa.2011.tm10314","2011","Stringing high-dimensional data for functional analysis","0","We propose stringing, a class of methods where one views high-dimensional observations as functional data. Stringing takes advantage of the high dimension by representing such data as discretized and noisy observations that originate from a hidden smooth stochastic process. Assuming that the observations result from scrambling the original ordering of the observations of the process, stringing reorders the components of the high-dimensional vectors, followed by transforming the high-dimensional vector observations into functional data. Established techniques from functional data analysis can be applied for further statistical analysis once an underlying stochastic process and the corresponding random trajectory for each subject have been identified. Stringing of high-dimensional data is implemented with distance based metric multidimensional scaling, mapping high-dimensional data to locations on a real interval, such that predictors that are close in a suitable sample metric also are located close to each other on the interval. We provide some theoretical support, showing that under certain assumptions, an underlying stochastic process can be constructed asymptotically, as the dimension p of the data tends to infinity. Stringing is illustrated for the analysis of tree ring data and for the prediction of survival time from high-dimensional gene expression data and is shown to lead to new insights. In regression applications involving high-dimensional predictors, stringing compares favorably with existing methods. The theoretical results and proofs and also additional simulation results are provided in online Supplemental Material."
"10.1198/jasa.2011.tm10284","2011","Tests for high-dimensional regression coefficients with factorial designs","0","We propose simultaneous tests for coefficients in high-dimensional linear regression models with factorial designs. The proposed tests are designed for the ""large p, small n"" situations where the conventional F-test is no longer applicable. We derive the asymptotic distribution of the proposed test statistic under the high-dimensional null hypothesis and various scenarios of the alternatives, which allow power evaluations. We also evaluate the power of the F-test for models of moderate dimension. The proposed tests are employed to analyze a microarray data on Yorkshire Gilts to find significant gene ontology terms which are significantly associated with the thyroid hormone after accounting for the designs of the experiment."
"10.1198/jasa.2011.tm09316","2011","On propagated scoring for semisupervised additive models","0","This article presents a semisupervised modeling framework that combines feature-based (x) data and graph-based (G) data for classification/regression of the response Y. In this semisupervised setting, Y is observed for a subset of the observations (labeled) and missing for the remainder (unlabeled). The Propagated Scoring algorithm proposed for fitting this model is a semisupervised fixed-point regularization approach that essentially extends the generalized additive model into the semisupervised setting. I first articulate when semisupervised degeneracies are expected within my framework, and then provide a general regularization strategy to address such circumstances. For statistical analysis, I establish that the approach uses shrinking smoothers, provide circumstances in which when the result is consistent, provide measures of inference and description, and establish clear connections to supervised models. Several semisupervised approaches have been considered for the classification problem posed, typically motivated from energy optimization perspective. In this work, I rigorously connect the statistically based propagated scoring framework to this class of approaches. This is particularly insightful, especially with regard to supervised comparisons, because this type of analysis is lacking for the previous work. Two applications are presented, one involving classification of protein location on a cell using a network of protein interaction data and the other involving classification of text documents with citation network information and text data. This article has supplementary material online."
"10.1198/jasa.2011.tm10113","2011","V{IF} regression: a fast regression algorithm for large data","0","We propose a fast and accurate algorithm. VIF regression, for doing feature selection in large regression problems. VIP regression is extremely fast: it uses a one-pass search over the predictors and a computationally efficient method of testing each potential predictor for addition to the model. VIE regression provably avoids model overfitting, controlling the marginal false discovery rate. Numerical results show that it is much faster than any other published algorithm for regression with feature selection and is as accurate as the best of the slower algorithms."
"10.1198/jasa.2011.tm09800","2011","Testing for threshold effects in regression models","0","In this article, we develop a general method for testing threshold effects in regression models, using sup-likelihood-ratio (LR)-type statistics. Although the sup-LR-type test statistic has been considered in the literature, our method for establishing the asymptotic null distribution is new and nonstandard. The standard approach in the literature for obtaining the asymptotic null distribution requires that there exist a certain quadratic approximation to the objective function. The article provides an alternative, novel method that can be used to establish the asymptotic null distribution, even when the usual quadratic approximation is intractable. We illustrate the usefulness of our approach in the examples of the maximum score estimation, maximum likelihood estimation, quantile regression, and maximum rank correlation estimation. We establish consistency and local power properties of the test. We provide some simulation results and also an empirical application to tipping in racial segregation. This article has supplementary materials online."
"10.1198/jasa.2011.tm10337","2011","Adaptive probability-based {L}atin hypercube designs","0","Adaptive sampling is an effective method developed mainly for regular regions. However, experimental regions in irregular shapes are commonly observed in practice. Motivated by a data center thermal management study, a new class of adaptive designs is proposed to accommodate a specific type of irregular region. Because the adaptive procedure introduces biases into conventional estimators, several design-unbiased estimators are given for estimating the population mean. Efficient and easy-to-compute unbiased estimators are also introduced. The proposed method is applied to obtain an adaptive sensor placement plan to monitor and study the thermal distribution in a data center. All of the supplemental materials used in this work are available online."
"10.1198/jasa.2011.tm09650","2011","Fast robust model selection in large datasets","0","Large datasets are increasingly common in many research fields. In particular, in the linear regression context, it is often the case that a huge number of potential covariates are available to explain a response variable, and the first step of a reasonable statistical analysis is to reduce the number of covariates. This can be clone in a forward selection procedure that includes selecting the variable to enter, deciding to retain it or stop the selection, and estimating the augmented model. Least squares plus t tests can be fast, but the outcome of a forward selection might be suboptimal when there are outliers. In this article we propose a complete algorithm for fast robust model selection, including considerations for huge sample sizes. Because simply replacing the classical statistical criteria with robust ones is not computationally possible, we develop simplified robust estimators, selection criteria, and testing procedures for linear regression. The robust estimator is a one-step weighted M-estimator that can be biased if the covariates are not orthogonal. We show that the bias can be made smaller by iterating the M-estimator one or more steps further. In the variable selection process, we propose a simplified robust criterion based on a robust t statistic that we compare with a false discovery rate-adjusted level. We carry out a simulation study to show the good performance of our approach. We also analyze two datasets and show that the results obtained by our method outperform those from robust least angle regression and random forests. Supplemental materials are available online."
"10.1198/jasa.2011.tm10355","2011","Testing and estimating shape-constrained nonparametric density and regression in the presence of measurement error","0","In many applications we can expect that, or are interested to know if, a density function or a regression curve satisfies some specific shape constraints. For example, when the explanatory variable, X, represents the value taken by a treatment or dosage, the conditional mean of the response, Y, is often anticipated to be a monotone function of X. Indeed, if this regression mean is not monotone (in the appropriate direction) :hen the medical or commercial value of the treatment is likely to be significantly curtailed, at least for values of X that lie beyond the point at which monotonicity fails. In the case of a density, common shape constraints include log-concavity and unimodality. If we can correctly guess the shape of a curve, then nonparametric estimators can be improved by taking this information into account. Addressing such problems requires a method for testing the hypothesis that the curve of interest satisfies a shape constraint, and, if the conclusion of the test is positive, a technique for estimating the curve subject to the constraint. Nonparametric methodology for solving these problems already exists, but only in cases where the covariates are observed precisely. However in many problems, data can only be observed with measurement errors, and the methods employed in the error-free case typically do not carry over to this error context. In this article we develop a novel approach to hypothesis testing and function estimation under shape constraints, which is valid in the context of measurement errors. Our method is based on tilting an estimator of the density or the regression mean until it satisfies the shape constraint, and we take as our test statistic the distance through which it is tilted. Bootstrap methods are used to calibrate the test. The constrained curve estimators that we develop are also based on tilting, and in that context our work has points of contact with methodology in the error-free case."
"10.1198/jasa.2011.tm08250","2011","Inverse regression estimation for censored data","0","An inverse regression methodology for assessing predictor performance in the censored data setup is developed along with inference procedures and a computational algorithm. The technique developed here allows for conditioning on the unobserved failure time along with a weighting mechanism that accounts for the censoring. The implementation is nonparametric and computationally fast. This provides an efficient methodological tool that can be used especially in cases where the usual modeling assumptions are not applicable to the data under consideration. It can also be a good diagnostic tool that can be used in the model selection process. We have provided theoretical justification of consistency and asymptotic normality of the methodology. Simulation studies and two data analyses are provided to illustrate the practical utility of the procedure."
"10.1198/jasa.2011.tm10319","2011","Hard or soft classification? {L}arge-margin unified machines","0","Margin-based classifiers have been popular in both machine learning and statistics for classification problems. Among numerous classifiers, some are hard classifiers while some are soft ones. Soft classifiers explicitly estimate the class conditional probabilities and then perform classification based on estimated probabilities. In contrast, hard classifiers directly target the classification decision boundary without producing the probability estimation. These two types of classifiers are based on different philosophies and each has its own merits. In this article, we propose a novel family of large-margin classifiers, namely large-margin unified machines (LUMs), which covers a broad range of margin-based classifiers including both hard and soft ones. By offering a natural bridge from soft to hard classification, the LUM provides a unified algorithm to fit various classifiers and hence a convenient platform to compare hard and soft classification. Both theoretical consistency and numerical performance of LUMs are explored. Our numerical study sheds some light on the choice between hard and soft classifiers in various classification problems."
"10.1198/jasa.2011.tm10104","2011","A semiparametric estimation of mean functionals with nonignorable missing data","0","Parameter estimation with nonignorable missing data is a challenging problem in statistics. The fully parametric approach for joint modeling of the response model and the population model can produce results that are quite sensitive to the failure of the assumed model. We propose a more robust modeling approach by considering the model for the nonresponding part as an exponential tilting of the model for the responding part. The exponential tilting model can be justified under the assumption that the response probability can be expressed as a semiparametric logistic regression model.In this paper, based on the exponential tilting model, we propose a semiparametric estimation method of mean functionals with nonignorable missing data. A semiparametric logistic regression model is assumed for the response probability and a nonparametric regression approach for missing data discussed in Cheng (1994) is used in the estimator. By adopting nonparametric components for the model, the estimation method can be made robust. Variance estimation is also discussed and results from a simulation study are presented. The proposed method is applied to real income data from the Korean Labor and Income Panel Survey."
"10.1198/jasa.2011.tm10031","2011","Saddlepoint test in measurement error models","0","We develop second-order hypothesis testing procedures in functional measurement error models for small or moderate sample sizes, where the classical first-order asymptotic analysis often fails to provide accurate results. In functional models no distributional assumptions are made on the unobservable covariates and this leads to semiparametric models. Our testing procedure is derived using saddlepoint techniques and is based on an empirical distribution estimation subject to the null hypothesis constraints, in combination with a set of estimating equations which avoid a distribution approximation. The validity of the method is proved in theorems for both simple and composite hypothesis tests, and is demonstrated through simulation and a farm size data analysis."
"10.1198/jasa.2011.ap09475","2011","Improved inference for respondent-driven sampling data with application to {HIV} prevalence estimation","0","Respondent-driven sampling is a form of link-tracing network sampling, which is widely used to study hard-to-reach populations, often to estimate population proportions. Previous treatments of this process have used a with-replacement approximation, which we show induces bias in estimates for large sample fractions and differential network connectedness by characteristic of interest.We present a treatment of respondent-driven sampling as a successive sampling process. Unlike existing representations, our approach respects the essential without-replacement feature of the process, while converging to an existing with-replacement representation for small sample fractions, and to the sample mean for a full-population sample.We present a successive-sampling based estimator for population means based on respondent-driven sampling data, and demonstrate its super,or performance when the size of the hidden population is known. We present sensitivity analyses for unknown population sizes. In addition, we note that like other existing estimators, our new estimator is subject to bias induced by the selection of the initial sample. Using data collected among three populations in two countries, we illustrate the application of this approach to populations with varying characteristics. We conclude that the successive sampling estimator improves on existing estimators, and can also be used as a diagnostic tool when population size is not known. This article has supplementary material online."
"10.1198/jasa.2011.ap09735","2011","Meta analysis of functional neuroimaging data via {B}ayesian spatial point processes","0","As the discipline of functional neuroimaging grows there is an increasing interest in meta analysis of brain imaging studies. A typical neuroimaging meta analysis collects peak activation coordinates (foci) from several studies and identifies areas of consistent activation. Most imaging meta analysis methods only produce null hypothesis inferences and do not provide an interpretable fitted model. To overcome these limitations, we propose a Bayesian spatial hierarchical model using a marked independent cluster process. We model the foci as offspring of a latent study center process, and the study centers are in turn offspring of a latent population center process. The posterior intensity function of the population center process provides inference on the location of population centers, as well as the interstudy variability of foci about the population centers. We illustrate our model with a meta analysis consisting of 437 studies from 164 publications, show how two subpopulations of studies can be compared and assess our model via sensitivity analyses and simulation studies. Supplemental materials are available online."
"10.1198/jasa.2011.ap09769","2011","Dynamic trees for learning and design","0","Dynamic regression trees are an attractive option for automatic regression and classification with complicated response surfaces in online application settings. We create a sequential tree model whose state changes in time with the accumulation of new data, and provide particle learning algorithms that allow for the efficient online posterior filtering of tree states. A major advantage of tree regression is that it allows for the use of very simple models within each partition. The model also facilitates a natural division of labor in our sequential particle-based inference: tree dynamics are defined through a few potential changes that are local to each newly arrived observation, while global uncertainty is captured by the ensemble of particles. We consider both constant and linear mean functions at the tree leaves, along with multinomial leaves for classification problems, and propose default prior specifications that allow for prediction to be integrated over all model parameters conditional on a given tree. Inference is illustrated in some standard nonparametric regression examples, as well as in the setting of sequential experiment design, including both active learning and optimization applications, and in online classification. We detail implementation guidelines and problem specific methodology for each of these motivating applications. Throughout, it is demonstrated that our practical approach is able to provide better results compared to commonly used methods at a fraction of the cost."
"10.1198/jasa.2011.ap09546","2011","Self-exciting point process modeling of crime","0","Highly clustered event sequences are observed in certain types of crime data, such as burglary and gang violence, due to crime-specific patterns of criminal behavior. Similar clustering patterns are observed by seismologists, as earthquakes are well known to increase the risk of subsequent earthquakes, or aftershocks, near the location of an initial event. Space time clustering is modeled in seismology by self-exciting point processes and the focus of this article is to show that these methods are well suited for criminological applications. We first review self-exciting point processes in the context of seismology. Next, using residential burglary data provided by the Los Angeles Police Department, we illustrate the implementation of self-exciting point process models in the context of urban crime. For this purpose we use a fully nonparametric estimation methodology to gain insight into the form of the space time triggering function and temporal trends in the background rate of burglary."
"10.1198/jasa.2011.ap10023","2011","Changepoints in the {N}orth {A}tlantic tropical cyclone record","0","This article examines the North Atlantic tropical cyclone record for statistical discontinuities (changepoints). This is a controversial area and indeed, our end conclusions are opposite of those made in Dr. Kelvin Droegemeier's July 28, 2009 Senate testimonial. The methods developed here should help rigorize the debate. Elaborating, we develop a level-alpha test for a changepoint in a categorical data sequence sampled from a multinomial distribution. The proposed test statistic is the maximum of correlated Pearson chi-square statistics. This test statistic is linked to cumulative sum statistics and its null hypothesis asymptotic distribution is derived in terms of the supremum of squared Brownian bridges. The methods are used to identify changes in the tropical cyclone record in the North Atlantic Basin over the period 1851-2008. We find changepoints in both the storm frequencies and their strengths (wind speeds). The changepoint in wind speed is not found with standard cumulative sum mean shift changepoint methods, hence providing a dataset where categorical probabilities shift but means do not. While some of the identified shifts can be attributed to changes in data collection techniques, the hotly debated changepoint in cyclone frequency circa 1995 also appears to be significant."
"10.1198/jasa.2011.ap09587","2011","Multiple testing for pattern identification, with applications to microarray time-course experiments","0","In One-course experiments, it is often desirable to identify genes that exhibit a specific pattern of differential expression over time and thus gain insights into the mechanisms of the underlying biological processes. Two challenging issues in the pattern identification problem are: (i) how to combine the simultaneous inferences across multiple time points and (ii) how to control the multiplicity while accounting for the strong dependence. We formulate a compound decision-theoretic framework for set-wise multiple testing and propose a data-driven procedure that aims to minimize the missed set rate subject to a constraint on the false set rate. The hidden Markov model proposed in Yuan and Kendziorski (2006) is generalized to capture the temporal correlation in the gene expression data. Both theoretical and numerical results are presented to show that our data-driven procedure controls the multiplicity, provides an optimal way of combining simultaneous inferences across multiple time points, and greatly improves the conventional combined p-value methods. In particular, we demonstrate our method in an application to a study of systemic inflammation in humans for detecting early and late response genes."
"10.1198/jasa.2010.ap09504","2011","Modeling three-dimensional chromosome structures using gene expression data","0","Recent genomic studies have shown that significant chromosomal spatial correlation exists in gene expression of many organisms. Interestingly, coexpression has been observed among genes separated by a fixed interval in specific regions of a chromosome chain, which is likely caused by three-dimensional (3D) chromosome folding structures. Modeling such spatial correlation explicitly may lead to essential understandings of 3D chromosome structures and their roles in transcriptional regulation. In this paper, we explore chromosomal spatial correlation induced by 3D chromosome structures, and propose a hierarchical Bayesian method based on helical structures to formally model and incorporate the correlation into the analysis of gene expression microarray data. It is the first study to quantify and infer 3D chromosome structures in vivo using expression microarrays. Simulation studies show computing feasibility of the proposed method and that, under the assumption of helical chromosome structures, it can lead to precise estimation of structural parameters and gene expression levels. Real data applications demonstrate an intriguing biological phenomenon that functionally associated genes, which are far apart along the chromosome chain, are brought into physical proximity by chromosomal folding in 3D space to facilitate their coexpression. It leads to important: biological insight into relationship between chromosome structure and function."
"10.1198/jasa.2010.ap09545","2011","Online model-based clustering for crisis identification in distributed computing","0","Large-scale distributed computing systems can suffer from occasional severe violation of performance goals; due to the complexity of these systems, manual diagnosis of the cause of the crisis is too slow to inform interventions taken during the crisis. Rapid automatic recognition of the recurrence of a problem can lead to cause diagnosis and informed intervention. We frame this as an online clustering problem, where the labels (causes) of some of the previous crises may be known. We give a fast and accurate solution using model-based clustering based on a Dirichlet process mixture; the evolution of each crisis is modeled as a multivariate time series.In the periods between crises we perform full Bayesian inference for the past crises, and as a new crisis occurs we apply fast approximate Bayesian updating. These inferences allow real-time expected-cost-minimizing decision making that fully accounts for uncertainty in the crisis labels and other parameters. We apply and validate our methods using simulated data and data from a production computing center with hundreds of servers running a 24/7 email-related application."
"10.1198/jasa.2011.ap09653","2011","A hierarchical model for quantifying forest variables over large heterogeneous landscapes with uncertain forest areas","0","We are interested in predicting one or more continuous forest variables (e.g., biomass, volume, age) at a fine resolution (e.g., pixel level) across a specified domain. Given a definition of forest/nonforest, this prediction is typically a two-step process. The first step predicts which locations are forested. The second step predicts the value of the variable for only those forested locations. Rarely is the forest/nonforest status predicted without error. However, the uncertainty in this prediction is typically not propagated through to the subsequent prediction of the forest variable of interest. Failure to acknowledge this error can result in biased estimates of forest variable totals within a domain. In response to this problem, we offer a modeling framework that will allow propagation of this uncertainty. Here we envision two latent processes generating the data. The first is a continuous spatial process while the second is a binary spatial process. The continuous spatial process controls the spatial association structure of the forest variable of interest, while the binary process indicates presence of a possible nonzero value for the forest variable at a given location. The proposed models are applied to georeferenced National Forest Inventory (NFI) data and spatially coinciding remotely sensed predictor variables. Due to the large number of observed locations in this dataset we seek dimension reduction not just in the likelihood, but also for unobserved stochastic processes. We demonstrate how a low-rank predictive process can be adapted to our setting and reduce the dimensionality of the data and ease the computational burden."
"10.1198/jasa.2010.ap09318","2011","Statistical estimation of word acquisition with application to readability prediction","0","Models of language learning play a central role in a wide range of applications: from psycholinguistic theories of how people acquire new word knowledge, to information systems that can automatically match content to users' reading ability. Traditional methods for estimating word acquisition ages or content readability are typically based on linear regression over a small number of summary features derived from time-consuming user studies or costly expert judgments. With the increasing amounts of content available from the web and other sources, however, new statistical approaches are possible that can exploit this easily acquired data to learn more flexible, fine-grained models of language usage. We present a novel statistical model for document readability that is based on the logistic Rasch model and the quantiles of word acquisition age distributions. We use this model to estimate the distributions of word acquisition ages from empirical readability data collected from the web. We then demonstrate that the estimated acquisition distributions are very effective in predicting both global and local document readability. We also compare the estimated distributions with word acquisition data from existing oral studies, revealing interesting historical trends as well as differences between oral and written word acquisition grade levels."
"10.1198/jasa.2010.ap09237","2011","Bayesian spatial quantile regression","1","Tropospheric ozone is one of the six criteria pollutants regulated by the United States Environmental Protection Agency under the Clean Air Act and has been linked with several adverse health effects, including mortality. Due to the strong dependence on weather conditions, ozone may be sensitive to climate change and there is great interest in studying the potential effect of climate change on ozone, and how this change may affect public health. In this paper we develop a Bayesian spatial model to predict ozone under different meteorological conditions, and use this model to study spatial and temporal trends and to forecast ozone concentrations under different climate scenarios. We develop a spatial quantile regression model that does not assume normality and allows the covariates to affect the entire conditional distribution, rather than just the mean. The conditional distribution is allowed to vary from site-to-site and is smoothed with a spatial prior. For extremely large datasets our model is computationally infeasible, and we develop an approximate method. We apply the approximate version of our model to summer ozone from 1997-2005 in the Eastem U.S., and use deterministic climate models to project ozone under future climate conditions. Our analysis suggests that holding all other factors fixed, an increase in daily average temperature will lead to the largest increase in ozone in the Industrial Midwest and Northeast."
"10.1080/01621459.2012.656041","2012","Vast volatility matrix estimation using high-frequency data for portfolio selection","0","Portfolio allocation with gross-exposure constraint is an effective method to increase the efficiency and stability of portfolios selection among a vast pool of assets, as demonstrated by Fan, Zhang, and Yu. The required high-dimensional volatility matrix can be estimated by using high-frequency financial data. This enables us to better adapt to the local volatilities and local correlations among a vast number of assets and to increase significantly the sample size for estimating the volatility matrix. This article studies the volatility matrix estimation using high-dimensional, high-frequency data from the perspective of portfolio selection. Specifically, we propose the use of ""pairwise-refresh time"" and ""all-refresh time"" methods based on the concept of ""refresh time"" proposed by Barndorff-Nielsen, Hansen, Lunde, and Shephard for the estimation of vast covariance matrix and compare their merits in the portfolio selection. We establish the concentration inequalities of the estimates, which guarantee desirable properties of the estimated volatility matrix in vast asset allocation with gross-exposure constraints. Extensive numerical studies are made via carefully designed simulations. Comparing with the methods based on low-frequency daily data, our methods can capture the most recent trend of the time varying volatility and correlation, hence provide more accurate guidance for the portfolio allocation in the next time period. The advantage of using high-frequency data is significant in our simulation and empirical studies, which consist of 50 simulated assets and 30 constituent stocks of Dow Jones Industrial Average index."
"10.1080/01621459.2012.656035","2012","Optimal designs for rational function regression","0","We consider consider the problem of finding optimal nonsequential designs for a large class of regression models involving polynomials and rational functions with heteroscedastic noise also given by a polynomial or rational weight function. Since the design weights can be found easily by existing methods once the support is known, we concentrate on determining the support of the optimal design. The proposed method treats D-, E-, A-, and Phi(p)-optimal designs in a unified manner, and generates a polynomial whose zeros are the support points of the optimal approximate design, generalizing a number of previously known results of the same flavor. The method is based on a mathematical optimization model that can incorporate various criteria of optimality and can be solved efficiently by well-established numerical optimization methods. In contrast to optimization-based methods previously proposed for the solution of similar design problems, our method also has theoretical guarantee of its algorithmic efficiency; in concordance with the theory, the actual running times of all numerical examples considered in the paper are negligible. The numerical stability of the method is demonstrated in an example involving high-degree polynomials. As a corollary, an upper bound on the size of the support set of the minimally supported optimal designs is also found."
"10.1080/01621459.2011.644132","2012","Sliced {L}atin hypercube designs","0","This article proposes a method for constructing a new type of space-filling design, called a sliced Latin hypercube design, intended for running computer experiments. Such a design is a special Latin hypercube design that can be partitioned into slices of smaller Latin hypercube designs. It is desirable to use the constructed designs for collective evaluations of computer models and ensembles of multiple computer models. The proposed construction method is easy to implement, capable of accommodating any number of factors, and flexible in run size. Examples are given to illustrate the method. Sampling properties of the constructed designs are examined. Numerical illustration is provided to corroborate the derived theoretical results."
"10.1080/01621459.2011.646935","2012","Bootstrapping for significance of compact clusters in multidimensional datasets","0","This article proposes a bootstrap approach for assessing significance in the clustering of multidimensional datasets. The procedure compares two models and declares the more complicated model a better candidate if there is significant evidence in its favor. The performance of the procedure is illustrated on two well-known classification datasets and comprehensively evaluated in terms of its ability to estimate the number of components via extensive simulation studies, with excellent results. The methodology is also applied to the problem of k-means color quantization of several standard images in the literature and is demonstrated to be a viable approach for determining the minimal and optimal numbers of colors needed to display an image without significant loss in resolution. Additional illustrations and performance evaluations are provided in the online supplementary material."
"10.1080/01621459.2011.646934","2012","Simplex factor models for multivariate unordered categorical data","0","Gaussian latent factor models are routinely used for modeling of dependence in continuous, binary, and ordered categorical data. For unordered categorical variables, Gaussian latent factor models lead to challenging computation and complex modeling structures. As an alternative, we propose a novel class of simplex factor models. In the single-factor case, the model treats the different categorical outcomes as independent with unknown marginals. The model can characterize flexible dependence structures parsimoniously with few factors, and as factors are added, any multivariate categorical data distribution can be accurately approximated. Using a Bayesian approach for computation and inferences, a Markov chain Monte Carlo (MCMC) algorithm is proposed that scales well with increasing dimension, with the number of factors treated as unknown. We develop an efficient proposal for updating the base probability vector in hierarchical Dirichlet models. Theoretical properties are described, and we evaluate the approach through simulation examples. Applications are described for modeling dependence in nucleotide sequences and prediction from high-dimensional categorical features."
"10.1080/01621459.2011.646916","2012","On fractile transformation of covariates in regression","0","The need for comparing two regression functions arises frequently in statistical applications. Comparison of the usual regression functions is not very meaningful in situations where the distributions and the ranges of the covariates are different for the populations. For instance, in econometric studies, the prices of commodities and people's incomes observed at different time points may not be on comparable scales due to inflation and other economic factors. In this article, we describe a method of standardizing the covariates and estimating the transformed regression function, which then become comparable. We develop smooth estimates of the fractile regression function and study its statistical properties analytically as well as numerically. We also provide a few real examples that illustrate the difficulty in comparing the usual regression functions and motivate the need for the fractile transformation. Our analysis of the real examples leads to new and useful statistical conclusions that are missed by comparison of the usual regression functions."
"10.1080/01621459.2011.644141","2012","Interim design modifications in time-to-event studies","0","We propose a flexible method for interim design modifications in time-to-event studies. With this method, it is possible to inspect the data at any time during the course of the study, without the need for prespecification of a learning phase, and to make certain types of design modifications depending on the interim data without compromising the Type I error risk. The method can be applied to studies designed with a conventional statistical test, fixed sample, or group sequential, even when no adaptive interim analysis and no specific method for design adaptations (such as combination tests) had been foreseen in the protocol. Currently, the method supports design changes such as an extension of the recruitment or follow-up period, as well as certain modifications of the number and the schedule of interim analyses as well as changes of inclusion criteria. In contrast to existing methods offering the same flexibility, our approach allows us to make use of the full interim information collected until the time of the adaptive data inspection. This includes time-to-event data from patients who have already experienced an event at the time of the data inspection, and preliminary information from patients still alive, even if this information is predictive for survival, such as early treatment response in a cancer clinical trial. Our method is an extension of the so-called conditional rejection probability (CRP) principle. It is based on the conditional distribution of the test statistic given the final value of the same test statistic from a subsample, namely the learning sample. It is developed in detail for the example of the logrank statistic, for which we derive this conditional distribution using martingale techniques."
"10.1080/01621459.2011.637468","2012","Recursively imputed survival trees","0","We propose recursively imputed survival tree (RIST) regression for right-censored data. This new nonparametric regression procedure uses a novel recursive imputation approach combined with extremely randomized trees that allows significantly better use of censored data than previous tree-based methods, yielding improved model fit and reduced prediction error. The proposed method can also be viewed as a type of Monte Carlo EM algorithm, which generates extra diversity in the tree-based fitting process. Simulation studies and data analyses demonstrate the superior performance of RIST compared with previous methods."
"10.1080/01621459.2012.656021","2012","Estimating regression parameters in an extended proportional odds model","0","The proportional odds model may serve as a useful alternative to the Cox proportional hazards model to study association between covariates and their survival functions in medical studies. In this article, we study an extended proportional odds model that incorporates the so-called ""external"" time-varying covariates. In the extended model, regression parameters have a direct interpretation of comparing survival functions, without specifying the baseline survival odds function. Semiparametric and maximum likelihood estimation procedures are proposed to estimate the extended model. Our methods are demonstrated by Monte Carlo simulations, and applied to a landmark randomized clinical trial of a short-course nevirapine (NVP) for mother-to-child transmission (MTCT) of human immunodeficiency virus type-1 (HIV-1). Additional application includes an analysis of the well-known Veterans Administration (VA) lung cancer trial."
"10.1080/01621459.2012.656011","2012","A {H}eckman selection-{$t$} model","0","Sample selection arises often in practice as a result of the partial observability of the outcome of interest in a study. In the presence of sample selection, the observed data do not represent a random sample from the population, even after controlling for explanatory variables. That is, data are missing not at random. Thus, standard analysis using only complete cases will lead to biased results. Heckman introduced a sample selection model to analyze such data and proposed a full maximum likelihood estimation method under the assumption of normality. The method was criticized in the literature because of its sensitivity to the normality assumption. In practice, data, such as income or expenditure data, often violate the normality assumption because of heavier tails. We first establish a new link between sample selection models and recently studied families of extended skew-elliptical distributions. Then, this allows us to introduce a selection-t (SLt) model, which models the error distribution using a Student's t distribution. We study its properties and investigate the finite-sample performance of the maximum likelihood estimators for this model. We compare the performance of the SLt model to the conventional Heckman selection-normal (SLN) model and apply it to analyze ambulatory expenditures. Unlike the SLN model, our analysis using the SLt model provides statistical evidence for the existence of sample selection bias in these data. We also investigate the performance of the test for sample selection bias based on the SLt model and compare it with the performances of several tests used with the SLN model. Our findings indicate that the latter tests can be misleading in the presence of heavy-tailed data."
"10.1080/01621459.2011.644501","2012","Estimation of copula models with discrete margins via {B}ayesian data augmentation","0","Estimation of copula models with discrete margins can be difficult beyond the bivariate case. We show how this can be achieved by augmenting the likelihood with continuous latent variables, and computing inference using the resulting augmented posterior. To evaluate this, we propose two efficient Markov chain Monte Carlo sampling schemes. One generates the latent variables as a block using a Metropolis Hastings step with a proposal that is close to its target distribution, the other generates them one at a time. Our method applies to all parametric copulas where the conditional copula functions can be evaluated, not just elliptical copulas as in much previous work. Moreover, the copula parameters can be estimated joint with any marginal parameters. and Bayesian selection ideas can be employed. We establish the effectiveness of the estimation method by modeling consumer behavior in online retail using Archimedean and Gaussian copulas. The example shows that elliptical copulas can be poor at modeling dependence in discrete data, just as they can be in the continuous case. To demonstrate the potential in higher dimensions, we estimate 16-dimensional D-vine copulas for a longitudinal model of usage of a bicycle path in the city of Melbourne, Australia. The estimates reveal an interesting serial dependence structure that can be represented in a parsimonious fashion using Bayesian selection of independence pair-copula components. Finally, we extend our results and method to the case where some margins are discrete and others continuous. Supplemental materials for the article are also available online."
"10.1080/01621459.2011.646919","2012","Modeling nonstationary processes through dimension expansion","0","In this article, we propose a novel approach to modeling nonstationary spatial fields. The proposed method works by expanding the geographic plane over which these processes evolve into higher-dimensional spaces, transforming and clarifying complex patterns in the physical plane. By combining aspects of multidimensional scaling, group lasso, and latent variable models, a dimensionally sparse projection is found in which the originally nonstationary field exhibits stationarity. Following a comparison with existing methods in a simulated environment, dimension expansion is studied on a classic test-bed dataset historically used to study nonstationary models. Following this, we explore the use of dimension expansion in modeling air pollution in the United Kingdom, a process known to be strongly influenced by rural/urban effects, amongst others, which gives rise to a nonstationary field."
"10.1080/01621459.2011.646928","2012","Estimating space and space-time covariance functions for large data sets: a weighted composite likelihood approach","0","In this article, we propose two methods for estimating space and space-time covariance functions from a Gaussian random field, based on the composite likelihood idea. The first method relies on the maximization of a weighted version of the composite likelihood function, while the second one is based on the solution of a weighted composite score equation. This last scheme is quite general and could be applied to any kind of composite likelihood. An information criterion for model selection based on the first estimation method is also introduced. The methods are useful for practitioners looking for a good balance between computational complexity and statistical efficiency. The effectiveness of the methods is illustrated through examples, simulation experiments, and by analyzing a dataset on ozone measurements."
"10.1080/01621459.2011.640592","2012","One-sided and two-sided tolerance intervals in general mixed and random effects models using small-sample asymptotics","0","The computation of tolerance intervals in mixed and random effects models has not been satisfactorily addressed in a general setting when the data are unbalanced and/or when covariates are present. This article derives satisfactory one-sided and two-sided tolerance intervals in such a general scenario, by applying small-sample asymptotic procedures. In the case of one-sided tolerance limits, the problem reduces to the interval estimation of a percentile, and accurate confidence limits are derived using small-sample asymptotics. In the case of a two-sided tolerance interval, the problem does not reduce to an interval estimation problem; however, it is possible to derive an approximate margin of error statistic that is an upper confidence limit for a linear combination of the variance components. For the latter problem. small-sample asymptotic procedures can once again be used to arrive at an accurate upper confidence limit. In the article, balanced and unbalanced data situations are treated separately, and computational issues are addressed in detail. Extensive numerical results show that the tolerance intervals derived based on small-sample asymptotics exhibit satisfactory performance regardless of the sample size. The results are illustrated using some examples. Some technical derivations, additional simulation results, and R codes are available online as supplementary materials."
"10.1080/01621459.2012.656009","2012","Semiparametric double balancing score estimation for incomplete data with ignorable missingness","0","When estimating the marginal mean response with missing observations, a critical issue is robustness to model misspecification. In this article, we propose a semiparametric estimation method with extended double robustness that attains the optimal efficiency under less stringent requirement for model specifications than the doubly robust estimators. In this semiparametric estimation, covariate information is collapsed into a two-dimensional score S. with one dimension for (i) the pattern of missingness and the other for (ii) the pattern of response, both estimated from some working parametric models. The mean response E(Y) is then estimated by the sample mean of E(Y vertical bar S), which is estimated via nonparametric regression. The semiparametric estimator is consistent if either the ""core"" of (i) or the ""core"" of (ii) is captured by S, and attains the optimal efficiency if both are captured by S. As the ""cores"" can be obtained without correctly specifying the full parametric models for (i) or (ii), the proposed estimator can be more robust than other doubly robust estimators. As S contains the propensity score as one component, the proposed estimator avoids the use and the shortcomings of inverse propensity weighting. This semiparametric estimator is most appealing for high-dimensional covariates, where fully correct model specification is challenging and nonparametric estimation is not feasible due to the problem of dimensionality. Numerical performance is investigated by simulation studies."
"10.1080/01621459.2011.646929","2012","Block bootstraps for time series with fixed regressors","0","This article examines block bootstrap methods in linear regression models with weakly dependent error variables and nonstochastic regressors. Contrary to intuition, the tapered block bootstrap (TB B) with a smooth taper not only loses its superior bias properties but may also fail to be consistent in the regression problem. A similar problem, albeit at a smaller scale, is shown to exist for the moving and the circular block bootstrap (MBB and CBB, respectively). As a remedy, an additional block randomization step is introduced that balances out the effects of nonuniform regression weights, and restores the superiority of the (modified) TBB. The randomization step also improves the MBB or CB B. Interestingly, the stationary bootstrap (SB) automatically balances out regression weights through its probabilistic blocking mechanism, without requiring any modification, and enjoys a kind of robustness. Optimal block sizes are explicitly determined for block bootstrap variance estimators under regression. Finite sample performance and practical uses of the methods are illustrated through a simulation study and two data examples, respectively. Supplementary materials are available online."
"10.1080/01621459.2011.645783","2012","Likelihood-based selection and sharp parameter estimation","0","In high-dimensional data analysis, feature selection becomes one effective means for dimension reduction, which proceeds with parameter estimation. Concerning accuracy of selection and estimation, we study nonconvex constrained and regularized likelihoods in the presence of nuisance parameters. Theoretically, we show that constrained L-0 likelihood and its computational surrogate are optimal in that they achieve feature selection consistency and sharp parameter estimation, under one necessary condition required for any method to be selection consistent and to achieve sharp parameter estimation. It permits up to exponentially many candidate features. Computationally, we develop difference convex methods to implement the computational surrogate through prime and dual subproblems. These results establish a central role of L-0 constrained and regularized likelihoods in feature selection and parameter estimation involving selection. As applications of the general method and theory, we perform feature selection in linear regression and logistic regression, and estimate a precision matrix in Gaussian graphical models. In these situations, we gain a new theoretical insight and obtain favorable numerical results. Finally, we discuss an application to predict the metastasis status of breast cancer patients with their gene expression profiles. This article has online supplementary material."
"10.1080/01621459.2012.656014","2012","Quantile regression for analyzing heterogeneity in ultra-high dimension","0","Ultra-high dimensional data often display heterogeneity due to either heteroscedastic variance or other forms of non-location-scale covariate effects. To accommodate heterogeneity, we advocate a more general interpretation of sparsity, which assumes that only a small number of covariates influence the conditional distribution of the response variable, given all candidate covariates; however, the sets of relevant covariates may differ when we consider different segments of the conditional distribution. In this framework, we investigate the methodology and theory of nonconvex, penalized quantile regression in ultra-high dimension. The proposed approach has two distinctive features: (1) It enables us to explore the entire conditional distribution of the response variable, given the ultra-high-dimensional covariates, and provides a more realistic picture of the sparsity pattern; (2) it requires substantially weaker conditions compared with alternative methods in the literature; thus, it greatly alleviates the difficulty of model checking in the ultra-high dimension. In theoretic development, it is challenging to deal with both the nonsmooth loss function and the nonconvex penalty function in ultra-high-dimensional parameter space. We introduce a novel, sufficient optimality condition that relies on a convex differencing representation of the penalized loss function and the subdifferential calculus. Exploring this optimality condition enables us to establish the oracle property for sparse quantile regression in the ultra-high dimension under relaxed conditions. The proposed method greatly enhances existing tools for ultra-high-dimensional data analysis. Monte Carlo simulations demonstrate the usefulness of the proposed procedure. The real data example we analyzed demonstrates that the new approach reveals substantially more information as compared with alternative methods. This article has online supplementary material."
"10.1080/01621459.2011.645785","2012","Information ratio test for model misspecification in quasi-likelihood inference","0","In this article, we focus on the circumstances in quasi-likelihood inference that the estimation accuracy of mean structure parameters is guaranteed by correct specification of the first moment, but the estimation efficiency could be diminished due to misspecification of the second moment. We propose an information ratio (IR) statistic to test for model misspecification of the variance/covariance structure through a comparison between two forms of information matrix: the negative sensitivity matrix and the variability matrix. We establish asymptotic distributions of the proposed IR test statistics. We also suggest an approximation to the asymptotic distribution of the IR statistic via a perturbation resampling method. Moreover, we propose a selection criterion based on the IR test to select the best fitting variance/covariance structure from a class of candidates. Through simulation studies, it is shown that the IR statistic provides a powerful statistical tool to detect different scenarios of misspecification of the variance/covariance structures. In addition, the IR test as well as the proposed model selection procedure shows substantial improvement over some of the existing statistical methods. The IR-based model selection procedure is illustrated by analyzing the Madras Longitudinal Schizophrenia data. Appendices are included in the supplemental materials, which are available online."
"10.1080/01621459.2011.643198","2012","Multiple imputation for {$M$}-regression with censored covariates","0","We develop a new multiple imputation approach for M-regression models with censored covariates. Instead of specifying parametric likelihoods, our method imputes the censored covariates by their conditional quantiles given the observed data, where the conditional quantiles are estimated through fitting a censored quantile regression process. The resulting estimator is shown to be consistent and asymptotically normal, and it improves the estimation efficiency by using information from cases with censored covariates. Compared with existing methods, the proposed method is more flexible as it does not require stringent parametric assumptions on the distributions of either the regression errors or the covariates. The finite sample performance of the proposed method is assessed through a simulation study and the analysis of a c-reactive protein dataset in the 2007-2008 National Health and Nutrition Examination Survey. This article has supplementary material online."
"10.1080/01621459.2011.643197","2012","A valid {M}at\'ern class of cross-covariance functions for multivariate random fields with any number of components","0","We introduce a valid parametric family of cross-covariance functions for multivariate spatial random fields where each component has a covariance function from a well-celebrated Matern class. Unlike previous attempts, our model indeed allows for various smoothnesses and rates of correlation decay for any number of vector components. We present the conditions on the parameter space that result in valid models with varying degrees of complexity. We discuss practical implementations, including reparameterizations to reflect the conditions on the parameter space and an iterative algorithm to increase the computational efficiency. We perform various Monte Carlo simulation experiments to explore the performances of our approach in terms of estimation and cokriging. The application of the proposed multivariate Matern model is illustrated on two meteorological datasets: temperature/pressure over the Pacific Northwest (bivariate) and wind/temperature/pressure in Oklahoma (trivariate). In the latter case, our flexible trivariate Matern model is valid and yields better predictive scores compared with a parsimonious model with common scale parameters."
"10.1080/01621459.2011.646925","2012","A semiparametric approach to dimension reduction","0","We provide a novel and completely different approach to dimension-reduction problems from the existing literature. We cast the dimension-reduction problem in a semiparametric estimation framework and derive estimating equations. Viewing this problem from the new angle allows us to derive a rich class of estimators, and obtain the classical dimension reduction techniques as special cases in this class. The semiparametric approach also reveals that in the inverse regression context while keeping the estimation structure intact, the common assumption of linearity and/or constant variance on the covariates can be removed at the cost of performing additional nonparametric regression. The semiparametric estimators without these common assumptions are illustrated through simulation studies and a real data example. This article has online supplementary material."
"10.1080/01621459.2011.644498","2012","Sparse estimation of conditional graphical models with application to gene networks","0","In many applications the graph structure in a network arises from two sources: intrinsic connections and connections due to external effects. We introduce a sparse estimation procedure for graphical models that is capable of isolating the intrinsic connections by removing the external effects. Technically, this is formulated as a conditional graphical model, in which the external effects are modeled as predictors, and the graph is determined by the conditional precision matrix. We introduce two sparse estimators of this matrix using the reproduced kernel Hilbert space combined with lasso and adaptive lasso. We establish the sparsity, variable selection consistency, oracle property, and the asymptotic distributions of the proposed estimators. We also develop their convergence rate when the dimension of the conditional precision matrix goes to infinity. The methods are compared with sparse estimators for unconditional graphical models, and with the constrained maximum likelihood estimate that assumes a known graph structure. The methods are applied to a genetic data set to construct a gene network conditioning on single-nucleotide polymorphisms."
"10.1080/01621459.2011.645777","2012","Cross-dimensional inference of dependent high-dimensional data","0","A growing number of modern scientific problems in areas such as genomics, neurobiology, and spatial epidemiology involve the measurement and analysis of thousands of related features that may be stochastically dependent at arbitrarily strong levels. In this work, we consider The scenario where the features follow a multivariate Normal distribution. We demonstrate that dependence is manifested as random variation shared among features, and that standard methods may yield highly unstable inference due to dependence, even when the dependence: is fully parameterized and utilized in the procedure. We propose a ""cross-dimensional inference"" framework that alleviates the problems due to dependence by modeling and removing the variation shared among features, while also properly regularizing estimation across features. We demonstrate the framework on both simultaneous point estimation and multiple hypothesis testing in scenarios derived from the scientific applications of interest."
"10.1198/jasa.2011.ap10446","2012","Bias-corrected hierarchical {B}ayesian classification with a selected subset of high-dimensional features","0","Class prediction based on high-dimensional features has received a great deal of attention in many areas of application. For example, biologists are interested in using microarray gene expression profiles for diagnosis or prognosis of a certain disease (e.g., cancer). For computational and other reasons, it is necessary to select a subset of features before fitting a statistical model, by evaluating how strongly the features are related to the response. However, such a feature selection procedure will result in overconfident predictive probabilities for future cases, because the signal-to-noise ratio in the retained features is exacerbated by the feature selection. In this article we develop a hierarchical Bayesian classification method that can correct for this feature selection bias. Our method, which we term bias-corrected Bayesian classification with selected features (BCBCSF), uses the partial information from the feature selection procedure, in addition to the retained features, to form a correct (unbiased) posterior distribution of certain hyperparameters in the hierarchical Bayesian model that control the signal-to-noise ratio of the dataset. We take a Markov chain Monte Carlo (MCMC) approach to inferring the model parameters. We then use MCMC samples to make predictions for future cases. Because of the simplicity of the models, the inferred parameters from MCMC are easy to interpret, and the computation is very fast. Simulation studies and tests with two real microarray datasets related to complex human diseases show that our BCBCSF method provides better predictions than two widely used high-dimensional classification methods, prediction analysis for microarrays and diagonal linear discriminant analysis. The R package BCBCSF for the method described here is available from http://math.usask.ca/similar to longhai/software/BCBCSF and CRAN."
"10.1198/jasa.2011.ap09529","2012","M{RI} tissue classification using high-resolution {B}ayesian hidden {M}arkov normal mixture models","0","Magnetic resonance imaging (MRI) is used to identify the major tissues within a subject's brain. Classification is usually based on a single image providing one measurement for each volume element, or voxel, in a discretization of the brain. A simple model views each voxel as homogeneous, belonging entirely to one of the three major tissue types: gray matter, white matter, or cerebrospinal fluid. The measurements are normally distributed, with means and variances depending on the tissue types of their voxels. Because nearby voxels tend to be of the same tissue type, a Markov random field model can be used to capture the spatial similarity of voxels. A more realistic model takes into account the fact that some voxels are not homogeneous and contain tissues of more than one type. Our approach to this problem is to construct a higher-resolution image in which each voxel is divided into subvoxels and subvoxels are in turn assumed to be homogeneous and to follow a Markov random field model. In the present work we used a Bayesian hierarchical model to perform MRI tissue classification. Conditional independence was exploited to improve the speed of sampling. The subvoxel approach provides more accurate tissue classification and also allows more effective estimation of the proportion of major tissue types present in each voxel for both simulated and real datasets."
"10.1080/01621459.2011.644496","2012","Bayesian estimation and prediction for inhomogeneous spatiotemporal log-{G}aussian {C}ox processes using low-rank models, with application to criminal surveillance","0","In this article, we propose a method for conducting likelihood-based inference for a class of nonstationary spatiotemporal log-Gaussian Cox processes. The method uses convolution-based models to capture spatiotemporal correlation structure, is computationally feasible even for large datasets, and does not require knowledge of the underlying spatial intensity of the process. We describe an application to a surveillance system for detecting emergent spatiotemporal clusters of homicides in Belo Horizonte, Brazil, and discuss the advantages and drawbacks of our model-based approach by comparison with other spatiotemporal surveillance methods that have been proposed in the literature."
"10.1080/01621459.2011.643747","2012","Bayesian inference for dynamic treatment regimes: mobility, equity, and efficiency in student tracking","0","Policies in health, education, and economics often unfold sequentially and adapt to changing conditions. Such time-varying treatments pose problems for standard program evaluation methods because intermediate outcomes are simultaneously pretreatment confounders and posttreatment outcomes. This article extends the Bayesian perspective on causal inference and optimal treatment to these types of dynamic treatment regimes. A unifying idea remains ignorable treatment assignment, which now sequentially includes selection on intermediate outcomes. I present methods to estimate the causal effect of arbitrary regimes, recover the optimal regime, and characterize the set of feasible outcomes under different regimes. I demonstrate these methods through an application to optimal student tracking in ninth and tenth grade mathematics. For the sample considered, student mobility under the status-quo regime is significantly below the optimal rate and existing policies reinforce between-student inequality. An easy to implement optimal dynamic tracking regime, which promotes more students to honors in tenth grade, increases average final achievement to 0.07 standard deviations above the status quo while lowering inequality; there is no binding equity-efficiency tradeoff. The proposed methods provide a flexible and principled approach to causal inference for time-varying treatments and optimal treatment choice under uncertainty. This article has online supplementary material."
"10.1080/01621459.2011.643745","2012","Using conditional kernel density estimation for wind power density forecasting","0","Of the various renewable energy resources, wind power is widely recognized as one of the most promising. The management of wind farms and electricity systems can benefit greatly from the availability of estimates of the probability distribution of wind power generation. However, most research has focused on point forecasting of wind power. In this article, we develop an approach to producing density forecasts for the wind power generated at individual wind farms. Our interest is in intraday data and prediction from I to 72 hours ahead. We model wind power in terms of wind speed and wind direction. In this framework, there are two key uncertainties. First, there is the inherent uncertainty in wind speed and direction, and we model this using a bivariate vector autoregressive moving average-generalized autoregressive conditional heteroscedastic (VARMA-GARCH) model, with a Student t error distribution, in the Cartesian space of wind speed and direction. Second, there is the stochastic nature of the relationship of wind power to wind speed (described by the power curve), and to wind direction. We model this using conditional kernel density (CKD) estimation, which enables a nonparametric modeling of the conditional density of wind power. Using Monte Carlo simulation of the VARMA-GARCH model and CKD estimation, density forecasts of wind speed and direction are converted to wind power density forecasts. Our work is novel in several respects: previous wind power studies have not modeled a stochastic power curve; to accommodate time evolution in the power curve, we incorporate a time decay factor within the CKD method; and the CKD method is conditional on a density, rather than a single value. The new approach is evaluated using datasets from four Greek wind farms."
"10.1080/01621459.2011.643743","2012","Partially hidden {M}arkov model for time-varying principal stratification in {HIV} prevention trials","0","It is frequently of interest to estimate the intervention effect that adjusts for post-randomization variables in clinical trials. In the recently completed HPTN 035 trial, there is differential condom use between the three microbicide gel arms and the no-gel control arm, so intention-to-treat (ITT) analyses only assess the net treatment effect that includes the indirect treatment effect mediated through differential condom use. Various statistical methods in causal inference have been developed to adjust for post-randomization variables. We extend the principal stratification framework to time-varying behavioral variables in HIV prevention trials with a time-to-event endpoint, using a partially hidden Markov model (pHMM). We formulate the causal estimand of interest, establish assumptions that enable identifiability of the causal parameters, and develop maximum likelihood methods for estimation. Application of our model on the HPTN 035 trial reveals an interesting pattern of prevention effectiveness among different condom-use principal strata."
"10.1080/01621459.2011.643739","2012","Adjustment for missing confounders using external validation data and propensity scores","0","Reducing bias from missing confounders is a challenging problem in the analysis of observational data. Information about missing variables is sometimes available from external validation data, such as surveys or secondary samples drawn from the same source population. In principle, the validation data permit us to recover information about the missing data, but the difficulty is in eliciting a valid model for the nuisance distribution of the missing confounders. Motivated by a British study of the effects of trihalomethane exposure on risk of full-term low birthweight, we describe a flexible Bayesian procedure for adjusting for a vector of missing confounders using external validation data. We summarize the missing confounders with a scalar summary score using the propensity score methodology of Rosenbaum and Rubin. The score has the property that it induces conditional independence between the exposure and the missing confounders, given the measured confounders. It balances the unmeasured confounders across exposure groups, within levels of measured covariates. To adjust for bias, we need only model and adjust for the summary score during Markov chain Monte Carlo computation. Simulation results illustrate that the proposed method reduces bias from several missing confounders over a range of different sample sizes for the validation data. Appendices A C are available as online supplementary material."
"10.1080/01621459.2011.643732","2012","Modeling waves of extreme temperature: the changing tails of four cities","0","Heat waves are a serious threat to society, the environment, and the economy. Estimates of the recurrence probabilities of heat waves may be obtained following the successful modeling of daily maximum temperature, but working with the latter is difficult as we have to recognize, and allow for, not only a time trend but also seasonality in the mean and in the variability, as well as serial correlation. Furthermore, as the extreme values of daily maximum temperature have a different form of nonstationarity from the body, additional modeling is required to completely capture the realities. We present a time series model for the daily maximum temperature and use an exceedance over high thresholds approach to model the upper tail of the distribution of its scaled residuals. We show how a change-point analysis can be used to identify seasons of constant crossing rates and how a time-dependent shape parameter can then be introduced to capture a change in the distribution of the exceedances. Daily maximum temperature series for Des Moines, New York, Portland, and Tucson are analyzed. In-sample and out-of-sample goodness-of-fit measures show that the proposed model is an excellent fit to the data. The fitted model is then used to estimate the recurrence probabilities of runs over seasonally high temperatures, and we show that the probability of long and intense heat waves has increased considerably over 50 years. We also find that the increases vary by city and by time of year."
"10.1080/01621459.2011.643710","2012","Intrinsic regression models for medial representation of subcortical structures","0","The aim of this article is to develop a semiparametric model to describe the variability of the medial representation of subcortical structures, which belongs to a Riemannian manifold, and establish its association with covariates of interest, such as diagnostic status, age, and gender. We develop a two-stage estimation procedure to calculate the parameter estimates. The first stage is to calculate an intrinsic least squares estimator of the parameter vector using the annealing evolutionary stochastic approximation Monte Carlo algorithm, and then the second stage is to construct a set of estimating equations to obtain a more efficient estimate with the intrinsic least squares estimate as the starting point. We use Wald statistics to test linear hypotheses of unknown parameters and establish their limiting distributions. Simulation studies are used to evaluate the accuracy of our parameter estimates and the finite sample performance of the Wald statistics. We apply our methods to the detection of the difference in the morphological changes of the left and right hippocampi between schizophrenia patients and healthy controls using a medial shape description. This article has online supplementary material."
"10.1080/01621459.2011.643707","2012","Nonparametric covariate-adjusted association tests based on the generalized {K}endall's tau","0","Identifying the risk factors for comorbidity is important in psychiatric research. Empirically, studies have shown that testing multiple correlated traits simultaneously is more powerful than testing a single trait at a time in association analysis. Furthermore, for complex diseases, especially mental illnesses and behavioral disorders, the traits are often recorded in different scales, such as dichotomous, ordinal, and quantitative. In the absence of covariates, nonparametric association tests have been developed for multiple complex traits to study comorbidity. However, genetic studies generally contain measurements of some covariates that may affect the relationship between the risk factors of major interest (such as genes) and the outcomes. While it is relatively easy to adjust for these covariates in a parametric model for quantitative traits, it is challenging to adjust for covariates when there are multiple complex traits with possibly different scales. In this article, we propose a nonparametric test for multiple complex traits that can adjust for covariate effects. The test aims to achieve an optimal scheme of adjustment by using a maximum statistic calculated from multiple adjusted test statistics. We derive the asymptotic null distribution of the maximum test statistic and also propose a resampling approach, both of which can be used to assess the significance of our test. Simulations are conducted to compare the Type I error and power of the nonparametric adjusted test to the unadjusted test and other existing adjusted tests. The empirical results suggest that our proposed test increases the power through adjustment for covariates when there exist environmental effects and is more robust to model misspecifications than some existing parametric adjusted tests. We further demonstrate the advantage of our test by analyzing a dataset on genetics of alcoholism."
"10.1111/j.1467-9868.2012.01029.x","2012","A road to classification in high dimensional space: the regularized optimal affine discriminant","0",". For high dimensional classification, it is well known that naively performing the Fisher discriminant rule leads to poor results due to diverging spectra and accumulation of noise. Therefore, researchers proposed independence rules to circumvent the diverging spectra, and sparse independence rules to mitigate the issue of accumulation of noise. However, in biological applications, often a group of correlated genes are responsible for clinical outcomes, and the use of the covariance information can significantly reduce misclassification rates. In theory the extent of such error rate reductions is unveiled by comparing the misclassification rates of the Fisher discriminant rule and the independence rule. To materialize the gain on the basis of finite samples, a regularized optimal affine discriminant (ROAD) is proposed. The ROAD selects an increasing number of features as the regularization relaxes. Further benefits can be achieved when a screening method is employed to narrow the feature pool before applying the ROAD method. An efficient constrained co-ordinate descent algorithm is also developed to solve the associated optimization problems. Sampling properties of oracle type are established. Simulation studies and real data analysis support our theoretical results and demonstrate the advantages of the new classification procedure under a variety of correlation structures. A delicate result on continuous piecewise linear solution paths for the ROAD optimization problem at the population level justifies the linear interpolation of the constrained co-ordinate descent algorithm."
"10.1111/j.1467-9868.2011.01027.x","2012","Inference with transposable data: modelling the effects of row and column correlations","0",". We consider the problem of large-scale inference on the row or column variables of data in the form of a matrix. Many of these data matrices are transposable meaning that neither the row variables nor the column variables can be considered independent instances. An example of this scenario is detecting significant genes in microarrays when the samples may be dependent because of latent variables or unknown batch effects. By modelling this matrix data by using the matrix variate normal distribution, we study and quantify the effects of row and column correlations on procedures for large-scale inference. We then propose a simple solution to the myriad of problems that are presented by unexpected correlations: we simultaneously estimate row and column covariances and use these to sphere or decorrelate the noise in the underlying data before conducting inference. This procedure yields data with approximately independent rows and columns so that test statistics more closely follow null distributions and multiple-testing procedures correctly control the desired error rates. Results on simulated models and real microarray data demonstrate major advantages of this approach: increased statistical power, less bias in estimating the false discovery rate and reduced variance of the false discovery rate estimators."
"10.1111/j.1467-9868.2011.01022.x","2012","Local polynomial regression for symmetric positive definite matrices","0",". Local polynomial regression has received extensive attention for the non-parametric estimation of regression functions when both the response and the covariate are in Euclidean space. However, little has been done when the response is in a Riemannian manifold. We develop an intrinsic local polynomial regression estimate for the analysis of symmetric positive definite matrices as responses that lie in a Riemannian manifold with covariate in Euclidean space. The primary motivation and application of the methodology proposed is in computer vision and medical imaging. We examine two commonly used metrics, including the trace metric and the log-Euclidean metric on the space of symmetric positive definite matrices. For each metric, we develop a cross-validation bandwidth selection method, derive the asymptotic bias, variance and normality of the intrinsic local constant and local linear estimators, and compare their asymptotic mean-square errors. Simulation studies are further used to compare the estimators under the two metrics and to examine their finite sample performance. We use our method to detect diagnostic differences between diffusion tensors along fibre tracts in a study of human immunodeficiency virus."
"10.1111/j.1467-9868.2011.01021.x","2012","The relative frailty variance and shared frailty models","0",". The relative frailty variance among survivors provides a readily interpretable measure of how the heterogeneity of a population, as represented by a frailty model, evolves over time. We discuss the properties of the relative frailty variance, show that it characterizes frailty distributions and that, suitably rescaled, it may be used to compare patterns of dependence across models and data sets. In shared frailty models, the relative frailty variance is closely related to the cross-ratio function, which is estimable from bivariate survival data. We investigate the possible shapes of the relative frailty variance function for the purpose of model selection, and we review available frailty distribution families in this context. We introduce several new families with contrasting properties, including simple but flexible time varying frailty models. The benefits of the approach that we propose are illustrated with two applications to bivariate current status data obtained from serological surveys."
"10.1111/j.1467-9868.2011.01020.x","2012","Probabilistic index models","0",". We present a semiparametric statistical model for the probabilistic index which can be defined as P(YY*), where Y and Y* are independent random response variables associated with covariate patterns X and X* respectively. A link function defines the relationship between the probabilistic index and a linear predictor. Asymptotic normality of the estimators and consistency of the covariance matrix estimator are established through semiparametric theory. The model is illustrated with several examples, and the estimation theory is validated in a simulation study."
"10.1111/j.1467-9868.2011.01023.x","2012","High dimensional variable selection via tilting","0",". The paper considers variable selection in linear regression models where the number of covariates is possibly much larger than the number of observations. High dimensionality of the data brings in many complications, such as (possibly spurious) high correlations between the variables, which result in marginal correlation being unreliable as a measure of association between the variables and the response. We propose a new way of measuring the contribution of each variable to the response which takes into account high correlations between the variables in a data-driven way. The proposed tilting procedure provides an adaptive choice between the use of marginal correlation and tilted correlation for each variable, where the choice is made depending on the values of the hard thresholded sample correlation of the design matrix. We study the conditions under which this measure can successfully discriminate between the relevant and the irrelevant variables and thus be used as a tool for variable selection. Finally, an iterative variable screening algorithm is constructed to exploit the theoretical properties of tilted correlation, and its good practical performance is demonstrated in a comparative simulation study."
"10.1111/j.1467-9868.2011.01018.x","2012","The phylogenetic {K}antorovich-{R}ubinstein metric for environmental sequence samples","0",". It is now common to survey microbial communities by sequencing nucleic acid material extracted in bulk from a given environment. Comparative methods are needed that indicate the extent to which two communities differ given data sets of this type. UniFrac, which gives a somewhat ad hoc phylogenetics-based distance between two communities, is one of the most commonly used tools for these analyses. We provide a foundation for such methods by establishing that, if we equate a metagenomic sample with its empirical distribution on a reference phylogenetic tree, then the weighted UniFrac distance between two samples is just the classical KantorovichRubinstein, or earth mover's, distance between the corresponding empirical distributions. We demonstrate that this KantorovichRubinstein distance and extensions incorporating uncertainty in the sample locations can be written as a readily computable integral over the tree, we develop Lp Zolotarev-type generalizations of the metric, and we show how the p-value of the resulting natural permutation test of the null hypothesis no difference between two communities can be approximated by using a Gaussian process functional. We relate the L2-case to an analysis-of-variance type of decomposition, finding that the distribution of its associated Gaussian functional is that of a computable linear combination of independent random variables."
"10.1111/j.1467-9868.2011.01017.x","2012","Likelihood-based procedures for threshold diagnostics and uncertainty in extreme value modelling","0",". For extreme value modelling based on threshold techniques, a well-documented issue is the sensitivity of inference from the model to the choice of threshold. The threshold above which we assume a non-homogeneous Poisson process, or equivalently generalized Pareto representation, to be a reasonable approximation to the distribution is traditionally selected before analysis and subsequently treated as fixed and known. In doing so, the analyst cannot account for the subjective judgement that has already taken place before formal inference begins. We propose an asymptotically motivated model to account for uncertainty in choice of threshold, under assumptions generated by a penultimate form of extreme value theory. To assess the sensitivity of the conclusions to these assumptions, we additionally present a purely likelihood-based diagnostic for the choice of threshold, developing a non-standard likelihood ratio test which supplements the current suite of tools. We show that the likelihood ratio procedure quantifies evidence derived from traditional threshold diagnostic plots, and that the full model for threshold uncertainty identifies the same features as the diagnostic. We apply our procedures to both simulated data, and a data set of flow rates from the River Nidd."
"10.1111/j.1467-9868.2011.01016.x","2012","Adjusted {B}ayesian inference for selected parameters","0",". We address the problem of providing inference from a Bayesian perspective for parameters selected after viewing the data. We present a Bayesian framework for providing inference for selected parameters, based on the observation that providing Bayesian inference for selected parameters is a truncated data problem. We show that if the prior for the parameter is non-informative, or if the parameter is a fixed unknown constant, then it is necessary to adjust the Bayesian inference for selection. Our second contribution is the introduction of Bayesian false discovery rate controlling methodology, which generalizes existing Bayesian false discovery rate methods that are only defined in the two-group mixture model. We illustrate our results by applying them to simulated data and data from a microarray experiment."
"10.1111/j.1467-9868.2011.01024.x","2012","Learning out of leaders","0",". The paper investigates the estimation problem in a regression-type model. To be able to deal with potential high dimensions, we provide a procedure called LOLfor learning out of leaderswith no optimization step. LOL is an autodriven algorithm with two thresholding steps. A first adaptive thresholding helps to select leaders among the initial regressors to obtain a first reduction of dimensionality. Then a second thresholding is performed on the linear regression on the leaders. The consistency of the procedure is investigated. Exponential bounds are obtained, leading to minimax and adaptive results for a wide class of sparse parameters, with (quasi) no restriction on the number p of possible regressors. An extensive computational experiment is conducted to emphasize the practical good performances of LOL."
"10.1111/j.1467-9868.2011.01010.x","2012","Constructing summary statistics for approximate {B}ayesian computation: semi-automatic approximate {B}ayesian computation","0",". Many modern statistical applications involve inference for complex stochastic models, where it is easy to simulate from the models, but impossible to calculate likelihoods. Approximate Bayesian computation (ABC) is a method of inference for such models. It replaces calculation of the likelihood by a step which involves simulating artificial data for different parameter values, and comparing summary statistics of the simulated data with summary statistics of the observed data. Here we show how to construct appropriate summary statistics for ABC in a semi-automatic manner. We aim for summary statistics which will enable inference about certain parameters of interest to be as accurate as possible. Theoretical results show that optimal summary statistics are the posterior means of the parameters. Although these cannot be calculated analytically, we use an extra stage of simulation to estimate how the posterior means vary as a function of the data; and we then use these estimates of our summary statistics within ABC. Empirical results show that our approach is a robust method for choosing summary statistics that can result in substantially more accurate ABC analyses than the ad hoc choices of summary statistics that have been proposed in the literature. We also demonstrate advantages over two alternative methods of simulation-based inference."
"10.1111/j.1467-9868.2011.01025.x","2012","Catching up faster by switching sooner: a predictive approach to adaptive estimation with an application to the {AIC}-{BIC} dilemma","0",". Prediction and estimation based on Bayesian model selection and model averaging, and derived methods such as the Bayesian information criterion BIC, do not always converge at the fastest possible rate. We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian methods, which inspires a modification of the Bayesian predictive distribution, called the switch distribution. When used as an adaptive estimator, the switch distribution does achieve optimal cumulative risk convergence rates in non-parametric density estimation and Gaussian regression problems. We show that the minimax cumulative risk is obtained under very weak conditions and without knowledge of the underlying degree of smoothness. Unlike other adaptive model selection procedures such as the Akaike information criterion AIC and leave-one-out cross-validation, BIC and Bayes factor model selection are typically statistically consistent. We show that this property is retained by the switch distribution, which thus solves the AICBIC dilemma for cumulative risk. The switch distribution has an efficient implementation. We compare its performance with AIC, BIC and Bayesian model selection and averaging on a regression problem with simulated data."
"10.1111/j.1467-9868.2011.01014.x","2012","Fast subset scan for spatial pattern detection","0",". We propose a new fast subset scan approach for accurate and computationally efficient event detection in massive data sets. We treat event detection as a search over subsets of data records, finding the subset which maximizes some score function. We prove that many commonly used functions (e.g. Kulldorff's spatial scan statistic and extensions) satisfy the linear time subset scanning property, enabling exact and efficient optimization over subsets. In the spatial setting, we demonstrate that proximity-constrained subset scans substantially improve the timeliness and accuracy of event detection, detecting emerging outbreaks of disease 2 days faster than existing methods."
"10.1111/j.1467-9868.2011.01013.x","2012","The dynamic `expectation-conditional maximization either' algorithm","0",". The expectationconditional maximization either (ECME) algorithm has proven to be an effective way of accelerating the expectationmaximization algorithm for many problems. Recognizing the limitation of using prefixed acceleration subspaces in the ECME algorithm, we propose a dynamic ECME (DECME) algorithm which allows the acceleration subspaces to be chosen dynamically. The simplest DECME implementation is what we call DECME-1, which uses the line that is determined by the two most recent estimates as the acceleration subspace. The investigation of DECME-1 leads to an efficient, simple, stable and widely applicable DECME implementation, which uses two-dimensional acceleration subspaces and is referred to as DECME-2. The fast convergence of DECME-2 is established by the theoretical result that, in a small neighbourhood of the maximum likelihood estimate, it is equivalent to a conjugate direction method. The remarkable accelerating effect of DECME-2 and its variant is also demonstrated with several numerical examples."
"10.1111/j.1467-9868.2011.01015.x","2012","Local shrinkage rules, {L}\'evy processes and regularized regression","0",". We use Levy processes to generate joint prior distributions, and therefore penalty functions, for a location parameter as p grows large. This generalizes the class of localglobal shrinkage rules based on scale mixtures of normals, illuminates new connections between disparate methods and leads to new results for computing posterior means and modes under a wide class of priors. We extend this framework to large-scale regularized regression problems where p>n, and we provide comparisons with other methodologies."
"10.1111/j.1467-9868.2011.01003.x","2012","Achieving near perfect classification for functional data","1",". We show that, in functional data classification problems, perfect asymptotic classification is often possible, making use of the intrinsic very high dimensional nature of functional data. This performance is often achieved by linear methods, which are optimal in important cases. These results point to a marked contrast between classification for functional data and its counterpart in conventional multivariate analysis, where the dimension is kept fixed as the sample size diverges. In the latter setting, linear methods can sometimes be quite inefficient, and there are no prospects for asymptotically perfect classification, except in pathological cases where, for example, a variance vanishes. By way of contrast, in finite samples of functional data, good performance can be achieved by truncated versions of linear methods. Truncation can be implemented by partial least squares or projection onto a finite number of principal components, using, in both cases, cross-validation to determine the truncation point. We establish consistency of the cross-validation procedure."
"10.1111/j.1467-9868.2011.01004.x","2012","Strong rules for discarding predictors in lasso-type problems","0",". We consider rules for discarding predictors in lasso regression and related problems, for computational efficiency. El Ghaoui and his colleagues have proposed SAFE rules, based on univariate inner products between each predictor and the outcome, which guarantee that a coefficient will be 0 in the solution vector. This provides a reduction in the number of variables that need to be entered into the optimization. We propose strong rules that are very simple and yet screen out far more predictors than the SAFE rules. This great practical improvement comes at a price: the strong rules are not foolproof and can mistakenly discard active predictors, i.e. predictors that have non-zero coefficients in the solution. We therefore combine them with simple checks of the KarushKuhnTucker conditions to ensure that the exact solution to the convex problem is delivered. Of course, any (approximate) screening method can be combined with the KarushKuhnTucker conditions to ensure the exact solution; the strength of the strong rules lies in the fact that, in practice, they discard a very large number of the inactive predictors and almost never commit mistakes. We also derive conditions under which they are foolproof. Strong rules provide substantial savings in computational time for a variety of statistical optimization problems."
"10.1111/j.1467-9868.2011.01011.x","2012","Semiparametric tests for sufficient cause interaction","1",". A sufficient cause interaction between two exposures signals the presence of individuals for whom the outcome would occur only under certain values of the two exposures. When the outcome is dichotomous and all exposures are categorical, then, under certain no confounding assumptions, empirical conditions for sufficient cause interactions can be constructed on the basis of the sign of linear contrasts of conditional outcome probabilities between differently exposed subgroups, given confounders. It is argued that logistic regression models are unsatisfactory for evaluating such contrasts, and that Bernoulli regression models with linear link are prone to misspecification. We therefore develop semiparametric tests for sufficient cause interactions under models which postulate probability contrasts in terms of a finite dimensional parameter, but which are otherwise unspecified. Estimation is often not feasible in these models because it would require non-parametric estimation of auxiliary conditional expectations given high dimensional variables. We therefore develop multiply robust tests under a union model which assumes that at least one of several working submodels holds. In the special case of a randomized experiment or a family-based genetic study in which the joint exposure distribution is known by design or Mendelian inheritance, the procedure leads to asymptotically distribution-free tests of the null hypothesis of no sufficient cause interaction."
"10.1111/j.1467-9868.2011.01002.x","2012","Reduced rank stochastic regression with a sparse singular value decomposition","0",". For a reduced rank multivariate stochastic regression model of rank r*, the regression coefficient matrix can be expressed as a sum of r* unit rank matrices each of which is proportional to the outer product of the left and right singular vectors. For improving predictive accuracy and facilitating interpretation, it is often desirable that these left and right singular vectors be sparse or enjoy some smoothness property. We propose a regularized reduced rank regression approach for solving this problem. Computation algorithms and regularization parameter selection methods are developed, and the properties of the new method are explored both theoretically and by simulation. In particular, the regularization method proposed is shown to be selection consistent and asymptotically normal and to enjoy the oracle property. We apply the proposed model to perform biclustering analysis with microarray gene expression data."
"10.1111/j.1467-9868.2011.01012.x","2012","Cumulative incidence association models for bivariate competing risks data","0",". Association models, like frailty and copula models, are frequently used to analyse clustered survival data and to evaluate within-cluster associations. The assumption of non-informative censoring is commonly applied to these models, though it may not be true in many situations. We consider bivariate competing risk data and focus on association models specified for the bivariate cumulative incidence function (CIF), which is a non-parametrically identifiable quantity. Copula models are proposed which relate the bivariate CIF to its corresponding univariate CIFs, similarly to independently right-censored data, and accommodate frailty models for the bivariate CIF. Two estimating equations are developed to estimate the association parameter, permitting the univariate CIFs to be estimated either parametrically or non-parametrically. Goodness-of-fit tests are presented for formally evaluating the parametric models. Both estimators perform well with moderate sample sizes in simulation studies. The practical use of the methodology is illustrated in an analysis of dementia associations."
"10.1111/j.1467-9868.2011.01001.x","2012","Adaptive and dynamic adaptive procedures for false discovery rate control and estimation","0",". Many methods for estimation or control of the false discovery rate (FDR) can be improved by incorporating information about p0, the proportion of all tested null hypotheses that are true. Estimates of p0 are often based on the number of p-values that exceed a threshold ?. We first give a finite sample proof for conservative point estimation of the FDR when the ?-parameter is fixed. Then we establish a condition under which a dynamic adaptive procedure, whose ?-parameter is determined by data, will lead to conservative p0- and FDR estimators. We also present asymptotic results on simultaneous conservative FDR estimation and control for a class of dynamic adaptive procedures. Simulation results show that a novel dynamic adaptive procedure achieves more power through smaller estimation errors for p0 under independence and mild dependence conditions. We conclude by discussing the connection between estimation and control of the FDR and show that several recently developed FDR control procedures can be cast in a unifying framework where the strength of the procedures can be easily evaluated."
"10.1111/j.1467-9868.2011.01000.x","2012","Control variates for estimation based on reversible {M}arkov chain {M}onte {C}arlo samplers","0",". A general methodology is introduced for the construction and effective application of control variates to estimation problems involving data from reversible Markov chain Monte Carlo samplers. We propose the use of a specific class of functions as control variates, and we introduce a new consistent estimator for the values of the coefficients of the optimal linear combination of these functions. For a specific Markov chain Monte Carlo scenario, the form and proposed construction of the control variates is shown to provide an exact solution of the associated Poisson equation. This implies that the estimation variance in this case (in the central limit theorem regime) is exactly zero. The new estimator is derived from a novel, finite dimensional, explicit representation for the optimal coefficients. The resulting variance reduction methodology is primarily (though certainly not exclusively) applicable when the simulated data are generated by a random-scan Gibbs sampler. Markov chain Monte Carlo examples of Bayesian inference problems demonstrate that the corresponding reduction in the estimation variance is significant, and that in some cases it can be quite dramatic. Extensions of this methodology are discussed and simulation examples are presented illustrating the utility of the methods proposed. All methodological and asymptotic arguments are rigorously justified under essentially minimal conditions."
"10.1111/j.1467-9868.2011.01007.x","2012","A full scale approximation of covariance functions for large spatial data sets","0",". Gaussian process models have been widely used in spatial statistics but face tremendous computational challenges for very large data sets. The model fitting and spatial prediction of such models typically require O(n3) operations for a data set of size n. Various approximations of the covariance functions have been introduced to reduce the computational cost. However, most existing approximations cannot simultaneously capture both the large- and the small-scale spatial dependence. A new approximation scheme is developed to provide a high quality approximation to the covariance function at both the large and the small spatial scales. The new approximation is the summation of two parts: a reduced rank covariance and a compactly supported covariance obtained by tapering the covariance of the residual of the reduced rank approximation. Whereas the former part mainly captures the large-scale spatial variation, the latter part captures the small-scale, local variation that is unexplained by the former part. By combining the reduced rank representation and sparse matrix techniques, our approach allows for efficient computation for maximum likelihood estimation, spatial prediction and Bayesian inference. We illustrate the new approach with simulated and real data sets."
"10.1111/j.1467-9868.2011.01006.x","2012","Hybrid confidence regions based on data depth","0",". We consider the general problem of constructing confidence regions for, possibly multi-dimensional, parameters when we have available more than one approach for the construction. These approaches may be motivated by different model assumptions, different levels of approximation, different settings of tuning parameters or different Monte Carlo algorithms. Their effectiveness is often governed by different sets of conditions which are difficult to vindicate in practice. We propose two procedures for constructing hybrid confidence regions which endeavour to integrate all such individual approaches. The procedures employ the concept of data depth to calibrate the confidence region in two different ways, the first rendering its coverage error minimax and the second rendering its coverage error conservative. The resulting region reconciles in many important aspects the discrepancies between the various approaches, and is robust against misspecification of their governing conditions. Theoretical and empirical properties of our procedures are investigated in comparison with those of the constituent individual approaches."
"10.1111/j.1467-9868.2011.01008.x","2012","Conditional quantile analysis when covariates are functions, with application to growth data","0",". Motivated by the conditional growth charts problem, we develop a method for conditional quantile analysis when predictors take values in a functional space. The method proposed aims at estimating conditional distribution functions under a generalized functional regression framework. This approach facilitates balancing of model flexibility and the curse of dimensionality for the infinite dimensional functional predictors. Its good performance in comparison with other methods, both for sparsely and for densely observed functional covariates, is demonstrated through theory as well as in simulations and an application to growth curves, where the method proposed can, for example, be used to assess the entire growth pattern of a child by relating it to the predicted quantiles of adult height."
"10.1111/j.1467-9868.2011.01005.x","2012","Variance estimation using refitted cross-validation in ultrahigh dimensional regression","0",". Variance estimation is a fundamental problem in statistical modelling. In ultrahigh dimensional linear regression where the dimensionality is much larger than the sample size, traditional variance estimation techniques are not applicable. Recent advances in variable selection in ultrahigh dimensional linear regression make this problem accessible. One of the major problems in ultrahigh dimensional regression is the high spurious correlation between the unobserved realized noise and some of the predictors. As a result, the realized noises are actually predicted when extra irrelevant variables are selected, leading to a serious underestimate of the level of noise. We propose a two-stage refitted procedure via a data splitting technique, called refitted cross-validation, to attenuate the influence of irrelevant variables with high spurious correlations. Our asymptotic results show that the resulting procedure performs as well as the oracle estimator, which knows in advance the mean regression function. The simulation studies lend further support to our theoretical claims. The naive two-stage estimator and the plug-in one-stage estimators using the lasso and smoothly clipped absolute deviation are also studied and compared. Their performances can be improved by the refitted cross-validation method proposed."
"10.1111/j.1467-9868.2011.01009.x","2012","New consistent and asymptotically normal parameter estimates for random-graph mixture models","0",". Random-graph mixture models are very popular for modelling real data networks. Parameter estimation procedures usually rely on variational approximations, either combined with the expectationmaximization (EM) algorithm or with Bayesian approaches. Despite good results on synthetic data, the validity of the variational approximation is, however, not established. Moreover, these variational approaches aim at approximating the maximum likelihood or the maximum a posteriori estimators, whose behaviour in an asymptotic framework (as the sample size increases to 8) remains unknown for these models. In this work, we show that, in many different affiliation contexts (for binary or weighted graphs), parameter estimators based either on moment equations or on the maximization of some composite likelihood are strongly consistent and vn convergent, when the number n of nodes increases to 8. As a consequence, our result establishes that the overall structure of an affiliation model can be (asymptotically) caught by the description of the network in terms of its number of triads (order 3 structures) and edges (order 2 structures). Moreover, these parameter estimates are either explicit (as for the moment estimators) or may be approximated by using a simple EM algorithm, whose convergence properties are known. We illustrate the efficiency of our method on simulated data and compare its performances with other existing procedures. A data set of cross-citations among economics journals is also analysed."
"10.1111/j.1467-9868.2011.00775.x","2011","Data-driven density estimation in the presence of additive noise with unknown distribution","0","We study the model Y = X + epsilon. We assume that we have at our disposal independent identically distributed observations Y(1), ... ,Y(n) and epsilon(-1), ... ,epsilon(-M). The (X(j))(1 <= j <= n) are independent identically distributed with density f, independent of the (epsilon(j))(1 <= j <= n), independent identically distributed with density f(epsilon). The aim of the paper is to estimate f without knowing f(epsilon). We first define an estimator, for which we provide bounds for the integrated L(2)-risk. We consider ordinary smooth and supersmooth noise epsilon with regard to ordinary smooth and supersmooth densities f. Then we present an adaptive estimator of the density of f. This estimator is obtained by penalization of a projection contrast and yields to model selection. Lastly, we present simulation experiments to illustrate the good performances of our estimator and study from the empirical point of view the importance of theoretical constraints."
"10.1111/j.1467-9868.2011.00776.x","2011","Bayesian smoothing of photon-limited images with applications in astronomy","0","We consider a multiscale model for intensities in photon-limited images using a Bayesian framework. A typical Dirichlet prior on relative intensities is not efficient in picking up structures owing to the continuity of intensities. We propose a novel prior using the so-called 'Chinese restaurant process' to create structures in the form of equal intensities of some neighbouring pixels. Simulations are conducted using several photon-limited images, which are common in X-ray astronomy and other high energy photon-based images. Applications to astronomical images from the Chandra X-ray Observatory satellite are shown. The new methodology outperforms most existing methods in terms of image processing quality, speed and the ability to select smoothing parameters automatically."
"10.1111/j.1467-9868.2010.00767.x","2011","Multiscale adaptive regression models for neuroimaging data","0","Neuroimaging studies aim to analyse imaging data with complex spatial patterns in a large number of locations (called voxels) on a two-dimensional surface or in a three-dimensional volume. Conventional analyses of imaging data include two sequential steps: spatially smoothing imaging data and then independently fitting a statistical model at each voxel. However, conventional analyses suffer from the same amount of smoothing throughout the whole image, the arbitrary choice of extent of smoothing and low statistical power in detecting spatial patterns. We propose a multiscale adaptive regression model to integrate the propagation-separation approach with statistical modelling at each voxel for spatial and adaptive analysis of neuroimaging data from multiple subjects. The multiscale adaptive regression model has three features: being spatial, being hierarchical and being adaptive. We use a multiscale adaptive estimation and testing procedure to utilize imaging observations from the neighbouring voxels of the current voxel to calculate parameter estimates and test statistics adaptively. Theoretically, we establish consistency and asymptotic normality of the adaptive parameter estimates and the asymptotic distribution of the adaptive test statistics. Our simulation studies and real data analysis confirm that the multiscale adaptive regression model significantly outperforms conventional analyses of imaging data."
"10.1111/j.1467-9868.2010.00768.x","2011","Adaptive inference for the mean of a {G}aussian process in functional data","0","We propose and analyse fully data-driven methods for inference about the mean function of a Gaussian process from a sample of independent trajectories of the process, observed at random time points and corrupted by additive random error. Our methods are based on thresholded least squares estimators relative to an approximating function basis. The variable threshold levels are determined from the data and the resulting estimates adapt to the unknown sparsity of the mean function relative to the approximating basis. These results are obtained via novel oracle inequalities, which are further used to derive the rates of convergence of our mean estimates. In addition, we construct confidence balls that adapt to the unknown regularity of the mean and covariance function of the stochastic process. They are easy to compute since they do not require explicit estimation of the covariance operator of the process. A simulation study shows that the new method performs very well in practice and is robust against large variations that may be introduced by the random-error terms."
"10.1111/j.1467-9868.2011.00773.x","2011","Thick pen transformation for time series","0","Traditional visualization of time series data often consists of plotting the time series values against time and 'connecting the dots'. We propose an alternative, multiscale visualization technique, motivated by the scale-space approach in computer vision. In brief, our method also 'connects the dots' but uses a range of pens of varying thicknesses for this. The resulting multiscale map, which is termed the thick pen transform, corresponds to viewing the time series from a range of distances. We formally prove that the thick pen transform is a discriminatory statistic for two Gaussian time series with distinct correlation structures. Further, we show interesting possible applications of the thick pen transform to measuring cross-dependence in multivariate time series, classifying time series and testing for stationarity. In particular, we derive the asymptotic distribution of our test statistic and argue that the test is applicable to both linear and non-linear processes under low moment assumptions. Various other aspects of the methodology, including other possible applications, are also discussed."
"10.1111/j.1467-9868.2011.00777.x","2011","An explicit link between {G}aussian fields and {G}aussian {M}arkov random fields: the stochastic partial differential equation approach","1","Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gaussian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in R-2 only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Matern class, provide an explicit link, for any triangulation of R-d, between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere."
"10.1111/j.1467-9868.2011.00782.x","2011","Estimation of direct effects for survival data by using the {A}alen additive hazards model","0","We extend the definition of the controlled direct effect of a point exposure on a survival outcome, other than through some given, time-fixed intermediate variable, to the additive hazard scale. We propose two-stage estimators for this effect when the exposure is dichotomous and randomly assigned and when the association between the intermediate variable and the survival outcome is confounded only by measured factors, which may themselves be affected by the exposure. The first stage of the estimation procedure involves assessing the effect of the intermediate variable on the survival outcome via Aalen's additive regression for the event time, given exposure, intermediate variable and confounders. The second stage involves applying Aalen's additive model, given the exposure alone, to a modified stochastic process (i.e. a modification of the observed counting process based on the first-stage estimates). We give the large sample properties of the estimator proposed and investigate its small sample properties by Monte Carlo simulation. A real data example is provided for illustration."
"10.1111/j.1467-9868.2011.00783.x","2011","Penalized classification using {F}isher's linear discriminant","1","We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high dimensional setting where p >> n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule that is obtained from LDA, since it involves all p features. We propose penalized LDA, which is a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach to optimize it efficiently when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L(1) and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high dimensional setting and explore their relationships with our proposal."
"10.1111/j.1467-9868.2011.00780.x","2011","Augmented designs to assess principal strata direct effects","0","Many studies involving causal questions are often concerned with understanding the causal pathways by which a treatment affects an outcome. Thus, the concept of 'direct' versus 'indirect' effects comes into play. We tackle the problem of disentangling direct and indirect effects by investigating new augmented experimental designs, where the treatment is randomized, and the mediating variable is not forced, but only randomly encouraged. There are two key features of our framework: we adopt a principal stratification approach, and we mainly focus on principal strata effects, avoiding involving a priori counterfactual outcomes. Using nonparametric identification strategies, we provide a set of assumptions, which allow us to identify partially the causal estimands of interest: the principal strata direct effects. Some examples are shown to illustrate our design and causal estimands of interest. Large sample bounds for the principal strata average direct effects are provided, and a simple hypothetical example is used to show how our augmented design can be implemented and how the bounds can be calculated. Finally our augmented design is compared and contrasted with a standard randomized design."
"10.1111/j.1467-9868.2011.00779.x","2011","Banded regularization of autocovariance matrices in application to parameter estimation and forecasting of time series","0","The paper addresses a 'large p-small n' problem in a time series framework and considers properties of banded regularization of an empirical autocovariance matrix of a time series process. Utilizing the banded autocovariance matrix enables us to fit a much longer auto-regressive AR(p) model to the observed data than typically suggested by the Akaike information criterion, while controlling how many parameters are to be estimated precisely and the level of accuracy. We present results on asymptotic consistency of banded autocovariance matrices under the Frobenius norm and provide a theoretical justification on optimal band selection by using cross-validation. Remarkably, the cross-validation loss function for banded prediction is related to the conditional mean-square prediction error and, thus, may be viewed as an alternative model selection criterion. The procedure proposed is illustrated by simulations and application to predicting the sea surface temperature index in the Nino 3.4 region."
"10.1111/j.1467-9868.2011.00781.x","2011","Asymptotic behaviour of the posterior distribution in overfitted mixture models","0","We study the asymptotic behaviour of the posterior distribution in a mixture model when the number of components in the mixture is larger than the true number of components: a situation which is commonly referred to as an overfitted mixture. We prove in particular that quite generally the posterior distribution has a stable and interesting behaviour, since it tends to empty the extra components. This stability is achieved under some restriction on the prior, which can be used as a guideline for choosing the prior. Some simulations are presented to illustrate this behaviour."
"10.1111/j.1467-9868.2011.00774.x","2011","Dynamic multiscale spatiotemporal models for {G}aussian areal data","0","We introduce a new class of dynamic multiscale models for spatiotemporal processes arising from Gaussian areal data. Specifically, we use nested geographical structures to decompose the original process into multiscale coefficients which evolve through time following state space equations. Our approach naturally accommodates data that are observed on irregular grids as well as heteroscedasticity. Moreover, we propose a multiscale spatiotemporal clustering algorithm that facilitates estimation of the nested geographical multiscale structure. In addition, we present a singular forward filter backward sampler for efficient Bayesian estimation. Our multiscale spatiotemporal methodology decomposes large data analysis problems into many smaller components and thus leads to scalable and highly efficient computational procedures. Finally, we illustrate the utility and flexibility of our dynamic multiscale framework through two spatiotemporal applications. The first example considers mortality ratios in the state of Missouri whereas the second example examines agricultural production in Espirito Santo State, Brazil."
"10.1111/j.1467-9868.2011.00778.x","2011","Optimal detection of heterogeneous and heteroscedastic mixtures","0","The problem of detecting heterogeneous and heteroscedastic Gaussian mixtures is considered. The focus is on how the parameters of heterogeneity, heteroscedasticity and proportion of non-null component influence the difficulty of the problem. We establish an explicit detection boundary which separates the detectable region where the likelihood ratio test is shown to detect the presence of non-null effects reliably from the undetectable region where no method can do so. In particular, the results show that the detection boundary changes dramatically when the proportion of non-null component shifts from the sparse regime to the dense regime. Furthermore, it is shown that the higher criticism test, which does not require specific information on model parameters, is optimally adaptive to the unknown degrees of heterogeneity and heteroscedasticity in both the sparse and the dense cases."
"10.1111/j.1467-9868.2011.00772.x","2011","Self-consistent method for density estimation","0","The estimation of a density profile from experimental data points is a challenging problem, which is usually tackled by plotting a histogram. Prior assumptions on the nature of the density, from its smoothness to the specification of its form, allow the design of more accurate estimation procedures, such as maximum likelihood. Our aim is to construct a procedure that makes no explicit assumptions, but still providing an accurate estimate of the density. We introduce the self-consistent estimate: the power spectrum of a candidate density is given, and an estimation procedure is constructed on the assumption, to be released a posteriori, that the candidate is correct. The self-consistent estimate is defined as a prior candidate density that precisely reproduces itself. Our main result is to derive the exact expression of the self-consistent estimate for any given data set, and to study its properties. Applications of the method require neither priors on the form of the density nor the subjective choice of parameters. A cutoff frequency, akin to a bin size or a kernel bandwidth, emerges naturally from the derivation. We apply the self-consistent estimate to artificial data generated from various distributions and show that it reaches the theoretical limit for the scaling of the square error with the size of the data set."
"10.1111/j.1467-9868.2010.00770.x","2011","Non-parametric {B}ayesian inference on bivariate extremes","0","The tail of a bivariate distribution function in the domain of attraction of a bivariate extreme value distribution may be approximated by that of its extreme value attractor. The extreme value attractor has margins that belong to a three-parameter family and a dependence structure which is characterized by a probability measure on the unit interval with mean equal to 1/2, which is called the spectral measure. Inference is done in a Bayesian framework using a censored likelihood approach. A prior distribution is constructed on an infinite dimensional model for this measure, the model being at the same time dense and computationally manageable. A trans-dimensional Markov chain Monte Carlo algorithm is developed and convergence to the posterior distribution is established. In simulations, the Bayes estimator for the spectral measure is shown to compare favourably with frequentist non-parametric estimators. An application to a data set of Danish fire insurance claims is provided."
"10.1111/j.1467-9868.2010.00766.x","2011","Regression for compositional data by using distributions defined on the hypersphere","0","Compositional data can be transformed to directional data by the square-root transformation and then modelled by using distributions defined on the hypersphere. One advantage of this approach is that zero components are catered for naturally in the models. The Kent distribution for directional data is a good candidate model because it has a sufficiently general covariance structure. We propose a new regression model which models the mean direction of the Kent distribution as a function of a vector of covariates. Our estimators can be regarded as asymptotic maximum likelihood estimators. We show that these estimators perform well and are suitable for typical compositional data sets, including those with some zero components."
"10.1111/j.1467-9868.2010.00764.x","2011","Penalized composite quasi-likelihood for ultrahigh dimensional variable selection","0","In high dimensional model selection problems, penalized least square approaches have been extensively used. The paper addresses the question of both robustness and efficiency of penalized model selection methods and proposes a data-driven weighted linear combination of convex loss functions, together with weighted L(1)-penalty. It is completely data adaptive and does not require prior knowledge of the error distribution. The weighted L(1)-penalty is used both to ensure the convexity of the penalty term and to ameliorate the bias that is caused by the L(1)-penalty. In the setting with dimensionality much larger than the sample size, we establish a strong oracle property of the method proposed that has both the model selection consistency and estimation efficiency for the true non-zero coefficients. As specific examples, we introduce a robust method of composite L(1)-L(2), and an optimal composite quantile method and evaluate their performance in both simulated and real data examples."
"10.1111/j.1467-9868.2010.00769.x","2011","Functional singular component analysis","0","Aiming at quantifying the dependence of pairs of functional data (X, Y), we develop the concept of functional singular value decomposition for covariance and functional singular component analysis, building on the concept of 'canonical expansion' of compact operators in functional analysis. We demonstrate the estimation of the resulting singular values, functions and components for the practically relevant case of sparse and noise-contaminated longitudinal data and provide asymptotic consistency results. Expanding bivariate functional data into singular functions emerges as a natural extension of the popular functional principal component analysis for single processes to the case of paired processes. A natural application of the functional singular value decomposition is a measure of functional correlation. Owing to the involvement of an inverse operation, most previously considered functional correlation measures are plagued by numerical instabilities and strong sensitivity to the choice of smoothing parameters. These problems are exacerbated for the case of sparse longitudinal data, on which we focus. The functional correlation measure that is derived from the functional singular value decomposition behaves well with respect to numerical stability and statistical error, as we demonstrate in a simulation study. Practical feasibility for applications to longitudinal data is illustrated with examples from a study on aging and on-line auctions."
"10.1111/j.1467-9868.2010.00761.x","2011","Robustness and accuracy of methods for high dimensional data analysis based on {S}tudent's {$t$}-statistic","0","Student's t-statistic is finding applications today that were never envisaged when it was introduced more than a century ago. Many of these applications rely on properties, e.g. robustness against heavy-tailed sampling distributions, that were not explicitly considered until relatively recently. We explore these features of the t-statistic in the context of its application to very high dimensional problems, including feature selection and ranking, the simultaneous testing of many different hypotheses and sparse, high dimensional signal detection. Robustness properties of the t-ratio are highlighted, and it is established that those properties are preserved under applications of the bootstrap. In particular, bootstrap methods correct for skewness and therefore lead to second-order accuracy, even in the extreme tails. Indeed, it is shown that the bootstrap and also the more popular but less accurate t-distribution and normal approximations are more effective in the tails than towards the middle of the distribution. These properties motivate new methods, e.g. bootstrap-based techniques for signal detection, that confine attention to the significant tail of a statistic."
"10.1111/j.1467-9868.2011.00771.x","2011","Regression shrinkage and selection via the lasso: a retrospective","0","In the paper I give a brief review of the basic idea and some history and then discuss some developments since the original paper on regression shrinkage and selection via the lasso."
"10.1111/j.1467-9868.2010.00762.x","2011","Efficient probabilistic forecasts for counts","0","Efficient probabilistic forecasts of integer-valued random variables are derived. The optimality is achieved by estimating the forecast distribution non-parametrically over a given broad model class and proving asymptotic (non-parametric) efficiency in that setting. The method is developed within the context of the integer auto-regressive class of models, which is a suitable class for any count data that can be interpreted as a queue, stock, birth-and-death process or branching process. The theoretical proofs of asymptotic efficiency are supplemented by simulation results that demonstrate the overall superiority of the non-parametric estimator relative to a misspecified parametric alternative, in large but finite samples. The method is applied to counts of stock market iceberg orders. A subsampling method is used to assess sampling variation in the full estimated forecast distribution and a proof of its validity is given."
"10.1111/j.1467-9868.2010.00757.x","2011","A geometric characterization of optimal designs for regression models with correlated observations","0","We consider the problem of optimal design of experiments for random-effects models, especially population models, where a small number of correlated observations can be taken on each individual, whereas the observations corresponding to different individuals are assumed to be uncorrelated. We focus on c-optimal design problems and show that the classical equivalence theorem and the famous geometric characterization of Elfving from the case of uncorrelated data can be adapted to the problem of selecting optimal sets of observations for the n individual patients. The theory is demonstrated by finding optimal designs for a linear model with correlated observations and a non-linear random-effects population model, which is commonly used in pharmaco-kinetics."
"10.1111/j.1467-9868.2010.00763.x","2011","Robustness of design in dose-response studies","0","We construct experimental designs for dose-response studies. The designs are robust against possibly misspecified link functions; for this they minimize the maximum mean-squared error of the estimated dose required to attain a response in 100p% of the target population. Here p might be one particular value-p=0.5 corresponds to ED50-estimation-or it might range over an interval of values of interest. The maximum of the mean-squared error is evaluated over a Kolmogorov neighbourhood of the fitted link. Both the maximum and the minimum must be evaluated numerically; the former is carried out by quadratic programming and the latter by simulated annealing."
"10.1111/j.1467-9868.2010.00765.x","2011","Riemann manifold {L}angevin and {H}amiltonian {M}onte {C}arlo methods","0","The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis-Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from http://www.ucl.ac.uk/statistics/research/rmhmc allows replication of all the results reported."
"10.1111/j.1467-9868.2010.00758.x","2011","Modelling non-homogeneous {P}oisson processes with almost periodic intensity functions","0","We propose a model for the analysis of non-stationary point processes with almost periodic rate of occurrence. The model deals with the arrivals of events which are unequally spaced and show a pattern of periodicity or almost periodicity, such as stock transactions and earthquakes. We model the rate of occurrence of a non-homogeneous Poisson process as the sum of sinusoidal functions plus a baseline. Consistent estimates of frequencies, phases and amplitudes which form the sinusoidal functions are constructed mainly by the Bartlett periodogram. The estimates are shown to be asymptotically normally distributed. Computational issues are discussed and it is shown that the frequency estimates must be resolved with order o(T-1) to guarantee the asymptotic unbiasedness and consistency of the estimates of phases and amplitudes, where T is the length of the observation period. The prediction of the next occurrence is carried out and the mean-squared prediction error is calculated by Monte Carlo integration. Simulation and real data examples are used to illustrate the theoretical results and the utility of the model."
"10.1111/j.1467-9868.2010.00751.x","2011","Local and omnibus goodness-of-fit tests in classical measurement error models","0","We consider functional measurement error models, i.e. models where covariates are measured with error and yet no distributional assumptions are made about the mismeasured variable. We propose and study a score-type local test and an orthogonal series-based, omnibus goodness-of-fit test in this context, where no likelihood function is available or calculated-i.e. all the tests are proposed in the semiparametric model framework. We demonstrate that our tests have optimality properties and computational advantages that are similar to those of the classical score tests in the parametric model framework. The test procedures are applicable to several semiparametric extensions of measurement error models, including when the measurement error distribution is estimated non-parametrically as well as for generalized partially linear models. The performance of the local score-type and omnibus goodness-of-fit tests is demonstrated through simulation studies and analysis of a nutrition data set."
"10.1111/j.1467-9868.2010.00759.x","2011","Inference on the primary parameter of interest with the aid of dimension reduction estimation","0","As high dimensional data become routinely available in applied sciences, sufficient dimension reduction has been widely employed and its research has received considerable attention. However, with the majority of sufficient dimension reduction methodology focusing on the dimension reduction step, complete analysis and inference after dimension reduction have yet to receive much attention. We couple the strategy of sufficient dimension reduction with a flexible semiparametric model. We concentrate on inference with respect to the primary variables of interest, and we employ sufficient dimension reduction to bring down the dimension of the regression effectively. Extensive simulations demonstrate the efficacy of the method proposed, and a real data analysis is presented for illustration."
"10.1111/j.1467-9868.2010.00756.x","2011","Bayesian non-parametric hidden {M}arkov models with applications in genomics","4","We propose a flexible non-parametric specification of the emission distribution in hidden Markov models and we introduce a novel methodology for carrying out the computations. Whereas current approaches use a finite mixture model, we argue in favour of an infinite mixture model given by a mixture of Dirichlet processes. The computational framework is based on auxiliary variable representations of the Dirichlet process and consists of a forward-backward Gibbs sampling algorithm of similar complexity to that used in the analysis of parametric hidden Markov models. The algorithm involves analytic marginalizations of latent variables to improve the mixing, facilitated by exchangeability properties of the Dirichlet process that we uncover in the paper. A by-product of this work is an efficient Gibbs sampler for learning Dirichlet process hierarchical models. We test the Monte Carlo algorithm proposed against a wide variety of alternatives and find significant advantages. We also investigate by simulations the sensitivity of the proposed model to prior specification and data-generating mechanisms. We apply our methodology to the analysis of genomic copy number variation. Analysing various real data sets we find significantly more accurate inference compared with state of the art hidden Markov models which use finite mixture emission distributions."
"10.1111/j.1467-9868.2010.00749.x","2011","Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models","0","Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton-Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection."
"10.1111/j.1467-9868.2009.00735.x","2010","Minimally informative prior distributions for non-parametric {B}ayesian analysis","0","We address the problem of how to conduct a minimally informative, non-parametric Bayesian analysis. The central question is how to devise a model so that the posterior distribution satisfies a few basic properties. The concept of 'local mass' provides the key to the development of the limiting Dirichlet process model. This model is then used to provide an engine for inference in the compound decision problem and for multiple-comparisons inference in a one-way analysis-of-variance setting. Our analysis in this setting may be viewed as a limit of the analyses that were developed by Escobar and by Gopalan and Berry. Computations for the analysis are described, and the predictive performance of the model is compared with that of mixture of Dirichlet processes models."
"10.1111/j.1467-9868.2009.00734.x","2010","Semiparametric marginal regression analysis for dependent competing risks under an assumed copula","0","Competing risks problems arise in many fields of science, where two or more types of event may occur on a subject, but only the event occurring first is observed together with its occurrence time, and other events are censored. The marginal and joint distributions of event times for competing risks cannot be identified from the observed data without assuming the relationship between events. The commonly adopted independent censoring assumption may be easily violated. An alternative is to assume that the joint distribution of event times follows a known copula, which is an explicit function of the marginal distributions. On the basis of the latter assumption, we consider marginal regression analysis for dependent competing risks, with the marginal regressions performed via semiparametric transformation models, including the proportional hazards and proportional odds models. We propose a non-parametric maximum likelihood analysis, which provides explicit expressions for the score functions and information matrix, and facilitates convenient computations. Large and finite sample properties are studied. We report an illustration with data from an acquired immune deficiency syndrome clinical trial where the censoring may be dependent. The proposal can be readily used as a sensitivity analysis for assessing effects of potential dependent censoring and can incorporate external information on the association of competing risks."
"10.1111/j.1467-9868.2009.00733.x","2010","Simultaneous estimation and reduction of nonconformity in interlaboratory studies","0","Several procedures that are designed to reduce nonconformity in interlaboratory studies by shrinking data towards a consensus weighted mean are suggested. Some of them are shown to have a smaller quadratic risk than the vector sample means. Shrinkage towards a weighted mean in a random-effects model and a statistic appearing in models which allow for systematic errors are also considered. The results are illustrated by two examples of collaborative studies."
"10.1111/j.1467-9868.2009.00732.x","2010","Likelihood for statistically equivalent models","0","In likelihood inference we usually assume that the model is fixed and then base inference on the corresponding likelihood function. Often, however, the choice of model is rather arbitrary, and there may be other models which fit the data equally well. We study robustness of likelihood inference over such 'statistically equivalent' models and suggest a simple 'envelope likelihood' to capture this aspect of model uncertainty. Robustness depends critically on how we specify the parameter of interest. Some asymptotic theory is presented, illustrated by three examples."
"10.1111/j.1467-9868.2009.00731.x","2010","Local additive estimation","0","Additive models are popular in high dimensional regression problems owing to their flexibility in model building and optimality in additive function estimation. Moreover, they do not suffer from the so-called curse of dimensionality generally arising in non-parametric regression settings. Less known is the model bias that is incurred from the restriction to the additive class of models. We introduce a new class of estimators that reduces additive model bias, yet preserves some stability of the additive estimator. The new estimator is constructed by localizing the additivity assumption and thus is named the local additive estimator. It follows the spirit of local linear estimation but is shown to be able to relieve partially the dimensionality problem. Implementation can be easily made with any standard software for additive regression. For detailed analysis we explicitly use the smooth backfitting estimator of Mammen, Linton and Nielsen."
"10.1111/j.1467-9868.2009.00730.x","2010","On the use of non-local prior densities in {B}ayesian hypothesis tests","0","We examine philosophical problems and sampling deficiencies that are associated with current Bayesian hypothesis testing methodology, paying particular attention to objective Bayes methodology. Because the prior densities that are used to define alternative hypotheses in many Bayesian tests assign non-negligible probability to regions of the parameter space that are consistent with null hypotheses, resulting tests provide exponential accumulation of evidence in favour of true alternative hypotheses, but only sublinear accumulation of evidence in favour of true null hypotheses. Thus, it is often impossible for such tests to provide strong evidence in favour of a true null hypothesis, even when moderately large sample sizes have been obtained. We review asymptotic convergence rates of Bayes factors in testing precise null hypotheses and propose two new classes of prior densities that ameliorate the imbalance in convergence rates that is inherited by most Bayesian tests. Using members of these classes, we obtain analytic expressions for Bayes factors in linear models and derive approximations to Bayes factors in large sample settings."
"10.1111/j.1467-9868.2010.00752.x","2010","A multiresolution approach to time warping achieved by a {B}ayesian prior-posterior transfer fitting strategy","0","Warping is an approach to the reduction and analysis of phase variability in functional observations, by applying a smooth bijection to the function argument. We propose a natural representation of warping functions in terms of a new type of elementary functions named 'warping component functions', or 'warplets', which are combined into the warping function by composition. The inverse warping function is trivial and explicit to obtain. A sequential Bayesian estimation strategy is introduced which fits a series of models and transfers the posterior of the previous fit into the prior of the next fit. Model selection is based on a warping analogue to wavelet thresholding, combined with Bayesian inference."
"10.1111/j.1467-9868.2010.00748.x","2010","A {M}arkov process for circular data","0","We propose a discrete time Markov process which takes values on the unit circle. Some properties of the process, including the limiting behaviour and ergodicity, are investigated. Many computations associated with this process are shown to be greatly simplified if the variables and parameters of the model are represented in terms of complex numbers. A further discussion is given on some submodels, in particular on the stationary process. The proposed model is compared with some existing Markov processes for circular data. Some statistical issues of the model, such as statistical inference, model selection and diagnostic checks, are considered. Finally, an application of the model to wind direction data is provided."
"10.1111/j.1467-9868.2010.00750.x","2010","Default priors for {B}ayesian and frequentist inference","0","We investigate the choice of default priors for use with likelihood for Bayesian and frequentist inference. Such a prior is a density or relative density that weights an observed likelihood function, leading to the elimination of parameters that are not of interest and then a density-type assessment for a parameter of interest. For independent responses from a continuous model, we develop a prior for the full parameter that is closely linked to the original Bayes approach and provides an extension of the right invariant measure to general contexts. We then develop a modified prior that is targeted on a component parameter of interest and by targeting avoids the marginalization paradoxes of Dawid and co-workers. This modifies Jeffreys's prior and provides extensions to the development of Welch and Peers. These two approaches are combined to explore priors for a vector parameter of interest in the presence of a vector nuisance parameter. Examples are given to illustrate the computation of the priors."
"10.1111/j.1467-9868.2010.00742.x","2010","Non-parametric tests for right-censored data with biased sampling","0","Testing the equality of two survival distributions can be difficult in a prevalent cohort study when non-random sampling of subjects is involved. Owing to the biased sampling scheme, the independent censoring assumption is often violated. Although the issues about biased inference caused by length-biased sampling have been widely recognized in the statistical, epidemiological and economical literature, there is no satisfactory solution for efficient two-sample testing. We propose an asymptotic most efficient non-parametric test by properly adjusting for length-biased sampling. The test statistic is derived from a full likelihood function and can be generalized from the two-sample test to a k-sample test. The asymptotic properties of the test statistic under the null hypothesis are derived by using its asymptotic independent and identically distributed representation. We conduct extensive Monte Carlo simulations to evaluate the performance of the test statistics proposed and compare them with the conditional test and the standard log-rank test for various biased sampling schemes and right-censoring mechanisms. For length-biased data, empirical studies demonstrated that the test proposed is substantially more powerful than the existing methods. For general left-truncated data, the test proposed is robust, still maintains accurate control of the type I error rate and is also more powerful than the existing methods, if the truncation patterns and right censoring patterns are the same between the groups. We illustrate the methods by using two real data examples."
"10.1111/j.1467-9868.2010.00753.x","2010","Maximum likelihood estimation of a multi-dimensional log-concave density","2","Let X-1,...,X-n be independent and identically distributed random vectors with a (Lebesgue) density f. We first prove that, with probability 1, there is a unique log-concave maximum likelihood estimator f(n) of f. The use of this estimator is attractive because, unlike kernel density estimation, the method is fully automatic, with no smoothing parameters to choose. Although the existence proof is non-constructive, we can reformulate the issue of computing f(n) in terms of a non-differentiable convex optimization problem, and thus combine techniques of computational geometry with Shor's r-algorithm to produce a sequence that converges to f(n). An R version of the algorithm is available in the package LogConcDEAD-log-concave density estimation in arbitrary dimensions. We demonstrate that the estimator has attractive theoretical properties both when the true density is log-concave and when this model is misspecified. For the moderate or large sample sizes in our simulations, f(n) is shown to have smaller mean integrated squared error compared with kernel-based methods, even when we allow the use of a theoretical, optimal fixed bandwidth for the kernel estimator that would not be available in practice. We also present a real data clustering example, which shows that our methodology can be used in conjunction with the expectation-maximization algorithm to fit finite mixtures of log-concave densities."
"10.1111/j.1467-9868.2010.00747.x","2010","Bayesian pseudo-empirical-likelihood intervals for complex surveys","0","Bayesian methods for inference on finite population means and other parameters by using sample survey data face hurdles in all three phases of the inferential procedure: the formulation of a likelihood function, the choice of a prior distribution and the validity of posterior inferences under the design-based frequentist framework. In the case of independent and identically distributed observations, the profile empirical likelihood function of the mean and a non-informative prior on the mean can be used as the basis for inference on the mean and the resulting Bayesian empirical likelihood intervals are also asymptotically valid under the frequentist set-up. For complex survey data, we show that a pseudo-empirical-likelihood approach can be used to construct Bayesian pseudo-empirical-likelihood intervals that are asymptotically valid under the design-based set-up. The approach proposed compares favourably with a full Bayesian analysis under simple random sampling without replacement. It is also valid under general single-stage unequal probability sampling designs, unlike a full Bayesian analysis. Moreover, the approach is very flexible in using auxiliary population information and can accommodate two scenarios which are practically important: incorporation of known auxiliary population information for the construction of intervals by using the basic design weights; calculation of intervals by using calibration weights based on known auxiliary population means or totals."
"10.1111/j.1467-9868.2010.00743.x","2010","Simultaneous inference of linear models with time varying coefficients","0","The paper considers construction of simultaneous confidence tubes for time varying regression coefficients in functional linear models. Using a Gaussian approximation result for non-stationary multiple time series, we show that the constructed simultaneous confidence tubes have asymptotically correct nominal coverage probabilities. Our results are applied to the problem of testing whether the regression coefficients are of certain parametric forms, which is a fundamental problem in the inference of functional linear models. As an application, we analyse an environmental data set and study the association between levels of pollutants and hospital admissions."
"10.1111/j.1467-9868.2010.00744.x","2010","Random-weight particle filtering of continuous time processes","0","It is possible to implement importance sampling, and particle filter algorithms, where the importance sampling weight is random. Such random-weight algorithms have been shown to be efficient for inference for a class of diffusion models, as they enable inference without any (time discretization) approximation of the underlying diffusion model. One difficulty of implementing such random-weight algorithms is the requirement to have weights that are positive with probability 1. We show how Wald's identity for martingales can be used to ensure positive weights. We apply this idea to analysis of diffusion models from high frequency data. For a class of diffusion models we show how to implement a particle filter, which uses all the information in the data, but whose computational cost is independent of the frequency of the data. We use the Wald identity to implement a random-weight particle filter for these models which avoids time discretization error."
"10.1111/j.1467-9868.2010.00741.x","2010","Explicit estimating equations for semiparametric generalized linear latent variable models","0","We study generalized linear latent variable models without requiring a distributional assumption of the latent variables. Using a geometric approach, we derive consistent semiparametric estimators. We demonstrate that these models have a property which is similar to that of a sufficient complete statistic, which enables us to simplify the estimating procedure and explicitly to formulate the semiparametric estimating equations. We further show that the explicit estimators have the usual root n consistency and asymptotic normality. We explain the computational implementation of our method and illustrate the numerical performance of the estimators in finite sample situations via extensive simulation studies. The advantage of our estimators over the existing likelihood approach is also shown via numerical comparison. We employ the method to analyse a real data example from economics."
"10.1111/j.1467-9868.2010.00740.x","2010","Stability selection","0","Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data."
"10.1111/j.1467-9868.2010.00746.x","2010","Discovering the false discovery rate","0","I describe the background for the paper 'Controlling the false discovery rate: a new and powerful approach to multiple comparisons' by Benjamini and Hochberg that was published in the Journal of the Royal Statistical Society, Series B, in 1995. I review the progress since made on the false discovery rate, as well as the major conceptual developments that followed."
"10.1111/j.1467-9868.2010.00745.x","2010","Preface: `{R}etrospective read paper'","0",""
"10.1111/j.1467-9868.2010.00739.x","2010","On selection of spatial linear models for lattice data","0","Spatial linear models are popular for the analysis of data on a spatial lattice, but statistical techniques for selection of covariates and a neighbourhood structure are limited. Here we develop new methodology for simultaneous model selection and parameter estimation via penalized maximum likelihood under a spatial adaptive lasso. A computationally efficient algorithm is devised for obtaining approximate penalized maximum likelihood estimates. Asymptotic properties of penalized maximum likelihood estimates and their approximations are established. A simulation study shows that the method proposed has sound finite sample properties and, for illustration, we analyse an ecological data set in western Canada."
"10.1111/j.1467-9868.2010.00738.x","2010","Sufficient dimension reduction for spatial point processes directed by {G}aussian random fields","0",""
"10.1111/j.1467-9868.2009.00737.x","2010","A self-normalized approach to confidence interval construction in time series","0","We propose a new method to construct confidence intervals for quantities that are associated with a stationary time series, which avoids direct estimation of the asymptotic variances. Unlike the existing tuning-parameter-dependent approaches, our method has the attractive convenience of being free of any user-chosen number or smoothing parameter. The interval is constructed on the basis of an asymptotically distribution-free self-normalized statistic, in which the normalizing matrix is computed by using recursive estimates. Under mild conditions, we establish the theoretical validity of our method for a broad class of statistics that are functionals of the empirical distribution of fixed or growing dimension. From a practical point of view, our method is conceptually simple, easy to implement and can be readily used by the practitioner. Monte Carlo simulations are conducted to compare the finite sample performance of the new method with those delivered by the normal approximation and the block bootstrap approach."
"10.1111/j.1467-9868.2009.00736.x","2010","Particle {M}arkov chain {M}onte {C}arlo methods","0","Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a Levy-driven stochastic volatility model."
"10.1111/j.1467-9868.2009.00722.x","2010","Report of the editors---2009","0",""
"10.1111/j.1467-9868.2009.00729.x","2010","Criteria for surrogate end points based on causal distributions","0","When a treatment has a positive average causal effect (ACE) on an intermediate variable or surrogate end point which in turn has a positive ACE on a true end point, the treatment may have a negative ACE on the true end point due to the presence of unobserved confounders, which is called the surrogate paradox. A criterion for surrogate end points based on ACEs has recently been proposed to avoid the surrogate paradox. For a continuous or ordinal discrete end point, the distributional causal effect (DCE) may be a more appropriate measure for a causal effect than the ACE. We discuss criteria for surrogate end points based on DCEs. We show that commonly used models, such as generalized linear models and Cox's proportional hazard models, can make the sign of the DCE of the treatment on the true end point determinable by the sign of the DCE of the treatment on the surrogate even if the models include unobserved confounders. Furthermore, for a general distribution without any assumption of parametric models, we give a sufficient condition for a distributionally consistent surrogate and prove that it is almost necessary."
"10.1111/j.1467-9868.2009.00728.x","2010","Signed directed acyclic graphs for causal inference","0","Formal rules governing signed edges on causal directed acyclic graphs are described and it is shown how these rules can be useful in reasoning about causality. Specifically, the notions of a monotonic effect, a weak monotonic effect and a signed edge are introduced. Results are developed relating these monotonic effects and signed edges to the sign of the causal effect of an intervention in the presence of intermediate variables. The incorporation of signed edges in the directed acyclic graph causal framework furthermore allows for the development of rules governing the relationship between monotonic effects and the sign of the covariance between two variables. It is shown that when certain assumptions about monotonic effects can be made then these results can be used to draw conclusions about the presence of causal effects even when data are missing on confounding variables."
"10.1111/j.1467-9868.2009.00727.x","2010","Ordering and selecting components in multivariate or functional data linear prediction","0","The problem of component choice in regression-based prediction has a long history. The main cases where important choices must be made are functional data analysis, and problems in which the explanatory variables are relatively high dimensional vectors. Indeed, principal component analysis has become the basis for methods for functional linear regression. In this context the number of components can also be interpreted as a smoothing parameter, and so the viewpoint is a little different from that for standard linear regression. However, arguments for and against conventional component choice methods are relevant to both settings and have received significant recent attention. We give a theoretical argument, which is applicable in a wide variety of settings, justifying the conventional approach. Although our result is of minimax type, it is not asymptotic in nature; it holds for each sample size. Motivated by the insight that is gained from this analysis, we give theoretical and numerical justification for cross-validation choice of the number of components that is used for prediction. In particular we show that cross-validation leads to asymptotic minimization of mean summed squared error, in settings which include functional data analysis."
"10.1111/j.1467-9868.2009.00726.x","2010","Combining probability forecasts","0","Linear pooling is by far the most popular method for combining probability forecasts. However, any non-trivial weighted average of two or more distinct, calibrated probability forecasts is necessarily uncalibrated and lacks sharpness. In view of this, linear pooling requires recalibration, even in the ideal case in which the individual forecasts are calibrated. Towards this end, we propose a beta-transformed linear opinion pool for the aggregation of probability forecasts from distinct, calibrated or uncalibrated sources. The method fits an optimal non-linearly recalibrated forecast combination, by compositing a beta transform and the traditional linear opinion pool. The technique is illustrated in a simulation example and in a case-study on statistical and National Weather Service probability of precipitation forecasts."
"10.1111/j.1467-9868.2009.00725.x","2010","Local composite quantile regression smoothing: an efficient and safe alternative to local polynomial regression","0","Local polynomial regression is a useful non-parametric regression tool to explore fine data structures and has been widely used in practice. We propose a new non-parametric regression technique called local composite quantile regression smoothing to improve local polynomial regression further. Sampling properties of the estimation procedure proposed are studied. We derive the asymptotic bias, variance and normality of the estimate proposed. The asymptotic relative efficiency of the estimate with respect to local polynomial regression is investigated. It is shown that the estimate can be much more efficient than the local polynomial regression estimate for various non-normal errors, while being almost as efficient as the local polynomial regression estimate for normal errors. Simulation is conducted to examine the performance of the estimates proposed. The simulation results are consistent with our theoretical findings. A real data example is used to illustrate the method proposed."
"10.1111/j.1467-9868.2009.00724.x","2010","Combining information from multiple surveys by using regression for efficient small domain estimation","0","In sample surveys of finite populations, subpopulations for which the sample size is too small for estimation of adequate precision are referred to as small domains. Demand for small domain estimates has been growing in recent years among users of survey data. We explore the possibility of enhancing the precision of domain estimators by combining comparable information collected in multiple surveys of the same population. For this, we propose a regression method of estimation that is essentially an extended calibration procedure whereby comparable domain estimates from the various surveys are calibrated to each other. We show through analytic results and an empirical study that this method may greatly improve the precision of domain estimators for the variables that are common to these surveys, as these estimators make effective use of increased sample size for the common survey items. The design-based direct estimators proposed involve only domain-specific data on the variables of interest. This is in contrast with small domain (mostly small area) indirect estimators, based on a single survey, which incorporate through modelling data that are external to the targeted small domains. The approach proposed is also highly effective in handling the closely related problem of estimation for rare population characteristics."
"10.1111/j.1467-9868.2009.00723.x","2010","Sparse partial least squares regression for simultaneous dimension reduction and variable selection","0","Partial least squares regression has been an alternative to ordinary least squares for handling multicollinearity in several areas of scientific research since the 1960s. It has recently gained much attention in the analysis of high dimensional genomic data. We show that known asymptotic consistency of the partial least squares estimator for a univariate response does not hold with the very large p and small n paradigm. We derive a similar result for a multivariate response regression with partial least squares. We then propose a sparse partial least squares formulation which aims simultaneously to achieve good predictive performance and variable selection by producing sparse linear combinations of the original predictors. We provide an efficient implementation of sparse partial least squares regression and compare it with well-known variable selection and dimension reduction approaches via simulation experiments. We illustrate the practical utility of sparse partial least squares regression in a joint analysis of gene expression and genomewide binding data."
"10.1111/j.1467-9868.2009.00719.x","2009","Controlling the familywise error rate with plug-in estimator for the proportion of true null hypotheses","0","Estimation of the number or proportion of true null hypotheses in multiple-testing problems has become an interesting area of research. The first important work in this field was performed by Schweder and Spj circle divide tvoll. Among others, they proposed to use plug-in estimates for the proportion of true null hypotheses in multiple-test procedures to improve the power. We investigate the problem of controlling the familywise error rate FWER when such estimators are used as plug-in estimators in single-step or step-down multiple-test procedures. First we investigate the case of independent p-values under the null hypotheses and show that a suitable choice of plug-in estimates leads to control of FWER in single-step procedures. We also investigate the power and study the asymptotic behaviour of the number of false rejections. Although step-down procedures are more difficult to handle we briefly consider a possible solution to this problem. Anyhow, plug-in step-down procedures are not recommended here. For dependent p-values we derive a condition for asymptotic control of FWER and provide some simulations with respect to FWER and power for various models and hypotheses."
"10.1111/j.1467-9868.2009.00718.x","2009","Sparse additive models","0","We present a new class of methods for high dimensional non-parametric regression and classification called sparse additive models. Our methods combine ideas from sparse linear modelling and additive non-parametric regression. We derive an algorithm for fitting the models that is practical and effective even when the number of covariates is larger than the sample size. Sparse additive models are essentially a functional version of the grouped lasso of Yuan and Lin. They are also closely related to the COSSO model of Lin and Zhang but decouple smoothing and sparsity, enabling the use of arbitrary non-parametric smoothers. We give an analysis of the theoretical properties of sparse additive models and present empirical results on synthetic and real data, showing that they can be effective in fitting sparse non-parametric models in high dimensional data."
"10.1111/j.1467-9868.2009.00717.x","2009","Bayesian non-parametric inference for species variety with a two-parameter {P}oisson-{D}irichlet process prior","0","A Bayesian non-parametric methodology has been recently proposed to deal with the issue of prediction within species sampling problems. Such problems concern the evaluation, conditional on a sample of size n, of the species variety featured by an additional sample of size m. Genomic applications pose the additional challenge of having to deal with large values of both n and m. In such a case the computation of the Bayesian non-parametric estimators is cumbersome and prevents their implementation. We focus on the two-parameter Poisson-Dirichlet model and provide completely explicit expressions for the corresponding estimators, which can be easily evaluated for any sizes of n and m. We also study the asymptotic behaviour of the number of new species conditionally on the observed sample: such an asymptotic result, combined with a suitable simulation scheme, allows us to derive asymptotic highest posterior density intervals for the estimates of interest. Finally, we illustrate the implementation of the proposed methodology by the analysis of five expressed sequence tags data sets."
"10.1111/j.1467-9868.2009.00716.x","2009","A hierarchical eigenmodel for pooled covariance estimation","0","Although the covariance matrices corresponding to different populations are unlikely to be exactly equal they can still exhibit a high degree of similarity. For example, some pairs of variables may be positively correlated across most groups, whereas the correlation between other pairs may be consistently negative. In such cases much of the similarity across covariance matrices can be described by similarities in their principal axes, which are the axes that are defined by the eigenvectors of the covariance matrices. Estimating the degree of across-population eigenvector heterogeneity can be helpful for a variety of estimation tasks. For example, eigenvector matrices can be pooled to form a central set of principal axes and, to the extent that the axes are similar, covariance estimates for populations having small sample sizes can be stabilized by shrinking their principal axes towards the across-population centre. To this end, the paper develops a hierarchical model and estimation procedure for pooling principal axes across several populations. The model for the across-group heterogeneity is based on a matrix-valued antipodally symmetric Bingham distribution that can flexibly describe notions of 'centre' and 'spread' for a population of orthogonal matrices."
"10.1111/j.1467-9868.2009.00712.x","2009","Causal inference in outcome-dependent two-phase sampling designs","0","We consider estimation of the causal effect of a treatment on an outcome from observational data collected in two phases. In the first phase, a simple random sample of individuals is drawn from a population. On these individuals, information is obtained on treatment, outcome and a few low dimensional covariates. These individuals are then stratified according to these factors. In the second phase, a random subsample of individuals is drawn from each stratum, with known stratum-specific selection probabilities. On these individuals, a rich set of covariates is collected. In this setting, we introduce five estimators: simple inverse weighted; simple doubly robust; enriched inverse weighted; enriched doubly robust; locally efficient. We evaluate the finite sample performance of these estimators in a simulation study. We also use our methodology to estimate the causal effect of trauma care on in-hospital mortality by using data from the National Study of Cost and Outcomes of Trauma."
"10.1111/j.1467-9868.2009.00713.x","2009","Detecting changes in the mean of functional observations","0","Principal component analysis has become a fundamental tool of functional data analysis. It represents the functional data as X(i)(t)=mu(t)+Sigma(1 < l <infinity)eta(i, l)+ v(l)(t), where mu is the common mean, v(l) are the eigenfunctions of the covariance operator and the eta(i, l) are the scores. Inferential procedures assume that the mean function mu(t) is the same for all values of i. If, in fact, the observations do not come from one population, but rather their mean changes at some point(s), the results of principal component analysis are confounded by the change(s). It is therefore important to develop a methodology to test the assumption of a common functional mean. We develop such a test using quantities which can be readily computed in the R package fda. The null distribution of the test statistic is asymptotically pivotal with a well-known asymptotic distribution. The asymptotic test has excellent finite sample performance. Its application is illustrated on temperature data from England."
"10.1111/j.1467-9868.2009.00714.x","2009","A {B}ayesian discovery procedure","0","We discuss a Bayesian discovery procedure for multiple-comparison problems. We show that, under a coherent decision theoretic framework, a loss function combining true positive and false positive counts leads to a decision rule that is based on a threshold of the posterior probability of the alternative. Under a semiparametric model for the data, we show that the Bayes rule can be approximated by the optimal discovery procedure, which was recently introduced by Storey. Improving the approximation leads us to a Bayesian discovery procedure, which exploits the multiple shrinkage in clusters that are implied by the assumed non-parametric model. We compare the Bayesian discovery procedure and the optimal discovery procedure estimates in a simple simulation study and in an assessment of differential gene expression based on microarray data from tumour samples. We extend the setting of the optimal discovery procedure by discussing modifications of the loss function that lead to different single-thresholding statistics. Finally, we provide an application of the previous arguments to dependent (spatial) data."
"10.1111/j.1467-9868.2009.00710.x","2009","Adaptively varying-coefficient spatiotemporal models","0","We propose an adaptive varying-coefficient spatiotemporal model for data that are observed irregularly over space and regularly in time. The model is capable of catching possible non-linearity (both in space and in time) and non-stationarity (in space) by allowing the auto-regressive coefficients to vary with both spatial location and an unknown index variable. We suggest a two-step procedure to estimate both the coefficient functions and the index variable, which is readily implemented and can be computed even for large spatiotemporal data sets. Our theoretical results indicate that, in the presence of the so-called nugget effect, the errors in the estimation may be reduced via the spatial smoothing-the second step in the estimation procedure proposed. The simulation results reinforce this finding. As an illustration, we apply the methodology to a data set of sea level pressure in the North Sea."
"10.1111/j.1467-9868.2009.00709.x","2009","Bootstrapping frequency domain tests in multivariate time series with an application to comparing spectral densities","0","We propose a general bootstrap procedure to approximate the null distribution of non-parametric frequency domain tests about the spectral density matrix of a multivariate time series. Under a set of easy-to-verify conditions, we establish asymptotic validity of the bootstrap procedure proposed. We apply a version of this procedure together with a new statistic to test the hypothesis that the spectral densities of not necessarily independent time series are equal. The test statistic proposed is based on an L-2-distance between the non-parametrically estimated individual spectral densities and an overall, 'pooled' spectral density, the latter being obtained by using the whole set of m time series considered. The effects of the dependence between the time series on the power behaviour of the test are investigated. Some simulations are presented and a real life data example is discussed."
"10.1111/j.1467-9868.2009.00711.x","2009","Robust discrimination designs","0","We study the construction of experimental designs, the purpose of which is to aid in the discrimination between two possibly non-linear regression models, each of which might be only approximately specified. A rough description of our approach is that we impose neighbourhood structures on each regression response and determine the members of these neighbourhoods which are least favourable in the sense of minimizing the Kullback-Leibler divergence. Designs are obtained which maximize this minimum divergence. Both static and sequential approaches are studied. We then consider sequential designs whose purpose is initially to discriminate, but which move their emphasis towards efficient estimation or prediction as one model becomes favoured over the other."
"10.1111/j.1467-9868.2009.00701.x","2009","Tilting methods for assessing the influence of components in a classifier","0","Many contemporary classifiers are constructed to provide good performance for very high dimensional data. However, an issue that is at least as important as good classification is determining which of the many potential variables provide key information for good decisions. Responding to this issue can help us to determine which aspects of the datagenerating mechanism (e.g. which genes in a genomic study) are of greatest importance in terms of distinguishing between populations. We introduce tilting methods for addressing this problem. We apply weights to the components of data vectors, rather than to the data vectors themselves (as is commonly the case in related work). In addition we tilt in a way that is governed by L(2)-distance between weight vectors, rather than by the more commonly used Kullback-Leibler distance. It is shown that this approach, together with the added constraint that the weights should be non-negative, produces an algorithm which eliminates vector components that have little influence on the classification decision. In particular, use of the L(2)-distance in this problem produces properties that are reminiscent of those that arise when L(1)-penalties are employed to eliminate explanatory variables in very high dimensional prediction problems, e.g. those involving the lasso. We introduce techniques that can be implemented very rapidly, and we show how to use bootstrap methods to assess the accuracy of our variable ranking and variable elimination procedures."
"10.1111/j.1467-9868.2009.00708.x","2009","Hybrid {D}irichlet mixture models for functional data","0","In functional data analysis, curves or surfaces are observed, up to measurement error, at a finite set of locations, for, say, a sample of n individuals. Often, the curves are homogeneous, except perhaps for individual-specific regions that provide heterogeneous behaviour (e.g. 'damaged' areas of irregular shape on an otherwise smooth surface). Motivated by applications with functional data of this nature, we propose a Bayesian mixture model, with the aim of dimension reduction, by representing the sample of n curves through a smaller set of canonical curves. We propose a novel prior on the space of probability measures for a random curve which extends the popular Dirichlet priors by allowing local clustering: non-homogeneous portions of a curve can be allocated to different clusters and the n individual curves can be represented as recombinations (hybrids) of a few canonical curves. More precisely, the prior proposed envisions a conceptual hidden factor with k-levels that acts locally on each curve. We discuss several models incorporating this prior and illustrate its performance with simulated and real data sets. We examine theoretical properties of the proposed finite hybrid Dirichlet mixtures, specifically, their behaviour as the number of the mixture components goes to infinity and their connection with Dirichlet process mixtures."
"10.1111/j.1467-9868.2009.00715.x","2009","Semiparametric estimation and inference for distributional and general treatment effects","0","There is a large literature on methods of analysis for randomized trials with noncompliance which focuses on the effect of treatment on the average outcome. The paper considers evaluating the effect of treatment on the entire distribution and general functions of this effect. For distributional treatment effects, fully non-parametric and fully parametric approaches have been proposed. The fully non-parametric approach could be inefficient but the fully parametric approach is not robust to the violation of distribution assumptions. We develop a semiparametric instrumental variable method based on the empirical likelihood approach. Our method can be applied to general outcomes and general functions of outcome distributions and allows us to predict a subject's latent compliance class on the basis of an observed outcome value in observed assignment and treatment received groups. Asymptotic results for the estimators and likelihood ratio statistic are derived. A simulation study shows that our estimators of various treatment effects are substantially more efficient than the currently used fully non-parametric estimators. The method is illustrated by an analysis of data from a randomized trial of an encouragement intervention to improve adherence to prescribed depression treatments among depressed elderly patients in primary care practices."
"10.1111/j.1467-9868.2009.00707.x","2009","Copula structure analysis","0","We extend the standard approach of correlation structure analysis for dimension reduction of high dimensional statistical data. The classical assumption of a linear model for the distribution of a random vector is replaced by the weaker assumption of a model for the copula. For elliptical copulas a correlation-like structure remains, but different margins and non-existence of moments are possible. After introducing the new concept and deriving some theoretical results we observe in a simulation study the performance of the estimators: the theoretical asymptotic behaviour of the statistics can be observed even for small sample sizes. Finally, we show our method at work for a financial data set and explain differences between our copula-based approach and the classical approach. Our new method yielear models also."
"10.1111/j.1467-9868.2009.00703.x","2009","A general dynamical statistical model with causal interpretation","2","We develop a general dynamical model as a framework for causal interpretation. We first state a criterion of local independence in terms of measurability of processes that are involved in the Doob-Meyer decomposition of stochastic processes; then we define direct and indirect influence. We propose a definition of causal influence using the concepts of a 'physical system'. This framework makes it possible to link descriptive and explicative statistical models, and encompasses quantitative processes and events. One of the features of the paper is the clear distinction between the model for the system and the model for the observation. We give a dynamical representation of a conventional joint model for human immunodeficiency virus load and CD4 cell counts. We show its inadequacy to capture causal influences whereas in contrast known mechanisms of infection by the human immunodeficiency virus can be expressed directly through a system of differential equations."
"10.1111/j.1467-9868.2009.00705.x","2009","Deconvolution methods for non-parametric inference in two-level mixed models","0","We develop a general non-parametric approach to the analysis of clustered data via random effects. Assuming only that the link function is known, the regression functions and the distributions of both cluster means and observation errors are treated non-parametrically. Our argument proceeds by viewing the observation error at the cluster mean level as though it were a measurement error in an errors-in-variables problem, and using a deconvolution argument to access the distribution of the cluster mean. A Fourier deconvolution approach could be used if the distribution of the error-in-variables were known. In practice it is unknown, of course, but it can be estimated from repeated measurements, and in this way deconvolution can be achieved in an approximate sense. This argument might be interpreted as implying that large numbers of replicates are necessary for each cluster mean distribution, but that is not so; we avoid this requirement by incorporating statistical smoothing over values of nearby explanatory variables. Empirical rules are developed for the choice of smoothing parameter. Numerical simulations, and an application to real data, demonstrate small sample performance for this package of methodology. We also develop theory establishing statistical consistency."
"10.1111/j.1467-9868.2008.00702.x","2009","Two-step estimation for inhomogeneous spatial point processes","1","The paper is concerned with parameter estimation for inhomogeneous spatial point processes with a regression model for the intensity function and tractable second-order properties (K-function). Regression parameters are estimated by using a Poisson likelihood score estimating function and in the second step minimum contrast estimation is applied for the residual clustering parameters. Asymptotic normality of parameter estimates is established under certain mixing conditions and we exemplify how the results may be applied in ecological studies of rainforests."
"10.1111/j.1467-9868.2008.00693.x","2009","Shrinkage tuning parameter selection with a diverging number of parameters","4","Contemporary statistical research frequently deals with problems involving a diverging number of parameters. For those problems, various shrinkage methods (e.g. the lasso and smoothly clipped absolute deviation) are found to be particularly useful for variable selection. Nevertheless, the desirable performances of those shrinkage methods heavily hinge on an appropriate selection of the tuning parameters. With a fixed predictor dimension, Wang and co-worker have demonstrated that the tuning parameters selected by a Bayesian information criterion type criterion can identify the true model consistently. In this work, similar results are further extended to the situation with a diverging number of parameters for both unpenalized and penalized estimators. Consequently, our theoretical results further enlarge not only the scope of applicabilityation criterion type criteria but also that of those shrinkage estimation methods."
"10.1111/j.1467-9868.2009.00696.x","2009","Splines for financial volatility","0","We propose a flexible generalized auto-regressive conditional heteroscedasticity type of model for the prediction of volatility in financial time series. The approach relies on the idea of using multivariate B-splines of lagged observations and volatilities. Estimation of such a B-spline basis expansion is constructed within the likelihood framework for non-Gaussian observations. As the dimension of the B-spline basis is large, i.e. many parameters, we use regularized and sparse model fitting with a boosting algorithm. Our method is computationally attractive and feasible for large dimensions. We demonstrate its strong predictive potential for financial volatility on simulated and real data, and also in comparison with other approaches, and we present some supporting asymptotic arguments."
"10.1111/j.1467-9868.2008.00704.x","2009","Fully exponential {L}aplace approximations for the joint modelling of survival and longitudinal data","0","A common objective in longitudinal studies is the joint modelling of a longitudinal response with a time-to-event outcome. Random effects are typically used in the joint modelling framework to explain the interrelationships between these two processes. However, estimation in the presence of random effects involves intractable integrals requiring numerical integration. We propose a new computational approach for fitting such models that is based on the Laplace method for integrals that makes the consideration of high dimensional random-effects structures feasible. Contrary to the standard Laplace approximation, our method requires much fewer repeated measurements per individual to produce reliable results."
"10.1111/j.1467-9868.2009.00699.x","2009","Covariance-regularized regression and classification for high dimensional problems","4","We propose covariance-regularized regression, a family of methods for prediction in high dimensional settings that uses a shrunken estimate of the inverse covariance matrix of the features to achieve superior prediction. An estimate of the inverse covariance matrix is obtained by maximizing the log-likelihood of the data, under a multivariate normal model, subject to a penalty; it is then used to estimate coefficients for the regression of the response onto the features. We show that ridge regression, the lasso and the elastic net are special cases of covariance-regularized regression, and we demonstrate that certain previously unexplored forms of covariance-regularized regression can outperform existing methods in a range of situations. The covariance-regularized regression framework is extended to generalized linear models and linear discriminant analysis, and is used to analyse gene expression data sets with multiple class and survival outcomes."
"10.1111/j.1467-9868.2009.00698.x","2009","On-line expectation-maximization algorithm for latent data models","1","We propose a generic on-line (also sometimes called adaptive or recursive) version of the expectation-maximization (EM) algorithm applicable to latent variable models of independent observations. Compared with the algorithm of Titterington, this approach is more directly connected to the usual EM algorithm and does not rely on integration with respect to the complete-data distribution. The resulting algorithm is usually simpler and is shown to achieve convergence to the stationary points of the Kullback-Leibler divergence between the marginal distribution of the observation and the model distribution at the optimal rate, i.e. that of the maximum likelihood estimator. In addition, the approach proposed is also suitable for conditional (or regression) models, as illustrated in the case of the mixture of linear regressions model."
"10.1111/j.1467-9868.2009.00706.x","2009","Invariant co-ordinate selection","0","A general method for exploring multivariate data by comparing different estimates of multivariate scatter is presented. The method is based on the eigenvalue-eigenvector decomposition of one scatter matrix relative to another. In particular, it is shown that the eigenvectors can be used to generate an affine invariant co-ordinate system for the multivariate data. Consequently, we view this method as a method for invariant co-ordinate selection. By plotting the data with respect to this new invariant co-ordinate system, various data structures can be revealed. For example, under certain independent components models, it is shown that the invariant co- ordinates correspond to the independent components. Another example pertains to mixtures of elliptical distributions. In this case, it is shown that a subset of the invariant co-ordinates corresponds to Fisher's linear discriminant subspace, even though the class identifications of the data points are unknown. Some illustrative examples are given."
"10.1111/j.1467-9868.2008.00692.x","2009","Finding an unknown number of multivariate outliers","1","We use the forward search to provide robust Mahalanobis distances to detect the presence of outliers in a sample of multivariate normal data. Theoretical results on order statistics and on estimation in truncated samples provide the distribution of our test statistic. We also introduce several new robust distances with associated distributional results. Comparisons of our procedure with tests using other robust Mahalanobis distances show the good size and high power of our procedure. We also provide a unification of results on correction factors for estimation from truncated samples."
"10.1111/j.1467-9868.2008.00695.x","2009","Smoothing parameter selection for a class of semiparametric linear models","3","Spline-based approaches to non-parametric and semiparametric regression, as well as to regression of scalar outcomes on functional predictors, entail choosing a parameter controlling the extent to which roughness of the fitted function is penalized. We demonstrate that the equations determining two popular methods for smoothing parameter selection, generalized cross-validation and restricted maximum likelihood, share a similar form that allows us to prove several results which are common to both, and to derive a condition under which they yield identical values. These ideas are illustrated by application of functional principal component regression, a method for regressing scalars on functions, to two chemometric data sets."
"10.1111/j.1467-9868.2008.00697.x","2009","On distribution-weighted partial least squares with diverging number of highly correlated predictors","4","Because highly correlated data arise from many scientific fields, we investigate parameter estimation in a semiparametric regression model with diverging number of predictors that are highly correlated. For this, we first develop a distribution-weighted least squares estimator that can recover directions in the central subspace, then use the distribution-weighted least squares estimator as a seed vector and project it onto a Krylov space by partial least squares to avoid computing the inverse of the covariance of predictors. Thus, distrbution-weighted partial least squares can handle the cases with high dimensional and highly correlated predictors. Furthermore, we also suggest an iterative algorithm for obtaining a better initial value before implementing partial least squares. For theoretical investigation, we obtain strong consistency and asymptotic normality when the dimension p of predictors is of convergence rate O{n(1/2)/ log (n)} and o(n(1/3)) respectively where n is the sample size. When there are no other constraints on the covariance of predictors, the rates n(1/2) and n(1/3) are optimal. We also propose a Bayesian information criterion type of criterion to estimate the dimension of the Krylov space in the partial least squares procedure. Illustrative examples with a real data set and comprehensive simulations demonstrate that the method is robust to non-ellipticity and works well even in 'small n-large p' problems."
"10.1111/j.1467-9868.2008.00691.x","2009","Some asymptotic results on generalized penalized spline smoothing","3","The paper discusses asymptotic properties of penalized spline smoothing if the spline basis increases with the sample size. The proof is provided in a generalized smoothing model allowing for non-normal responses. The results are extended in two ways. First, assuming the spline coefficients to be a priori normally distributed links the smoothing framework to generalized linear mixed models. We consider the asymptotic rates such that the Laplace approximation is justified and the resulting fits in the mixed model correspond to penalized spline estimates. Secondly, we make use of a fully Bayesian viewpoint by imposing an a priori distribution on all parameters and coefficients. We argue that with the postulated rates at which the spline basis dimension increases with the sample size the posterior distribution of the spline coefficients is approximately normal. The validity of this result is investigated in finite samples by comparing Markov chain Monte Carlo results with their asymptotic approximation in a simulation study."
"10.1111/j.1467-9868.2008.00687.x","2009","Efficient estimation of auto-regression parameters and innovation distributions for semiparametric integer-valued {${\rm AR}(p)$} models","1","Integer-valued auto-regressive (INAR) processes have been introduced to model non-negative integer-valued phenomena that evolve over time. The distribution of an INAR(p) process is essentially described by two parameters: a vector of auto-regression coefficients and a probability distribution on the non-negative integers, called an immigration or innovation distribution. Traditionally, parametric models are considered where the innovation distribution is assumed to belong to a parametric family. The paper instead considers a more realistic semiparametric INAR(p) model where there are essentially no restrictions on the innovation distribution. We provide an (semiparametrically) efficient estimator of both the auto-regression parameters and the innovation distribution."
"10.1111/j.1467-9868.2008.00690.x","2009","Variance estimation in the analysis of microarray data","1","Microarrays are one of the most widely used high throughput technologies. One of the main problems in the area is that conventional estimates of the variances that are required in the t-statistic and other statistics are unreliable owing to the small number of replications. Various methods have been proposed in the literature to overcome this lack of degrees of freedom problem. In this context, it is commonly observed that the variance increases proportionally with the intensity level, which has led many researchers to assume that the variance is a function of the mean. Here we concentrate on estimation of the variance as a function of an unknown mean in two models: the constant coefficient of variation model and the quadratic variance-mean model. Because the means are unknown and estimated with few degrees of freedom, naive methods that use the sample mean in place of the true mean are generally biased because of the errors-in-variables phenomenon. We propose three methods for overcoming this bias. The first two are variations on the theme of the so-called heteroscedastic simulation-extrapolation estimator, modified to estimate the variance function consistently. The third class of estimators is entirely different, being based on semiparametric information calculations. Simulations show the power of our methods and their lack of bias compared with the naive method that ignores the measurement error. The methodology is illustrated by using microarray data from leukaemia patients."
"10.1111/j.1467-9868.2008.00694.x","2009","Large-scale multiple testing under dependence","3","The paper considers the problem of multiple testing under dependence in a compound decision theoretic framework. The observed data are assumed to be generated from an underlying two-state hidden Markov model. We propose oracle and asymptotically optimal data-driven procedures that aim to minimize the false non-discovery rate FNR subject to a constraint on the false discovery rate FDR. It is shown that the performance of a multiple-testing procedure can be substantially improved by adaptively exploiting the dependence structure among hypotheses, and hence conventional FDR procedures that ignore this structural information are inefficient. Both theoretical properties and numerical performances of the procedures proposed are investigated. It is shown that the procedures proposed control FDR at the desired level, enjoy certain optimality properties and are especially powerful in identifying clustered non-null cases. The new procedure is applied to an influenza-like illness surveillance study for detecting the timing of epidemic periods."
"10.1111/j.1467-9868.2008.00700.x","2009","Approximate {B}ayesian inference for latent {G}aussian models by using integrated nested {L}aplace approximations","7","Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged."
"10.1111/j.1467-9868.2008.00682.x","2009","Robust linear clustering","0","Non-hierarchical clustering methods are frequently based on the idea of forming groups around 'objects'. The main exponent of this class of methods is the k-means method, where these objects are points. However, clusters in a data set may often be due to certain relationships between the measured variables. For instance, we can find linear structures such as straight lines and planes, around which the observations are grouped in a natural way. These structures are not well represented by points. We present a method that searches for linear groups in the presence of outliers. The method is based on the idea of impartial trimming. We search for the 'best' subsample containing a proportion 1-alpha of the data and the best k affine subspaces fitting to those non-discarded observations by measuring discrepancies through orthogonal distances. The population version of the sample problem is also considered. We prove the existence of solutions for the sample and population problems together with their consistency. A feasible algorithm for solving the sample problem is described as well. Finally, some examples showing how the method proposed works in practice are provided."
"10.1111/j.1467-9868.2008.00686.x","2009","Shrinkage inverse regression estimation for model-free variable selection","0","The family of inverse regression estimators that was recently proposed by Cook and Ni has proven effective in dimension reduction by transforming the high dimensional predictor vector to its low dimensional projections. We propose a general shrinkage estimation strategy for the entire inverse regression estimation family that is capable of simultaneous dimension reduction and variable selection. We demonstrate that the new estimators achieve consistency in variable selection without requiring any traditional model, meanwhile retaining the root n estimation consistency of the dimension reduction basis. We also show the effectiveness of the new estimators through both simulation and real data analysis."
"10.1111/j.1467-9868.2008.00681.x","2009","Empirical {B}ayes confidence intervals shrinking both means and variances","0","We construct empirical Bayes intervals for a large number p of means. The existing intervals in the literature assume that variances O2/1 are either equal or unequal but known. When the variances are unequal and unknown, the suggestion is typically to replace them by unbiased estimators S2/1. However, when p is large, there would be advantage in 'borrowing strength' from each other. We derive double-shrinkage intervals for means on the basis of our empirical Bayes estimators that shrink both the means and the variances. Analytical and simulation studies and application to a real data set show that, compared with the t-intervals, our intervals have higher coverage probabilities while yielding shorter lengths on average. The double-shrinkage intervals are on average shorter than the intervals from shrinking the means alone and are always no longer than the intervals from shrinking the variances alone. Also, the intervals are explicitly defined and can be computed immediately."
"10.1111/j.1467-9868.2008.00683.x","2009","Non-parametric tests for distributional treatment effect for randomly censored responses","0","For a binary treatment nu=0, 1 and the corresponding 'potential response'Y(0) for the control group (nu=0) and Y(1) for the treatment group (nu=1), one definition of no treatment effect is that Y(0) and Y(1) follow the same distribution given a covariate vector X. Koul and Schick have provided a non-parametric test for no distributional effect when the realized response (1-nu)Y(0)+nu Y(1) is fully observed and the distribution of X is the same across the two groups. This test is thus not applicable to censored responses, nor to non-experimental (i.e. observational) studies that entail different distributions of X across the two groups. We propose 'X-matched' non-parametric tests generalizing the test of Koul and Schick following an idea of Gehan. Our tests are applicable to non-experimental data with randomly censored responses. In addition to these motivations, the tests have several advantages. First, they have the intuitive appeal of comparing all available pairs across the treatment and control groups, instead of selecting a number of matched controls (or treated) in the usual pair or multiple matching. Second, whereas most matching estimators or tests have a non-overlapping support (of X) problem across the two groups, our tests have a built-in protection against the problem. Third, Gehan's idea allows the tests to make good use of censored observations. A simulation study is conducted, and an empirical illustration for a job training effect on the duration of unemployment is provided."
"10.1111/j.1467-9868.2008.00684.x","2009","A new class of models for bivariate joint tails","0","A fundamental issue in applied multivariate extreme value analysis is modelling dependence within joint tail regions. The primary focus of this work is to extend the classical pseudopolar treatment of multivariate extremes to develop an asymptotically motivated representation of extremal dependence that also encompasses asymptotic independence. Starting with the usual mild bivariate regular variation assumptions that underpin the coefficient of tail dependence as a measure of extremal dependence, our main result is a characterization of the limiting structure of the joint survivor function in terms of an essentially arbitrary non-negative measure that must satisfy some mild constraints. We then construct parametric models from this new class and study in detail one example that accommodates asymptotic dependence, asymptotic independence and asymmetry within a straightforward parsimonious parameterization. We provide a fast simulation algorithm for this example and detail likelihood-based inference including tests for asymptotic dependence and symmetry which are useful for submodel selection. We illustrate this model by application to both simulated and real data. In contrast with the classical multivariate extreme value approach, which concentrates on the limiting distribution of normalized componentwise maxima, our framework focuses directly on the structure of the limiting joint survivor function and provides significant extensions of both the theoretical and the practical tools that are available for joint tail modelling."
"10.1111/j.1467-9868.2008.00685.x","2009","Fourier analysis of irregularly spaced data on {$\Bbb{R}^d$}","1","The purpose of the paper is to propose a frequency domain approach for irregularly spaced data on R-d. We extend the original definition of a periodogram for time series to that for irregularly spaced data and define non-parametric and parametric spectral density estimators in a way that is similar to the classical approach. Introduction of the mixed asymptotics, which are one of the asymptotics for irregularly spaced data, makes it possible to provide asymptotic theories to the spectral estimators. The asymptotic result for the parametric estimator is regarded as a natural extension of the classical result for regularly spaced data to that for irregularly spaced data. Empirical studies are also included to illustrate the frequency domain approach in comparisons with the existing spatial and frequency domain approaches."
"10.1111/j.1467-9868.2008.00679.x","2009","Consistent model selection and data-driven smooth tests for longitudinal data in the estimating equations approach","1","Model selection for marginal regression analysis of longitudinal data is challenging owing to the presence of correlation and the difficulty of specifying the full likelihood, particularly for correlated categorical data. The paper introduces a novel Bayesian information criterion type model selection procedure based on the quadratic inference function, which does not require the full likelihood or quasi-likelihood. With probability approaching 1, the criterion selects the most parsimonious correct model. Although a working correlation matrix is assumed, there is no need to estimate the nuisance parameters in the working correlation matrix; moreover, the model selection procedure is robust against the misspecification of the working correlation matrix. The criterion proposed can also be used to construct a data-driven Neyman smooth test for checking the goodness of fit of a postulated model. This test is especially useful and often yields much higher power in situations where the classical directional test behaves poorly. The finite sample performance of the model selection and model checking procedures is demonstrated through Monte Carlo studies and analysis of a clinical trial data set."
"10.1111/j.1467-9868.2008.00677.x","2009","A {B}ayesian approach to non-parametric monotone function estimation","1","The paper proposes two Bayesian approaches to non-parametric monotone function estimation. The first approach uses a hierarchical Bayes framework and a characterization of smooth monotone functions given by Ramsay that allows unconstrained estimation. The second approach uses a Bayesian regression spline model of Smith and Kohn with a mixture distribution of constrained normal distributions as the prior for the regression coefficients to ensure the monotonicity of the resulting function estimate. The small sample properties of the two function estimators across a range of functions are provided via simulation and compared with existing methods. Asymptotic results are also given that show that Bayesian methods provide consistent function estimators for a large class of smooth functions. An example is provided involving economic demand functions that illustrates the application of the constrained regression spline estimator in the context of a multiple-regression model where two functions are constrained to be monotone."
"10.1111/j.1467-9868.2008.00678.x","2009","Bayesian model selection using test statistics","0","Existing Bayesian model selection procedures require the specification of prior distributions on the parameters appearing in every model in the selection set. In practice, this requirement limits the application of Bayesian model selection methodology. To overcome this limitation, we propose a new approach towards Bayesian model selection that uses classical test statistics to compute Bayes factors between possible models. In several test cases, our approach produces results that are similar to previously proposed Bayesian model selection and model averaging techniques in which prior distributions were carefully chosen. In addition to eliminating the requirement to specify complicated prior distributions, this method offers important computational and algorithmic advantages over existing simulation-based methods. Because it is easy to evaluate the operating characteristics of this procedure for a given sample size and specified number of covariates, our method facilitates the selection of hyperparameter values through prior-predictive simulation."
"10.1111/j.1467-9868.2008.00668.x","2009","D{ASSO}: connections between the {D}antzig selector and lasso","4","We propose a new algorithm, DASSO, for fitting the entire coefficient path of the Dantzig selector with a similar computational cost to the least angle regression algorithm that is used to compute the lasso. DASSO efficiently constructs a piecewise linear path through a sequential simplex-like algorithm, which is remarkably similar to the least angle regression algorithm. Comparison of the two algorithms sheds new light on the question of how the lasso and Dantzig selector are related. In addition, we provide theoretical conditions on the design matrix X under which the lasso and Dantzig selector coefficient estimates will be identical for certain tuning parameters. As a consequence, in many instances, we can extend the powerful non-asymptotic bounds that have been developed for the Dantzig selector to the lasso. Finally, through empirical studies of simulated and real world data sets we show that in practice, when the bounds hold for the Dantzig selector, they almost always also hold for the lasso."
"10.1111/j.1467-9868.2008.00672.x","2009","Multiscale methods for data on graphs and irregular multidimensional situations","0","For regularly spaced one-dimensional data, wavelet shrinkage has proven to be a compelling method for non-parametric function estimation. We create three new multiscale methods that provide wavelet-like transforms both for data arising on graphs and for irregularly spaced spatial data in more than one dimension. The concept of scale still exists within these transforms, but as a continuous quantity rather than dyadic levels. Further, we adapt recent empirical Bayesian shrinkage techniques to enable us to perform multiscale shrinkage for function estimation both on graphs and for irregular spatial data. We demonstrate that our methods perform very well when compared with several other methods for spatial regression for both real and simulated data. Although we concentrate on multiscale shrinkage (regression) we present our new 'wavelet transforms' as generic tools intended to be the basis of methods that might benefit from a multiscale representation of data either on graphs or for irregular spatial data."
"10.1111/j.1467-9868.2008.00671.x","2009","Testing in semiparametric models with interaction, with applications to gene-environment interactions","1","Motivated from the problem of testing for genetic effects on complex traits in the presence of gene-environment interaction, we develop score tests in general semiparametric regression problems that involves Tukey style 1 degree-of-freedom form of interaction between parametrically and non-parametrically modelled covariates. We find that the score test in this type of model, as recently developed by Chatterjee and co-workers in the fully parametric setting, is biased and requires undersmoothing to be valid in the presence of non-parametric components. Moreover, in the presence of repeated outcomes, the asymptotic distribution of the score test depends on the estimation of functions which are defined as solutions of integral equations, making implementation difficult and computationally taxing. We develop profiled score statistics which are unbiased and asymptotically efficient and can be performed by using standard bandwidth selection methods. In addition, to overcome the difficulty of solving functional equations, we give easy interpretations of the target functions, which in turn allow us to develop estimation procedures that can be easily implemented by using standard computational methods. We present simulation studies to evaluate type I error and power of the method proposed compared with a naive test that does not consider interaction. Finally, we illustrate our methodology by analysing data from a case-control study of colorectal adenoma that was designed to investigate the association between colorectal adenoma and the candidate gene NAT2 in relation to smoking history."
"10.1111/j.1467-9868.2008.00689.x","2009","Parameter estimation for partially observed hypoelliptic diffusions","0","Hypoelliptic diffusion processes can be used to model a variety of phenomena in applications ranging from molecular dynamics to audio signal analysis. We study parameter estimation for such processes in situations where we observe some components of the solution at discrete times. Since exact likelihoods for the transition densities are typically not known, approximations are used that are expected to work well in the limit of small intersample times Delta t and large total observation times N Delta t. Hypoellipticity together with partial observation leads to ill conditioning requiring a judicious combination of approximate likelihoods for the various parameters to be estimated. We combine these in a deterministic scan Gibbs sampler alternating between missing data in the unobserved solution components, and parameters. Numerical experiments illustrate asymptotic consistency of the method when applied to simulated data. The paper concludes with an application of the Gibbs sampler to molecular dynamics data."
"10.1111/j.1467-9868.2008.00670.x","2009","Testing for lack of fit in inverse regression---with applications to biophotonic imaging","0","We propose two test statistics for use in inverse regression problems Y=K theta+epsilon, where K is a given linear operator which cannot be continuously inverted. Thus, only noisy, indirect observations Y for the function theta are available. Both test statistics have a counterpart in classical hypothesis testing, where they are called the order selection test and the data-driven Neyman smooth test. We also introduce two model selection criteria which extend the classical Akaike information criterion and Bayes information criterion to inverse regression problems. In a simulation study we show that the inverse order selection and Neyman smooth tests outperform their direct counterparts in many cases. The theory is motivated by data arising in confocal fluorescence microscopy. Here, images are observed with blurring, modelled as convolution, and stochastic error at subsequent times. The aim is then to reduce the signal-to-noise ratio by averaging over the distinct images. In this context it is relevant to decide whether the images are still equal, or have changed by outside influences such as moving of the object table."
"10.1111/j.1467-9868.2008.00680.x","2009","Pseudomartingale estimating equations for modulated renewal process models","0","We adapt martingale estimating equations based on gap time information to a general intensity model for a single realization of a modulated renewal process. The consistency and asymptotic normality of the estimators is proved under ergodicity conditions. Previous work has considered either parametric likelihood analysis or semiparametric multiplicative models using partial likelihood. The framework is generally applicable to semiparametric and parametric models, including additive and multiplicative specifications, and periodic models. It facilitates a semiparametric extension of a popular parametric earthquake model. Simulations and empirical analyses of Taiwanese earthquake sequences illustrate the methodology's practical utility."
"10.1111/j.1467-9868.2008.00659.x","2008","Clarification: regression model selection---a residual likelihood approach","0",""
"10.1111/j.1467-9868.2008.00673.x","2008","Estimation of controlled direct effects","3","When regression models adjust for mediators on the causal path from exposure to outcome, the regression coefficient of exposure is commonly viewed as a measure of the direct exposure effect. This interpretation can be misleading, even with a randomly assigned exposure. This is because adjustment for post-exposure measurements introduces bias whenever their association with the outcome is confounded by more than just the exposure. By the same token, adjustment for such confounders stays problematic when these are themselves affected by the exposure. Robins accommodated this by introducing linear structural nested direct effect models with direct effect parameters that can be estimated by using inverse probability weighting by a conditional distribution of the mediator. The resulting estimators are consistent, but inefficient, and can be extremely unstable when the mediator is absolutely continuous. We develop direct effect estimators which are not only more efficient but also consistent under a less demanding model for a conditional expectation of the outcome. We find that the one estimator which avoids inverse probability weighting altogether performs best. This estimator is intuitive, computationally straightforward and, as demonstrated by simulation, competes extremely well with ordinary least squares estimators in settings where standard regression is valid."
"10.1111/j.1467-9868.2008.00669.x","2008","Non-parametric heteroscedastic transformation regression models for skewed data with an application to health care costs","0","We develop a new non-parametric heteroscedastic transformation regression model for predicting the expected value of the outcome of a patient with given patient's covariates when the distribution of the outcome is highly skewed with a heteroscedastic variance. In our model, we allow both the transformation function and the error distribution function to be unknown. We show that under some regularity conditions the estimators for regression parameters, the expected value of the original outcome and the transformation function converge to their true values at the rate n(-1/2). In our simulation studies, we demonstrate that our proposed non-parametric method is robust with little loss of efficiency. Finally, we apply our model to a study on health care costs."
"10.1111/j.1467-9868.2008.00666.x","2008","Graphical {G}aussian models with edge and vertex symmetries","1","We introduce new types of graphical Gaussian models by placing symmetry restrictions on the concentration or correlation matrix. The models can be represented by coloured graphs, where parameters that are associated with edges or vertices of the same colour are restricted to being identical. We study the properties of such models and derive the necessary algorithms for calculating maximum likelihood estimates. We identify conditions for restrictions on the concentration and correlation matrices being equivalent. This is for example the case when symmetries are generated by permutation of variable labels. For such models a particularly simple maximization of the likelihood function is available."
"10.1111/j.1467-9868.2008.00667.x","2008","Generalization of {J}effreys divergence-based priors for {B}ayesian hypothesis testing","0","We introduce objective proper prior distributions for hypothesis testing and model selection based on measures of divergence between the competing models; we call them divergence-based (DB) priors. DB priors have simple forms and desirable properties like information (finite sample) consistency and are often similar to other existing proposals like intrinsic priors. Moreover, in normal linear model scenarios, they reproduce the Jeffreys-Zellner-Siow priors exactly, Most importantly, in challenging scenarios such as irregular models and mixture models, DB priors are well defined and very reasonable, whereas alternative proposals are not. We derive approximations to the DB priors as well as Markov chain Monte Carlo and asymptotic expressions for the associated Bayes factors."
"10.1111/j.1467-9868.2008.00664.x","2008","Separation measures and the geometry of {B}ayes factor selection for classification","0","Conjugacy assumptions are often used in Bayesian selection over a partition because they allow the otherwise unfeasibly large model space to be searched very quickly. The implications of such models can be analysed algebraically. We use the explicit forms of the associated Bayes factors to demonstrate that such methods can be unstable under common settings of the associated hyperparameters. We then prove that the regions of instability can be removed by setting the hyperparameters in an unconventional way. Under this family of assignments we prove that model selection is determined by an implicit separation measure: a function of the hyperparameters and the sufficient statistics of clusters in a given partition. We show that this family of separation measures has plausible properties. The methodology proposed is illustrated through the selection of clusters of longitudinal gene expression profiles."
"10.1111/j.1467-9868.2008.00665.x","2008","Soap film smoothing","1","Conventional smoothing methods sometimes perform badly when used to smooth data over complex domains, by smoothing inappropriately across boundary features, such as peninsulas. Solutions to this smoothing problem tend to be computationally complex, and not to provide model smooth functions which are appropriate for incorporating as components of other models, such as generalized additive models or mixed additive models. We propose a class of smoothers that are appropriate for smoothing over difficult regions of R(2) which can be represented in terms of a low rank basis and one or two quadratic penalties. The key features of these smoothers are that they do not 'smooth across' boundary features, that their representation in terms of a basis and penalties allows straightforward incorporation as components of generalized additive models, mixed models and other non-standard models, that smoothness selection for these model components is straightforward to accomplish in a computationally efficient manner via generalized cross-validation, Akaike's information criterion or restricted maximum likelihood, for example, and that their low rank means that their use is computationally efficient."
"10.1111/j.1467-9868.2008.00655.x","2008","Semiparametric inference in generalized mixed effects models","0","The paper presents a study of the generalized partially linear model including random effects in its linear part. We propose an estimator that combines likelihood approaches for mixed effects models, with kernel methods. Following the methodology of Hardle and co-workers, we introduce a test for the hypothesis of a parametric mixed effects model against the alternative of a semiparametric mixed effects model. The critical values are estimated by using a bootstrap procedure. The asymptotic theory for the methods is provided, as are the results of a simulation study. These verify the feasibility and the excellent behaviour of the methods for samples of even moderate size. The usefulness of the methodology is illustrated with an application in which the objective is to estimate forest coverage in Galicia, Spain."
"10.1111/j.1467-9868.2008.00674.x","2008","Sure independence screening for ultrahigh dimensional feature space","35","Variable selection plays an important role in high dimensional statistical modelling which nowadays appears in many areas and is key to various scientific discoveries. For problems of large scale or dimensionality p, accuracy of estimation and computational cost are two top concerns. Recently, Candes and Tao have proposed the Dantzig selector using L(1)-regularization and showed that it achieves the ideal risk up to a logarithmic factor log(p). Their innovative procedure and remarkable result are challenged when the dimensionality is ultrahigh as the factor log(p) can be large and their uniform uncertainty principle can fail. Motivated by these concerns, we introduce the concept of sure screening and propose a sure screening method that is based on correlation learning, called sure independence screening, to reduce dimensionality from high to a moderate scale that is below the sample size. In a fairly general asymptotic framework, correlation learning is shown to have the sure screening property for even exponentially growing dimensionality. As a methodological extension, iterative sure independence screening is also proposed to enhance its finite sample performance. With dimension reduced accurately from high to below sample size, variable selection can be improved on both speed and accuracy, and can then be accomplished by a well-developed method such as smoothly clipped absolute deviation, the Dantzig selector, lasso or adaptive lasso. The connections between these penalized least squares methods are also elucidated."
"10.1111/j.1467-9868.2008.00663.x","2008","Gaussian predictive process models for large spatial data sets","10","With scientific data available at geocoded locations, investigators are increasingly turning to spatial process models for carrying out statistical inference. Over the last decade, hierarchical models implemented through Markov chain Monte Carlo methods have become especially popular for spatial modelling, given their flexibility and power to fit models that would be infeasible with classical methods as well as their avoidance of possibly inappropriate asymptotics. However, fitting hierarchical spatial models often involves expensive matrix decompositions whose computational complexity increases in cubic order with the number of spatial locations, rendering such models infeasible for large spatial data sets. This computational burden is exacerbated in multivariate settings with several spatially dependent response variables. It is also aggravated when data are collected at frequent time points and spatiotemporal process models are used. With regard to this challenge, our contribution is to work with what we call predictive process models for spatial and spatiotemporal data. Every spatial (or spatiotemporal) process induces a predictive process model (in fact, arbitrarily many of them). The latter models project process realizations of the former to a lower dimensional subspace, thereby reducing the computational burden. Hence, we achieve the flexibility to accommodate non-stationary, non-Gaussian, possibly multivariate, possibly spatiotemporal processes in the context of large data sets. We discuss attractive theoretical properties of these predictive processes. We also provide a computational template encompassing these diverse settings. Finally, we illustrate the approach with simulated and real data sets."
"10.1111/j.1467-9868.2008.00662.x","2008","Improving semiparametric estimation by using surrogate data","2","The paper considers estimating a parameter beta that defines an estimating function U(y, x, beta) for an outcome variable y and its covariate x when the outcome is missing in some of the observations. We assume that, in addition to the outcome and the covariate, a surrogate outcome is available in every observation. The efficiency of existing estimators for beta depends critically on correctly specifying the conditional expectation of U given the surrogate and the covariate. When the conditional expectation is not correctly specified, which is the most likely scenario in practice, the efficiency of estimation can be severely compromised even if the propensity function (of missingness) is correctly specified. We propose an estimator that is robust against the choice of the conditional expectation via an empirical likelihood. We demonstrate that the estimator proposed achieves a gain in efficiency whether the conditional score is correctly specified or not. When the conditional score is correctly specified, the estimator reaches the semiparametric variance bound within the class of estimating functions that are generated by U. The practical performance of the estimator is evaluated by using simulation and a data set that is based on the 1996 US presidential election."
"10.1111/j.1467-9868.2008.00657.x","2008","Robust estimation in the normal mixture model based on robust clustering","0","We introduce a robust estimation procedure that is based on the choice of a representative trimmed subsample through an initial robust clustering procedure, and subsequent improvements based on maximum likelihood. To obtain the initial trimming we resort to the trimmed k-means, a simple procedure designed for finding the core of the clusters under appropriate configurations. By handling the trimmed data as censored, maximum likelihood estimation provides in each step the location and shape of the next trimming. Data-driven restrictions on the parameters, requiring that every distribution in the mixture must be sufficiently represented in the initial clustered region, allow singularities to be avoided and guarantee the existence of the estimator. Our analysis includes robustness properties and asymptotic results as well as worked examples."
"10.1111/j.1467-9868.2008.00661.x","2008","Particle filters for partially observed diffusions","2","We introduce a novel particle filter scheme for a class of partially observed multivariate diffusions. We consider a variety of observation schemes, including diffusion observed with error, observation of a subset of the components of the multivariate diffusion and arrival times of a Poisson process whose intensity is a known function of the diffusion (Cox process). Unlike currently available methods, our particle filters do not require approximations of the transition and/or the observation density by using time discretizations. Instead, they build on recent methodology for the exact simulation of the diffusion process and the unbiased estimation of the transition density. We introduce the generalized Poisson estimator, which generalizes the Poisson estimator of Beskos and co-workers. A central limit theorem is given for our particle filter scheme."
"10.1111/j.1467-9868.2008.00653.x","2008","Isotropic spectral additive models of the covariogram","1","A class of additive covariance models of an isotropic random process is proposed, motivated by the spectral representation of the covariance function. Model parameters are estimated by using a special case of the minimum norm quadratic estimation estimator, whose asymptotic moments have convenient expressions in terms of spectral densities. Fitting a model in this class is equivalent to fitting an additive model of the spectral density. The class of spectral additive models proposed is dense in the set of summable covariance functions having a spectral density, allowing approximately unbiased estimation of an arbitrary covariance function and its spectral density. Theoretical results are supported by numerical comparison with commonly used models. A procedure to assist model selection is proposed. The techniques are illustrated with an application to contaminant data."
"10.1111/j.1467-9868.2008.00658.x","2008","Non-parametric inference for clustered binary and count data when only summary information is available","1","Data in the form of pairs (X,Y), where the response Y is a count, arise in many applications, including problems involving stratified or two-stage sampling. Such data are often analysed by using random-effects models, where the distribution of Y, conditional on X and on an unobserved random parameter Theta, is taken to be either binomial or Poisson, and the distribution of Theta is connected through a link function to a random effect. The latter is sometimes supposed to be normally distributed, but that assumption can lead to serious biases if it is not satisfied. This difficulty has motivated an extensive literature on non-parametric techniques for cases where the random-effect distribution is unknown, but that methodology requires detailed cluster level data. No non-parametric techniques are available for instances where only summary data are accessible. We show that the random-effect distribution is actually non-parametrically identifiable from crude summary data, and we suggest a sieve method for inference in this case. Non-parametric approaches to both parameter estimation and prediction are introduced, and empirical techniques are developed for choosing tuning parameters. These methods are shown to outperform their parametric counterparts when the model is misspecified, and to perform almost as well as parametric methods when the model is correct."
"10.1111/j.1467-9868.2008.00656.x","2008","Modelling sparse generalized longitudinal observations with latent {G}aussian processes","1","In longitudinal data analysis one frequently encounters non-Gaussian data that are repeatedly collected for a sample of individuals over time. The repeated observations could be binomial, Poisson or of another discrete type or could be continuous. The timings of the repeated measurements are often sparse and irregular. We introduce a latent Gaussian process model for such data, establishing a connection to functional data analysis. The functional methods proposed are non-parametric and computationally straightforward as they do not involve a likelihood. We develop functional principal components analysis for this situation and demonstrate the prediction of individual trajectories from sparse observations. This method can handle missing data and leads to predictions of the functional principal component scores which serve as random effects in this model. These scores can then be used for further statistical analysis, such as inference, regression, discriminant analysis or clustering. We illustrate these non-parametric methods with longitudinal data on primary biliary cirrhosis and show in simulations that they are competitive in comparisons with generalized estimating equations and generalized linear mixed models."
"10.1111/j.1467-9868.2008.00654.x","2008","Modelling multivariate volatilities via conditionally uncorrelated components","2","We propose to model multivariate volatility processes on the basis of the newly defined conditionally uncorrelated components (CUCs). This model represents a parsimonious representation for matrix-valued processes. It is flexible in the sense that each CUC may be fitted separately with any appropriate univariate volatility model. Computationally it splits one high dimensional optimization problem into several lower dimensional subproblems. Consistency for the estimated CUCs has been established. A bootstrap method is proposed for testing the existence of CUCs. The methodology proposed is illustrated with both simulated and real data sets."
"10.1111/j.1467-9868.2007.00660.x","2008","Sampling bias and logistic models","1","In a regression model, the joint distribution for each finite sample of units is determined by a function p(x)(y) depending only on the list of covariate values x=(x(u(1)),...,x(u(n))) on the sampled units. No random sampling of units is involved. In biological work, random sampling is frequently unavoidable, in which case the joint distribution p(y,x) depends on the sampling scheme. Regression models can be used for the study of dependence provided that the conditional distribution p(y vertical bar x) for random samples agrees with p(x)(y) as determined by the regression model for a fixed sample having a non-random configuration x. The paper develops a model that avoids the concept of a fixed population of units, thereby forcing the sampling plan to be incorporated into the sampling distribution. For a quota sample having a predetermined covariate configuration x, the sampling distribution agrees with the standard logistic regression model with correlated components. For most natural sampling plans such as sequential or simple random sampling, the conditional distribution p(y vertical bar x) is not the same as the regression distribution unless p(x)(y) has independent components. In this sense, most natural sampling schemes involving binary random-effects models are biased. The implications of this formulation for subject-specific and population-averaged procedures are explored."
"10.1111/j.1467-9868.2007.00652.x","2008","On casting random-effects models in a survival framework","0","Logistic random-effects models are often employed in the analysis of correlated binary data. However, fitting these models is challenging, since the marginal distribution of the response variables is analytically intractable. Often, the random effects are treated as missing data for constructing traditional data augmentation algorithms. We create a novel alternative data augmentation scheme that simplifies the likelihood-based inference for logistic random-effects models. We cast the random-effects model in a 'survival framework', where each binary response is the censoring indicator for a survival time that is treated as additional missing data. Under this augmentation framework, the conditional expectations are free of unknown regression parameters. Such a construction has a particular advantage that, in the case of discrete covariates, the score equations for regression parameters have analytical solutions. Consequently, one does not need to resort to a search algorithm in estimating the regression parameters. We further create a parameter expansion scheme for logistic random-effects models under this survival data augmentation framework. The proposed data augmentation is illustrated when the random-effects distribution follows a multivariate Gaussian and multivariate t-distribution. The performance of the method is assessed through simulation studies and a real data analysis."
"10.1111/j.1467-9868.2008.00651.x","2008","Non-crossing non-parametric estimates of quantile curves","3","Since the introduction by Koenker and Bassett, quantile regression has become increasingly important in many applications. However, many non-parametric conditional quantile estimates yield crossing quantile curves (calculated for various p is an element of (0, 1)). We propose a new non-parametric estimate of conditional quantiles that avoids this problem. The method uses an initial estimate of the conditional distribution function in the first step and solves the problem of inversion and monotonization with respect to p is an element of (0, 1) simultaneously. It is demonstrated that the new estimates are asymptotically normally distributed with the same asymptotic bias and variance as quantile estimates that are obtained by inversion of a locally constant or locally linear smoothed conditional distribution function. The performance of the new procedure is illustrated by means of a simulation study and some comparisons with the currently available procedures which are similar in spirit with the method proposed are presented."
"10.1111/j.1467-9868.2007.00650.x","2008","Marginal likelihood estimation via power posteriors","0","Model choice plays an increasingly important role in statistics. From a Bayesian perspective a crucial goal is to compute the marginal likelihood of the data for a given model. However, this is typically a difficult task since it amounts to integrating over all model parameters. The aim of the paper is to illustrate how this may be achieved by using ideas from thermodynamic integration or path sampling. We show how the marginal likelihood can be computed via Markov chain Monte Carlo methods on modified posterior distributions for each model. This then allows Bayes factors or posterior model probabilities to be calculated. We show that this approach requires very little tuning and is straightforward to implement. The new method is illustrated in a variety of challenging statistical settings."
"10.1111/j.1467-9868.2007.00649.x","2008","A wavelet- or lifting-scheme-based imputation method","1","The paper proposes a new approach to imputation using the expected sparse representation of a surface in a wavelet or lifting scheme basis. Our method incorporates a Bayesian mixture prior for these wavelet coefficients into a Gibbs sampler to generate a complete posterior distribution for the variable of interest. Intuitively, the estimator operates by borrowing strength from those observed neighbouring values to impute at the unobserved sites. We demonstrate the strong performance of our estimator in both one- and two-dimensional imputation problems where we also compare its application with the standard imputation techniques of kriging and thin plate splines."
"10.1111/j.1467-9868.2007.00648.x","2008","Dated ancenstral trees from binary trait data and their application to the diversification of languages","0","Binary trait data record the presence or absence of distinguishing traits in individuals. We treat the problem of estimating ancestral trees with time depth from binary trait data. Simple analysis of such data is problematic. Each homology class of traits has a unique birth event on the tree, and the birth event of a trait that is visible at the leaves is biased towards the leaves. We propose a model-based analysis of such data and present a Markov chain Monte Carlo algorithm that can sample from the resulting posterior distribution. Our model is based on using a birth-death process for the evolution of the elements of sets of traits. Our analysis correctly accounts for the removal of singleton traits, which are commonly discarded in real data sets. We illustrate Bayesian inference for two binary trait data sets which arise in historical linguistics. The Bayesian approach allows for the incorporation of information from ancestral languages. The marginal prior distribution of the root time is uniform. We present a thorough analysis of the robustness of our results to model misspecification, through analysis of predictive distributions for external data, and fitting data that are simulated under alternative observation models. The reconstructed ages of tree nodes are relatively robust, whereas posterior probabilities for topology are not reliable."
"10.1111/j.1467-9868.2007.00647.x","2008","A semiparametric approach to canonical analysis","0","Classical canonical correlation analysis is one of the fundamental tools in statistics to investigate the linear association between two sets of variables. We propose a method, called semiparametric canonical analysis, to generalize canonical correlation analysis to incorporate the important non-linear association. Semiparametric canonical analysis is easy to implement and interpret. Statistical properties are proved. A consistent estimation method is developed. Selection of significant semiparametric canonical analysis components is discussed. Simulations suggest that the methods proposed have satisfactory performance in finite samples. One environmental data set and one data set in social science are investigated, in which non-linear canonical associations are observed and interpreted."
"10.1111/j.1467-9868.2007.00646.x","2008","Fast stable direct fitting and smoothness selection for generalized additive models","2","Existing computationally efficient methods for penalized likelihood generalized additive model fitting employ iterative smoothness selection on working linear models (or working mixed models). Such schemes fail to converge for a non-negligible proportion of models, with failure being particularly frequent in the presence of concurvity. If smoothness selection is performed by optimizing 'whole model' criteria these problems disappear, but until now attempts to do this have employed finite-difference-based optimization schemes which are computationally inefficient and can suffer from false convergence. The paper develops the first computationally efficient method for direct generalized additive model smoothness selection. It is highly stable, but by careful structuring achieves a computational efficiency that leads, in simulations, to lower mean computation times than the schemes that are based on working model smoothness selection. The method also offers a reliable way of fitting generalized additive mixed models."
"10.1111/j.1467-9868.2007.00645.x","2008","Proportion of non-zero normal means: universal oracle equivalences and uniformly consistent estimators","1","Since James and Stein's seminal work, the problem of estimating n normal means has received plenty of enthusiasm in the statistics community. Recently, driven by the fast expansion of the field of large-scale multiple testing, there has been a resurgence of research interest in the n normal means problem. The new interest, however, is more or less concentrated on testing n normal means: to determine simultaneously which means are 0 and which are not. In this setting, the proportion of the non-zero means plays a key role. Motivated by examples in genomics and astronomy, we are particularly interested in estimating the proportion of non-zero means, i.e. given n independent normal random variables with individual means X-j similar to N(mu(j),1), j=1,...,n, to estimate the proportion epsilon(n)=(1/n) #{j:mu(j) /= 0}. We propose a general approach to construct the universal oracle equivalence of the proportion. The construction is based on the underlying characteristic function. The oracle equivalence reduces the problem of estimating the proportion to the problem of estimating the oracle, which is relatively easier to handle. In fact, the oracle equivalence naturally yields a family of estimators for the proportion, which are consistent under mild conditions, uniformly across a wide class of parameters. The approach compares favourably with recent works by Meinshausen and Rice, and Genovese and Wasserman. In particular, the consistency is proved for an unprecedentedly broad class of situations; the class is almost the largest that can be hoped for without further constraints on the model. We also discuss various extensions of the approach, report results on simulation experiments and make connections between the approach and several recent procedures in large-scale multiple testing, including the false discovery rate approach and the local false discovery rate approach."
"10.1111/j.1467-9868.2007.00644.x","2008","A new method for analysing discrete life history data with missing covariate values","0","Regular censusing of wild animal populations produces data for estimating their annual survival. However, there can be missing covariate data; for instance time varying covariates that are measured on individual animals often contain missing values. By considering the transitions that occur from each occasion to the next, we derive a novel expression for the likelihood for mark-recapture-recovery data, which is equivalent to the traditional likelihood in the case where no covariate data are missing, and which provides a natural way of dealing with covariate data that are missing, for whatever reason. Unlike complete-case analysis, this approach does not exclude incompletely observed life histories, uses all available data and produces consistent estimators. In a simulation study it performs better overall than alternative methods when there are missing covariate data."
"10.1111/j.1467-9868.2007.00643.x","2008","{$\alpha$}-investing: a procedure for sequential control of expected false discoveries","2","alpha-investing is an adaptive sequential methodology that encompasses a large family of procedures for testing multiple hypotheses. All control mFDR, which is the ratio of the expected number of false rejections to the expected number of rejections. mFDR is a weaker criterion than the false discovery rate, which is the expected value of the ratio. We compensate for this weakness by showing that alpha-investing controls mFDR at every rejected hypothesis. alpha-investing resembles alpha-spending that is used in sequential trials, but it has a key difference. When a test rejects a null hypothesis, alpha-investing earns additional probability towards subsequent tests. alpha-investing hence allows us to incorporate domain knowledge into the testing procedure and to improve the power of the tests. In this way, alpha-investing enables the statistician to design a testing procedure for a specific problem while guaranteeing control of mFDR."
"10.1111/j.1467-9868.2007.00642.x","2008","Practical filtering with sequential parameter learning","1","The paper develops a simulation-based approach to sequential parameter learning and filtering in general state space models. Our approach is based on approximating the target posterior by a mixture of fixed lag smoothing distributions. Parameter inference exploits a sufficient statistic structure and the methodology can be easily implemented by modifying state space smoothing algorithms. We avoid reweighting particles and hence sample degeneracy problems that plague particle filters that use sequential importance sampling. The method is illustrated by using two examples: a benchmark auto-regressive model with observation error and a high dimensional dynamic spatiotemporal model. We show that the method provides accurate inference in the presence of outliers, model misspecification and high dimensionality."
"10.1111/j.1467-9868.2007.00641.x","2008","Semiparametrically efficient inference based on signs and ranks for median-restricted models","0","the pioneering work of Koenker and Bassett, median-restricted models have attracted considerable interest. Attention in these models, so far, has focused on least absolute deviation (auto-)regression quantile estimation and the corresponding sign tests. These methods use a pseudolikelihood that is based on a double-exponential reference density and enjoy quite attractive properties of root n consistency (for estimators) and distribution freeness (for tests). The paper extends these results to general, i.e. not necessarily double-exponential, reference densities. Using residual signs and ranks (not signed ranks) and a general reference density f, we construct estimators that remain root n consistent, irrespective of the true underlying density g (i.e. also for g not equal f). However, instead of reaching semiparametric efficiency bounds under double-exponential g, they reach these bounds when g coincides with the chosen reference density f. Moreover, we show that choosing reference densities other than the double-exponential in applications can lead to sizable gains in efficiency. The particular case of median regression is treated in detail; extensions to general quantile regression, heteroscedastic errors and time series models are briefly described. The performance of the method is also assessed by simulation and illustrated on financial data."
"10.1111/j.1467-9868.2007.00640.x","2008","Every missingness not at random model has a missingness at random counterpart with equal fit","0","Over the last decade a variety of models to analyse incomplete multivariate and longitudinal data have been proposed, many of which allowing for the missingness to be not at random, in the sense that the unobserved measurements influence the process governing missingness, in addition to influences coming from observed measurements and/or covariates. The fundamental problems that are implied by such models, to which we refer as sensitivity to unverifiable modelling assumptions, has, in turn, sparked off various strands of research in what is now termed sensitivity analysis. The nature of sensitivity originates from the fact that a missingness not at random (MNAR) model is not fully verifiable from the data, rendering the empirical distinction between MNAR and missingness at random (MAR), where only covariates and observed outcomes influence missingness, difficult or even impossible, unless we are willing to accept the posited MNAR model in an unquestioning way. We show that the empirical distinction between MAR and MNAR is not possible, in the sense that each MNAR model fit to a set of observed data can be reproduced exactly by an MAR counterpart. Of course, such a pair of models will produce different predictions of the unobserved outcomes, given the observed outcomes. Theoretical considerations are supplemented with an illustration that is based on the Slovenian public opinion survey, which has been analysed before in the context of sensitivity analysis."
"10.1111/j.1467-9868.2008.00639.x","2008","Variable selection in semiparametric linear regression with censored data","2","We describe two procedures for selecting variables in the semiparametric linear regression model for censored data. One procedure penalizes a vector of estimating equations and simultaneously estimates regression coefficients and selects submodels. A second procedure controls systematically the proportion of unimportant variables through forward selection and the addition of pseudorandom variables. We explore both rank-based statistics and Buckley-James statistics in the setting proposed and evaluate the performance of all methods through extensive simulation studies and one real data set."
"10.1111/j.1467-9868.2007.00638.x","2008","Empirical-likelihood-based difference-in-differences estimators","0","Recently there has been a surge in econometric and epidemiologic works focusing on estimating average treatment effects under various sets of assumptions. Estimation of average treatment effects in observational studies often requires adjustment for differences in pretreatment variables. Rosenbaum and Rubin have proposed the propensity score method for estimating the average treatment effect by adjusting pretreatment variables. In this paper, the empirical likelihood method is used to estimate average treatment effects on the treated under the difference-in-differences framework. The advantage of this approach is that the common marginal covariate information can be incorporated naturally to enhance the estimation of average treatment effects. Compared with other approaches in the literature, the method proposed can provide more efficient estimation. A simulation study and a real economic data analysis are presented."
"10.1111/j.1467-9868.2007.00637.x","2008","Generalized linear models incorporating population level information: an empirical-likelihood-based approach","0","In many situations information from a sample of individuals can be supplemented by population level information on the relationship between a dependent variable and explanatory variables. Inclusion of the population level information can reduce bias and increase the efficiency of the parameter estimates. Population level information can be incorporated via constraints on functions of the model parameters. In general the constraints are non-linear, making the task of maximum likelihood estimation more difficult. We develop an alternative approach exploiting the notion of an empirical likelihood. It is shown that, within the framework of generalized linear models, the population level information corresponds to linear constraints, which are comparatively easy to handle. We provide a two-step algorithm that produces parameter estimates by using only unconstrained estimation. We also provide computable expressions for the standard errors. We give an application to demographic hazard modelling by combining panel survey data with birth registration data to estimate annual birth probabilities by parity."
"10.1111/j.1467-9868.2007.00636.x","2008","Binary models for marginal independence","0","Log-linear models are a classical tool for the analysis of contingency tables. In particular, the subclass of graphical log-linear models provides a general framework for modelling conditional independences. However, with the exception of special structures, marginal independence hypotheses cannot be accommodated by these traditional models. Focusing on binary variables, we present a model class that provides a framework for modelling marginal independences in contingency tables. The approach that is taken is graphical and draws on analogies with multivariate Gaussian models for marginal independence. For the graphical model representation we use bidirected graphs, which are in the tradition of path diagrams. We show how the models can be parameterized in a simple fashion, and how maximum likelihood estimation can be performed by using a version of the iterated conditional fitting algorithm. Finally we consider combining these models with symmetry restrictions."
"10.1111/j.1467-9868.2007.00635.x","2008","Non-parametric small area estimation using penalized spline regression","0","The paper proposes a small area estimation approach that combines small area random effects with a smooth, non-parametrically specified trend. By using penalized splines as the representation for the non-parametric trend, it is possible to express the non-parametric small area estimation problem as a mixed effect model regression. The resulting model is readily fitted by using existing model fitting approaches such as restricted maximum likelihood. We present theoretical results on the prediction mean-squared error of the estimator proposed and on likelihood ratio tests for random effects, and we propose a simple non-parametric bootstrap approach for model inference and estimation of the small area prediction mean-squared error. The applicability of the method is demonstrated on a survey of lakes in north-eastern USA."
"10.1111/j.1467-9868.2007.00634.x","2008","Graphical models for marked point processes based on local independence","0","A new class of graphical models capturing the dependence structure of events that occur in time is proposed. The graphs represent so-called local independences, meaning that the intensities of certain types of events are independent of some (but not necessarilly all) events in the past. This dynamic concept of independence is asymmetric, similar to Granger non-causality, so the corresponding local independence graphs differ considerably from classical graphical models. Hence a new notion of graph separation, which is called delta-separation, is introduced and implications for the underlying model as well as for likelihood inference are explored. Benefits regarding facilitation of reasoning about and understanding of dynamic dependences as well as computational simplifications are discussed."
"10.1111/j.1467-9868.2007.00625.x","2008","A new reconstruction of multivariate normal orthant probabilities","0","A new method is introduced for geometrically reconstructing orthant probabilities for non-singular multivariate normal distributions. Orthant probabilities are expressed in terms of those for auto-regressive sequences and an efficient method is developed for numerical approximation of the latter. The approach allows more efficient accurate evaluation of the multivariate normal cumulative distribution function than previously, for many situations where the original distribution arises from a graphical model. An implementation is available as a package for the statistical software R and an application is given to multivariate probit models."
"10.1111/j.1467-9868.2007.00633.x","2008","Fixed rank kriging for very large spatial data sets","0","Spatial statistics for very large spatial data sets is challenging. The size of the data set, n, causes problems in computing optimal spatial predictors such as kriging, since its computational cost is of order n(3). In addition, a large data set is often defined on a large spatial domain, so the spatial process of interest typically exhibits non-stationary behaviour over that domain. A flexible family of non-stationary covariance functions is defined by using a set of basis functions that is fixed in number, which leads to a spatial prediction method that we call fixed rank kriging. Specifically, fixed rank kriging is kriging within this class of non-stationary covariance functions. It relies on computational simplifications when n is very large, for obtaining the spatial best linear unbiased predictor and its mean-squared prediction error for a hidden spatial process. A method based on minimizing a weighted Frobenius norm yields best estimators of the covariance function parameters, which are then substituted into the fixed rank kriging equations. The new methodology is applied to a very large data set of total column ozone data, observed over the entire globe, where n is of the order of hundreds of thousands."
"10.1111/j.1467-9868.2007.00622.x","2008","A two-stage procedure for comparing hazard rate functions","0","Comparison of two hazard rates is important in applications that are related to times to occurrence of a specific event. Conventional comparison procedures, such as the log-rank, Gehan-Wilcoxon and Peto-Peto tests, are powerful only when the two hazard rates do not cross each other. Because crossing hazard rates are common in practice, several procedures have been proposed in the literature for comparing such rates. However, most of these procedures consider only the alternative hypothesis with crossing hazard rates; many other realistic cases, including those when the two hazard rates run parallel to each other, are excluded from consideration. We propose a two-stage procedure that considers all possible alternatives, including ones with crossing or running parallel hazard rates. To define its significance level and p-value properly, a new procedure for handling the crossing hazard rates problem is suggested, which has the property that its test statistic is asymptotically independent of the test statistic of the log-rank test. We show that the two-stage procedure, with the log-rank test and the suggested procedure for handling the crossing hazard rates problem used in its two stages, performs well in applications in comparing two hazard rates."
"10.1111/j.1467-9868.2007.00632.x","2008","Variance estimation for statistics computed from inhomogeneous spatial point processes","0","The paper introduces a new approach to estimate the variance of statistics that are computed from an inhomogeneous spatial point process. The approach proposed is based on the assumption that the observed point process can be thinned to be a second-order stationary point process, where the thinning probability depends only on the first-order intensity function of the (unthinned) original process. The resulting variance estimator is proved to be asymptotically consistent for the target parameter under some very mild conditions. The use of the approach proposed is demonstrated in two important applications of modelling inhomogeneous spatial point processes: residual diagnostics of a fitted model and inference on the unknown regression coefficients. A simulation study and an application to a real data example are used to demonstrate the efficacy of the approach proposed."
"10.1111/j.1467-9868.2007.00631.x","2008","Theoretical measures of relative performance of classifiers for high dimensional data with small sample sizes","0","We suggest a technique, related to the concept of 'detection boundary' that was developed by Ingster and by Donoho and Jin, for comparing the theoretical performance of classifiers constructed from small training samples of very large vectors. The resulting 'classification boundaries' are obtained for a variety of distance-based methods, including the support vector machine, distance-weighted discrimination and kth-nearest-neighbour classifiers, for thresholded forms of those methods, and for techniques based on Donoho and Jin's higher criticism approach to signal detection. Assessed in these terms, standard distance-based methods are shown to be capable only of detecting differences between populations when those differences can be estimated consistently. However, the thresholded forms of distance-based classifiers can do better, and in particular can correctly classify data even when differences between distributions are only detectable, not estimable. Other methods, including higher criticism classifiers, can on occasion perform better still, but they tend to be more limited in scope, requiring substantially more information about the marginal distributions. Moreover, as tail weight becomes heavier the classification boundaries of methods designed for particular distribution types can converge to, and achieve, the boundary for thresholded nearest neighbour approaches. For example, although higher criticism has a lower classification boundary, and in this sense performs better, in the case of normal data, the boundaries are identical for exponentially distributed data when both sample sizes equal 1."
"10.1111/j.1467-9868.2007.00630.x","2008","Partially linear hazard regression with varying coefficients for multivariate survival data","0","The paper studies estimation of partially linear hazard regression models with varying coefficients for multivariate survival data. A profile pseudo-partial-likelihood estimation method is proposed. The estimation of the parameters of the linear part is accomplished via maximization of the profile pseudo-partial-likelihood, whereas the varying-coefficient functions are considered as nuisance parameters that are profiled out of the likelihood. It is shown that the estimators of the parameters are root n consistent and the estimators of the non-parametric coefficient functions achieve optimal convergence rates. Asymptotic normality is obtained for the estimators of the finite parameters and varying-coefficient functions. Consistent estimators of the asymptotic variances are derived and empirically tested, which facilitate inference for the model. We prove that the varying-coefficient functions can be estimated as well as if the parametric components were known and the failure times within each subject were independent. Simulations are conducted to demonstrate the performance of the estimators proposed. A real data set is analysed to illustrate the methodology proposed."
"10.1111/j.1467-9868.2007.00629.x","2008","Clustering using objective functions and stochastic search","0","A new approach to clustering multivariate data, based on a multilevel linear mixed model, is proposed. A key feature of the model is that observations from the same cluster are correlated, because they share cluster-specific random effects. The inclusion of cluster-specific random effects allows parsimonious departure from an assumed base model for cluster mean profiles. This departure is captured statistically via the posterior expectation, or best linear unbiased predictor. One of the parameters in the model is the true underlying partition of the data, and the posterior distribution of this parameter, which is known up to a normalizing constant, is used to cluster the data. The problem of finding partitions with high posterior probability is not amenable to deterministic methods such as the EM algorithm. Thus, we propose a stochastic search algorithm that is driven by a Markov chain that is a mixture of two Metropolis-Hastings algorithms-one that makes small scale changes to individual objects and another that performs large scale moves involving entire clusters. The methodology proposed is fundamentally different from the well-known finite mixture model approach to clustering, which does not explicitly include the partition as a parameter, and involves an independent and identically distributed structure."
"10.1111/j.1467-9868.2007.00623.x","2008","Model selection in high dimensions: a quadratic-risk-based approach","0","We propose a general class of risk measures which can be used for data-based evaluation of parametric models. The loss function is defined as the generalized quadratic distance between the true density and the model proposed. These distances are characterized by a simple quadratic form structure that is adaptable through the choice of a non-negative definite kernel and a bandwidth parameter. Using asymptotic results for the quadratic distances we build a quick-to-compute approximation for the risk function. Its derivation is analogous to the Akaike information criterion but, unlike the Akaike information criterion, the quadratic risk is a global comparison tool. The method does not require resampling, which is a great advantage when point estimators are expensive to compute. The method is illustrated by using the problem of selecting the number of components in a mixture model, where it is shown that, by using an appropriate kernel, the method is computationally straightforward in arbitrarily high data dimensions. In this same context it is shown that the method has some clear advantages over the Akaike information criterion and Bayesian information criterion."
"10.1111/j.1467-9868.2007.00628.x","2008","The combination of ecological and case-control data","0","Ecological studies, in which data are available at the level of the group, rather than at the level of the individual, are susceptible to a range of biases due to their inability to characterize within-group variability in exposures and confounders. To overcome these biases, we propose a hybrid design in which ecological data are supplemented with a sample of individual level case-control data. We develop the likelihood for this design and illustrate its benefits via simulation, both in bias reduction when compared with an ecological study and in efficiency gains relative to a conventional case-control study. An interesting special case of the design proposed is the situation where ecological data are supplemented with case-only data. The design is illustrated by using a data set of county-specific lung cancer mortality rates in the state of Ohio from 1988."
"10.1111/j.1467-9868.2007.00627.x","2008","The group {L}asso for logistic regression","0","The group lasso is an extension of the lasso to do variable selection on (predefined) groups of variables in linear regression models. The estimates have the attractive property of being invariant under groupwise orthogonal reparameterizations. We extend the group lasso to logistic regression models and present an efficient algorithm, that is especially suitable for high dimensional problems, which can also be applied to generalized linear models to solve the corresponding convex optimization problem. The group lasso estimator for logistic regression is shown to be statistically consistent even if the number of predictors is much larger than sample size but with sparse true underlying structure. We further use a two-stage procedure which aims for sparser models than the group lasso, leading to improved prediction performance for some cases. Moreover, owing to the two-stage nature, the estimates can be constructed to be hierarchical. The methods are used on simulated and real data sets about splice site detection in DNA sequences."
"10.1111/j.1467-9868.2007.00620.x","2008","Tail index estimation for heavy-tailed models: accommodation of bias in weighted log-excesses","0","We are interested in the derivation of the distributional properties of a weighted log-excesses estimator of a positive tail index gamma. One of the main objectives of such an estimator is the accommodation of the dominant component of asymptotic bias, together with the maintenance of the asymptotic variance of the maximum likelihood estimator of gamma, under a strict Pareto model. We consider the external estimation not only of a second-order shape parameter rho but also of a second-order scale parameter beta. This will enable us to reduce the asymptotic variance of the final estimators under consideration, compared with second-order reduced bias estimators that are already available in the literature. The second-order reduced bias estimators that are considered are also studied for finite samples, through Monte Carlo techniques, as well as applied to real data in the field of finance."
"10.1111/j.1467-9868.2007.00624.x","2008","The analysis of randomized response sum score variables","0","Randomized response (RR) is an interview technique that ensures confidentiality when questions are sensitive. In RR the answer to a sensitive question depends to a certain extent on a probability mechanism. As a result the observed data are partially misclassified, and the true status of the respondent is obscured. RR data are commonly analysed in a univariate way, with models that relate the observed responses to the prevalence of the sensitive characteristic, and with the more recent logistic regression models that relate the sensitive characteristic to a set of covariates. In an RR design with multiple sensitive questions, interest is usually not confined to the univariate prevalence and regression parameter estimates. Additional multivariate information may be obtained from an RR sum score variable, assessing the sum of sensitive characteristics that are associated with the respondent. However, the construction of an RR sum score variable is by no means straightforward, which might explain why sum scores have not yet been used within the context of RR. We present two models for RR sum score variables: the RR sum score model that relates the observed sum scores to the true sum scores and the RR proportional odds model that relates the true sum scores to covariates. The models are applied to RR data from a Dutch survey on non-compliance with social security regulations."
"10.1111/j.1467-9868.2007.00621.x","2008","Regression analysis based on semicompeting risks data","0","Semicompeting risks data are commonly seen in biomedical applications in which a terminal event censors a non-terminal event. Possible dependent censoring complicates statistical analysis. We consider regression analysis based on a non-terminal event, say disease progression, which is subject to censoring by death. The methodology proposed is developed for discrete covariates under two types of assumption. First, separate copula models are assumed for each covariate group and then a flexible regression model is imposed on the progression time which is of major interest. Model checking procedures are also proposed to help to choose a best-fitted model. Under a two-sample setting, Lin and co-workers proposed a competing method which requires an additional marginal assumption on the terminal event and implicitly assumes that the dependence structures in the two groups are the same. Using simulations, we compare the two approaches on the basis of their finite sample performances and robustness properties under model misspecification. The method proposed is applied to a bone marrow transplant data set."
"10.1111/j.1467-9868.2007.00617.x","2007","Criteria for surrogate end points","0","A surrogate end point is often used to evaluate the effects of treatments or exposures on the true end point in medical researches. Various criteria for the statistical surrogate, principal surrogate and strong surrogate have been proposed. We first illustrate that, with a surrogate end point that is defined by these criteria, it is possible that a treatment has a positive effect on the surrogate, which in turn has a positive effect on the true end point, but the treatment has a negative effect on the true end point. We define such a phenomenon as a surrogate paradox. The surrogate paradox also means that the sign of the treatment effect on the true end point is unpredictable by the effect signs of both the treatment on the surrogate and the surrogate on the true end point. Then we propose two notions for a consistent surrogate and a strictly consistent surrogate to avoid the surrogate paradox. With the causal network that was presented by Lauritzen, we discuss the conditions for a strong surrogate to be a consistent surrogate and a strictly consistent surrogate."
"10.1111/j.1467-9868.2007.00616.x","2007","A new test for the parametric form of the variance function in non-parametric regression","0","In the common non-parametric regression model the problem of testing for the parametric form of the conditional variance is considered. A stochastic process based on the difference between the empirical processes that are obtained from the standardized non-parametric residuals under the null hypothesis (of a specific parametric form of the variance function) and the alternative is introduced and its weak convergence established. This result is used for the construction of a Kolmogorov-Smimov and a Cramer-von Mises type of statistic for testing the parametric form of the conditional variance. The consistency of a bootstrap approximation is established, and the finite sample properties of this approximation are investigated by means of a simulation study. In particular the new procedure is compared with some of the currently available methods for this problem."
"10.1111/j.1467-9868.2007.00615.x","2007","Semiparametric estimation of treatment effects given base-line covariates on an outcome measured after a post-randomization event occurs","1","We consider estimation, from a double-blind randomized trial, of treatment effect within levels of base-line covariates on an outcome that is measured after a post-treatment event E has occurred in the subpopulation P-E,P-E that would experience event E regardless of treatment. Specifically, we consider estimation of the parameters gamma indexing models for the outcome mean conditional on treatment and base-line covariates in the subpopulation P-E,P-E. Such parameters are not identified from randomized trial data but become identified if additionally it is assumed that the subpopulation P-(E) over bar ,P-E of subjects that would experience event E under the second treatment but not under the first is empty and a parametric model for the conditional probability that a subject experiences event E if assigned to the first treatment given that the subject would experience the event if assigned to the second treatment, his or her outcome under the second treatment and his or her pretreatment covariates. We develop a class of estimating equations whose solutions comprise, up to asymptotic equivalence, all consistent and asymptotically normal estimators of gamma under these two assumptions. In addition, we derive a locally semiparametric efficient estimator of gamma. We apply our methods to estimate the effect on mean viral load of vaccine versus placebo after infection with human immunodeficiency virus (the event E) in a placebo-controlled randomized acquired immune deficiency syndrome vaccine trial."
"10.1111/j.1467-9868.2007.00614.x","2007","Non-parametric regression estimation from data contaminated by a mixture of {B}erkson and classical errors","2","Estimation of a regression function is a well-known problem in the context of errors in variables, where the explanatory variable is observed with random noise. This noise can be of two types, which are known as classical or Berkson, and it is common to assume that the error is purely of one of these two types. In practice, however, there are many situations where the explanatory variable is contaminated by a mixture of the two errors. In such instances, the Berkson component typically arises because the variable of interest is not directly available and can only be assessed through a proxy, whereas the inaccuracy that is related to the observation of the latter causes an error of classical type. We propose a non-parametric estimator of a regression function from data that are contaminated by a mixture of the two errors. We prove consistency of our estimator, derive rates of convergence and suggest a data-driven implementation. Finite sample performance is illustrated via simulated and real data examples."
"10.1111/j.1467-9868.2007.00613.x","2007","Statistical classification with missing covariates","0","Some results related to statistical classification in the presence of missing covariates are presented. We derive representations for the best (Bayes) classifier when some of the covariates can be missing; this is done without imposing any assumptions on the underlying missing probability mechanism. Furthermore, without assuming any missingness-at-random type of conditions, we also construct Bayes consistent classifiers that do not require any imputation-based techniques. Both parametric and non-parametric situations are considered but the emphasis is on the latter. In addition to simple missingness patterns, we also consider the full Swiss cheese model, where the missing covariates can be anywhere. Both mechanics and the theoretical validity of our results are discussed."
"10.1111/j.1467-9868.2007.00612.x","2007","Order-free co-regionalized areal data models with application to multiple-disease mapping","1","With the ready availability of spatial databases and geographical information system software, statisticians are increasingly encountering multivariate modelling settings featuring associations of more than one type: spatial associations between data locations and associations between the variables within the locations. Although flexible modelling of multivariate point-referenced data has recently been addressed by using a linear model of co-regionalization, existing methods for multivariate areal data typically suffer from unnecessary restrictions on the covariance structure or undesirable dependence on the conditioning order of the variables. We propose a class of Bayesian hierarchical models for multivariate areal data that avoids these restrictions, permitting flexible and order-free modelling of correlations both between variables and across areal units. Our framework encompasses a rich class of multivariate conditionally autoregressive models that are computationally feasible via modern Markov chain Monte Carlo methods. We illustrate the strengths of our approach over existing models by using simulation studies and also offer a real data application involving annual lung, larynx and oesophageal cancer death-rates in Minnesota counties between 1990 and 2000."
"10.1111/j.1467-9868.2007.00611.x","2007","Measurement error modelling with an approximate instrumental variable","1","Consider using regression modelling to relate an exposure (predictor) variable to a disease outcome (response) variable. If the exposure variable is measured with error, but this error is ignored in the analysis, then misleading inferences can result. This problem is well known and has spawned a large literature on methods which adjust for measurement error in predictor variables. One theme is that the requisite assumptions about the nature of the measurement error can be stronger than what is actually known in many practical situations. In particular, the assumptions that are required to yield a model which is formally identified from the observable data can be quite strong. The paper deals with one particular strategy for measurement error modelling, namely that of seeking an instrumental variable, i.e. a covariate S which is associated with exposure and conditionally independent of the outcome given exposure. If these two conditions hold exactly, then we call S an exact instrumental variable, and an identified model results. However, the second is not checkable empirically, since the actual exposure is unobserved. In practice then, investigators typically seek a covariate which is plausibly thought to satisfy it. We study inferences which acknowledge the approximate nature of this assumption. In particular, we consider Bayesian inference with a prior distribution that posits that S is probably close to conditionally independent of outcome given exposure. We refer to this as an approximate instrumental variable assumption. Although the approximate instrumental variable assumption is more realistic for most applications, concern arises that a non-identified model may result. Thus the paper contrasts inferences arising from the approximate instrumental variable assumption with their exact instrumental variable counterparts, with particular emphasis on the benefit of basing inferences on a more realistic model versus the cost of basing inferences on a non-identified model."
"10.1111/j.1467-9868.2007.00610.x","2007","Parameter estimation for differential equations: a generalized smoothing approach","9","We propose a new method for estimating parameters in models that are defined by a system of non-linear differential equations. Such equations represent changes in system outputs by linking the behaviour of derivatives of a process to the behaviour of the process itself. Current methods for estimating parameters in differential equations from noisy data are computationally intensive and often poorly suited to the realization of statistical objectives such as inference and interval estimation. The paper describes a new method that uses noisy measurements on a subset of variables to estimate the parameters defining a system of non-linear differential equations. The approach is based on a modification of data smoothing methods along with a generalization of profiled estimation. We derive estimates and confidence intervals, and show that these have low bias and good coverage properties respectively for data that are simulated from models in chemical engineering and neurobiology. The performance of the method is demonstrated by using real world data from chemistry and from the progress of the autoimmune disease lupus."
"10.1111/j.1467-9868.2007.00609.x","2007","Controlling the reinforcement in {B}ayesian non-parametric mixture models","0","The paper deals with the problem of determining the number of components in a mixture model. We take a Bayesian non-parametric approach and adopt a hierarchical model with a suitable non-parametric prior for the latent structure. A commonly used model for such a problem is the mixture of Dirichlet process model. Here, we replace the Dirichlet process with a more general non-parametric prior obtained from a generalized gamma process. The basic feature of this model is that it yields a partition structure for the latent variables which is of Gibbs type. This relates to the well-known (exchangeable) product partition models. If compared with the usual mixture of Dirichlet process model the advantage of the generalization that we are examining relies on the availability of an additional parameter a belonging to the interval (0,1): it is shown that such a parameter greatly influences the clustering behaviour of the model. A value of a that is close to 1 generates a large number of clusters, most of which are of small size. Then, a reinforcement mechanism which is driven by (T acts on the mass allocation by penalizing clusters of small size and favouring those few groups containing a large number of elements. These features turn out to be very useful in the context of mixture modelling. Since it is difficult to specify a priori the reinforcement rate, it is reasonable to specify a prior for sigma. Hence, the strength of the reinforcement mechanism is controlled by the data."
"10.1111/j.1467-9868.2007.00608.x","2007","Continuous time modelling of dynamical spatial lattice data observed at sparsely distributed times","0","We consider statistical and computational aspects of simulation-based Bayesian inference for a spatial-temporal model based on a multivariate point process which is only observed at sparsely distributed times. The point processes are indexed by the sites of a spatial lattice, and they exhibit spatial interaction. For specificity we consider a particular dynamical spatial lattice data set which has previously been analysed by a discrete time model involving unknown normalizing constants. We discuss the advantages and disadvantages of using continuous time processes compared with discrete time processes in the setting of the present paper as well as other spatial-temporal situations."
"10.1111/j.1467-9868.2007.00605.x","2007","Functional clustering and identifying substructures of longitudinal data","3","A functional clustering (FC) method, k-centres FC, for longitudinal data is proposed. The k-centres FC approach accounts for both the means and the modes of variation differentials between clusters by predicting cluster membership with a reclassification step. The cluster membership predictions are based on a non-parametric random-effect model of the truncated Karhunen-Lobve expansion, coupled with a non-parametric iterative mean and covariance updating scheme. We show that, under the identifiability conditions derived, the k-centres FC method proposed can greatly improve cluster quality as compared with conventional clustering algorithms. Moreover, by exploring the mean and covariance functions of each cluster, the k-centres FC method provides an additional insight into cluster structures which facilitates functional cluster analysis. Practical performance of the k-centres FC method is demonstrated through simulation studies and data applications including growth curve and gene expression profile data."
"10.1111/j.1467-9868.2007.00607.x","2007","{$L_1$}-regularization path algorithm for generalized linear models","12","We introduce a path following algorithm for L-1-regularized generalized linear models. The L-1-regularization procedure is useful especially because it, in effect, selects variables according to the amount of penalization on the L-1-norm of the coefficients, in a manner that is less greedy than forward selection-backward deletion. The generalized linear model path algorithm efficiently computes solutions along the entire regularization path by using the predictor-corrector method of convex optimization. Selecting the step length of the regularization parameter is critical in controlling the overall accuracy of the paths; we suggest intuitive and flexible strategies for choosing appropriate values. We demonstrate the implementation with several simulated and real data sets."
"10.1111/j.1467-9868.2007.00604.x","2007","Statistical inference for evolving periodic functions","1","In the study of variable stars, where the light reaching an observer fluctuates over time, it can be difficult to explain the nature of the variation unless it follows a regular pattern. In this respect, so-called periodic variable stars are particularly amenable to analysis. There, radiation varies in a perfectly periodic fashion, and period length is a major focus of interest. We develop methods for conducting inference about features that might account for departures from strict periodicity. These include variation, over time, of the period or amplitude of radiation. We suggest methods for estimating the parameters of this evolution, and for testing the hypothesis that the evolution is present. This problem has some unusual features, including subtle issues of identifiability."
"10.1111/j.1467-9868.2007.00603.x","2007","Nested generalized linear mixed models: an orthodox best linear unbiased predictor approach","0","We introduce a new class of generalized linear mixed models based on the Tweedie exponential dispersion model distributions, accommodating a wide range of discrete, continuous and mixed data. Using the best linear unbiased predictor of random effects, we obtain an optimal estimating function for the regression parameters in the sense of Godambe, allowing an efficient common fitting algorithm for the whole class. Although allowing full parametric inference, our main results depend only on the first- and second-moment assumptions of unobserved random effects. In addition, we obtain consistent estimators for both regression and dispersion parameters. We illustrate the method by analysing the epilepsy data and cake baking data. Along with simulations and asymptotic justifications, this shows the usefulness of the method for analysis of clustered non-normal data."
"10.1111/j.1467-9868.2007.00602.x","2007","Convergence rates and asymptotic standard errors for {M}arkov chain {M}onte {C}arlo algorithms for {B}ayesian probit regression","5","Consider a probit regression problem in which Y1,..., Y-n are independent Bernoulli random variables such that Pr(Y-j = 1) = Phi(x(i)(T) beta) where x(i) is a p-dimensional vector of known covariates that are associated with Yj, 0 is' a p-dimensional vector of unknown regression coefficients and Phi(.) denotes the standard normal distribution function. We study Markov chain Monte Carlo algorithms for exploring the intractable posterior density that results when the probit regression likelihood is combined with a flat prior on beta. We prove that Albert and Chib's data augmentation algorithm and Liu and Wu's PX-DA algorithm both converge at a geometric rate, which ensures the existence of central limit theorems for ergodic averages under a second-moment condition. Although these two algorithms are essentially equivalent in terms of computational complexity, results of Hobert and Marchev imply that the PX-DA algorithm is theoretically more efficient in the sense that the asymptotic variance in the central limit theorem under the PX-DA algorithm is no larger than that under Albert and Chib's algorithm. We also construct minorization conditions that allow us to exploit regenerative simulation techniques for the consistent estimation of asymptotic variances. As an illustration, we apply our results to van Dyk and Meng's lupus data. This example demonstrates that huge gains in efficiency are possible by using the PX-DA algorithm instead of Albert and Chib's algorithm."
"10.1111/j.1467-9868.2007.00601.x","2007","On-line inference for multiple changepoint problems","1","We propose an on-line algorithm for exact filtering of multiple changepoint problems. This algorithm enables simulation from the true joint posterior distribution of the number and position of the changepoints for a class of changepoint models. The computational cost of this exact algorithm is quadratic in the number of observations. We further show how resampling ideas from particle filters can be used to reduce the computational cost to linear in the number of observations, at the expense of introducing small errors, and we propose two new, optimum resampling algorithms for this problem. One, a version of rejection control, allows the particle filter to choose the number of particles that are required at each time step automatically. The new resampling algorithms substantially outperform standard resampling algorithms on examples that we consider; and we demonstrate how the resulting particle filter is practicable for segmentation of human G+C content."
"10.1111/j.1467-9868.2007.00600.x","2007","Estimating the effect of treatment in a proportional hazards model in the presence of non-compliance and contamination","1","Methods for adjusting for non-compliance and contamination, which respect the randomization, are extended from binary outcomes to time-to-event analyses by using a proportional hazards model. A simple non-iterative method is developed when there are no covariates, which is a generalization of the Mantel-Haenszel estimator. More generally, a 'partial likelihood' is developed which accommodates covariates under the assumption that they are independent of compliance. A key feature is that the proportion of contaminators and non-compliers in the risk set is updated at each failure time. When covariates are not independent of compliance, a full likelihood is developed and explored, but this leads to a complex estimator. Estimating equations and information matrices are derived for these estimators and they are evaluated by simulation studies."
"10.1111/j.1369-7412.2007.00606.x","2007","Maximum likelihood estimation in semiparametric regression models with censored data","13","Semiparametric regression models play a central role in formulating the effects of covariates on potentially censored failure times and in the joint modelling of incomplete repeated measures and failure times in longitudinal studies. The presence of infinite dimensional parameters poses considerable theoretical and computational challenges in the statistical analysis of such models. We present several classes of semiparametric regression models, which extend the existing models in important directions. We construct appropriate likelihood functions involving both finite dimensional and infinite dimensional parameters. The maximum likelihood estimators are consistent and asymptotically normal with efficient variances. We develop simple and stable numerical techniques to implement the corresponding inference procedures. Extensive simulation experiments demonstrate that the inferential and computational methods proposed perform well in practical settings. Applications to three medical studies yield important new insights. We conclude that there is no reason, theoretical or numerical, not to use maximum likelihood estimation for semiparametric regression models. We discuss several areas that need further research."
"10.1111/j.1467-9868.2007.599.x","2007","Non-parametric confidence bands in deconvolution density estimation","1","Uniform confidence bands for densities f via non-parametric kernel estimates were first constructed by Bickel and Rosenblatt. In this paper this is extended to confidence bands in the deconvolution problem g=f*psi for an ordinary smooth error density psi. Under certain regularity conditions, we obtain asymptotic uniform confidence bands based on the asymptotic distribution of the maximal deviation (L-infinity-distance) between a deconvolution kernel estimator (f) over cap and f. Further consistency of the simple non-parametric bootstrap is proved. For our theoretical developments the bias is simply corrected by choosing an undersmoothing bandwidth. For practical purposes we propose a new data-driven bandwidth selector that is based on heuristic arguments, which aims at minimizing the L-infinity-distance between (f) over cap and f. Although not constructed explicitly to undersmooth the estimator, a simulation study reveals that the bandwidth selector suggested performs well in finite samples, in terms of both area and coverage probability of the resulting confidence bands. Finally the methodology is applied to measurements of the metallicity of local F and G dwarf stars. Our results confirm the 'G dwarf problem', i.e. the lack of metal poor G dwarfs relative to predictions from 'closed box models' of stellar formation."
"10.1111/j.1467-9868.2007.00598.x","2007","Estimation of treatment effects in randomized trials with non-compliance and a dichotomous outcome","1","We propose a class of estimators of the treatment effect on a dichotomous outcome among the treated subjects within covariate and treatment arm strata in randomized trials with non-compliance. Recent papers by Vansteelandt and Goetghebeur, and Robins and Rotnitzky have presented consistent and asymptotically linear estimators of a causal odds ratio, which rely, beyond correct specification of a model for the causal odds ratio, on a correctly specified model for a potentially high dimensional nuisance parameter. In this paper we propose consistent, asymptotically linear and locally efficient estimators of a causal relative risk and a new parameter-called a switch causal relative risk-which relies only on the correct specification of a model for the parameter of interest. Our estimators are always consistent and asymptotically linear at the null hypothesis of no-treatment effect, thereby providing valid testing procedures. We examine the finite sample properties of these instrumental-variable-based estimators and the associated testing procedures in simulations and a data analysis of decaffeinated coffee consumption and miscarriage."
"10.1111/j.1467-9868.2007.00597.x","2007","Coefficient sign prediction methods for model selection","0","We consider a Bayesian model selection strategy based on predicting the signs of the coefficients in a regression model, i.e. we consider identification of coefficients in a full or encompassing model for which we can confidently predict whether they are positive or negative. This is useful when our main purpose in doing model selection is interpretation, since the sign of a coefficient is often of primary importance for this task. In the case of a linear model with standard non-informative prior, we connect our sign coefficient prediction approach to the classical Zheng-Loh procedure for model selection. One advantage of our approach is that only specification of a prior on the full model is required, unlike standard Bayesian variable selection approaches which require specification of prior distributions on parameters in all submodels, and specification of a prior on the model itself. We consider applying our method with proper hierarchical shrinkage priors, which makes the procedure more useful in 'large p, small n' regression problems with more predictors than observations and in problems involving multicollinearity. In these problems we may wish to do prediction by using shrinkage methods in the full model, but interpreting which variables are important is also of interest. We compare selection by using our coefficient sign prediction approach with the recently proposed elastic net procedure of Zou and Hastie and observe that our method shares some of the features of the elastic net such as a group selection property. The method can be extended to more complex model selection problems such as selection on variance components in random-effects models. For selection on variance components where the parameter of interest is non-negative and hence prediction of the sign of the parameter not the appropriate way to proceed, we consider instead prediction of the sign of the score component for the parameter at zero, obtaining a method that is related to classical score tests on variance components."
"10.1111/j.1467-9868.2007.00596.x","2007","Semiparametric estimators of functional measurement error models with unknown error","2","We consider functional measurement error models where the measurement error distribution is estimated non-parametrically. We derive a locally efficient semiparametric estimator but propose not to implement it owing to its numerical complexity. Instead, a plug-in estimator is proposed, where the measurement error distribution is estimated through non-parametric kernel methods based on multiple measurements. The root n consistency and asymptotic normality of the plug-in estimator are derived. Despite the theoretical inefficiency of the plug-in estimator, simulations demonstrate its near optimal performance. Computational advantages relative to the theoretically efficient estimator make the plug-in estimator practically appealing. Application of the estimator is illustrated by using the Framingham data example."
"10.1111/j.1467-9868.2007.00595.x","2007","Orthogonal locally ancillary estimating functions for matched pair studies and errors in covariates","0","We propose an estimating function method for two related applications, matched pair studies and studies with errors in covariates under a functional model, where a mismeasured unknown scalar covariate is treated as a fixed nuisance parameter. Our method addresses the severe inferential problem that is posed by an abundance of nuisance parameters in these two applications. We propose orthogonal locally ancillary estimating functions for these two applications that depend on merely the mean model and partial modelling of the variances of the observations (and observed mismeasured covariate, if applicable), and we achieve first-order bias correction of inferences under a 'small dispersion and large sample size' asymptotic. Simulation results confirm that the estimator proposed is largely improved over that using a regular profile estimating function. We apply the approach proposed to a length of hospital stay study with a mismeasured covariate."
"10.1111/j.1467-9868.2007.00594.x","2007","Inference of trends in time series","4","We consider statistical inference of trends in mean non-stationary models. A test statistic is proposed for the existence of structural breaks in trends. On the basis of a strong invariance principle of stationary processes, we construct simultaneous confidence bands with asymptotically correct nominal coverage probabilities. The results are applied to global warming temperature data and Nile river flow data. Our confidence band of the trend of the global warming temperature series supports the claim that the trend is increasing over the last 150 years."
"10.1111/j.1467-9868.2007.00593.x","2007","Bootstrapping clustered data","2","Various bootstraps have been proposed for bootstrapping clustered data from one-way arrays. The simulation results in the literature suggest that some of these methods work quite well in practice; the theoretical results are limited and more mixed in their conclusions. For example, McCullagh reached negative conclusions about the use of non-parametric bootstraps for one-way arrays. The purpose of this paper is to extend our understanding of the issues by discussing the effect of different ways of modelling clustered data, the criteria for successful bootstraps used in the literature and extending the theory from functions of the sample mean to include functions of the between and within sums of squares and non-parametric bootstraps to include model-based bootstraps. We determine that the consistency of variance estimates for a bootstrap method depends on the choice of model with the residual bootstrap giving consistency under the transformation model whereas the cluster bootstrap gives consistent estimates under both the transformation and the random-effect model. In addition we note that the criteria based on the distribution of the bootstrap observations are not really useful in assessing consistency."
"10.1111/j.1467-9868.2007.005592.x","2007","The optimal discovery procedure: a new approach to simultaneous significance testing","10","The Neyman-Pearson lemma provides a simple procedure for optimally testing a single hypothesis when the null and alternative distributions are known. This result has played a major role in the development of significance testing strategies that are used in practice. Most of the work extending single-testing strategies to multiple tests has focused on formulating and estimating new types of significance measures, such as the false discovery rate. These methods tend to be based on p-values that are calculated from each test individually, ignoring information from the other tests. I show here that one can improve the overall performance of multiple significance tests by borrowing information across all the tests when assessing the relative significance of each one, rather than calculating p-values for each test individually. The 'optimal discovery procedure' is introduced, which shows how to maximize the number of expected true positive results for each fixed number of expected false positive results. The optimality that is achieved by this procedure is shown to be closely related to optimality in terms of the false discovery rate. The optimal discovery procedure motivates a new approach to testing multiple hypotheses, especially when the tests are related. As a simple example, a new simultaneous procedure for testing several normal means is defined; this is surprisingly demonstrated to outperform the optimal single-test procedure, showing that a method which is optimal for single tests may no longer be optimal for multiple tests. Connections to other concepts in statistics are discussed, including Stein's paradox, shrinkage estimation and the Bayesian approach to hypothesis testing."
"10.1111/j.1467-9868.2007.00591.x","2007","Dimension reduction and coefficient estimation in multivariate linear regression","4","We introduce a general formulation for dimension reduction and coefficient estimation in the multivariate linear model. We argue that many of the existing methods that are commonly used in practice can be formulated in this framework and have various restrictions. We continue to propose a new method that is more flexible and more generally applicable. The method proposed can be formulated as a novel penalized least squares estimate. The penalty that we employ is the coefficient matrix's Ky Fan norm. Such a penalty encourages the sparsity among singular values and at the same time gives shrinkage coefficient estimates and thus conducts dimension reduction and coefficient estimation simultaneously in the multivariate linear model. We also propose a generalized cross-validation type of criterion for the selection of the tuning parameter in the penalized least squares. Simulations and an application in financial econometrics demonstrate competitive performance of the new method. An extension to the non-parametric factor model is also discussed."
"10.1111/j.1467-9868.2007.00590.x","2007","Analysis of failure time data under competing censoring mechanisms","0","We derive estimators of the survival curve of a failure time in the presence of competing right censoring mechanisms. Our approach allows for the possibility that some or all of the competing censoring mechanisms are associated with the end point, even after adjustment for recorded prognostic factors. It also allows the degree of residual association to be possibly different for distinct censoring processes. Our methods generalize from one to several competing censoring mechanisms the methods of Scharfstein and Robins."
"10.1111/j.1467-9868.2007.00589.x","2007","Mixture cure survival models with dependent censoring","3","The paper is motivated by cure detection among the prostate cancer patients in the National Institutes of Health surveillance epidemiology and end results programme, wherein the main end point (e.g. deaths from prostate cancer) and the censoring causes (e.g. deaths from heart diseases) may be dependent. Although many researchers have studied the mixture survival model to analyse survival data with non-negligible cure fractions, none has studied the mixture cure model in the presence of dependent censoring. To account for such dependence, we propose a more general cure model that allows for dependent censoring. We derive the cure models from the perspective of competing risks and model the dependence between the censoring time and the survival time by using a class of Archimedean copula models. Within this framework, we consider the parameter estimation, the cure detection and the two-sample comparison of latency distributions in the presence of dependent censoring when a proportion of patients is deemed cured. Large sample results by using martingale theory are obtained. We examine the finite sample performance of the proposed methods via simulation and apply them to analyse the surveillance epidemiology and end results prostate cancer data."
"10.1111/j.1467-9868.2007.00588.x","2007","Inference and model choice for sequentially ordered hidden {M}arkov models","0","The system equation of a hidden Markov model is rewritten to label the components by order of appearance, and to make explicit the random behaviour of the number of components, m(t). We argue that this reformulation is often a good way to achieve identifiability, as it facilitates the interpretation of the posterior density, and the estimation of the number of components that have appeared in a given sample. We develop a sequential Monte Carlo algorithm for estimating the reformulated model, which relies on particle filtering and Gibbs sampling. Our algorithm has a computational cost that is similar to that of a Markov chain Monte Carlo sampler and is much less likely to be affected by label switching, i.e. the possibility of becoming trapped in a local mode of the posterior density. The extension to transdimensional priors is also considered. The approach is illustrated by two real data examples."
"10.1111/j.1467-9868.2007.00587.x","2007","Probabilistic forecasts, calibration and sharpness","10","Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection."
"10.1111/j.1467-9868.2007.00586.x","2007","An optimal experimental design criterion for discriminating between non-normal models","1","Typically T-optimality is used to obtain optimal designs to discriminate between homoscedastic models with normally distributed observations. Some extensions of this criterion have been made for the heteroscedastic case and binary response models in the literature. In this paper, a new criterion based on the Kullback-Leibler distance is proposed to discriminate between rival models with non-normally distributed observations. The criterion is coherent with the approaches mentioned above. An equivalence theorem is provided for this criterion and an algorithm to compute optimal designs is developed. The criterion is applied to discriminate between the popular Michaelis-Menten model and a typical extension of it under the log-normal and the gamma distributions."
"10.1111/j.1467-9868.2007.00585.x","2007","A mixture model for multivariate extremes","1","The spectral density function plays a key role in fitting the tail of multivariate extre-mal data and so in estimating probabilities of rare events. This function satisfies moment con-straints but unlike the univariate extreme value distributions has no simple parametric form. Parameterized subfamilies of spectral densities have been suggested for use in applications, and non-parametric estimation procedures have been proposed, but semiparametric models for multivariate extremes have hitherto received little attention. We show that mixtures of Dirichlet distributions satisfying the moment constraints are weakly dense in the class of all non-parametric spectral densities, and discuss frequentist and Bayesian inference in this class based on the EM algorithm and reversible jump Markov chain Monte Carlo simulation. We illustrate the ideas using simulated and real data."
"10.1111/j.1467-9868.2007.00584.x","2007","Identifying direct and indirect effects in a non-counterfactual framework","2","Identifying direct and indirect effects is a common problem in the social science and medical literature and can be described as follows. A treatment is administered and a response is recorded. However, another variable mediates the effect of the treatment on the response, in some way channelling a part of the treatment effect. The question is how to extricate the direct and channelled (indirect) effects from one another when it is not possible to intervene on the mediating variable. The aim of the paper is to tackle this problem by using a model for direct and indirect effects based on the decision theoretic framework for causal inference."
"10.1111/j.1467-9868.2007.00583.x","2007","On fast computation of the non-parametric maximum likelihood estimate of a mixing distribution","2","A fast algorithm for computing the non-parametric maximum likelihood estimate of a mixing distribution is presented. At each iteration, the algorithm adds new important points to the support set as guided by the gradient function, updates all mixing proportions via a quadratically convergent method and discards redundant support points straightaway. With its convergence being theoretically established, numerical studies show that it is very fast and stable, compared with several other algorithms that are available in the literature."
"10.1111/j.1467-9868.2007.00582.x","2007","Bayesian density regression","14","The paper considers Bayesian methods for density regression, allowing a random probability distribution to change flexibly with multiple predictors. The conditional response distribution is expressed as a non-parametric mixture of regression models, with the mixture distribution changing with predictors. A class of weighted mixture of Dirichlet process priors is proposed for the uncountable collection of mixture distributions. It is shown that this specification results in a generalized Polya urn scheme, which incorporates weights that are dependent on the distance between subjects' predictor values. To allow local dependence in the mixture distributions, we propose a kernel-based weighting scheme. A Gibbs sampling algorithm is developed for posterior computation. The methods are illustrated by using simulated data examples and an epidemiologic application."
"10.1111/j.1467-9868.2007.00581.x","2007","On the non-negative garrote estimator","9","We study the non-negative garrotte estimator from three different aspects: consistency, computation and flexibility. We argue that the non-negative garrotte is a general procedure that can be used in combination with estimators other than the original least squares estimator as in its original form. In particular, we consider using the lasso, the elastic net and ridge regression along with ordinary least squares as the initial estimate in the non-negative garrotte. We prove that the non-negative garrotte has the nice property that, with probability tending to 1, the solution path contains an estimate that correctly identifies the set of important variables and is consistent for the coefficients of the important variables, whereas such a property may not be valid for the initial estimators. In general, we show that the non-negative garrotte can turn a consistent estimate into an estimate that is not only consistent in terms of estimation but also in terms of variable selection. We also show that the non-negative garrotte has a piecewise linear solution path. Using this fact, we propose an efficient algorithm for computing the whole solution path for the non-negative garrotte. Simulations and a real example demonstrate that the non-negative garrotte is very effective in improving on the initial estimator in terms of variable selection and estimation accuracy."
"10.1111/j.1467-9868.2007.00580.x","2007","Maximum likelihood inference on a mixed conditionally and marginally specified regression model for genetic epidemiologic studies with two-phase sampling","2","Two-phase stratified sampling designs can reduce the cost of genetic epidemiologic studies by limiting expensive ascertainments of genetic and environmental exposure to an efficiently selected subsample (phase II) of the main study (phase I). Family history and some covariate information, which may be cheaply gathered for all subjects at phase I, can be used for sampling of informative subjects at phase II. We develop alternative maximum likelihood methods for analysis of data from such studies by using a novel regression model that permits the estimation of 'marginal' risk parameters that are associated with the genetic and environmental covariates of interest, while simultaneously characterizing the 'conditional' risk of the disease associated with family history after adjusting for the other covariates. The methods and appropriate asymptotic theories are developed with and without an assumption of gene-environment independence, allowing the distribution of the environmental factors to remain non-parametric. The performance of the alternative methods and of sampling strategies is studied by using simulated data involving rare and common genetic variants. An application of the methods proposed is illustrated by using a case-control study of colorectal adenoma embedded within the prostate, lung, colorectal and ovarian cancer screening trial."
"10.1111/j.1467-9868.2007.00579.x","2007","Empirical-likelihood-based inference in missing response problems and its application in observational studies","0","The problem of missing response data is ubiquitous in medical and social science studies. In the case of responses that are missing at random (depending on some covariate information), analyses focused only on the complete data may lead to biased results. Various debias methods have been extensively studied in the literature, particularly the weighting method that was motivated by Horvitz and Thompson's estimators. To improve efficiency, Robins, Rotnitzky and Zhao proposed augmented estimating equations based on corrected complete-case analyses. A nice feature of the augmented method is its 'double robustness', i.e. the estimator that is derived from the augmented method is asymptotically unbiased if either the underlying missing data mechanism or the underlying regression function is correctly specified. Furthermore, the augmented estimator can achieve full efficiency if both the missing data mechanism and the regression function are correctly specified. In general, however, it is very difficult to specify the regression function correctly, especially when the dimension of covariates is high- this is the so-called curse of dimensionality problem. The augmented estimator has much lower efficiency if the 'working regression model' is not close to the true regression model. In this paper, the empirical likelihood method is employed to seek a constrained empirical likelihood estimation of mean response with the assumption that responses are missing at random. The empirical-likelihood-based estimators enjoy the double-robustness property. Moreover, it is possible that the empirical-likelihood-based inference can produce asymptotically unbiased and efficient estimators even if the true regression function is not completely known. Simulation results indicate that the empirical-likelihood-based estimators are very robust to a misspecification of the propensity score and dominate other competitors in the sense of having smaller mean-square errors. Methods that are developed in this paper have a nice application in observational causal inferences. The propensity score is used to adjust for differences in pretreatment variables in the estimation of average treatment effects."
"10.1111/j.1467-9868.2007.00578.x","2007","Marginal regression analysis of longitudinal data with time-dependent covariates: a generalized method-of-moments approach","0","We develop a new approach to using estimating equations to estimate marginal regression models for longitudinal data with time-dependent covariates. Our approach classifies time-dependent covariates into three types-types I, II and III. The type of covariate determines what estimating equations can be used involving the covariate. We use the generalized method of moments to make optimal use of the estimating equations that are made available by the covariates. Previous literature has suggested the use of generalized estimating equations with the independent working correlation when there are time-dependent covariates. We conduct a simulation study that shows that our approach can provide substantial gains in efficiency over generalized estimating equations with the independent working correlation when a time-dependent covariate is of types I or II, and our approach remains consistent and comparable in efficiency with generalized estimating equations with the independent working correlation when a time-dependent covariate is of type III. We apply our approach to analyse the relationship between the body mass index and future morbidity among children in the Philippines."
"10.1111/j.1467-9868.2007.00577.x","2007","Regression coefficient and autoregressive order shrinkage and selection via the lasso","0",""
"10.1111/j.1467-9868.2007.00576.x","2007","Optimal additions to and deletions from two-level orthogonal arrays","0","Consider the problem of selecting a two-level factorial design. It is well known that two-level orthogonal arrays of strength 4 or more with e extra runs have various optimality properties including generalized Cheng (type 1) optimality when e=1, restricted Cheng (type 1) optimality when e=2 and E-optimality when 3 <= e <= 5. More general Schur optimality results are derived for more general values of e within the more restricted class of augmented two-level orthogonal arrays. Similar results are derived for the class of orthogonal arrays with deletions. Examples are used to illustrate the results and in many cases the designs are confirmed to be optimal across all two-level designs."
"10.1111/j.1467-9868.2007.00575.x","2007","On least squares fitting for stationary spatial point processes","0","The K-function is a popular tool for fitting spatial point process models owing to its simplicity and wide applicability. In this work we study the properties of least squares estimators of model parameters and propose a new method of model fitting via the K-function by using subsampling. We demonstrate consistency and asymptotic normality of our estimators of model parameters and compare the efficiency of our procedure with existing procedures. This is done through asymptotic theory, simulation experiments and an application to a data set on long leaf pine-trees."
"10.1111/j.1467-9868.2007.00574.x","2007","Towards a coherent statistical framework for dense deformable template estimation","0","The problem of estimating probabilistic deformable template models in the field of computer vision or of probabilistic atlases in the field of computational anatomy has not yet received a coherent statistical formulation and remains a challenge. We provide a careful definition and analysis of a well-defined statistical model based on dense deformable templates for grey level images of deformable objects. We propose a rigorous Bayesian framework for which we prove asymptotic consistency of the maximum a posteriori estimate and which leads to an effective iterative estimation algorithm of the geometric and photometric parameters in the small sample setting. The model is extended to mixtures of finite numbers of such components leading to a fine description of the photometric and geometric variations of an object class. We illustrate some of the ideas with images of handwritten digits and apply the estimated models to classification through maximum likelihood."
"10.1111/j.1467-9868.2006.00570.x","2006","Separating between- and within-cluster covariate effects by using conditional and partitioning methods","1","We consider the situation where the random effects in a generalized linear mixed model may be correlated with one of the predictors, which leads to inconsistent estimators. We show that conditional maximum likelihood can eliminate this bias. Conditional likelihood leads naturally to the partitioning of the covariate into between- and within-cluster components and models that include separate terms for these components also eliminate the source of the bias. Another viewpoint that we develop is the idea that many violations of the assumptions (including correlation between the random effects and a covariate) in a generalized linear mixed model may be cast as misspecified mixing distributions. We illustrate the results with two examples and simulations."
"10.1111/j.1467-9868.2006.00569.x","2006","A functional wavelet-kernel approach for time series prediction","0","We consider the prediction problem of a time series on a whole time interval in terms of its past. The approach that we adopt is based on functional kernel nonparametric regression estimation techniques where observations are discrete recordings of segments of an underlying stochastic process considered as curves. These curves are assumed to lie within the space of continuous functions, and the discretized time series data set consists of a relatively small, compared with the number of segments, number of measurements made at regular times. We estimate conditional expectations by using appropriate wavelet decompositions of the segmented sample paths. A notion of similarity, based on wavelet decompositions, is used to calibrate the prediction. Asymptotic properties when the number of segments grows to infinity are investigated under mild conditions, and a nonparametric resampling procedure is used to generate, in a flexible way, valid asymptotic pointwise prediction intervals for the trajectories predicted. We illustrate the usefulness of the proposed functional wavelet-kernel methodology in finite sample situations by means of a simulated example and two real life data sets, and we compare the resulting predictions with those obtained by three other methods in the literature, in particular with a smoothing spline method, with an exponential smoothing procedure and with a seasonal autoregressive integrated moving average model."
"10.1111/j.1467-9868.2006.00568.x","2006","Bounds on causal effects in three-arm trials with non-compliance","5","The paper considers the analysis of three-arm randomized trials with non-compliance. In these trials, the average causal effects of treatments within principal strata of compliance behaviour are of interest for better understanding the effect of the treatment. Unfortunately, even with the usual assumptions, the average causal effects of treatments within principal strata are not point identified. However, the observable data do provide useful information on the bounds of the identification regions of the parameters of interest. Under two sets of assumptions, we derive sharp bounds for the causal effects within principal strata for binary outcomes and construct confidence intervals to cover the identification regions. The methods are illustrated by an analysis of data from a randomized study of treatments for alcohol dependence."
"10.1111/j.1467-9868.2006.00567.x","2006","Network delay tomography using flexicast experiments","0","Estimating and monitoring the quality of service of computer and communications networks is a problem of considerable interest. The paper focuses on estimating link level delay distributions from end-to-end path level data collected by using active probing experiments. This is an interesting large scale statistical inverse (deconvolution) problem. We describe a flexible class of probing experiments ('flexicast') for data collection and develop conditions under which the link level delay distributions are identifiable. Maximum likelihood estimation using the EM algorithm is studied. It does not scale well for large trees, so a faster algorithm based on solving for local maximum likehood estimators and combining their information is proposed. The usefulness of the methods is illustrated on real voice over Internet protocol data that were collected from the University of North Carolina campus network."
"10.1111/j.1467-9868.2006.00566.x","2006","An exact {G}ibbs sampler for the {M}arkov-modulated {P}oisson process","1","A Markov-modulated Poisson process is a Poisson process whose intensity varies according to a Markov process. We present a novel technique for simulating from the exact distribution of a continuous time Markov chain over an interval given the start and end states and the infinitesimal generator, and we use this to create a Gibbs sampler which samples from the exact distribution of the hidden Markov chain in a Markov-modulated Poisson process. We apply the Gibbs sampler to modelling the occurrence of a rare DNA motif (the Chi site) and to inferring regions of the genome with evidence of high or low intensities for occurrences of this site."
"10.1111/j.1467-9868.2006.00565.x","2006","The complex {B}ingham quartic distribution and shape analysis","0","The complex Bingham distribution was introduced by Kent as a tractable model for landmark-based shape analysis. It forms an exponential family with a sufficient statistic which is quadratic in the data. However, the distribution has too much symmetry to be widely useful. In particular, under high concentration it behaves asymptotically as a normal distribution, but where the covariance matrix is constrained to have complex symmetry. To overcome this limitation and to provide a full range of asymptotic normal behaviour, we introduce a new 'complex Bingham quartic distribution' by adding a selection of quartic terms to the log-density. In the simplest case this new distribution corresponds to Kent's FB5-distribution. Asymptotic and saddlepoint methods are developed for the normalizing constant to facilitate maximum likelihood estimation. Examples are given to show the usefulness of this new distribution."
"10.1111/j.1467-9868.2006.00564.x","2006","Semiparametric estimation by model selection for locally stationary processes","0","Over recent decades increasingly more attention has been paid to the problem of how to fit a parametric model of time series with time-varying parameters. A typical example is given by autoregressive models with time-varying parameters. We propose a procedure to fit such time-varying models to general non-stationary processes. The estimator is a maximum Whittle likelihood estimator on sieves. The results do not assume that the observed process belongs to a specific class of time-varying parametric models. We discuss in more detail the fitting of time-varying AR(p) processes for which we treat the problem of the selection of the order p, and we propose an iterative algorithm for the computation of the estimator. A comparison with model selection by Akaike's information criterion is provided through simulations."
"10.1111/j.1467-9868.2006.00563.x","2006","Correlograms for non-stationary autoregressions","0","Analysis of time series often involves correlograms and partial correlograms as graphical descriptions of temporal dependence. Two methods are available for computing these statistics: one based on autocorrelations and the other on scaled autocovariances. For a stationary time series the resulting plots are nearly identical. When it comes to time series exhibiting non-stationary features these methods can lead to very different results. This has two consequences: incorrect inferences can be drawn when confusing these concepts; better discrimination between stationary and non-stationarity is achieved when using autocorrelations instead of, or along with, the autocovariances which are commonly used in statistical software."
"10.1111/j.1467-9868.2006.00562.x","2006","Assessing the finite dimensionality of functional data","2","If a problem in functional data analysis is low dimensional then the methodology for its solution can often be reduced to relatively conventional techniques in multivariate analysis. Hence, there is intrinsic interest in assessing the finite dimensionality of functional data. We show that this problem has several unique features. From some viewpoints the problem is trivial, in the sense that continuously distributed functional data which are exactly finite dimensional are immediately recognizable as such, if the sample size is sufficiently large. However, in practice, functional data are almost always observed with noise, for example, resulting from rounding or experimental error. Then the problem is almost insolubly difficult. In such cases a part of the average noise variance is confounded with the true signal and is not identifiable. However, it is possible to define the unconfounded part of the noise variance. This represents the best possible lower bound to all potential values of average noise variance and is estimable in low noise settings. Moreover, bootstrap methods can be used to describe the reliability of estimates of unconfounded noise variance, under the assumption that the signal is finite dimensional. Motivated by these ideas, we suggest techniques for assessing the finiteness of dimensionality. In particular, we show how to construct a critical point (V) over capq such that, if the distribution of our functional data has fewer than q - 1 degrees of freedom, then we should be willing to assume that the average variance of the added noise is at least (V) over capq. If this level seems too high then we must conclude that the dimension is at least q - 1. We show that simpler, more conventional techniques, based on hypothesis testing, are generally not effective."
"10.1111/j.1467-9868.2006.00561.x","2006","Free-knot spline smoothing for functional data","3","The paper introduces free-knot regression spline estimators for the mean and the variance components of a sample of curves. The asymptotic distribution of the mean estimator is derived, and asymptotic confidence bands are constructed. A comparative simulation study shows that free-knot splines estimate salient features of the functions (such as sharp peaks) more accurately than smoothing splines. This adaptive behaviour is also illustrated by an analysis of weather data."
"10.1111/j.1467-9868.2006.00560.x","2006","Minimum volume confidence regions for a multivariate normal mean vector","2","Since Stein's original proposal in 1962, a series of papers have constructed confidence regions of smaller volume than the standard spheres for the mean vector of a multivariate normal distribution. A general approach to this problem is developed here and used to calculate a lower bound on the attainable volume. Bayes and fiducial methods are involved in the calculation. Scheffe-type problems are used to show that low volume by itself does not guarantee favourable inferential properties."
"10.1111/j.1467-9868.2006.00559.x","2006","Second-order residual analysis of spatiotemporal point processes and applications in model evaluation","3","The paper gives first-order residual analysis for spatiotemporal point processes that is similar to the residual analysis that has been developed by Baddeley and co-workers for spatial point processes and also proposes principles for second-order residual analysis based on the viewpoint of martingales. Examples are given for both first- and second-order residuals. In particular, residual analysis can be used as a powerful tool in model improvement. Taking a spatiotemporal epidemic-type aftershock sequence model for earthquake occurrences as the base-line model, second-order residual analysis can be useful for identifying many features of the data that are not implied in the base-line model, providing us with clues about how to formulate better models."
"10.1111/j.1467-9868.2006.00558.x","2006","Haar-{F}isz estimation of evolutionary wavelet spectra","3","We propose a new 'Haar-Fisz' technique for estimating the time-varying, piecewise constant local variance of a locally stationary Gaussian time series. We apply our technique to the estimation of the spectral structure in the locally stationary wavelet model. Our method combines Haar wavelets and the variance stabilizing Fisz transform. The resulting estimator is mean square consistent, rapidly computable and easy to implement, and performs well in practice. We also introduce the 'Haar-Fisz transform', a device for stabilizing the variance of scaled chi(2)-data and bringing their distribution close to Gaussianity."
"10.1111/j.1467-9868.2006.00557.x","2006","Multiple randomizations","4","Multitiered experiments are characterized by involving multiple randomizations, in a sense that we make explicit. We compare and contrast six types of multiple randomizations, using a wide range of examples, and discuss their use in designing experiments. We outline a system of describing the randomizations in terms of sets of objects, their associated tiers and the factor nesting, using randomization diagrams, which give a convenient and readily assimilated summary of an experiment's randomization. We also indicate how to formulate a randomization-based mixed model for the analysis of data from such experiments."
"10.1111/j.1467-9868.2006.00556.x","2006","Empirical likelihood confidence regions in a partially linear single-index model","6","Empirical-likelihood-based inference for the parameters in a partially linear single-index model is investigated. Unlike existing empirical likelihood procedures for other simpler models, if there is no bias correction the limit distribution of the empirical likelihood ratio cannot be asymptotically tractable. To attack this difficulty we propose a bias correction to achieve the standard chi(2)-limit. The bias-corrected empirical likelihood ratio shares some of the desired features of the existing least squares method: the estimation of the parameters is not needed; when estimating nonparametric functions in the model, undersmoothing for ensuring root n-consistency of the estimator of the parameters is avoided; the bias-corrected empirical likelihood is self-scale invariant and no plug-in estimator for the limiting variance is needed. Furthermore, since the index is of norm 1, we use this constraint as information to increase the accuracy of the confidence regions (smaller regions at the same nominal level). As a by-product, our approach of using bias correction may also shed light on nonparametric estimation in model checking for other semiparametric regression models. A simulation study is carried out to assess the performance of the bias-corrected empirical likelihood. An application to a real data set is illustrated."
"10.1111/j.1467-9868.2006.00555.x","2006","Adjusted jackknife for imputation under unequal probability sampling without replacement","1","Imputation is commonly used to compensate for item non-response in sample surveys. If we treat the imputed values as if they are true values, and then compute the variance estimates by using standard methods, such as the jackknife, we can seriously underestimate the true variances. We propose a modified jackknife variance estimator which is defined for any without-replacement unequal probability sampling design in the presence of imputation and non-negligible sampling fraction. Mean, ratio and random-imputation methods will be considered. The practical advantage of the method proposed is its breadth of applicability."
"10.1111/j.1467-9868.2006.00554.x","2006","A new randomized response model","0","We suggest an efficient randomized response model that can easily be adjusted to be more efficient than the Warner, Mangat and Singh, and Mangat methods by selecting certain parameters of the proposed randomization device."
"10.1111/j.1467-9868.2006.00546.x","2006","On the bias of the multiple-imputation variance estimator in survey sampling","4","Multiple imputation is a method of estimating the variances of estimators that are constructed with some imputed data. We give an expression for the bias of the multiple-imputation variance estimator for data that are collected with a complex sample design. The bias may be sizable for certain estimators, such as domain means, when a large fraction of the values are imputed. A bias-adjusted variance estimator is suggested."
"10.1111/j.1467-9868.2006.00548.x","2006","Improved likelihood inference for discrete data","4","Discrete data, particularly count and contingency table data, are typically analysed by using methods that are accurate to first order, such as normal approximations for maximum likelihood estimators. By contrast continuous data can quite generally be analysed by using third-order procedures, with major improvements in accuracy and with intrinsic separation of information concerning parameter components. The paper extends these higher order results to discrete data, yielding a methodology that is widely applicable and accurate to second order. The extension can be described in terms of an approximating exponential model that is expressed in terms of a score variable. The development is outlined and the flexibility of the approach is illustrated by examples."
"10.1111/j.1467-9868.2006.00551.x","2006","Testing against a high dimensional alternative","3","As the dimensionality of the alternative hypothesis increases, the power of classical tests tends to diminish quite rapidly. This is especially true for high dimensional data in which there are more parameters than observations. We discuss a score test on a hyperparameter in an empirical Bayesian model as an alternative to classical tests. It gives a general test statistic which can be used to test a point null hypothesis against a high dimensional alternative, even when the number of parameters exceeds the number of samples. This test will be shown to have optimal power on average in a neighbourhood of the null hypothesis, which makes it a proper generalization of the locally most powerful test to multiple dimensions. To illustrate this new locally most powerful test we investigate the case of testing the global null hypothesis in a linear regression model in more detail. The score test is shown to have significantly more power than the F-test whenever under the alternative the large variance principal components of the design matrix explain substantially more of the variance of the outcome than do the small variance principal components. The score test is also useful for detecting sparse alternatives in truly high dimensional data, where its power is comparable with the test based on the maximum absolute t-statistic."
"10.1111/j.1467-9868.2006.00549.x","2006","A new approach to cluster analysis: the clustering-function-based method","0","The purpose of the paper is to present a new statistical approach to hierarchical cluster analysis with n objects measured on p variables. Motivated by the model of multivariate analysis of variance and the method of maximum likelihood, a clustering problem is formulated as a least squares optimization problem, simultaneously solving for both an n-vector of unknown group membership of objects and a linear clustering function. This formulation is shown to be linked to linear regression analysis and Fisher linear discriminant analysis and includes principal component regression for tackling multicollinearity or rank deficiency, polynomial or B-splines regression for handling non-linearity and various variable selection methods to eliminate irrelevant variables from data analysis. Algorithmic issues are investigated by using sign eigenanalysis."
"10.1111/j.1467-9868.2006.00547.x","2006","Testing for order-restricted hypotheses in longitudinal data","0","In many biomedical studies, we are interested in comparing treatment effects with an inherent ordering. We propose a quadratic score test (QST) based on a quadratic inference function for detecting an order in treatment effects for correlated data. The quadratic inference function is similar to the negative of a log-likelihood, and it provides test statistics in the spirit of a chi(2)-test for testing nested hypotheses as well as for assessing the goodness of fit of model assumptions. Under the null hypothesis of no order restriction, it is shown that the QST statistic has a Wald-type asymptotic representation and that the asymptotic distribution of the QST statistic is a weighted chi(2)-distribution. Furthermore, an asymptotic distribution of the QST statistic under an arbitrary convex cone alternative is provided. The performance of the QST is investigated through Monte Carlo simulation experiments. Analysis of the polyposis data demonstrates that the QST outperforms the Wald test when data are highly correlated with a small sample size and there is a significant amount of missing data with a small number of clusters. The proposed test statistic accommodates both time-dependent and time-independent covariates in a model."
"10.1111/j.1467-9868.2006.00553.x","2006","Sequential {M}onte {C}arlo samplers","10","We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference."
"10.1111/j.1467-9868.2006.00550.x","2006","Regularized semiparametric model identification with application to nuclear magnetic resonance signal quantification with unknown macromolecular base-line","0","We formulate and solve a semiparametric fitting problem with regularization constraints. The model that we focus on is composed of a parametric non-linear part and a nonparametric part that can be reconstructed via splines. Regularization is employed to impose a certain degree of smoothness on the nonparametric part. Semiparametric regression is presented as a generalization of non-linear regression, and all important differences that arise from the statistical and computational points of view are highlighted. We motivate the problem formulation with a biomedical signal processing application."
"10.1111/j.1467-9868.2006.00552.x","2006","Exact and computationally efficient likelihood-based estimation for discretely observed diffusion processes","10","The objective of the paper is to present a novel methodology for likelihood-based inference for discretely observed diffusions. We propose Monte Carlo methods, which build on recent advances on the exact simulation of diffusions, for performing maximum likelihood and Bayesian estimation."
"10.1111/j.1467-9868.2006.00545.x","2006","Functional clustering by {B}ayesian wavelet methods","6","We propose a nonparametric Bayes wavelet model for clustering of functional data. The wavelet-based methodology is aimed at the resolution of generic global and local features during clustering and is suitable for clustering high dimensional data. Based on the Dirichlet process, the nonparametric Bayes model extends the scope of traditional Bayes wavelet methods to functional clustering and allows the elicitation of prior belief about the regularity of the functions and the number of clusters by suitably mixing the Dirichlet processes. Posterior inference is carried out by Gibbs sampling with conjugate priors, which makes the computation straightforward. We use simulated as well as real data sets to illustrate the suitability of the approach over other alternatives."
"10.1111/j.1467-9868.2006.00544.x","2006","Variance stabilization for a scalar parameter","0","We present a variance stabilizing transformation for inference about a scalar parameter that is estimated by a function of a multivariate M-estimator. The transformation proposed is automatic, computationally simple and can be applied quite generally. Though it is based on an intuitive notion and entirely empirical, the transformation is shown to have an appropriate justification in providing variance stabilization when viewed from both parametric and nonparametric perspectives. Further, the transformation repairs deficiencies of existing methods for variance stabilization. The transformation proposed is illustrated in a range of examples, and its effectiveness to yield confidence limits having low coverage error is demonstrated in a numerical example."
"10.1111/j.1467-9868.2006.00543.x","2006","Generalized linear array models with applications to multidimensional smoothing","1","Data with an array structure are common in statistics, and the design or regression matrix for analysis of such data can often be written as a Kronecker product. Factorial designs, contingency tables and smoothing of data on multidimensional grids are three such general classes of data and models. In such a setting, we develop an arithmetic of arrays which allows us to define the expectation of the data array as a sequence of nested matrix operations on a coefficient array. We show how this arithmetic leads to low storage, high speed computation in the scoring algorithm of the generalized linear model. We refer to a generalized linear array model and apply the methodology to the smoothing of multidimensional arrays. We illustrate our procedure with the analysis of three data sets: mortality data indexed by age at death and year of death, spatially varying microarray background data and disease incidence data indexed by age at death, year of death and month of death."
"10.1111/j.1467-9868.2006.00542.x","2006","Mean-squared error estimation in transformed {F}ay-{H}erriot models","1","The problem of accurately estimating the mean-squared error of small area estimators within a Fay-Herriot normal error model is studied theoretically in the common setting where the model is fitted to a logarithmically transformed response variable. For bias-corrected empirical best linear unbiased predictor small area point estimators, mean-squared error formulae and estimators are provided, with biases of smaller order than the reciprocal of the number of small areas. The performance of these mean-squared error estimators is illustrated by a simulation study and a real data example relating to the county level estimation of child poverty rates in the US Census Bureau's on-going 'Small area income and poverty estimation' project."
"10.1111/j.1467-9868.2006.00541.x","2006","On parametric bootstrap methods for small area prediction","5","The particularly wide range of applications of small area prediction, e.g. in policy making decisions, has meant that this topic has received substantial attention in recent years. The problems of estimating mean-squared predictive error, of correcting that estimator for bias and of constructing prediction intervals have been addressed by various workers, although existing methodology is still restricted to a narrow range of models. To overcome this difficulty we develop new, bootstrap-based methods, which are applicable in very general settings, for constructing bias-corrected estimators of mean-squared error and for computing prediction regions. Unlike existing techniques, which are based largely on Taylor expansions, our bias-corrected mean-squared error estimators do not require analytical calculation. They also have the property that they are non-negative. Our prediction intervals have a high degree of coverage accuracy, O(n(-3)), where n is the number of areas, if double-bootstrap methods are employed. The techniques do not depend on the form of the small area estimator and are applicable to general two-level, small area models, where the variables at either level can be discrete or continuous and, in particular, can be non-normal. Most importantly, the new methods are simple and easy to apply."
"10.1111/j.1467-9868.2006.00540.x","2006","Nonparametric methods for solving the {B}erkson errors-in-variables problem","3","It is common, in errors-in-variables problems in regression, to assume that the errors are incurred 'after the experiment', in that the observed value of the explanatory variable is an independent perturbation of its true value. However, if the errors are incurred 'before the experiment' then the true value of the explanatory variable equals a perturbation of its observed value. This is the context of the Berkson model, which is increasingly attracting attention in parametric and semiparametric settings. We introduce and discuss nonparametric techniques for analysing data that are generated by the Berkson model. Our approach permits both random and regularly spaced values of the target doses. In the absence of data on dosage error it is necessary to propose a distribution for the latter, but we show numerically that our method is robust against that assumption. The case of dosage error data is also discussed. A practical method for smoothing parameter choice is suggested. Our techniques for errors-in-variables regression are shown to achieve theoretically optimal convergence rates."
"10.1111/j.1467-9868.2006.00539.x","2006","Wavelet-based functional mixed models","13","Increasingly, scientific studies yield functional data, in which the ideal units of observation are curves and the observed data consist of sets of curves that are sampled on a fine grid. We present new methodology that generalizes the linear mixed model to the functional mixed model framework, with model fitting done by using a Bayesian wavelet-based approach. This method is flexible, allowing functions of arbitrary form and the full range of fixed effects structures and between-curve covariance structures that are available in the mixed model framework. It yields nonparametric estimates of the fixed and random-effects functions as well as the various between-curve and within-curve covariance matrices. The functional fixed effects are adaptively regularized as a result of the non-linear shrinkage prior that is imposed on the fixed effects' wavelet coefficients, and the random-effect functions experience a form of adaptive regularization because of the separately estimated variance components for each wavelet coefficient. Because we have posterior samples for all model quantities, we can perform pointwise or joint Bayesian inference or prediction on the quantities of the model. The adaptiveness of the method makes it especially appropriate for modelling irregular functional data that are characterized by numerous local features like peaks."
"10.1111/j.1467-9868.2006.00538.x","2006","Likelihood inference for a class of latent {M}arkov models under linear hypotheses on the transition probabilities","1","For a class of latent Markov models for discrete variables having a longitudinal structure, we introduce an approach for formulating and testing linear hypotheses on the transition probabilities of the latent process. For the maximum likelihood estimation of a latent Markov model under hypotheses of this type, we outline an EM algorithm that is based on well-known recursions in the hidden Markov literature. We also show that, under certain assumptions, the asymptotic null distribution of the likelihood ratio statistic for testing a linear hypothesis on the transition probabilities of a latent Markov model, against a less stringent linear hypothesis on the transition probabilities of the same model, is of (x) over bar (2) type. As a particular case, we derive the asymptotic distribution of the likelihood ratio statistic between a latent class model and its latent Markov version, which may be used to test the hypothesis of absence of transition between latent states. The approach is illustrated through a series of simulations and two applications, the first of which is based on educational testing data that have been collected within the National Assessment of Educational Progress 1996, and the second on data, concerning the use of marijuana, which have been collected within the National Youth Survey 1976-1980."
"10.1111/j.1467-9868.2005.00537.x","2006","Bandwidth selection in local polynomial regression using eigenvalues","0","Local polynomial regression is commonly used for estimating regression functions. In practice, however, with rough functions or sparse data, a poor choice of bandwidth can lead to unstable estimates of the function or its derivatives. We derive a new expression for the leading term of the bias by using the eigenvalues of the weighted design matrix where the bias depends on the arrangement of the X-values in the bandwidth window. We then use this result to determine a local data-driven bandwidth selection method and to provide a diagnostic for poor bandwidths that are chosen by using other methods. We show that our data-driven bandwidth is asymptotically equivalent to the optimal local bandwidth and that it performs well for relatively small samples when compared with other methods. In addition, we provide simulation results for first-derivative estimation. We illustrate its performance with data from Mars Global Surveyor."
"10.1111/j.1467-9868.2005.00536.x","2006","Collapsibility of distribution dependence","1","Cox and Wermuth proposed that the partial derivative of the conditional distribution function of a random variable Y given another X is used for measuring association between two variables with arbitrary distributions. The paper presents a necessary and sufficient condition for uniform collapsibility of this association measure over a third variable W."
"10.1111/j.1467-9868.2005.00535.x","2006","On properties of functional principal components analysis","19","Functional data analysis is intrinsically infinite dimensional; functional principal component analysis reduces dimension to a finite level, and points to the most significant components of the data. However, although this technique is often discussed, its properties are not as well understood as they might be. We show how the properties of functional principal component analysis can be elucidated through stochastic expansions and related results. Our approach quantifies the errors that arise through statistical approximation, in successive terms of orders n(-1/2), n(-1), n(-3/2), ..., where n denotes sample size. The expansions show how spacings among eigenvalues impact on statistical performance. The term of size n(-1/2) illustrates first-order properties and leads directly to limit theory which describes the dominant effect of spacings. Thus, for example, spacings are seen to have an immediate, first-order effect on properties of eigenfunction estimators, but only a second-order effect on eigenvalue estimators. Our results can be used to explore properties of existing methods, and also to suggest new techniques. In particular, we suggest bootstrap methods for constructing simultaneous confidence regions for an infinite number of eigenvalues, and also for individual eigenvalues and eigenvectors."
"10.1111/j.1467-9868.2005.00534.x","2006","Sufficient dimension reduction in regressions across heterogeneous subpopulations","0","Sliced inverse regression is one of the widely used dimension reduction methods. Chiaromonte and co-workers extended this method to regressions with qualitative predictors and developed a method, partial sliced inverse regression, under the assumption that the covariance matrices of the continuous predictors are constant across the levels of the qualitative predictor. We extend partial sliced inverse regression by removing the restrictive homogeneous covariance condition. This extension, which significantly expands the applicability of the previous methodology, is based on a new estimation method that makes use of a non-linear least squares objective function."
"10.1111/j.1467-9868.2005.00533.x","2006","Semiparametric estimation in general repeated measures problems","9","The paper considers a wide class of semiparametric problems with a parametric part for some covariate effects and repeated evaluations of a nonparametric function. Special cases in our approach include marginal models for longitudinal or clustered data, conditional logistic regression for matched case-control studies, multivariate measurement error models, generalized linear mixed models with a semiparametric component, and many others. We propose profile kernel and backfitting estimation methods for these problems, derive their asymptotic distributions and show that in likelihood problems the methods are semiparametric efficient. Although generally not true, it transpires that with our methods profiling and backfitting are asymptotically equivalent. We also consider pseudolikelihood methods where some nuisance parameters are estimated from a different algorithm. The methods proposed are evaluated by using simulation studies and applied to the Kenya haemoglobin data."
"10.1111/j.1467-9868.2005.00532.x","2006","Model selection and estimation in regression with grouped variables","33","We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods."
"10.1111/j.1467-9868.2005.00531.x","2006","Multiscale {P}oisson data smoothing","1","The paper introduces a framework for non-linear multiscale decompositions of Poisson data that have piecewise smooth intensity curves. The key concept is conditioning on the sum of the observations that are involved in the computation of a given multiscale coefficient. Within this framework, most classical wavelet thresholding schemes for data with additive homoscedastic noise can be used. Any family of wavelet transforms (orthogonal, biorthogonal or second generation) can be incorporated in this framework. Our second contribution is to propose a Bayesian shrinkage approach with an original prior for coefficients of this decomposition. As such, the method combines the advantages of the Haar-Fisz transform with wavelet smoothing and (Bayesian) multiscale likelihood models, with additional benefits, such as extendability towards arbitrary wavelet families. Simulations show an important reduction in average squared error of the output, compared with the present techniques of Anscombe or Fisz variance stabilization or multiscale likelihood modelling."
"10.1111/j.1467-9868.2005.00530.x","2006","Penalized spline models for functional principal component analysis","9","We propose an iterative estimation procedure for performing functional principal component analysis. The procedure aims at functional or longitudinal data where the repeated measurements from the same subject are correlated. An increasingly popular smoothing approach, penalized spline regression, is used to represent the mean function. This allows straightforward incorporation of covariates and simple implementation of approximate inference procedures for coefficients. For the handling of the within-subject correlation, we develop an iterative procedure which reduces the dependence between the repeated measurements that are made for the same subject. The resulting data after iteration are theoretically shown to be asymptotically equivalent (in probability) to a set of independent data. This suggests that the general theory of penalized spline regression that has been developed for independent data can also be applied to functional data. The effectiveness of the proposed procedure is demonstrated via a simulation study and an application to yeast cell cycle gene expression data."
"10.1111/j.1467-9868.2005.00525.x","2005","Good randomized sequential probability forecasting is always possible","3","Building on the game theoretic framework for probability, we show that it is possible, using randomization, to make sequential probability forecasts that will pass any given battery of statistical tests. This result, an easy consequence of von Neumann's minimax theorem, simplifies and generalizes work by earlier researchers."
"10.1111/j.1467-9868.2005.00524.x","2005","Bayesian inference for stochastic multitype epidemics in structured populations via random graphs","0","The paper is concerned with new methodology for statistical inference for final outcome infectious disease data using certain structured population stochastic epidemic models. A major obstacle to inference for such models is that the likelihood is both analytically and numerically intractable. The approach that is taken here is to impute missing information in the form of a random graph that describes the potential infectious contacts between individuals. This level of imputation overcomes various constraints of existing methodologies and yields more detailed information about the spread of disease. The methods are illustrated with both real and test data."
"10.1111/j.1467-9868.2005.00523.x","2005","Bayesian likelihood methods for estimating the end point of a distribution","0","We consider maximum likelihood methods for estimating the end point of a distribution. The likelihood function is modified by a prior distribution that is imposed on the location parameter. The prior is explicit and meaningful, and has a general form that adapts itself to different settings. Results on convergence rates and limiting distributions are given. In particular, it is shown that the limiting distribution is non-normal in non-regular cases. Parametric bootstrap techniques are suggested for quantifying the accuracy of the estimator. We illustrate performance by applying the method to multiparameter Weibull and gamma distributions."
"10.1111/j.1467-9868.2005.00522.x","2005","Maximum likelihood estimation of linear continuous time long memory processes with discrete time data","0","We develop a new class of time continuous autoregressive fractionally integrated moving average (CARFIMA) models which are useful for modelling regularly spaced and irregu-larly spaced discrete time long memory data. We derive the autocovariance function of a stationary CARFIMA model and study maximum likelihood estimation of a regression model with CARFIMA errors, based on discrete time data and via the innovations algorithm. It is shown that the maximum likelihood estimator is asymptotically normal, and its finite sample properties are studied through simulation. The efficacy of the approach proposed is demonstrated with a data set from an environmental study."
"10.1111/j.1467-9868.2005.00521.x","2005","Bayes factors based on test statistics","3","Traditionally, the use of Bayes factors has required the specification of proper prior distributions on model parameters that are implicit to both null and alternative hypotheses. I describe an approach to defining Bayes factors based on modelling test statistics. Because the distributions of test statistics do not depend on unknown model parameters, this approach eliminates much of the subjectivity that is normally associated with the definition of Bayes factors. For standard test statistics, including the chi(2)-, F-, t- and z-statistics, the values of Bayes factors that result from this approach have simple, closed form expressions."
"10.1111/j.1467-9868.2005.00520.x","2005","Statistical methods for regular monitoring data","3","Meteorological and environmental data that are collected at regular time intervals on a fixed monitoring network can be usefully studied combining ideas from multiple time series and spatial statistics, particularly when there are little or no missing data. This work investigates methods for modelling such data and ways of approximating the associated likelihood functions. Models for processes on the sphere crossed with time are emphasized, especially models that are not fully symmetric in space-time. Two approaches to obtaining such models are described. The first is to consider a rotated version of fully symmetric models for which we have explicit expressions for the covariance function. The second is based on a representation of space-time covariance functions that is spectral in just the time domain and is shown to lead to natural partially nonparametric asymmetric models on the sphere crossed with time. Various models are applied to a data set of daily winds at 11 sites in Ireland over 18 years. Spectral and space-time domain diagnostic procedures are used to assess the quality of the fits. The spectral-in-time modelling approach is shown to yield a good fit to many properties of the data and can be applied in a routine fashion relative to finding elaborate parametric models that describe the space-time dependences of the data about as well."
"10.1111/j.1467-9868.2005.00519.x","2005","Residual analysis for spatial point processes","5","We define residuals for point process models fitted to spatial point pattern data, and we propose diagnostic plots based on them. The residuals apply to any point process model that has a conditional intensity; the model may exhibit spatial heterogeneity, interpoint interaction and dependence on spatial covariates. Some existing ad hoc methods for model checking (quadrat counts, scan statistic, kernel smoothed intensity and Berman's diagnostic) are recovered as special cases. Diagnostic tools are developed systematically, by using an analogy between our spatial residuals and the usual residuals for (non-spatial) generalized linear models. The conditional intensity lambda plays the role of the mean response. This makes it possible to adapt existing knowledge about model validation for generalized linear models to the spatial point process context, giving recommendations for diagnostic plots. A plot of smoothed residuals against spatial location, or against a spatial covariate, is effective in diagnosing spatial trend or co-variate effects. Q-Q-plots of the residuals are effective in diagnosing interpoint interaction."
"10.1111/j.1467-9868.2005.00518.x","2005","Modelling directional dispersion through hyperspherical log-splines","0","We introduce the directionally dispersed class of multivariate distributions, a generalization of the elliptical class. By allowing dispersion of multivariate random variables to vary with direction it is possible to generate a very wide and flexible class of distributions. Directionally dispersed distributions have a simple form for their density, which extends a spherically symmetric density function by including a function D modelling directional dispersion. Under a mild condition, the class of distributions is shown to preserve both unimodality and moment existence. By adequately defining D, it is possible to generate skewed distributions. Using spline models on hyperspheres, we suggest a very flexible, yet practical, implementation for modelling directional dispersion in any dimension. Finally, we use the new class of distributions in a Bayesian regression set-up and analyse the distributions of a set of biomedical measurements and a sample of US manufacturing firms."
"10.1111/j.1467-9868.2005.00517.x","2005","A note on non-negative continuous time processes","0","Recently there has been much work on developing models that are suitable for analysing the volatility of a continuous time process. One general approach is to define a volatility process as the convolution of a kernel with a non-decreasing Levy process, which is non-negative if the kernel is non-negative. Within the framework of time continuous autoregressive moving average (CARMA) processes, we derive a necessary and sufficient condition for the kernel to be non-negative. This condition is in terms of the Laplace transform of the CARMA kernel, which has a simple form. We discuss some useful consequences of this result and delineate the parametric region of stationarity and non-negative kernel for some lower order CARMA models."
"10.1111/j.1467-9868.2005.00516.x","2005","On nonparametric maximum likelihood estimation with interval censoring and left truncation","0","A graph theoretical approach is employed to describe the support set of the nonparametric maximum likelihood estimator for the cumulative distribution function given interval-censored and left-truncated data. A necessary and sufficient condition for the existence of a nonparametric maximum likelihood estimator is then derived. Two previously analysed data sets are revisited."
"10.1111/j.1467-9868.2005.00515.x","2005","Estimating the proportion of true null hypotheses, with application to {DNA} microarray data","4","We consider the problem of estimating the proportion of true null hypotheses, pi(0), in a multiple-hypothesis set-up. The tests are based on observed p-values. We first review published estimators based on the estimator that was suggested by Schweder and Spjotvoll. Then we derive new estimators based on nonparametric maximum likelihood estimation of the p-value density, restricting to decreasing and convex decreasing densities. The estimators of pi(0) are all derived under the assumption of independent test statistics. Their performance under dependence is investigated in a simulation study. We find that the estimators are relatively robust with respect to the assumption of independence and work well also for test statistics with moderate dependence."
"10.1111/j.1467-9868.2005.00514.x","2005","Estimated estimating equations: semiparametric inference for clustered and longitudinal data","5","We introduce a flexible marginal modelling approach for statistical inference for clustered and longitudinal data under minimal assumptions. This estimated estimating equations approach is semiparametric and the proposed models are fitted by quasi-likelihood regression, where the unknown marginal means are a function of the fixed effects linear predictor with unknown smooth link, and variance-covariance is an unknown smooth function of the marginal means. We propose to estimate the nonparametric link and variance-covariance functions via smoothing methods, whereas the regression parameters are obtained via the estimated estimating equations. These are score equations that contain nonparametric function estimates. The proposed estimated estimating equations approach is motivated by its flexibility and easy implementation. Moreover, if data follow a generalized linear mixed model, with either a specified or an unspecified distribution of random effects and link function, the model proposed emerges as the corresponding marginal (population-average) version and can be used to obtain inference for the fixed effects in the underlying generalized linear mixed model, without the need to specify any other components of this generalized linear mixed model. Among marginal models, the estimated estimating equations approach provides a flexible alternative to modelling with generalized estimating equations. Applications of estimated estimating equations include diagnostics and link selection. The asymptotic distribution of the proposed estimators for the model parameters is derived, enabling statistical inference. Practical illustrations include Poisson modelling of repeated epileptic seizure counts and simulations for clustered binomial responses."
"10.1111/j.1467-9868.2005.00513.x","2005","An exact distribution-free test comparing two multivariate distributions based on adjacency","3",""
"10.1111/j.1467-9868.2005.00512.x","2005","Local model uncertainty and incomplete-data bias","3","Problems of the analysis of data with incomplete observations are all too familiar in statistics. They are doubly difficult if we are also uncertain about the choice of model. We propose a general formulation for the discussion of such problems and develop approximations to the resulting bias of maximum likelihood estimates on the assumption that model departures are small. Loss of efficiency in parameter estimation due to incompleteness in the data has a dual interpretation: the increase in variance when an assumed model is correct; the bias in estimation when the model is incorrect. Examples include non-ignorable missing data, hidden confounders in observational studies and publication bias in meta-analysis. Doubling variances before calculating confidence intervals or test statistics is suggested as a crude way of addressing the possibility of undetectably small departures from the model. The problem of assessing the risk of lung cancer from passive smoking is used as a motivating example."
"10.1111/j.1467-9868.2005.00511.x","2005","Calibrated imputation in surveys under a quasi-model-assisted approach","0","We propose to use calibrated imputation to compensate for missing values. This technique consists of finding final imputed values that are as close as possible to preliminary imputed values and are calibrated to satisfy constraints. Preliminary imputed values, potentially justified by an imputation model, are obtained through deterministic single imputation. Using appropriate constraints, the resulting imputed estimator is asymptotically unbiased for estimation of linear population parameters such as domain totals. A quasi-model-assisted approach is considered in the sense that inferences do not depend on the validity of an imputation model and are made with respect to the sampling design and a non-response model. An imputation model may still be used to generate imputed values and thus to improve the efficiency of the imputed estimator. This approach has the characteristic of handling naturally the situation where more than one imputation method is used owing to missing values in the variables that are used to obtain imputed values. We use the Taylor linearization technique to obtain a variance estimator under a general non-response model. For the logistic non-response model, we show that ignoring the effect of estimating the non-response model parameters leads to overestimating the variance of the imputed estimator. In practice, the overestimation is expected to be moderate or even negligible, as shown in a simulation study."
"10.1111/j.1467-9868.2005.00510.x","2005","Geometric representation of high dimension, low sample size data","14","High dimension, low sample size data are emerging in various areas of science. We find a common structure underlying many such data sets by using a non-standard type of asymptotics: the dimension tends to infinity while the sample size is fixed. Our analysis shows a tendency for the data to lie deterministically at the vertices of a regular simplex. Essentially all the randomness in the data appears only as a random rotation of this simplex. This geometric representation is used to obtain several new statistical insights."
"10.1111/j.1467-9868.2005.00509.x","2005","Variance of the number of false discoveries","9","In high throughput genomic work, a very large number d of hypotheses are tested based on n << d data samples. The large number of tests necessitates an adjustment for false discoveries in which a true null hypothesis was rejected. The expected number of false discoveries is easy to obtain. Dependences between the hypothesis tests greatly affect the variance of the number of false discoveries. Assuming that the tests are independent gives an inadequate variance formula. The paper presents a variance formula that takes account of the correlations between test statistics. That formula involves O(d(2)) correlations, and so a naive implementation has cost O(nd(2)). A method based on sampling pairs of tests allows the variance to be approximated at a cost that is independent of d."
"10.1111/j.1467-9868.2005.00508.x","2005","Statistical inference for discretely observed {M}arkov jump processes","1","Likelihood inference for discretely observed Markov jump processes with finite state space is investigated. The existence and uniqueness of the maximum likelihood estimator of the intensity matrix are investigated. This topic is closely related to the imbedding problem for Markov chains. It is demonstrated that the maximum likelihood estimator can be found either by the EM algorithm or by a Markov chain Monte Carlo procedure. When the maximum likelihood estimator does not exist, an estimator can be obtained by using a penalized likelihood function or by the Markov chain Monte Carlo procedure with a suitable prior. The methodology and its implementation are illustrated by examples and simulation studies."
"10.1111/j.1467-9868.2005.00507.x","2005","Self-weighted least absolute deviation estimation for infinite variance autoregressive models","3","How to undertake statistical inference for infinite variance autoregressive models has been a long-standing open problem. To solve this problem, we propose a self-weighted least absolute deviation estimator and show that this estimator is asymptotically normal if the density of errors and its derivative are uniformly bounded. Furthermore, a Wald test statistic is developed for the linear restriction on the parameters, and it is shown to have non-trivial local power. Simulation experiments are carried out to assess the performance of the theory and method in finite samples and a real data example is given. The results are entirely different from other published results and should provide new insights for future research on heavy-tailed time series."
"10.1111/j.1467-9868.2005.00506.x","2005","Properties of bagged nearest neighbour classifiers","1","It is shown that bagging, a computationally intensive method, asymptotically improves the performance of nearest neighbour classifiers provided that the resample size is less than 69% of the actual sample size, in the case of with-replacement bagging, or less than 50% of the sample size, for without-replacement bagging. However, for larger sampling fractions there is no asymptotic difference between the risk of the regular nearest neighbour classifier and its bagged version. In particular, neither achieves the large sample performance of the Bayes classifier. In contrast, when the sampling fractions converge to 0, but the resample sizes diverge to infinity, the bagged classifier converges to the optimal Bayes rule and its risk converges to the risk of the latter. These results are most readily seen when the two populations have well-defined densities, but they may also be derived in other cases, where densities exist in only a relative sense. Cross-validation can be used effectively to choose the sampling fraction. Numerical calculation is used to illustrate these theoretical properties."
"10.1111/j.1467-9868.2005.00505.x","2005","Small confidence sets for the mean of a spherically symmetric distribution","2","Suppose that X has a k-variate spherically symmetric distribution with mean vector theta and identity covariance matrix. We present two spherical confidence sets for theta, both centred at a positive part Stein estimator T-S(+)(X) In the first, we obtain the radius by approximating the upper alpha-point of the sampling distribution of parallel to T-S(+)(X)-theta parallel to(2) by the first two non-zero terms of its Taylor series about the origin. We can analyse some of the properties of this confidence set and see that it performs well in terms of coverage probability, volume and conditional behaviour. In the second method, we find the radius by using a parametric bootstrap procedure. Here, even greater improvement in terms of volume over the usual confidence set is possible, at the expense of having a less explicit radius function. A real data example is provided, and extensions to the unknown covariance matrix and elliptically symmetric cases are discussed."
"10.1111/j.1467-9868.2005.00504.x","2005","Structural learning with time-varying components: tracking the cross-section of the financial time series","0","When modelling multivariate financial data, the problem of structural learning is compounded by the fact that the covariance structure changes with time. Previous work has focused on modelling those changes by using multivariate stochastic volatility models. We present an alternative to these models that focuses instead on the latent graphical structure that is related to the precision matrix. We develop a graphical model for sequences of Gaussian random vectors when changes in the underlying graph occur at random times, and a new block of data is created with the addition or deletion of an edge. We show how a Bayesian hierarchical model incorporates both the uncertainty about that graph and the time variation thereof."
"10.1111/j.1467-9868.2005.00496.x","2005","Acknowledgement of priority: ``{S}trong control, conservative point estimation and simultaneous conservative consistency of false discovery rates: a unified approach'' [{J}. {R}. {S}tat. {S}oc. {S}er. {B} {S}tat. {M}ethodol. {\bf 66} (2004), no. 1, 187--205; MR2035766]","0",""
"10.1111/j.1467-9868.2005.00495.x","2005","Risk-reducing shrinkage estimation for generalized linear models","0","Empirical Bayes techniques for normal theory shrinkage estimation are extended to generalized linear models in a manner retaining the original spirit of shrinkage estimation, which is to reduce risk. The investigation identifies two classes of simple, all-purpose prior distributions, which supplement such non-informative priors as Jeffreys's prior with mechanisms for risk reduction. One new class of priors is motivated as optimizers of a core component of asymptotic risk. The methodology is evaluated in a numerical exploration and application to an existing data set."
"10.1111/j.1467-9868.2005.00494.x","2005","Asymptotic bias in the linear mixed effects model under non-ignorable missing data mechanisms","1","In longitudinal studies, missingness of data is often an unavoidable problem. Estimators from the linear mixed effects model assume that missing data are missing at random. However, estimators are biased when this assumption is not met. In the paper, theoretical results for the asymptotic bias are established under non-ignorable drop-out, drop-in and other missing data patterns. The asymptotic bias is large when the drop-out subjects have only one or no observation, especially for slope-related parameters of the linear mixed effects model. In the drop-in case, intercept-related parameter estimators show substantial asymptotic bias when subjects enter late in the study. Eight other missing data patterns are considered and these produce asymptotic biases of a variety of magnitudes."
"10.1111/j.1467-9868.2005.00493.x","2005","Improved unbiased estimators in adaptive cluster sampling","1","The usual design-unbiased estimators in adaptive cluster sampling are easy to compute but are not functions of the minimal sufficient statistic and hence can be improved. Improved unbiased estimators obtained by conditioning on sufficient statistics-not necessarily minimal-are described. First, estimators that are as easy to compute as the usual design-unbiased estimators are given. Estimators obtained by conditioning on the minimal sufficient statistic which are more difficult to compute are also discussed. Estimators are compared in examples."
"10.1111/j.1467-9868.2005.00492.x","2005","Analysis of longitudinal data unbalanced over time","0","The paper considers modelling, estimating and diagnostically verifying the response process generating longitudinal data, with emphasis on association between repeated meas-ures from unbalanced longitudinal designs. Our model is based on separate specifications of the moments for the mean, standard deviation and correlation, with different components possibly sharing common parameters. We propose a general class of correlation structures that comprise random effects, measurement errors and a serially correlated process. These three elements are combined via flexible time-varying weights, whereas the serial correlation can depend flexibly on the mean time and lag. When the measurement schedule is independent of the response process, our estimation procedure yields consistent and asymptotically normal estimates for the mean parameters even when the standard deviation and correlation are misspecified, and for the standard deviation parameters even when the correlation is misspecified. A generic diagnostic method is developed for verifying the models for the mean, standard deviation and, in particular, the correlation, which is applicable even when the data are severely unbalanced. The methodology is illustrated by an analysis of data from a longitudinal study that was designed to characterize pulmonary growth in girls."
"10.1111/j.1467-9868.2005.00491.x","2005","An invitation to quantum tomography","1","We describe quantum tomography as an inverse statistical problem in which the quantum state of a light beam is the unknown parameter and the data are given by results of measurements performed on identical quantum systems. The state can be represented as an infinite dimensional density matrix or equivalently as a density on the plane called the Wigner function. We present consistency results for pattern function projection estimators and for sieve maximum likelihood estimators for both the density matrix of the quantum state and its Wigner function. We illustrate the performance of the estimators on simulated data. An EM algorithm is proposed for practical implementation. There remain many open problems, e.g. rates of convergence, adaptation and studying other estimators; a main purpose of the paper is to bring these to the attention of the statistical community."
"10.1111/j.1467-9868.2005.00490.x","2005","Sparsity and smoothness via the fused lasso","9","The lasso penalizes a least squares regression by the sum of the absolute values (L-1-norm) of the coefficients. The form of this penalty encourages sparse solutions (with many coefficients equal to 0). We propose the 'fused lasso', a generalization that is designed for problems with features that can be ordered in some meaningful way. The fused lasso penalizes the L-1-norm of both the coefficients and their successive differences. Thus it encourages sparsity of the coefficients and also sparsity of their differences-i.e. local constancy of the coefficient profile. The fused lasso is especially useful when the number of features p is much greater than N, the sample size. The technique is also extended to the 'hinge' loss function that underlies the support vector classifier. We illustrate the methods on examples from protein mass spectroscopy and gene expression data."
"10.1111/j.1467-9868.2005.00489.x","2005","A jackknife variance estimator for unequal probability sampling","3","The jackknife method is often used for variance estimation in sample surveys but has only been developed for a limited class of sampling designs. We propose a jackknife variance estimator which is defined for any without-replacement unequal probability sampling design. We demonstrate design consistency of this estimator for a broad class of point estimators. A Monte Carlo study shows how the proposed estimator may improve on existing estimators."
"10.1111/j.1467-9868.2005.00488.x","2005","Bivariate location-scale models for regression analysis, with applications to lifetime data","0","The literature on multivariate linear regression includes multivariate normal models, models that are used in survival analysis and a variety of models that are used in other areas such as econometrics. The paper considers the class of location-scale models, which includes a large proportion of the preceding models. It is shown that, for complete data, the maximum likelihood estimators for regression coefficients in a linear location-scale framework are consistent even when the joint distribution is misspecified. In addition, gains in efficiency arising from the use of a bivariate model, as opposed to separate univariate models, are studied. A major area of application for multivariate regression models is to clustered, 'parallel' lifetime data, so we also study the case of censored responses. Estimators of regression coefficients are no longer consistent under model misspecification, but we give simulation results that show that the bias is small in many practical situations. Gains in efficiency from bivariate models are also examined in the censored data setting. The methodology in the paper is illustrated by using lifetime data from the Diabetic Retinopathy Study."
"10.1111/j.1467-9868.2005.00487.x","2005","Smooth backfitting in practice","8","Compared with the classical backfitting of Buja, Hastie and Tibshirani, the smooth backfitting estimator (SBE) of Mammen, Linton and Nielsen not only provides complete asymptotic theory under weaker conditions but is also more efficient, robust and easier to calculate. However, the original paper describing the SBE method is complex and the practical as well as the theoretical advantages of the method have still neither been recognized nor accepted by the statistical community. We focus on a clear presentation of the idea, the main theoretical results and practical aspects like implementation and simplification of the algorithm. We introduce a feasible cross-validation procedure and apply it to the problem of data-driven bandwidth choice for the SBE. By simulations it is shown that the SBE and our cross-validation work very well indeed. In particular, the SBE is less affected by sparseness of data in high dimensional regression problems or strongly correlated designs. The SBE has reasonable performance even in 100-dimensional additive regression problems."
"10.1111/j.1467-9868.2005.00486.x","2005","On difference-based variance estimation in nonparametric regression when the covariate is high dimensional","3","We consider the problem of estimating the noise variance in homoscedastic nonparametric regression models. For low dimensional covariates t is an element of R-d, d=1, 2, difference-based estimators have been investigated in a series of papers. For a given length of such an estimator, difference schemes which minimize the asymptotic mean-squared error can be computed for d=1 and d=2. However, from numerical studies it is known that for finite sample sizes the performance of these estimators may be deficient owing to a large finite sample bias. We provide theoretical support for these findings. In particular, we show that with increasing dimension d this becomes more drastic. If dgreater than or equal to4, these estimators even fail to be consistent. A different class of estimators is discussed which allow better control of the bias and remain consistent when dgreater than or equal to4. These estimators are compared numerically with kernel-type estimators (which are asymptotically efficient), and some guidance is given about when their use becomes necessary."
"10.1111/j.1467-9868.2005.00485.x","2005","{$T$}-optimum designs for discrimination between two multiresponse dynamic models","2","The paper is concerned with a problem of finding an optimum experimental design for discriminating between two rival multiresponse models. The criterion of optimality that we use is based on the sum of squares of deviations between the models and picks up the design points for which the divergence is maximum. An important part of our criterion is an additional vector of experimental conditions, which may affect the design. We give the necessary conditions for the design and the additional parameters of the experiment to be optimum, we present the algorithm for the numerical optimization procedure and we show the relevance of these methods to dynamic systems, especially to chemical kinetic models."
"10.1111/j.1467-9868.2005.00503.x","2005","Regularization and variable selection via the elastic net","46","We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p>n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso."
"10.1111/j.1467-9868.2005.00502.x","2005","Model-free variable selection","4","The importance of variable selection in regression has grown in recent years as computing power has encouraged the modelling of data sets of ever-increasing size. Data mining applications in finance, marketing and bioinformatics are obvious examples. A limitation of nearly all existing variable selection methods is the need to specify the correct model before selection. When the number of predictors is large, model formulation and validation can be difficult or even infeasible. On the basis of the theory of sufficient dimension reduction, we propose a new class of model-free variable selection approaches. The methods proposed assume no model of any form, require no nonparametric smoothing and allow for general predictor effects. The efficacy of the methods proposed is demonstrated via simulation, and an empirical example is given."
"10.1111/j.1467-9868.2005.00501.x","2005","Model determination for categorical data with factor level merging","0","We deal with contingency table data that are used to examine the relationships between a set of categorical variables or factors. We assume that such relationships can be adequately described by the cond`itional independence structure that is imposed by an undirected graphical model. If the contingency table is large, a desirable simplified interpretation can be achieved by combining some categories, or levels, of the factors. We introduce conditions under which such an operation does not alter the Markov properties of the graph. Implementation of these conditions leads to Bayesian model uncertainty procedures based on reversible jump Markov chain Monte Carlo methods. The methodology is illustrated on a 2x3x4 and up to a 4x5x5x2x2 contingency table."
"10.1111/j.1467-9868.2005.00500.x","2005","Scaling limits for the transient phase of local {M}etropolis-{H}astings algorithms","2","The paper considers high dimensional Metropolis and Langevin algorithms in their initial transient phase. In stationarity, these algorithms are well understood and it is now well known how to scale their proposal distribution variances. For the random-walk Metropolis algorithm, convergence during the transient phase is extremely regular-to the extent that the algo-rithm's sample path actually resembles a deterministic trajectory. In contrast, the Langevin algorithm with variance scaled to be optimal for stationarity performs rather erratically. We give weak convergence results which explain both of these types of behaviour and practical guidance on implementation based on our theory."
"10.1111/j.1467-9868.2005.00499.x","2005","Ascent-based {M}onte {C}arlo expectation-maximization","1","The expectation-maximization (EM) algorithm is a popular tool for maximizing likelihood functions in the presence of missing data. Unfortunately, EM often requires the evaluation of analytically intractable and high dimensional integrals. The Monte Carlo EM (MCEM) algorithm is the natural extension of EM that employs Monte Carlo methods to estimate the relevant integrals. Typically, a very large Monte Carlo sample size is required to estimate these integrals within an acceptable tolerance when the algorithm is near convergence. Even if this sample size were known at the onset of implementation of MCEM, its use throughout all iterations is wasteful, especially when accurate starting values are not available. We propose a data-driven strategy for controlling Monte Carlo resources in MCEM. The algorithm proposed improves on similar existing methods by recovering EM's ascent (i.e. likelihood increasing) property with high probability, being more robust to the effect of user-defined inputs and handling classical Monte Carlo and Markov chain Monte Carlo methods within a common framework. Because of the first of these properties we refer to the algorithm as 'ascent-based MCEM'. We apply ascent-based MCEM to a variety of examples, including one where it is used to accelerate the convergence of deterministic EM dramatically."
"10.1111/j.1467-9868.2005.00498.x","2005","Bayesian classification of tumours by using gene expression data","0","Precise classification of tumours is critical for the diagnosis and treatment of cancer. Diagnostic pathology has traditionally relied on macroscopic and microscopic histology and tumour morphology as the basis for the classification of tumours. Current classification frameworks, however, cannot discriminate between tumours with similar histopathologic features, which vary in clinical course and in response to treatment. In recent years, there has been a move towards the use of complementary deoxyribonucleic acid microarrays for the classi-fication of tumours. These high throughput assays provide relative messenger ribonucleic acid expression measurements simultaneously for thousands of genes. A key statistical task is to perform classification via different expression patterns. Gene expression profiles may offer more information than classical morphology and may provide an alternative to classical tumour diagnosis schemes. The paper considers several Bayesian classification methods based on reproducing kernel Hilbert spaces for the analysis of microarray data. We consider the logistic likelihood as well as likelihoods related to support vector machine models. It is shown through simulation and examples that support vector machine models with multiple shrinkage parameters produce fewer misclassification errors than several existing classical methods as well as Bayesian methods based on the logistic likelihood or those involving only one shrinkage parameter."
"10.1111/j.1467-9868.2005.00497.x","2005","Stopping-time resampling for sequential {M}onte {C}arlo methods","0","Motivated by the statistical inference problem in population genetics, we present a new sequential importance sampling with resampling strategy. The idea of resampling is key to the recent surge of popularity of sequential Monte Carlo methods in the statistics and engin-eering communities, but existing resampling techniques do not work well for coalescent-based inference problems in population genetics. We develop a new method called 'stopping-time resampling', which allows us to compare partially simulated samples at different stages to terminate unpromising partial samples and to multiply promising samples early on. To illustrate the idea, we first apply the new method to approximate the solution of a Dirichlet problem and the likelihood function of a non-Markovian process. Then we focus on its application in population genetics. All our examples show that the new resampling method can significantly improve the computational efficiency of existing sequential importance sampling methods."
"10.1111/j.1467-9868.2004.B5582.x","2004","Self-modelling warping functions","7","The paper introduces a semiparametric model for functional data. The warping functions are assumed to be linear combinations of q common components, which are estimated from the data (hence the name 'self-modelling'). Even small values of q provide remarkable model flexibility, comparable with nonparametric methods. At the same time, this approach avoids overfitting because the common components are estimated combining data across individuals. As a convenient by-product, component scores are often interpretable and can be used for statistical inference (an example of classification based on scores is given)."
"10.1111/j.1467-9868.2004.05480.x","2004","Merging information for semiparametric density estimation","0","The density ratio model specifies that the likelihood ratio of m-1 probability density functions with respect to the mth is of known parametric form without reference to any parametric model. We study the semiparametric inference problem that is related to the density ratio model by appealing to the methodology of empirical likelihood. The combined data from all the samples leads to more efficient kernel density estimators for the unknown distributions. We adopt variants of well-established techniques to choose the smoothing parameter for the density estimators proposed."
"10.1111/j.1467-9868.2004.B5604.x","2004","Real nonparametric regression using complex wavelets","2","Wavelet shrinkage is an effective nonparametric regression technique, especially when the underlying curve has irregular features such as spikes or discontinuities. The basic idea is simple: take the discrete wavelet transform of data consisting of a signal corrupted by noise; shrink or remove the wavelet coefficients to remove the noise; then invert the discrete wavelet transform to form an estimate of the true underlying curve. Various researchers have proposed increasingly sophisticated methods of doing this by using real-valued wavelets. Complex-valued wavelets exist but are rarely used. We propose two new complex-valued wavelet shrinkage techniques: one based on multiwavelet style shrinkage and the other using Bayesian methods. Extensive simulations show that our methods almost always give significantly more accurate estimates than methods based on real-valued wavelets. Further, our multiwavelet style shrinkage method is both simpler and dramatically faster than its competitors. To understand the excellent performance of this method we present a new risk bound on its hard thresholded coefficients."
"10.1111/j.1467-9868.2004.05421.x","2004","Restricted likelihood ratio lack-of-fit tests using mixed spline models","2","Penalized regression spline models afford a simple mixed model representation in which variance components control the degree of non-linearity in the smooth function estimates. This motivates the study of lack-of-fit tests based on the restricted maximum likelihood ratio statistic which tests whether variance components are 0 against the alternative of taking on positive values. For this one-sided testing problem a further complication is that the variance component belongs to the boundary of the parameter space under the null hypothesis. Conditions are obtained on the design of the regression spline models under which asymptotic distribution theory applies, and finite sample approximations to the asymptotic distribution are provided. Test statistics are studied for simple as well as multiple-regression models."
"10.1111/j.1467-9868.2004.05627.x","2004","Estimation of generalized linear latent variable models","3","Generalized linear latent variable models (GLLVMs), as defined by Bartholomew and Knott, enable modelling of relationships between manifest and latent variables. They extend structural equation modelling techniques, which are powerful tools in the social sciences. However, because of the complexity of the log-likelihood function of a GLLVM, an approximation such as numerical integration must be used for inference. This can limit drastically the number of variables in the model and can lead to biased estimators. We propose a new estimator for the parameters of a GLLVM, based on a Laplace approximation to the likelihood function and which can be computed even for models with a large number of variables. The new estimator can be viewed as an M-estimator, leading to readily available asymptotic properties and correct inference. A simulation study shows its excellent finite sample properties, in particular when compared with a well-established approach such as LISREL. A real data example on the measurement of wealth for the computation of multidimensional inequality is analysed to highlight the importance of the methodology."
"10.1111/j.1467-9868.2004.B5590.x","2004","Approximating hidden {G}aussian {M}arkov random fields","2","Gaussian Markov random-field (GMRF) models are frequently used in a wide variety of applications. In most cases parts of the GMRF are observed through mutually independent data; hence the full conditional of the GMRF, a hidden GMRF (HGMRF), is of interest. We are concerned with the case where the likelihood is non-Gaussian, leading to non-Gaussian HGMRF models. Several researchers have constructed block sampling Markov chain Monte Carlo schemes based on approximations of the HGMRF by a GMRF, using a second-order expansion of the log-density at or near the mode. This is possible as the GMRF approximation can be sampled exactly with a known normalizing constant. The Markov property of the GMRF approximation yields computational efficiency.The main contribution in the paper is to go beyond the GMRF approximation and to construct a class of non-Gaussian approximations which adapt automatically to the particular HGMRF that is under study. The accuracy can be tuned by intuitive parameters to nearly any precision. These non-Gaussian approximations share the same computational complexity as those which are based on GMRFs and can be sampled exactly with computable normalizing constants. We apply our approximations in spatial disease mapping and model-based geostatistical models with different likelihoods, obtain procedures for block updating and construct Metropolized independence samplers."
"10.1111/j.1467-9868.2004.B5725.x","2004","Nonparametric inference about service time distribution from indirect measurements","0","In studies of properties of queues, for example in relation to Internet traffic, a subject that is of particular interest is the 'shape' of service time distribution. For example, we might wish to know whether the service time density is unimodal, suggesting that service time distribution is possibly homogeneous, or whether it is multimodal, indicating that there are two or more distinct customer populations. However, even in relatively controlled experiments we may not have access to explicit service time data. Our only information might be the durations of service time clusters, i.e. of busy periods. We wish to 'deconvolve' these concatenations, and to construct empirical approximations to the distribution and, particularly, the density function of service time. Explicit solutions of these problems will be suggested. In particular, a kernel-based 'deconvolution' estimator of service time density will be introduced, admitting conventional approaches to the choice of bandwidth."
"10.1111/j.1467-9868.2004.05741.x","2004","Efficiency of generalized estimating equations for binary responses","0","Using standard correlation bounds, we show that in generalized estimation equations (GEEs) the so-called 'working correlation matrix'R(alpha) for analysing binary data cannot in general be the true correlation matrix of the data. Methods for estimating the correlation param-eter in current GEE software for binary responses disregard these bounds. To show that the GEE applied on binary data has high efficiency, we use a multivariate binary model so that the covariance matrix from estimating equation theory can be compared with the inverse Fisher information matrix. But R(alpha) should be viewed as the weight matrix, and it should not be confused with the correlation matrix of the binary responses. We also do a comparison with more general weighted estimating equations by using a matrix Cauchy-Schwarz inequality. Our analysis leads to simple rules for the choice of alpha in an exchangeable or autoregressive AR(1) weight matrix R(alpha), based on the strength of dependence between the binary variables. An example is given to illustrate the assessment of dependence and choice of alpha."
"10.1111/j.1467-9868.2004.02059.x","2004","Clustering objects on subsets of attributes","3","A new procedure is proposed for clustering attribute value data. When used in conjunction with conventional distance-based clustering algorithms this procedure encourages those algorithms to detect automatically subgroups of objects that preferentially cluster on subsets of the attribute variables rather than on all of them simultaneously. The relevant attribute subsets for each individual cluster can be different and partially (or completely) overlap with those of other clusters. Enhancements for increasing sensitivity for detecting especially low cardinality groups clustering on a small subset of variables are discussed. Applications in different domains, including gene expression arrays, are presented."
"10.1111/j.1467-9868.2004.b5543.x","2004","Analysis of longitudinal data with irregular, outcome-dependent follow-up","2","A frequent problem in longitudinal studies is that subjects may miss scheduled visits or be assessed at self-selected points in time. As a result, observed outcome data may be highly unbalanced and the availability of the data may be directly related to the outcome measure and/or some auxiliary factors that are associated with the outcome. If the follow-up visit and outcome processes are correlated, then marginal regression analyses will produce biased estimates. Building on the work of Robins, Rotnitzky and Zhao, we propose a class of inverse intensity, of-visit process-weighted estimators in marginal regression models for longitudinal responses that may be observed in continuous time. This allows us to handle arbitrary patterns of missing data as embedded in a subject's visit process. We derive the large sample distribution for our inverse visit-intensity-weighted estimators and investigate their finite sample behaviour by simulation. Our approach is illustrated with a data set from a health services research study in which homeless people with mental illness were randomized to three different treatments and measures of homelessness (as percentage days homeless in the past 3 months) and other auxiliary factors were recorded at follow-up times that are not fixed by design."
"10.1111/j.1467-9868.2004.05561.x","2004","Exact filtering for partially observed continuous time models","1","The forward-backward algorithm is an exact filtering algorithm which can efficiently calculate likelihoods, and which can be used to simulate from posterior distributions. Using a simple result which relates gamma random variables with different rates, wig show how the forward-backward algorithm can be used to calculate the distribution of a sum of gamma random variables, and to simulate from their joint distribution given their sum. One application is to calculating the density of the time of a specific event in a Markov process, as this time is the sum of exponentially distributed interevent times. This enables us to apply the forward-backward algorithm to a range of new problems. We demonstrate our method on three problems: calculating likelihoods and simulating allele frequencies under a non-neutral population genetic model, analysing a stochastic epidemic model and simulating speciation times in phylogenetics."
"10.1111/j.1467-9868.2004.05304.x","2004","Probabilistic sensitivity analysis of complex models: a {B}ayesian approach","4","In many areas of science and technology, mathematical models are built to simulate complex real world phenomena. Such models are typically implemented in large computer programs and are also very complex, such that the way that the model responds to changes in its inputs is not transparent. Sensitivity analysis is concerned with understanding how changes in the model inputs influence the outputs. This may be motivated simply by a wish to understand the implications of a complex model but often arises because there is uncertainty about the true values of the inputs that should be used for a particular application. A broad range of measures have been advocated in the literature to quantify and describe the sensitivity of a model's output to variation in its inputs. In practice the most commonly used measures are those that are based on formulating uncertainty in the model inputs by a joint probability distribution and then analysing the induced uncertainty in outputs, an approach which is known as probabilistic sensitivity analysis. We present a Bayesian framework which unifies the various tools of probabilistic sensitivity analysis. The Bayesian approach is computationally highly efficient. It allows effective sensitivity analysis to be achieved by using far smaller numbers of model runs than standard Monte Carlo methods. Furthermore, all measures of interest may be computed from a single set of runs."
"10.1111/j.1467-9868.2004.05564.x","2004","A method for combining inference across related nonparametric {B}ayesian models","16","We consider the problem of combining inference in related nonparametric Bayes models. Analogous to parametric hierarchical models, the hierarchical extension formalizes borrowing strength across the related submodels. In the nonparametric context, modelling is complicated by the fact that the random quantities over which we define the hierarchy are infinite dimensional. We discuss a formal definition of such a hierarchical model. The approach includes a regression at the level of the nonparametric model. For the special case of Dirichlet process mixtures, we develop a Markov chain Monte Carlo scheme to allow efficient implementation of full posterior inference in the given model."
"10.1111/j.1467-9868.2004.b5364.x","2004","A nonparametric test to compare survival distributions with covariate adjustment","0","the analysis of covariance is a technique that is used to improve the power of a k-sample test by adjusting for concomitant variables. If the end point is the time of survival, and some observations are right censored, the score statistic from the Cox proportional hazards model is the method that is most commonly used to test the equality of conditional hazard functions. In many situations, however, the proportional hazards model assumptions are not satisfied. Specifically, the relative risk function is not time invariant or represented as a log-linear function of the covariates. We propose an asymptotically valid k-sample test statistic to compare conditional hazard functions which does not require the assumption of proportional hazards, a parametric, specification of the relative risk function or randomization of group assignment. Simulation results indicate that the performance of this statistic is satisfactory. The methodology is demonstrated on a data set in prostate cancer."
"10.1111/j.1467-9868.2004.b5161.x","2004","Joint response graphs and separation induced by triangular systems","4","We consider joint probability distributions generated recursively in terms of univariate conditional distributions satisfying conditional independence restrictions. The independences are captured by missing edges in a directed graph. A matrix form of such a graph, called the generating edge matrix, is triangular so the distributions that are generated over such graphs are called triangular systems. We study consequences of triangular systems after grouping or reordering of the variables for analyses as chain graph models, i.e. for alternative recursive factorizations of the given density using joint conditional distributions. For this we introduce families of linear triangular equations which do not require assumptions of distributional form. The strength of the associations that are implied by such linear families for chain graph models is derived. The edge matrices of chain graphs that are implied by any triangular system are obtained by appropriately transforming the generating edge matrix. It is shown how induced independences and dependences can be studied by graphs, by edge matrix calculations and via the properties of densities. Some ways of using the results are illustrated."
"10.1111/j.1467-9868.2004.04664.x","2004","Nonparametric multistep-ahead prediction in time series analysis","0","We consider the problem of multistep-ahead prediction in time series analysis by using nonparametric smoothing techniques. Forecasting is always one of the main objectives in time series analysis. Research has shown that non-linear time series models have certain advantages in multistep-ahead forecasting. Traditionally, nonparametric k-step-ahead least squares prediction for non-linear autoregressive AR(d) models is done by estimating E(Xt+k\X-t, . . . ,Xt-d+1) via nonparametric smoothing of Xt+k on (X-t, . . . ,Xt-d+1) directly. We propose a multistage nonparametric predictor. We show that the new predictor has smaller asymptotic mean-squared error than the direct smoother, though the convergence rate is the same. Hence, the predictor proposed is more efficient. Some simulation results, advice for practical bandwidth selection and a real data example are provided."
"10.1111/j.1467-9868.2004.B5595.x","2004","Smoothing spline estimation in varying-coefficient models","1","Smoothing spline estimators are considered for inference in varying-coefficient models with one effect modifying covariate. Bayesian 'confidence intervals' are developed for the coefficient curves and efficient computational methods are derived for computing the curve estimators, fitted values, posterior variances and data-adaptive methods for selecting the levels of Smoothing. The efficacy and utility of the methodology proposed are demonstrated through a small simulation study and the analysis of a real data set."
"10.1111/j.1467-9868.2004.02054.x","2004","Bayesian analysis of the scatterometer wind retrieval inverse problem: some new approaches","0","The retrieval of wind vectors from satellite scatterometer observations is a non-linear inverse problem. A common approach to solving inverse problems is to adopt a Bayesian framework and to infer the posterior distribution of the parameters of interest given the observations by using a likelihood model relating the observations to the parameters, and a prior distribution over the parameters. We show how Gaussian process priors can be used efficiently with a variety of likelihood models, using local forward (observation) models and direct inverse models for the scatterometer. We present an enhanced Markov chain Monte Carlo method to sample from the resulting multimodal posterior distribution. We go on to show how the computational complexity of the inference can be controlled by using a sparse, sequential Bayes algorithm for estimation with Gaussian processes. This helps to overcome the most serious barrier to the use of probabilistic, Gaussian process methods in remote sensing inverse problems, which is the prohibitively large size of the datasets. We contrast the sampling results with the approximations that are found by using the sparse, sequential Bayes algorithm."
"10.1111/j.1467-9868.2004.02053.x","2004","Markov chain {M}onte {C}arlo methods for high dimensional inversion in remote sensing","0","We discuss the inversion of the gas profiles (ozone, NO3, NO2, aerosols and neutral density) in the upper atmosphere from the spectral occultation measurements. The data are produced by the 'Global ozone monitoring of occultation of stars' instrument on board the Envisat satellite that was launched in March 2002. The instrument measures the attenuation of light spectra at various horizontal paths from about 100 km down to 10-20 km. The new feature is that these data allow the inversion of the gas concentration height profiles. A short introduction is given to the present operational data management procedure with examples of the first real data inversion. Several solution options for a more comprehensive statistical inversion are presented. A direct inversion leads to a non-linear model with hundreds of parameters to be estimated. The problem is solved with an adaptive single-step Markov chain Monte Carlo algorithm. Another approach is to divide the problem into several non-linear smaller dimensional problems, to run parallel adaptive Markov chain Monte Carlo chains for them and to solve the gas profiles in repetitive linear steps. The effect of grid size is discussed, and we present how the prior regularization takes the grid size into account in a way that effectively leads to a grid-independent inversion."
"10.1111/j.1467-9868.2004.02052.x","2004","Bayesian variable selection and regularization for time-frequency surface estimation","2","We describe novel Bayesian models for time-frequency inverse modelling of nonstationary signals. These models are based on the idea of a Gabor regression, in which a time series is represented as a superposition of translated, modulated versions of a window function exhibiting good time-frequency concentration. As a necessary consequence, the resultant set of potential predictors is in general overcomplete-constituting a frame rather than a basis-and hence the resultant models require careful regularization through appropriate choices of variable selection schemes and prior distributions. We introduce prior specifications that are tailored to representative time series, and we develop effective Markov chain Monte Carlo methods for inference. To highlight the potential applications of such methods, we provide examples using two of the most distinctive time-frequency surfaces-speech and music signals-as well as standard test functions from the wavelet regression literature."
"10.1111/j.1467-9868.2004.02056.x","2004","Wavelet deconvolution in a periodic setting","9","Deconvolution problems are naturally represented in the Fourier domain, whereas thresholding in wavelet bases is known to have broad adaptivity properties. We study a method which combines both fast Fourier and fast wavelet transforms and can recover a blurred function observed in white noise with O{n log(n)2} steps. In the periodic setting, the method applies to most deconvolution problems, including certain 'boxcar' kernels, which are important as a model of motion blur, but having poor Fourier characteristics. Asymptotic theory informs the choice of tuning parameters and yields adaptivity properties for the method over a wide class of measures of error and classes of function. The method is tested on simulated light detection and ranging data suggested by underwater remote sensing. Both visual and numerical results show an improvement over competing approaches. Finally, the theory behind our estimation paradigm gives a complete characterization of the 'maxiset' of the method: the set of functions where the method attains a near optimal rate of convergence for a variety of L-p loss functions."
"10.1111/j.1467-9868.2004.02050.x","2004","A conditional approach for multivariate extreme values","8","Multivariate extreme value theory and methods concern the characterization, estimation and extrapolation of the joint tail of the distribution of a d-dimensional random variable. Existing approaches are based on limiting arguments in which all components of the variable become large at the same rate. This limit approach is inappropriate when the extreme values of all the variables are unlikely to occur together or when interest is in regions of the support of the joint distribution where only a subset of components is extreme. In practice this restricts existing methods to applications where d is typically 2 or 3. Under an assumption about the asymptotic form of the joint distribution of a d-dimensional random variable conditional on its having an extreme component, we develop an entirely new semiparametric approach which overcomes these existing restrictions and can be applied to problems of any dimension. We demonstrate the performance of our approach and its advantages over existing methods by using theoretical examples and simulation studies. The approach is used to analyse air pollution data and reveals complex extremal dependence behaviour that is consistent with scientific understanding of the process. We find that the dependence structure exhibits marked seasonality, with extremal dependence between some pollutants being significantly greater than the dependence at non-extreme levels."
"10.1111/j.1369-7412.2004.05266.x","2004","Small area estimates for cross-classifications","0","We develop a class of log-linear structural models that is suited to estimation of small area cross-classified counts based on survey data. This allows us to account for various association structures within the data and includes as a special case the restricted log-linear model underlying structure preserving estimation. The effect of survey design can be incorporated into estimation through the specification of an unbiased direct estimator and its associated covariance structure. We illustrate our approach by applying it to estimation of small area labour force characteristics in Norway."
"10.1111/j.1369-7412.2004.05500.x","2004","Identification of non-linear additive autoregressive models","4","We propose a lag selection method for non-linear additive autoregressive models that is based on spline estimation and the Bayes information criterion. The additive structure of the autoregression function is used to overcome the 'curse of dimensionality', whereas the spline estimators effectively take into account such a structure in estimation. A stepwise procedure is suggested to implement the method proposed. A comprehensive Monte Carlo study demonstrates good performance of the method proposed and a substantial computational advantage over existing local-polynomial-based methods. Consistency of the lag selection method based on the Bayes information criterion is established under the assumption that the observations are from a stochastic process that is strictly stationary and strongly mixing, which provides the first theoretical result of this kind for spline smoothing of weakly dependent data."
"10.1111/j.1369-7412.2003.05341.x","2004","Estimating functions in indirect inference","2","There are models for which the evaluation of the likelihood is infeasible in practice. For these models the Metropolis-Hastings acceptance probability cannot be easily computed. This is the case, for instance, when only departure times from a G/G/1 queue are observed and inference on the arrival and service distributions are required. Indirect inference is a method to estimate a parameter theta in models whose likelihood function does not have an analytical closed form, but from which random samples can be drawn for fixed values of theta. First an auxiliary model is chosen whose parameter theta can be directly estimated. Next, the parameters in the auxiliary model are estimated for the original data, leading to an estimate (&beta;) over cap. The parameter beta is also estimated by using several sampled data sets, simulated from the original model for different values of the original parameter beta. Finally, the parameter beta which leads to the best match to is chosen as the indirect inference estimate. We analyse which properties an auxiliary model should have to give satisfactory indirect inference. We look at the situation where the data are summarized in a vector statistic T, and the auxiliary model is chosen so that inference on beta is drawn from T only. Under appropriate assumptions the asymptotic covariance matrix of the indirect estimators is proportional to the asymptotic covariance matrix of T and componentwise inversely proportional to the square of the derivative, with respect to theta, of the expected value of T. We discuss how these results can be used in selecting good estimating functions. We apply our findings to the queuing problem."
"10.1046/j.1369-7412.2003.05285.x","2004","Modelling spatial intensity for replicated inhomogeneous point patterns in brain imaging","2","Pharmacological experiments in brain microscopy study patterns of cellular activation in response to psychotropic drugs for connected neuroanatomic regions. A typical experimental design produces replicated point patterns having highly complex spatial variability. Modelling this variability hierarchically can enhance the inference for comparing treatments. We propose a semiparametric formulation that combines the robustness of a nonparametric kernel method with the efficiency of likelihood-based parameter estimation. In the convenient framework of a generalized linear mixed model, we decompose pattern variation by kriging the intensities of a hierarchically heterogeneous spatial point process. This approximation entails discretizing the inhomogeneous Poisson likelihood by Voronoi tiling of augmented point patterns. The resulting intensity-weighted log-linear model accommodates spatial smoothing through a reduced rank penalized linear spline. To correct for anatomic distortion between subjects, we interpolate point locations via an isomorphic mapping so that smoothing occurs relative to common neuroanatomical atlas co-ordinates. We propose a criterion for choosing the degree and spatial locale of smoothing based on truncating the ordered set of smoothing covariates to minimize residual extra-dispersion. Additional spatial covariates, experimental design factors, hierarchical random effects and intensity functions are readily accommodated in the linear predictor, enabling comprehensive analyses of the salient properties underlying replicated point patterns. We illustrate our method through application to data from a novel study of drug effects on neuronal activation patterns in the brain of rats."
"10.1046/j.1369-7412.2003.05329.x","2004","On the use of local optimizations within {M}etropolis-{H}astings updates","0","We propose new Metropolis-Hastings algorithms for sampling from multimodal distributions on R-n. Tjelmeland and Hegstad have obtained direct mode jumping proposals by optimization within Metropolis-Hastings updates and different proposals for 'forward' and 'backward' steps. We generalize their scheme by allowing the probability distribution for forward and backward kernels to depend on the current state. We use the new setting to combine mode jumping proposals and proposals from a prior approximation. We obtain that the frequency of proposals from the different proposal kernels is automatically adjusted to their quality. Mode jumping proposals include local optimizations. When combining this with a prior approximation it is tempting to use local optimization results not only for mode jumping proposals but also to improve the prior approximation. We show how this idea can be implemented. The resulting algorithm is adaptive but has a Markov structure. We evaluate the effectiveness of the proposed algorithms in two simulation examples."
"10.1046/j.1369-7412.2003.05398.x","2004","Stepwise likelihood ratio statistics in sequential studies","0","It is well known that in a sequential study the probability that the likelihood ratio for a simple alternative hypothesis H-1 versus a simple null hypothesis H-o will ever be greater than a positive constant c will not exceed /c under H-o. However, for a composite alternative hypothesis, this bound of 1/c will no longer hold when a generalized likelihood ratio statistic is used. We consider a stepwise likelihood ratio statistic which, for each new observation, is updated by cumulatively multiplying the ratio of the conditional likelihoods for the composite alternative hypothesis evaluated at an estimate of the parameter obtained from the preceding observations versus the simple null hypothesis. We show that, under the null hypothesis, the probability that this stepwise likelihood ratio will ever be greater than c will not exceed 1/c. In contrast, under the composite alternative hypothesis, this ratio will generally converge in probability to infinity. These results suggest that a stepwise likelihood ratio statistic can be useful in a sequential study for testing a composite alternative versus a simple null hypothesis. For illustration, we conduct two simulation studies, one for a normal response and one for an exponential response, to compare the performance of a sequential test based on a stepwise likelihood ratio statistic with a constant boundary versus some existing approaches."
"10.1111/j.1369-7412.2004.05695.x","2004","A simple procedure for the selection of significant effects","1","Given a large number of test statistics, a small proportion of which represent departures from the relevant null hypothesis, a simple rule is given for choosing those statistics that are indicative of departure. It is based on fitting by moments a mixture model to the set of test statistics and then deriving an estimated likelihood ratio. Simulation suggests that the procedure has good properties when the departure from an overall null hypothesis is not too small."
"10.1111/j.1369-7412.2004.05139.x","2004","Bayesian inference for non-{G}aussian {O}rnstein-{U}hlenbeck stochastic volatility processes","6","We develop Markov chain Monte Carlo methodology for Bayesian inference for non-Gaussian Ornstein-Uhlenbeck stochastic volatility processes. The approach introduced involves expressing the unobserved stochastic volatility process in terms of a suitable marked Poisson process. We introduce two specific classes of Metropolis-Hastings algorithms which correspond to different ways of jointly parameterizing the marked point process and the model parameters. The performance of the methods is investigated for different types of simulated data. The approach is extended to consider the case where the volatility process is expressed as a superposition of Ornstein-Uhlenbeck processes. We apply our methodology to the US dollar-Deutschmark exchange rate."
"10.1111/j.1369-7412.2003.04973.x","2004","Interval estimation for log-linear models with one variable subject to non-ignorable non-response","0","Log-linear models for multiway contingency tables where one variable is subject to non-ignorable non-response will often yield boundary solutions, with the probability of non-respondents being classified in some cells of the table estimated as 0. The paper considers the effect of this non-standard behaviour on two methods of interval estimation based on the distribution of the maximum likelihood estimator. The first method relies on the estimator being approximately normally distributed with variance equal to the inverse of the information matrix. It is shown that the information matrix is singular for boundary solutions, but intervals can be calculated after a simple transformation. For the second method, based on the bootstrap, asymptotic results suggest that the coverage properties may be poor for boundary solutions. Both methods are compared with profile likelihood intervals in a simulation study based on data from the British General Election Panel Study. The results of this study indicate that all three methods perform poorly for a parameter of the non-response model, whereas they all perform well for a parameter of the margin model, irrespective of whether or not there is a boundary solution."
"10.1046/j.1369-7412.2003.05316.x","2004","Smoothing spline {G}aussian regression: more scalable computation via efficient approximation","5","Smoothing splines via the penalized least squares method provide versatile and effective nonparametric models for regression with Gaussian responses. The computation of smoothing splines is generally of the order O(n(3)), n being the sample size, which severely limits its practical applicability. We study more scalable computation of smoothing spline regression via certain low dimensional approximations that are asymptotically as efficient. A simple algorithm is presented and the Bayes model that is associated with the approximations is derived, with the latter guiding the porting of Bayesian confidence intervals. The practical choice of the dimension of the approximating space is determined through simulation studies, and empirical comparisons of the approximations with the exact solution are presented. Also evaluated is a simple modification of the generalized cross-validation method for smoothing parameter selection, which to a large extent fixes the occasional undersmoothing problem that is suffered by generalized cross-validation."
"10.1111/j.1369-7412.2004.05303.x","2004","Semiparametric non-linear time series model selection","1","Semiparametric time series regression is often used without checking its suitability, resulting in an unnecessarily complicated model. In practice, one may encounter computational difficulties caused by the curse of dimensionality. The paper suggests that to provide more precise predictions we need to choose the most significant regressors for both the parametric and the nonparametric time series components. We develop a novel cross-validation-based model selection procedure for the simultaneous choice of both the parametric and the nonparametric time series components, and we establish some asymptotic properties of the model selection procedure proposed. In addition, we demonstrate how to implement it by using both simulated and real examples. Our empirical studies show that the procedure works well."
"10.1046/j.1369-7412.2003.05182.x","2004","Hazard-based nonparametric survivor function estimation","1","A representation is developed that expresses the bivariate survivor function as a function of the hazard function for truncated failure time variables. This leads to a class of nonparametric survivor function estimators that avoid negative mass. The transformation from hazard function to survivor function is weakly continuous and compact differentiable, so that such properties as strong consistency, weak convergence to a Gaussian process and boot-strap applicability for a hazard function estimator are inherited by the corresponding survivor function estimator. The set of point mass assignments for a survivor function estimator is readily obtained by using a simple matrix calculation on the set of hazard rate estimators. Special cases arise from a simple empirical hazard rate estimator, and from an empirical hazard rate estimator following the redistribution of singly censored observations within strips. The latter is shown to equal van der Laan's repaired nonparametric maximum likelihood estimator, for which a Greenwood-like variance estimator is given. Simulation studies are presented to compare the moderate sample performance of various nonparametric survivor function estimators."
"10.1111/j.1369-7412.2003.05527.x","2004","A note on the adaptive control of false discovery rates","3","The use of a fixed rejection region for multiple hypothesis testing has been shown to outperform standard fixed error rate approaches when applied to control of the false discovery rate. In this work it is demonstrated that, if the original step-up procedure of Benjamini and Hochberg is modified to exercise adaptive control of the false discovery rate, its performance is virtually identical to that of the fixed rejection region approach. In addition, the dependence of both methods on the proportion of true null hypotheses is explored, with a focus on the difficulties that are involved in the estimation of this quantity."
"10.1046/j.1369-7412.2003.05512.x","2004","Approximating likelihoods for large spatial data sets","13","Likelihood methods are often difficult to use with large, irregularly sited spatial data sets, owing to the computational burden. Even for Gaussian models, exact calculations of the likelihood for n observations require O(n(3)) operations. Since any joint density can be written as a product of conditional densities based on some ordering of the observations, one way to lessen the computations is to condition on only some of the 'past' observations when computing the conditional densities. We show how this approach can be adapted to approximate the restricted likelihood and we demonstrate how an estimating equations approach allows us to judge the efficacy of the resulting approximation. Previous work has suggested conditioning on those past observations that are closest to the observation whose conditional density we are approximating. Through theoretical, numerical and practical examples, we show that there can often be considerable benefit in conditioning on some distant observations as well."
"10.1046/j.1369-7412.2003.05300.x","2004","Composite conditional likelihood for sparse clustered data","1","Sparse clustered data arise in finely stratified genetic and epidemiologic studies and pose at least two challenges to inference. First, it is difficult to model and interpret the full joint probability of dependent discrete data, which limits the utility of full likelihood methods. Second, standard methods for clustered data, such as pairwise likelihood and the generalized estimating function approach, are unsuitable when the data are sparse owing to the presence of many nuisance parameters. We present a composite conditional likelihood for use with sparse clustered data that provides valid inferences about covariate effects on both the marginal response probabilities and the intracluster pairwise association. Our primary focus is on sparse clustered binary data, in which case the method proposed utilizes doubly discordant quadruplets drawn from each stratum to conduct inference about the intracluster pairwise odds ratios."
"10.1111/j.1467-9868.2004.00442.x","2004","Recurrent events analysis in the presence of time-dependent covariates and dependent censoring","5","Recurrent events models have had considerable attention recently. The majority of approaches show the consistency of parameter estimates under the assumption that censoring is independent of the recurrent events process of interest conditional on the covariates that are included in the model. We provide an overview of available recurrent events analysis methods and present an inverse probability of censoring weighted estimator for the regression parameters in the Andersen-Gill model that is commonly used for recurrent event analysis. This estimator remains consistent under informative censoring if the censoring mechanism is estimated consistently, and it generally improves on the naive estimator for the Andersen-Gill model in the case of independent censoring. We illustrate the bias of ad hoc estimators in the presence of informative censoring with a simulation study and provide a data analysis of recurrent lung exacerbations in cystic fibrosis patients when some patients are lost to follow-up."
"10.1046/j.1369-7412.2003.05220.x","2004","Estimation of global temperature fields from scattered observations by a spherical-wavelet-based spatially adaptive method","0","The paper considers the problem of estimating the entire temperature field for every location on the globe from scattered surface air temperatures observed by a network of weather-stations. Classical methods such as spherical harmonics and spherical smoothing splines are not efficient in representing data that have inherent multiscale structures. The paper presents an estimation method that can adapt to the multiscale characteristics of the data. The method is based on a spherical wavelet approach that has recently been developed for a multiscale representation and analysis of scattered data. Spatially adaptive estimators are obtained by coupling the spherical wavelets with different thresholding (selective reconstruction) techniques. These estimators are compared for their spatial adaptability and extrapolation performance by using the surface air temperature data."
"10.1046/j.1369-7412.2003.05305.x","2004","An estimating equation for parametric shared frailty models with marginal additive hazards","1","Multivariate failure time data arise when data consist of clusters in which the failure times may be dependent. A popular approach to such data is the marginal proportional hazards model with estimation under the working independence assumption. In some contexts, however, it may be more reasonable to use the marginal additive hazards model. We derive asymptotic properties of the Lin and Ying estimators for the marginal additive hazards model for multivariate failure time data. Furthermore we suggest estimating equations for the regression parameters and association parameters in parametric shared frailty models with marginal additive hazards by using the Lin and Ying estimators. We give the large sample properties of the estimators arising from these estimating equations and investigate their small sample properties by Monte Carlo simulation. A real example is provided for illustration."
"10.1111/j.1467-9868.2004.00439.x","2004","Strong control, conservative point estimation and simultaneous conservative consistency of false discovery rates: a unified approach","42","The false discovery rate (FDR) is a multiple hypothesis testing quantity that describes the expected proportion of false positive results among all rejected null hypotheses. Benjamini and Hochberg introduced this quantity and proved that a particular step-up p-value method controls the FDR. Storey introduced a point estimate of the FDR for fixed significance regions. The former approach conservatively controls the FDR at a fixed predetermined level, and the latter provides a conservatively biased estimate of the FDR for a fixed predetermined significance region. In this work, we show in both finite sample and asymptotic settings that the goals of the two approaches are essentially equivalent. In particular, the FDR point estimates can be used to define valid FDR controlling procedures. In the asymptotic setting, we also show that the point estimates can be used to estimate the FDR conservatively over all significance regions simultaneously, which is equivalent to controlling the FDR at all levels simultaneously. The main tool that we use is to translate existing FDR methods into procedures involving empirical processes. This simplifies finite sample proofs, provides a framework for asymptotic results and proves that these procedures are valid even under certain forms of dependence."
"10.1111/j.1467-9868.2004.00438.x","2004","Likelihood ratio tests in linear mixed models with one variance component","9","We consider the problem of testing null hypotheses that include restrictions on the variance component in a linear mixed model with one variance component and we derive the finite sample and asymptotic distribution of the likelihood ratio test and the restricted likelihood ratio test. The spectral representations of the likelihood ratio test and the restricted likelihood ratio test statistics are used as the basis of efficient simulation algorithms of their null distributions. The large sample chi(2) mixture approximations using the usual asymptotic theory for a null hypothesis on the boundary of the parameter space have been shown to be poor in simulation studies. Our asymptotic calculations explain these empirical results. The theory of Self and Liang applies only to linear mixed models for which the data vector can be partitioned into a large number of independent and identically distributed subvectors. One-way analysis of variance and penalized splines models illustrate the results."
"10.1111/j.1467-9868.2004.00437.x","2004","Penalized triograms: total variation regularization for bivariate smoothing","1","Hansen, Kooperberg and Sardy introduced a family of continuous, piecewise linear functions defined over adaptively selected triangulations of the plane as a general approach to statistical modelling of bivariate densities and regression and hazard functions. These triograms enjoy a natural affine equivariance that offers distinct advantages over competing tensor product methods that are more commonly used in statistical applications. Triograms employ basis functions consisting of linear 'tent functions' defined with respect to a triangulation of a given planar domain. As in knot selection for univariate splines, Hansen and colleagues adopted the regression spline approach of Stone. Vertices of the triangulation are introduced or removed sequentially in an effort to balance fidelity to the data and parsimony. We explore a smoothing spline variant of the triogram model based on a roughness penalty adapted to the piecewise linear structure of the triogram model. We show that the roughness penalty proposed may be interpreted as a total variation penalty on the gradient of the fitted function. The methods are illustrated with real and artificial examples, including an application to estimated quantile surfaces of land value in the Chicago metropolitan area."
"10.1046/j.1369-7412.2003.05211.x","2004","Application of `delete{$=$}replace' to deletion diagnostics for variance component estimation in the linear mixed model","0","'Delete = replace' is a powerful and intuitive modelling identity. This paper extends previous work by stating and proving the identity in more general terms and extending its application to deletion diagnostics for estimates of variance components obtained by restricted maximum likelihood estimation for the linear mixed model. We present a new, fast, transparent and approximate computational procedure, arising as a by-product of the fitting process. We illustrate the effect of the deletion of individual observations, of 'subjects' and of arbitrary subsets. Central to the identity and its application is the conditional residual."
"10.1111/j.1467-9868.2004.00435.x","2004","Power transformations to induce normality and their applications","1","Random variables which are positive linear combinations of positive independent random variables can have heavily right-skewed finite sample distributions even though they might be asymptotically normally distributed. We provide a simple method of determining an appropriate power transformation to improve the normal approximation in small samples. Our method contains the Wilson-Hilferty cube root transformation for chi(2) random variables as a special case. We also provide some important examples, including test statistics of goodness-of-fit and tail index estimators, where such power transformations can be applied. In particular, we study the small sample behaviour of two goodness-of-fit tests for time series models which have been proposed recently in the literature. Both tests are generalizations of the popular Box-Ljung-Pierce portmanteau test, one in the time domain and the other in the frequency domain. A power transformation with a finite sample mean and variance correction is proposed, which ameliorates the small sample effect. It is found that the corrected versions of the tests have markedly better size properties. The correction is also found to result in an overall increase in power which can be significant under certain alternatives. Furthermore, the corrected tests also have better power than the Box-Ljung-Pierce portmanteau test, unlike the uncorrected versions."
"10.1111/j.1467-9868.2004.00434.x","2004","Testing for a finite mixture model with two components","7","We consider a finite mixture model with k components and a kernel distribution from a general one-parameter family. The problem of testing the hypothesis k=2 versus kgreater than or equal to3 is studied. There has been no general statistical testing procedure for this problem. We propose a modified likelihood ratio statistic where under the null and the alternative hypotheses the estimates of the parameters are obtained from a modified likelihood function. It is shown that estimators of the support points are consistent. The asymptotic null distribution of the modified likelihood ratio test proposed is derived and found to be relatively simple and easily applied. Simulation studies for the asymptotic modified likelihood ratio test based on finite mixture models with normal, binomial and Poisson kernels suggest that the test proposed performs well. Simulation studies are also conducted for a bootstrap method with normal kernels. An example involving foetal movement data from a medical study illustrates the testing procedure."
"10.1046/j.1369-7412.2003.05343.x","2004","Detecting dependence between marks and locations of marked point processes","1","We introduce two characteristics for stationary and isotropic marked point proces- ses, E(h) and V(h), and describe their use in investigating mark-point interactions. These quantities are functions of the interpoint distance h and denote the conditional expectation and the conditional variance of a mark respectively, given that there is a further point of the process a distance h away. We present tests based on E and V for the hypothesis that the values of the marks can be modelled by a random field which is independent of the unmarked point process. We apply the methods to two data sets in forestry."
"10.1111/j.1467-9868.2004.00432.x","2004","Estimation and testing stationarity for double-autoregressive models","2","The paper considers the double-autoregressive model y(t) = phiy(t-1)+epsilon(t) with epsilon(t) = eta(t) root(omega + alphay(t-1)(2)). Consistency and asymptotic normality of the estimated parameters are proved under the condition E ln |phi +rootalphaeta(t)|<0, which includes the cases with |phi|=1 or |phi|>1 as well as E(epsilon(t)(2)) = infinity. It is well known that all kinds of estimators of phi in these cases are not normal when epsilon(t) are independent and identically distributed. Our result is novel and surprising. Two tests are proposed for testing stationarity of the model and their asymptotic distributions are shown to be a function of bivariate Brownian motions. Critical values of the tests are tabulated and some simulation results are reported. An application to the US 90-day treasury bill rate series is given."
"10.1111/j.1467-9868.2004.00431.x","2004","Compatible prior distributions for directed acyclic graph models","0","The application of certain Bayesian techniques, such as the Bayes factor and model averaging, requires the specification of prior distributions on the parameters of alternative models. We propose a new method for constructing compatible priors on the parameters of models nested in a given directed acyclic graph model, using a conditioning approach. We define a class of parameterizations that is consistent with the modular structure of the directed acyclic graph and derive a procedure, that is invariant within this class, which we name reference conditioning."
"10.1111/j.1467-9868.2004.00430.x","2004","Low order approximations in deconvolution and regression with errors in variables","5","We suggest two new methods, which are applicable to both deconvolution and regression with errors in explanatory variables, for nonparametric inference. The two approaches involve kernel or orthogonal series methods. They are based on defining a low order approximation to the problem at hand, and proceed by constructing relatively accurate estimators of that quantity rather than attempting to estimate the true target functions consistently. Of course, both techniques could be employed to construct consistent estimators, but in many contexts of importance (e.g. those where the errors are Gaussian) consistency is, from a practical viewpoint, an unattainable goal. We rephrase the problem in a form where an explicit, interpretable, low order approximation is available. The information that we require about the error distribution (the error-in-variables distribution, in the case of regression) is only in the form of low order moments and so is readily obtainable by a rudimentary analysis of indirect measurements of errors, e.g. through repeated measurements. In particular, we do not need to estimate a function, such as a characteristic function, which expresses detailed properties of the error distribution. This feature of our methods, coupled with the fact that all our estimators are explicitly defined in terms of readily computable averages, means that the methods are particularly economical in computing time."
"10.1046/j.1369-7412.2003.05282.x","2004","Local polynomial regression and simulation-extrapolation","7","The paper introduces a new local polynomial estimator and develops supporting asymptotic theory for nonparametric regression in the presence of covariate measurement error. We address the measurement error with Cook and Stefanski's simulation-extrapolation (SIMEX) algorithm. Our method improves on previous local polynomial estimators for this problem by using a bandwidth selection procedure that addresses SIMEX's particular estimation method and considers higher degree local polynomial estimators. We illustrate the accuracy of our asymptotic expressions with a Monte Carlo study, compare our method with other estimators with a second set of Monte Carlo simulations and apply our method to a data set from nutritional epidemiology. SIMEX was originally developed for parametric models. Although SIMEX is, in principle, applicable to nonparametric models, a serious problem arises with SIMEX in nonparametric situations. The problem is that smoothing parameter selectors that are developed for data without measurement error are no longer appropriate and can result in considerable undersmoothing. We believe that this is the first paper to address this difficulty."
"10.1046/j.1369-7412.2003.05379.x","2004","Hypothesis testing in mixture regression models","3","We establish asymptotic theory for both the maximum likelihood and the maximum modified likelihood estimators in mixture regression models. Moreover, under specific and reasonable conditions, we show that the optimal convergence rate of n(-1/4) for estimating the mixing distribution is achievable for both the maximum likelihood and the maximum modified likelihood estimators. We also derive the asymptotic distributions of two log-likelihood ratio test statistics for testing homogeneity and we propose a resampling procedure for approximating the p-value. Simulation studies are conducted to investigate the empirical performance of the two test statistics. Finally, two real data sets are analysed to illustrate the application of our theoretical results."
"10.1111/1467-9868.00424","2003","A general condition for avoiding effect reversal after marginalization","4","The paper examines the effect of marginalizing over a possibly unobserved background variable on the conditional relation between a response and an explanatory variable. In particular it is shown that some conclusions derived from least squares regression theory apply in general to testing independence for arbitrary distributions. It is also shown that the general condition of independence of the explanatory variable and the background ensures that monotonicity of dependence is preserved after marginalization. Relations with effect reversal and with collapsibility are sketched."
"10.1046/j.1369-7412.2003.00423.x","2003","Nonparametric estimation of the sojourn time distributions for a multipath model","0","We use a multipath (multistate) model to describe data with multiple end points. Statistical inference based on the intermediate end point is challenging because of the problems of nonidentifiability and dependent censoring. We study nonparametric estimation for the path probability and the sojourn time distributions between the states. The methodology proposed can be applied to analyse cure models which account for the competing risk of death. Asymptotic properties of the estimators proposed are derived. Simulation shows that the methods proposed have good finite sample performance. The methodology is applied to two data sets."
"10.1046/j.1369-7412.2003.00422.x","2003","Free-knot polynomial splines with confidence intervals","4","We construct approximate confidence intervals for a nonparametric regression function, using polynomial splines with free-knot locations. The number of knots is determined by generalized cross-validation. The estimates of knot locations and coefficients are obtained through a non-linear least squares solution that corresponds to the maximum likelihood estimate. Confidence intervals are then constructed based on the asymptotic distribution of the maximum likelihood estimator. Average coverage probabilities and the accuracy of the estimate are examined via simulation. This includes comparisons between our method and some existing methods such as smoothing spline and variable knots selection as well as a Bayesian version of the variable knots method. Simulation results indicate that our method works well for smooth underlying functions and also reasonably well for discontinuous functions. It also performs well for fairly small sample sizes."
"10.1111/1467-9868.00421","2003","On-line inference for hidden {M}arkov models via particle filters","3","We consider the on-line Bayesian analysis of data by using a hidden Markov model, where inference is tractable conditional on the history of the state of the hidden component. A new particle filter algorithm is introduced and shown to produce promising results when analysing data of this type. The algorithm is similar to the mixture Kalman filter but uses a different resampling algorithm. We prove that this resampling algorithm is computationally efficient and optimal, among unbiased resampling algorithms, in terms of minimizing a squared error loss function. In a practical example, that of estimating break points from well-log data, our new particle filter outperforms two other particle filters, one of which is the mixture Kalman filter, by between one and two orders of magnitude."
"10.1046/j.1369-7412.2003.00420.x","2003","Nonparametric methods for deconvolving multiperiodic functions","2","Multiperiodic functions, or functions that can be represented as finite additive mixtures of periodic functions, arise in problems related to stellar radiation. There they represent the overall variation in radiation intensity with time. The individual periodic components generally correspond to different sources of radiation and have intrinsic physical meaning provided that they can be 'deconvolved' from the mixture. We suggest a combination of kernel and orthogonal series methods for performing the deconvolution, and we show how to estimate both the sequence of periods and the periodic functions themselves. We pay particular attention to the issue of identifiability, in a nonparametric sense, of the components. This aspect of the problem is shown to exhibit particularly unusual features, and to have connections to number theory. The matter of rates of convergence of estimators also has links there, although we show that the rate-of-convergence problem can be treated from a relatively conventional viewpoint by considering an appropriate prior distribution for the periods."
"10.1046/j.1369-7412.2003.00419.x","2003","Extremes of {M}arkov chains with tail switching potential","0","A recent advance in the utility of extreme value techniques has been the characterization of the extremal behaviour of Markov chains. This has enabled the application of extreme value models to series whose temporal dependence is Markovian, subject to a limitation that prevents switching between extremely high and extremely low levels. For many applications this is sufficient, but for others, most notably in the field of finance, it is common to find series in which successive values switch between high and low levels. We term such series Markov chains with tail switching potential, and the scope of this paper is to generalize the previous theory to enable the characterization of the extremal properties of series displaying this type of behaviour. In addition to theoretical developments, a modelling procedure is proposed. A simulation study is made to assess the utility of the model in inferring the extremal dependence structure of autoregressive conditional heteroscedastic processes, which fall within the tail switching Markov family, and generalized autoregressive conditional heteroscedastic processes which do not, being non-Markov in general. Finally, the procedure is applied to model extremal aspects of a financial index extracted from the New York Stock Exchange compendium."
"10.1111/1467-9868.00418","2003","Accommodating stochastic departures from percentile invariance in causal models","0","Consider a clinical trial in which participants are randomized to a single-dose treatment or a placebo control and assume that the adherence level is accurately recorded. If the treatment is effective, then good adherers in the treatment group should do better than poor adherers because they received more drug; the treatment group data follow a dose-response curve. But, good adherers to the placebo often do better than poor adherers, so the observed adherence-response in the treatment group cannot be completely attributed to the treatment. Efron and Feldman proposed an adjustment to the observed adherence-response in the treatment group by using the adherence-response in the control group. It relies on a percentile invariance assumption under which each participant's adherence percentile within their assigned treatment group does not depend on the assigned group (active drug or placebo). The Efron and Feldman approach is valid under percentile invariance, but not necessarily under departures from it. We propose an analysis based on a generalization of percentile invariance that allows adherence percentiles to be stochastically permuted across treatment groups, using a broad class of stochastic permutation models. We show that approximate maximum likelihood estimates of the underlying dose-response curve perform well when the stochastic permutation process is correctly specified and are quite robust to model misspecification."
"10.1046/j.1369-7412.2003.00417.x","2003","Causal inference with generalized structural mean models","4","We estimate cause-effect relationships in empirical research where exposures are not completely controlled, as in observational studies or with patient non-compliance and self-selected treatment switches in randomized clinical trials. Additive and multiplicative structural mean models have proved useful for this but suffer from the classical limitations of linear and log-linear models when accommodating binary data. We propose the generalized structural mean model to overcome these limitations. This is a semiparametric two-stage model which extends the structural mean model to handle non-linear average exposure effects. The first-stage structural model describes the causal effect of received exposure by contrasting the means of observed and potential exposure-free outcomes in exposed subsets of the population. For identification of the structural parameters, a second stage 'nuisance' model is introduced. This takes the form of a classical association model for expected outcomes given observed exposure. Under the model, we derive estimating equations which yield consistent, asymptotically normal and efficient estimators of the structural effects. We examine their robustness to model misspecification and construct robust estimators in the absence of any exposure effect. The double-logistic structural mean model is developed in more detail to estimate the effect of observed exposure on the success of treatment in a randomized controlled blood pressure reduction trial with self-selected non-compliance."
"10.1111/1467-9868.00415","2003","On quantum statistical inference","3","Interest in problems of statistical inference connected to measurements of quantum systems has recently increased substantially, in step with dramatic new developments in experimental techniques for studying small quantum systems. Furthermore, developments in the theory of quantum measurements have brought the basic mathematical framework for the probability calculations much closer to that of classical probability theory. The present paper reviews this field and proposes and interrelates some new concepts for an extension of classical statistical inference to the quantum context."
"10.1111/1467-9868.00414","2003","Semiparametric models: a generalized self-consistency approach","3","In semiparametric models, the dimension d of the maximum likelihood problem is potentially unlimited. Conventional estimation methods generally behave like O(d(3)). A new O(d) estimation procedure is proposed for a large class of semiparametric models. Potentially unlimited dimension is handled in a numerically efficient way through a Nelson-Aalen-like estimator. Discussion of the new method is put in the context of recently developed minorization-maximization algorithms based on surrogate objective functions. The procedure for semiparametric models is used to demonstrate three methods to construct a surrogate objective function: using the difference of two concave functions, the EM way and the new quasi-EM (QEM) approach. The QEM approach is based on a generalization of the EM-like construction of the surrogate objective function so it does not depend on the missing data representation of the model. Like the EM algorithm, the QEM method has a dual interpretation, a result of merging the idea of surrogate maximization with the idea of imputation and self-consistency. The new approach is compared with other possible approaches by using simulations and analysis of real data. The proportional odds model is used as an example throughout the paper."
"10.1111/1467-9868.00413","2003","Bayesian inference for non-stationary spatial covariance structure via spatial deformations","6","In geostatistics it is common practice to assume that the underlying spatial process is stationary and isotropic, i.e. the spatial distribution is unchanged when the origin of the index set is translated and under rotation about the origin. However, in environmental problems, such assumptions are not realistic since local influences in the correlation structure of the spatial process may be found in the data. The paper proposes a Bayesian model to address the anisotropy problem. Following Sampson and Guttorp, we define the correlation function of the spatial process by reference to a latent space, denoted by D, where stationarity and isotropy hold. The space where the gauged monitoring sites lie is denoted by G. We adopt a Bayesian approach in which the mapping between G and D is represented by an unknown function d(.). A Gaussian process prior distribution is defined for d(.). Unlike the Sampson-Guttorp approach, the mapping of both gauged and ungauged sites is handled in a single framework, and predictive inferences take explicit account of uncertainty in the mapping. Markov chain Monte Carlo methods are used to obtain samples from the posterior distributions. Two examples are discussed: a simulated data set and the solar radiation data set that also was analysed by Sampson and Guttorp."
"10.1111/1467-9868.00412","2003","Efficient design of experiments in the {M}onod model","2","Estimation and experimental design in a non-linear regression model that is used in microbiology are studied. The Monod model is defined implicitly by a differential equation and has numerous applications in microbial growth kinetics, water research, pharmacokinetics and plant physiology. It is proved that least squares estimates are asymptotically unbiased and normally distributed. The asymptotic covariance matrix of the estimator is the basis for the construction of efficient designs of experiments. In particular locally D-, E- and c-optimal designs are determined and their properties are studied theoretically and by simulation. If certain intervals for the non-linear parameters can be specified, locally optimal designs can be constructed which are robust with respect to a misspecification of the initial parameters and which allow efficient parameter estimation. Parameter variances can be decreased by a factor of 2 by simply sampling at optimal times during the experiment."
"10.1111/1467-9868.00411","2003","Likelihood methods for missing covariate data in highly stratified studies","1","The paper considers canonical link generalized linear models with stratum-specific nuisance intercepts and missing covariate data. This family includes the conditional logistic regression model. Existing methods for this problem, each of which uses a conditioning argument to eliminate the nuisance intercept, model either the missing covariate data or the missingness process. The paper compares these methods under a common likelihood framework. The semiparametric efficient estimator is identified, and a new estimator, which reduces dependence on the model for the missing covariate, is proposed. A simulation study compares the methods with respect to efficiency and robustness to model misspecification."
"10.1111/1467-9868.00410","2003","The identifiability of the mixed proportional hazards competing risks model","2","We prove identification of dependent competing risks models in which each risk has a mixed proportional hazard specification with regressors, and the risks are dependent by way of the unobserved heterogeneity, or frailty, components. We show that the conditions for identification given by Heckman and Honore can be relaxed. We extend the results to the case in which multiple spells are observed for each subject."
"10.1111/1467-9868.00409","2003","Reversible jump, birth-and-death and more general continuous time {M}arkov chain {M}onte {C}arlo samplers","2","Reversible jump methods are the most commonly used Markov chain Monte Carlo tool for exploring variable dimension statistical models. Recently, however, an alternative approach based on birth-and-death processes has been proposed by Stephens for mixtures of distributions. We show that the birth-and-death setting can be generalized to include other types of continuous time jumps like split-and-combine moves in the spirit of Richardson and Green. We illustrate these extensions both for mixtures of distributions and for hidden Markov models. We demonstrate the strong similarity of reversible jump and continuous time methodologies by showing that, on appropriate rescaling of time, the reversible jump chain converges to a limiting continuous time birth-and-death process. A numerical comparison in the setting of mixtures of distributions highlights this similarity."
"10.1111/1467-9868.00408","2003","An empirical likelihood goodness-of-fit test for time series","3","Standard goodness-of-fit tests for a parametric regression model against a series of nonparametric alternatives are based on residuals arising from a fitted model. When a parametric regression model is compared with a nonparametric model, goodness-of-fit testing can be naturally approached by evaluating the likelihood of the parametric model within a nonparametric framework. We employ the empirical likelihood for an a-mixing process to formulate a test statistic that measures the goodness of fit of a parametric regression model. The technique is based on a comparison with kernel smoothing estimators. The empirical likelihood formulation of the test has two attractive features. One is its automatic consideration of the variation that is associated with the nonparametric fit due to empirical likelihood's ability to Studentize internally. The other is that the asymptotic distribution of the test statistic is free of unknown parameters, avoiding plug-in estimation. We apply the test to a discretized diffusion model which has recently been considered in financial market analysis."
"10.1111/1467-9868.00407","2003","Estimation of dependence between paired correlated failure times in the presence of covariate measurement error","0","dIn many biomedical studies, covariates are subject to measurement error. Although it is well known that the regression coefficients estimators can be substantially biased if the measurement error is not accommodated, there has been little study of the effect of covariate measurement error on the estimation of the dependence between bivariate failure times. We show that the dependence parameter estimator in the Clayton-Oakes model can be considerably biased if the measurement error in the covariate is not accommodated. In contrast with the typical bias towards the null for marginal regression coefficients, the dependence parameter can be biased in either direction. We introduce a bias reduction technique for the bivariate survival function in copula models while assuming an additive measurement error model and replicated measurement for the covariates, and we study the large and small sample properties of the dependence parameter estimator proposed."
"10.1111/1467-9868.00406","2003","Bayesian inversion of geoelectrical resistivity data","0","Enormous quantities of geoelectrical data are produced daily and often used for large scale reservoir modelling. To interpret these data requires reliable and efficient inversion methods which adequately incorporate prior information and use realistically complex modelling structures. We use models based on random coloured polygonal graphs as a powerful and flexible modelling framework for the layered composition of the Earth and we contrast our approach with earlier methods based on smooth Gaussian fields. We demonstrate how the reconstruction algorithm may be efficiently implemented through the use of multigrid Metropolis-coupled Markov chain Monte Carlo methods and illustrate the method on a set of field data."
"10.1111/1467-9868.00404","2003","A theory of statistical models for {M}onte {C}arlo integration","5","The task of estimating an integral by Monte Carlo methods is formulated as a statistical model using simulated observations as data. The difficulty in this exercise is that we ordinarily have at our disposal all of the information required to compute integrals exactly by calculus or numerical integration, but we choose to ignore some of the information for simplicity or computational feasibility. Our proposal is to use a semiparametric statistical model that makes explicit what information is ignored and what information is retained. The parameter space in this model is a set of measures on the sample space, which is ordinarily an infinite dimensional object. None-the-less, from simulated data the base-line measure can be estimated by maximum likelihood, and the required integrals computed by a simple formula previously derived by Vardi and by Lindsay in a closely related model for biased sampling. The same formula was also suggested by Geyer and by Meng and Wong using entirely different arguments. By contrast with Geyer's retrospective likelihood, a correct estimate of simulation error is available directly from the Fisher information. The principal advantage of the semiparametric model is that variance reduction techniques are associated with submodels in which the maximum likelihood estimator in the submodel may have substantially smaller variance than the traditional estimator. The method is applicable to Markov chain and more general Monte Carlo sampling schemes with multiple samplers."
"10.1111/1467-9868.00403","2003","A note on the prospective analysis of outcome-dependent samples","4","Two likelihood representations corresponding to the prospective and retrospective analyses of the case-control design are derived for general outcome-dependent samples with arbitrary discrete or continuous outcomes and possibly non-multiplicative models. Parameter identification in the general outcome-dependent design is reduced to the simple problem of parameter identification in the general odds ratio function. Both likelihoods are shown to generate the same profile likelihood for the common parameter of interest. Maximum likelihood estimators based on either likelihood are semiparametric efficient for the identifiable parameters."
"10.1111/1467-9868.00402","2003","Bayesian clustering and product partition models","8","We present a decision theoretic formulation of product partition models (PPMs) that allows a formal treatment of different decision problems such as estimation or hypothesis testing and clustering methods simultaneously. A key observation in our construction is the fact that PPMs can be formulated in the context of model selection. The underlying partition structure in these models is closely related to that arising in connection with Dirichlet processes. This allows a straightforward adaptation of some computational strategies-originally devised for nonparametric Bayesian problems-to our framework. The resulting algorithms are more flexible than other competing alternatives that are used for problems involving PPMs. We propose an algorithm that yields Bayes estimates of the quantities of interest and the groups of experimental units. We explore the application of our methods to the detection of outliers in normal and Student t regression models, with clustering structure equivalent to that induced by, a Dirichlet process prior. We also discuss the sensitivity of the results considering different prior distributions for the partitions."
"10.1111/1467-9868.00401","2003","Inference for clusters of extreme values","2","Inference for clusters of extreme values of a time series typically requires the identification of independent clusters of exceedances over a high threshold. The choice of declustering scheme often has a significant effect on estimates of cluster characteristics. We propose an automatic declustering scheme that is justified by an asymptotic result for the times between threshold exceedances. The scheme relies on the extremal index, which we show may be estimated before declustering, and supports a bootstrap procedure for assessing the variability of estimates."
"10.1111/1467-9868.00400","2003","Diagnostics for dependence within time series extremes","3","The analysis of extreme values within a stationary time series entails various assumptions concerning its long- and short-range dependence. We present a range of new diagnostic tools for assessing whether these assumptions are appropriate and for identifying structure within extreme events. These tools are based on tail characteristics of joint survivor functions but can be implemented by using existing estimation methods for extremes of univariate independent and identically distributed variables. Our diagnostic aids are illustrated through theoretical examples, simulation studies and by application to rainfall and exchange rate data. On the basis of these diagnostics we can explain characteristics that are found in the observed extreme events of these series and also gain insight into the properties of events that are more extreme than those observed."
"10.1111/1467-9868.00399","2003","Classical model selection via simulated annealing","3","The classical approach to statistical analysis is usually based upon finding values for model parameters that maximize the likelihood function. Model choice in this context is often also based on the likelihood function, but with the addition of a penalty term for the number of parameters. Though models may be compared pairwise by using likelihood ratio tests for example, various criteria such as the Akaike information criterion have been proposed as alternatives when multiple models need to be compared. In practical terms, the classical approach to model selection usually involves maximizing the likelihood function associated with each competing model and then calculating the corresponding criteria value(s). However, when large numbers of models are possible, this quickly becomes infeasible unless a method that simultaneously maximizes over both parameter and model space is available. We propose an extension to the traditional simulated annealing algorithm that allows for moves that not only change parameter values but also move between competing models. This transdimensional simulated annealing algorithm can therefore be used to locate models and parameters that minimize criteria such as the Akaike information criterion, but within a single algorithm, removing the need for large numbers of simulations to be run. We discuss the implementation of the transdimensional simulated annealing algorithm and use simulation studies to examine its performance in realistically complex modelling situations. We illustrate our ideas with a pedagogic example based on the analysis of an autoregressive time series and two more detailed examples: one on variable selection for logistic regression and the other on model selection for the analysis of integrated recapture-recovery data."
"10.1111/1467-9868.00398","2003","Maximum likelihood estimation for the proportional hazards model with partly interval-censored data","0","The maximum likelihood estimator (MLE) for the proportional hazards model with partly interval-censored data is studied. Under appropriate regularity conditions, the MLEs of the regression parameter and the cumulative hazard function are shown to be consistent and asymptotically normal. Two methods to estimate the variance-covariance matrix of the MILE of the regression parameter are considered, based on a generalized missing information principle and on a generalized profile information procedure. Simulation studies show that both methods work well in terms of the bias and variance for samples of moderate size. An example illustrates the methods."
"10.1111/1467-9868.00397","2003","Iterated residuals and time-varying covariate effects in {C}ox regression","2","The Cox proportional hazards model, which is widely used for the analysis of treatment and prognostic effects with censored survival data, makes the assumption that the hazard ratio is constant over time. Nonparametric estimators have been developed for an extended model in which the hazard ratio is allowed to change over time. Estimators based on residuals are appealing as they are easy to use and relate in a simple way to the more restricted Cox model estimator. After fitting a Cox model and calculating the residuals, one can obtain a crude estimate of the time-varying coefficients by adding a smooth of the residuals to the initial (constant) estimate. Treating the crude estimate as the fit, one can re-estimate the residuals. Iteration leads to consistent estimation of the nonparametric time-varying coefficients. This approach leads to clear guidelines for residual analysis in applications. The results are illustrated by an analysis of the Medical Research Council's myeloma trials, and by simulation."
"10.1111/1467-9868.00396","2003","Estimation of bivariate and marginal distributions with censored data","1","Consider a pair of random variables, both subject to random right censoring. New estimators for the bivariate and marginal distributions of these variables are proposed. The estimators of the marginal distributions are not the marginals of the corresponding estimator of the bivariate distribution. Both estimators require estimation of the conditional distribution when the conditioning variable is subject to censoring. Such a method of estimation is proposed. The weak convergence of the estimators proposed is obtained. A small simulation study suggests that the estimators of the marginal and bivariate distributions perform well relatively to respectively the Kaplan-Meier estimator for the marginal distribution and the estimators of Pruitt and van der Laan for the bivariate distribution. The use of the estimators in practice is illustrated by the analysis of a data set."
"10.1111/1467-9868.00395","2003","Using difference-based methods for inference in nonparametric regression with time series errors","0","We show that difference-based methods can be used to construct simple and explicit estimators of error covariance and autoregressive parameters in nonparametric regression with time series errors. When the error process is Gaussian our estimators are efficient, but they are available well beyond the Gaussian case. As an illustration of their usefulness we show that difference-based estimators can be used to produce a simplified version of time series cross-validation. This new approach produces a bandwidth selector that is equivalent, to both first and second orders, to that given by the full time series cross-validation algorithm. Other applications of difference-based methods are to variance estimation and construction of confidence bands in nonparametric regression."
"10.1111/1467-9868.00394","2003","Data tilting for time series","1","We develop a general methodology for tilting time series data. Attention is focused on a large class of regression problems, where errors are expressed through autoregressive processes. The class has a range of important applications and in the context of our work may be used to illustrate the application of tilting methods to interval estimation in regression, robust statistical inference and estimation subject to constraints. The method can be viewed as 'empirical likelihood with nuisance parameters'."
"10.1111/1467-9868.00393","2003","Functional quasi-likelihood regression models with smooth random effects","10","We propose a class of semiparametric functional regression models to describe the influence of vector-valued covariates on a sample of response curves. Each observed curve is viewed as the realization of a random process, composed of an overall mean function and random components. The finite dimensional covariates influence the random components of the eigenfunction expansion through single-index models that include unknown smooth link and variance functions. The parametric components of the single-index models are estimated via quasi-score estimating equations with link and variance functions being estimated nonparametrically. We obtain several basic asymptotic results. The functional regression models proposed are illustrated with the analysis of a data set consisting of egg laying curves for 1000 female Mediterranean fruit-flies (medflies)."
"10.1111/1467-9868.00392","2003","Interpreting statistical evidence by using imperfect models: robust adjusted likelihood functions","2","The strength of statistical evidence is measured by the likelihood ratio. Two key performance properties of this measure are the probability of observing strong misleading evidence and the probability of observing weak evidence. For the likelihood function associated with a parametric statistical model, these probabilities have a simple large sample structure when the model is correct. Here we examine how that structure changes when the model fails. This leads to criteria for determining whether a given likelihood function is robust (continuing to perform satisfactorily when the model fails), and to a simple technique for adjusting both likelihoods and profile likelihoods to make them robust. We prove that the expected information in the robust adjusted likelihood cannot exceed the expected information in the likelihood function from a true model. We note that the robust adjusted likelihood is asymptotically fully efficient when the working model is correct, and we show that in some important examples this efficiency is retained even when the working model fails. In such cases the Bayes posterior probability distribution based on the adjusted likelihood is robust, remaining correct asymptotically even when the model for the observable random variable does not include the true distribution. Finally we note a link to standard frequentist methodology-in large samples the adjusted likelihood functions provide robust likelihood-based confidence intervals."
"10.1111/1467-9868.00391","2003","Distributions generated by perturbation of symmetry with emphasis on a multivariate skew {$t$}-distribution","7","A fairly general procedure is studied to perturb a multivariate density satisfying a weak form of multivariate symmetry, and to generate a whole set of non-symmetric densities. The approach is sufficiently general to encompass some recent proposals in the literature, variously related to the skew normal distribution. The special case of skew elliptical densities is examined in detail, establishing connections with existing similar work. The final part of the paper specializes further to a form of multivariate skew t-density. Likelihood inference for this distribution is examined, and it is illustrated with numerical examples."
"10.1111/1467-9868.00389","2003","Optimal dynamic treatment regimes","6","A dynamic treatment regime is a list of decision rules, one per time interval, for how the level of treatment will be tailored through time to an individual's changing status. The goal of this paper is to use experimental or observational data to estimate decision regimes that result in a maximal mean response. To explicate our objective and to state the assumptions, we use the potential outcomes model. The method proposed makes smooth parametric assumptions only on quantities that are directly relevant to the goal of estimating the optimal rules. We illustrate the methodology proposed via a small simulation."
"10.1111/1467-9868.00379","2003","Estimating variance components by using survey data","1","Inflation-type weighted estimators for variance components can be badly biased. Modified weighted estimators suggested in the literature are also badly biased for certain sampling designs. We propose new estimators for variance components, some of which are approximately unbiased regardless of the sampling design. These estimators require knowledge of the joint inclusion probabilities of the observations. The small sample properties of the estimators are studied via simulation for the simple one-way random-effects model. An application is given by using data from the US Hispanic Health and Nutrition Examination Survey."
"10.1111/1467-9868.00388","2003","A simple estimator for a shared frailty regression model","0","We propose a simple estimation procedure for a proportional hazards frailty regression model for clustered survival data in which the dependence is generated by a positive stable distribution. Inferences for the frailty parameter can be obtained by using output from Cox regression analyses. The computational burden is substantially less than that of the other approaches to estimation. The large sample behaviour of the estimator is studied and simulations show that the approximations are appropriate for use with realistic sample sizes. The methods are motivated by studies of familial associations in the natural history of diseases. Their practical utility is illustrated with sib pair data from Beaver Dam, Wisconsin."
"10.1111/1467-9868.00387","2003","Conditional statistical inference and quantification of relevance","0","We argue that it can be fruitful to take a predictive view on notions such as the precision of a point estimator and the confidence of an interval estimator in frequentist inference. This predictive approach has implications for conditional inference, because it immediately allows a quantification of the concept of relevance for conditional inference. Conditioning on an ancillary statistic makes inference more relevant in this sense, provided that the ancillary is a precision index. Not all ancillary statistics satisfy this demand. We discuss the problem of choice between alternative ancillary statistics. The approach also has implications for the best choice of variance estimator, taking account of correlations with the squared error of estimation itself. The theory is illustrated by numerous examples, many of which are classical."
"10.1111/1467-9868.00386","2003","Pattern-mixture and selection models for analysing longitudinal data with monotone missing patterns","0","We examine three pattern-mixture models for making inference about parameters of the distribution of an outcome of interest Y that is to be measured at the end of a longitudinal study when this outcome is missing in some subjects. We show that these pattern-mixture models also have an interpretation as selection models., Because these models make unverifiable assumptions, we recommend that inference about the distribution of Y be repeated under a range of plausible assumptions. We argue that, of the three models considered, only one admits a parameterization that facilitates the examination of departures from the assumption of sequential ignorability The three models are nonparametric in the sense that they do not impose restrictions on the class of observed data distributions. Owing to the curse of dimensionality, the assumptions that are encoded in these models are sufficient for identification but not for inference. We describe additional flexible and easily interpretable assumptions under which it is possible to construct estimators that are well behaved with moderate sample sizes. These assumptions define semiparametric models for the distribution of the observed data. We describe a class of estimators which, up to asymptotic equivalence, comprise all the consistent and asymptotically normal estimators of the parameters of interest under the postulated semiparametric models. We illustrate our methods with the analysis of data from a randomized clinical trial of contracepting women."
"10.1111/1467-9868.00385","2003","Estimating the association parameter for copula models under dependent censoring","4","Many biomedical studies involve the analysis of multiple events. The dependence between the times to these end points is often of scientific interest. We investigate a situation when one end point is subject to censoring by the other. The model assumptions of Day and co-workers and Fine and co-workers are extended to more general structures where the level of association may vary with time. Two types of estimating function are proposed. Asymptotic properties of the proposed estimators are derived. Their finite sample performance is studied via simulations. The inference procedures are applied to two real data sets for illustration."
"10.1111/1467-9868.00384","2003","Posterior bimodality in the balanced one-way random-effects model","1","Although some researchers have examined posterior multimodality for specific richly parameterized models, multimodality is not well characterized for any such model. The paper characterizes bimodality of the joint and marginal posteriors for a conjugate analysis of the balanced one-way random-effects model with a flat prior on the mean. This apparently simple model has surprisingly complex and even bizarre mode behaviour. Bimodality usually arises when the data indicate a much larger between-groups variance than does the prior. We examine an example in detail, present a graphical display for describing bimodality and use real data sets from a statistical practice to shed light on the practical relevance of bimodality for these models."
"10.1111/1467-9868.00383","2003","Efficient calculation of the normalizing constant of the autologistic and related models on the cylinder and lattice","1","Motivated by the autologistic model for the analysis of spatial binary data on the two-dimensional lattice, we develop efficient computational methods for calculating the normalizing constant for models for discrete data defined on the cylinder and lattice. Because the normalizing constant is generally unknown analytically, statisticians have developed various ad hoc methods to overcome this difficulty. Our aim is to provide computationally and statistically efficient methods for calculating the normalizing constant so that efficient likelihood-based statistical methods are then available for inference. We extend the so-called transition method to find a feasible computational method of obtaining the normalizing constant for the cylinder boundary condition. To extend the result to the free-boundary condition on the lattice we use an efficient path sampling Markov chain Monte Carlo scheme. The methods are generally applicable to association patterns other than spatial, such as clustered binary data, and to variables taking three or more values described by, for example, Potts models."
"10.1111/1467-9868.00382","2003","The evaluation of general non-centred orthant probabilities","0","The evaluation of the cumulative distribution function of a multivariate normal distribution is considered. The multivariate normal distribution can have any positive definite correlation matrix and any mean vector. The approach taken has two stages. In the first stage, it is shown how non-centred orthoscheme probabilities can be evaluated by using a recursive integration method. In the second stage, some ideas of Schlafli and Abrahamson are extended to show that any non-centred orthant probability can be expressed as differences between at most (m - 1)! non-centred orthoscheme probabilities. This approach allows an accurate evaluation of many multivariate normal probabilities which have important applications in statistical practice."
"10.1111/1467-9868.00381","2003","Covariate selection for estimating the causal effect of control plans by using causal diagrams","0","Consider a case where cause-effect relationships between variables can be described by a causal path diagram and the corresponding linear structural equation model. The paper proposes a graphical selection criterion for covariates to estimate the causal effect of a control plan. For designing the control plan, it is essential to determine both covariates that are used for control and covariates that are used for identification. The selection of covariates used for control is only constrained by the requirement that the covariates be non-descendants of a treatment variable. However, the selection of covariates used for identification is dependent on the selection of covariates used for control and is not unique. In the paper, the difference between covariates that are used for identification is evaluated on the basis of the asymptotic variance of the estimated causal effect of an effective control plan. Furthermore, the results can be also described in terms of a graph structure."
"10.1111/1467-9868.00380","2003","Efficiency of projected score methods in rectangular array asymptotics","0","The paper considers a rectangular array asymptotic embedding for multistratum data sets, in which both the number of strata and the number of within-stratum replications increase, and at the same rate. It is shown that under this embedding the maximum likelihood estimator is consistent but not efficient owing to a non-zero mean in its asymptotic normal distribution. By using a projection operator on the score function, an adjusted maximum likelihood estimator can be obtained that is asymptotically unbiased and has a variance that attains the Cramer-Rao lower bound. The adjusted maximum likelihood estimator can be viewed as an approximation to the conditional maximum likelihood estimator."
"10.1111/1467-9868.00378","2003","A skew extension of the {$t$}-distribution, with applications","4","A tractable skew t-distribution on the real line is proposed. This includes as a special case the symmetric t-distribution, and otherwise provides skew extensions thereof. The distribution is potentially useful both for modelling data and in robustness studies. Properties of the new distribution are presented. Likelihood inference for the parameters of this skew t-distribution is developed. Application is made to two data modelling examples."
"10.1111/1467-9868.00377","2003","Sequential classification on partially ordered sets","0","A general theorem on the asymptotically optimal sequential selection of experiments is presented and applied to a Bayesian classification problem when the parameter space is a finite partially ordered set. The main results include establishing conditions under which the posterior probability of the true state converges to 1 almost surely and determining optimal rates of convergence. Properties of a class of experiment selection rules are explored."
"10.1111/1467-9868.00376","2003","Building adaptive estimating equations when inverse of covariance estimation is difficult","2","To construct an optimal estimating function by weighting a set of score functions, we must either know or estimate consistently the covariance matrix for the individual scores. In problems with high dimensional correlated data the estimated covariance matrix could be unreliable. The smallest eigenvalues of the covariance matrix will be the most important for weighting the estimating equations, but in high dimensions these will be poorly determined. Generalized estimating equations introduced the idea of a working correlation to minimize such problems. However, it can be difficult to specify the working correlation model correctly. We develop an adaptive estimating equation method which requires no working correlation assumptions. This methodology relies on finding a reliable approximation to the inverse of the variance matrix in the quasi-likelihood equations. We apply a multivariate generalization of the conjugate gradient method to find estimating equations that preserve the information well at fixed low dimensions. This approach is particularly useful when the estimator of the covariance matrix is singular or close to singular, or impossible to invert owing to its large size."
"10.1111/1467-9868.00375","2003","Goodness-of-fit tests based on maximum correlations and their orthogonal decompositions","0","We propose a goodness-of-fit statistic Q(n) based on the Hoeffding maximum correlation for testing uniformity and we show its relationship to Gini's mean difference. We compute exact and asymptotic critical values and study the power of the test proposed against a representative set of alternatives."
"10.1111/1467-9868.00374","2003","Thin plate regression splines","7","discuss the production of low rank smoothers for d greater than or equal to 1 dimensional data, which can be fitted by regression or penalized regression methods. The smoothers are constructed by a simple transformation and truncation of the basis that arises from the solution of the thin plate spline smoothing problem and are optimal in the sense that the truncation is designed to result in the minimum possible perturbation of the thin plate spline smoothing problem given the dimension of the basis used to construct the smoother. By making use of Lanczos iteration the basis change and truncation are computationally efficient. The smoothers allow the use of approximate thin plate spline models with large data sets, avoid the problems that are associated with 'knot placement' that usually complicate modelling with regression splines or penalized regression splines, provide a sensible way of modelling interaction terms in generalized additive models, provide low rank approximations to generalized smoothing spline models, appropriate for use with large data sets, provide a means for incorporating smooth functions of more than one variable into non-linear models and improve the computational efficiency of penalized likelihood models incorporating thin plate splines. Given that the approach produces spline-like models with a sparse basis, it also provides a natural way of incorporating unpenalized spline-like terms in linear and generalized linear models, and these can be treated just like any other model terms from the point of view of model selection, inference and diagnostics."
"10.1111/1467-9868.00373","2003","Comprehensive definitions of breakdown points for independent and dependent observations","3","We provide a new definition of breakdown in finite samples, with an extension to asymptotic breakdown. Previous definitions centre on defining a critical region for either the parameter or the objective function. If for a particular outlier configuration the critical region is entered, breakdown is said to occur. In contrast with the traditional approach, we leave the definition of the critical region implicit. Our proposal encompasses previous definitions of breakdown in linear and non-linear regression settings. In some cases, it leads to a different and more intuitive notion of breakdown than other procedures that are available. An important advantage of our new definition is that it also applies to models for dependent observations where current definitions of breakdown typically fail. We illustrate our suggestion by using examples from linear and non-linear regression, and time series."
"10.1111/1467-9868.00372","2003","Adaptive varying-coefficient linear models","10","Varying-coefficient linear models arise from multivariate nonparametric regression, non-linear time series modelling and forecasting, functional data analysis, longitudinal data analysis and others. It has been a common practice to assume that the varying coefficients are functions of a given variable, which is often called an index. To enlarge the modelling capacity substantially, this paper explores a class of varying-coefficient linear models in which the index is unknown and is estimated as a linear combination of regressors and/or other variables. We search for the index such that the derived varying-coefficient model provides the least squares approximation to the underlying unknown multidimensional regression function. The search is implemented through a newly proposed hybrid backfitting algorithm. The core of the algorithm is the alternating iteration between estimating the index through a one-step scheme and estimating coefficient functions through one-dimensional local linear smoothing. The locally significant variables are selected in terms of a combined use of the t-statistic and the Akaike information criterion. We further extend the algorithm for models with two indices. Simulation shows that the methodology proposed has appreciable flexibility to model complex multivariate nonlinear structure and is practically feasible with average modern computers. The methods are further illustrated through the Canadian mink-muskrat data in 1925-1994 and the pound-dollar exchange rates in 1974-1983."
"10.1111/1467-9868.03711","2003","Efficient construction of reversible jump {M}arkov chain {M}onte {C}arlo proposal distributions","8","The major implementational problem for reversible jump Markov chain Monte Carlo methods is that there is commonly no natural way to choose jump proposals since there is no Euclidean structure in the parameter space to guide our choice. We consider mechanisms for guiding the choice of proposal. The first group of methods is based on an analysis of acceptance probabilities for jumps. Essentially, these methods involve a Taylor series expansion of the acceptance probability around certain canonical jumps and turn out to have close connections to Langevin algorithms. The second group of methods generalizes the reversible jump algorithm by using the so-called saturated space approach. These allow the chain to retain some degree of memory so that, when proposing to move from a smaller to a larger model, information is borrowed from the last time that the reverse move was performed. The main motivation for this paper is that, in complex problems, the probability that the Markov chain moves between such spaces may be prohibitively small, as the probability mass can be very thinly spread across the space. Therefore, finding reasonable jump proposals becomes extremely important. We illustrate the procedure by using several examples of reversible jump Markov chain Monte Carlo applications including the analysis of autoregressive time series, graphical Gaussian modelling and mixture modelling."
